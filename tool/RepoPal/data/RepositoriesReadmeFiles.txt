neo4j-contrib/sparql-plugin	This is a [Neo4j Server](http://neo4j.org/download) plugin, providing [Sparql](http://en.wikipedia.org/wiki/SPARQL)  [Neo4j Server](http://neo4j.org). This effectively turns the Neo4j Server into a Triple Store.   For usage via the REST API, see the [current documentation](http://neo4j-contrib.github.io/sparql-plugin/).  Building from source and deploying into Neo4j Server -----------------------------------------------------      mvn clean package     unzip target/neo4j-sparql-plugin-0.2-SNAPSHOT-server-plugin.zip -d $NEO4J_HOME/plugins/sparql-plugin     cd $NEO4J_HOME     bin/neo4j restart
AskNowQA/AutoSPARQL	## Introduction  AutoSPARQL TBSL is a graphical user interface, which allows to answer natural language queries over RDF knowledge bases. It is based on algorithms implemented in the DL-Learner Semantic Web machine learning framework.  ## Requirements * Java 7 or higher * Maven 3 or higher * Git * an **IPv6-capable internet connection** (you can use a free IPv6 tunnel service like Miredo or Teredo if your connection does not support it natively)  ## Warning AutoSPARQL TBSL is a research prototype that is not actively developed anymore. As such, getting it to work may need some effort and depends on the availability of several other online services. Feel free to create issues if you encounter problems and please share your fixes using pull requests.  ## Installation and Execution 1. clone the git repository 2. run `./compile` and then `./run` (Linux, Mac) or `compile.bat` and then `run.bat` (Windows) * Manually: Do `mvn install -N` in the folders in that order: autosparql, commons, algorithm-tbsl. Then go into autosparql-tbsl and run `mvn jetty:run`.  Then, go into your browser and access `http://localhost:8080` and click on the link to the application.  ## Error Reporting If you encounter errors, please look at the issues if the problem is already reported. If not, please create a single issue including the command line output. If the error occurs during compilation, please use `./createcompillelog` instead of `./compile` to create the compile log. Feel free to create issues if you encounter problems and please share your fixes using pull requests.  ## Adding your own Dataset Using your own datasource instead of DBpedia or Oxford is nontrivial. It needs several days of work in addition to the time needed to familiarize yourself with the code base.  You need to:  - fork AutoSPARQL TBSL - add your own domain dependent lexicon as an LTAG grammar at algorithm-tbsl/src/main/resources/tbsl/lexicon (see http://pub.uni-bielefeld.de/publication/2002961 and http://pub.uni-bielefeld.de/publication/2278529 as well as the existing files in that folder) - add your knowledge base:  1. as a local model to algorithm-tbsl/src/main/resources/models/yourmodel (preferred for small knowledge bases as it is much faster and more reliable)  2. as a SPARQL (version 1.0 is enough) endpoint along with a SOLR server instance - create a singleton for your knowledge base (see package org.aksw.autosparql.tbsl.algorithm.knowledgebase) - extend org.aksw.autosparql.tbsl.algorithm.learning.TBSL with TbslYourKnowledgeBase - finally, in case the dataset isn't a private model, please do a pull request so that it can be integrated in the main project  ## Modules and Packages | Maven Module      | Package                             | Purpose       | |-------------------|-------------------------------------|---------------| | autosparql-parent |                  -                  | Parent Module | | algorithm-tbsl    | org.aksw.autosparql.tbsl.algorithm  | Algorithm     | | commons           | org.aksw.autosparql.commons         | Utilities     | | autosparql-tbsl   | org.aksw.autosparql.tbsl.gui.vaadin | Web Interface |
rdfhdt/hdt-java	[![Join the chat at https://gitter.im/rdfhdt](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/rdfhdt)  # HDT Library, Java Implementation. http://www.rdfhdt.org  ## Overview  HDT-lib is a Java Library that implements the W3C Submission (http://www.w3.org/Submission/2011/03/) of the RDF HDT (Header-Dictionary-Triples) binary format for publishing and exchanging RDF data at large scale. Its compact representation allows storing RDF in fewer space, providing at the same time direct access to the stored information. This is achieved by depicting the RDF graph in terms of three main components: Header, Dictionary and Triples. The Header includes extensible metadata required to describe the RDF data set and details of its internals. The Dictionary organizes the vocabulary of strings present in the RDF graph by assigning numerical IDs to each different string. The Triples component comprises the internal structure of the RDF graph in a compressed form.  It provides several components: - hdt-java-api: Abstract interface for dealing with HDT files. - hdt-java-core: Core library for accessing HDT files programmatically from java. It allows creating HDT files from RDF and converting HDT files back to RDF. It also provides a Search interface to find triples that match a specific triple pattern. - hdt-java-cli: Commandline tools to convert RDF to HDT and access HDT files from a terminal. - hdt-jena: Jena integration. Provides a Jena Graph implementation that allows accessing HDT files as normal Jena Models. In turn, this can be used with Jena ARQ to provide more advanced searches, such as SPARQL, and even setting up SPARQL Endpoints with Fuseki. - hdt-java-package: Generates a package with all the components and launcher scripts. - hdt-fuseki: Packages Apache Jena Fuseki with the HDT jars and a fast launcher, to start a SPARQL endpoint out of HDT files very easily.   ## Compiling  Use mvn install to let Apache Maven install the required jars in your system.  You can also run mvn assembly:single under hdt-java-package to generate a distribution directory with all the jars and launcher scripts.   ## Usage  Please refer to hdt-java-package/README for more information on how to use the library. You can also find useful information on our Web Page http://www.rdfhdt.org   ## License  Each module has a different License. Core is LGPL, examples and tools are Apache.  hdt-api: Apache License hdt-java-cli (Commandline tools and examples): Apache License hdt-java-core: Lesser General Public License hdt-jena: Lesser General Public License hdt-fuseki: Apache License   ## Authors  Mario Arias <mario.arias@gmailcom> Javier D. Fernandez <jfergar@infor.uva.es> Miguel A. Martinez-Prieto <migumar2@infor.uva.es>   ## Acknowledgements  RDF/HDT is a project developed by the Insight Centre for Data Analytics (www.insight-centre.org), University of Valladolid (www.uva.es), University of Chile (www.uchile.cl). Funded by Science Foundation Ireland: Grant No. SFI/08/CE/I1380, Lion-II; the Spanish Ministry of Economy and Competitiveness (TIN2009-14009-C02-02); and Chilean Fondecyt's 1110287 and 1-110066.
AKSW/Sparqlify	# Sparqlify SPARQL->SQL rewriter [![Build Status](http://ci.aksw.org/jenkins/job/Sparqlify/badge/icon)](http://ci.aksw.org/jenkins/job/Sparqlify/)   ## Introduction  Sparqlify is a scalable SPARQL-SQL rewriter whose development began in April 2011 in the course of the [LinkedGeoData](http://linkedgeodata.org) project.  This system's features/traits are: * Support of the ['Sparqlification Mapping Language' (SML)](http://sparqlify.org/wiki/SML), an intuitive language for expressing RDB-RDF mappings with only very little syntactic noise. * Scalability: Sparqlify does not evaluate expressions in memory. All SPARQL filters end up in the corresponding SQL statement, giving the underlying RDBMS has maximum control over query planning. * A powerful rewriting engine that analyzes filter expressions in order to eleminate self joins and joins with unsatisfiable conditions. * Initial support for spatial datatypes and predicates. * A subset of the SPARQL 1.0 query language plus sub queries are supported. * Tested with PostgreSQL/Postgis and H2. Support for further databases is planned. * CSV support * R2RML will be supported soon  ## Supported SPARQL language features * Join, LeftJoin (i.e. Optional), Union, Sub queries * Filter predicates: comparison: (<=, <, =, >, >=) logical: (!, &&; ||) arithmetic: (+, -) spatial: st\_intersects, geomFromText; other: regex, lang, langMatches   * Aggregate functions: Count(\*) * Order By is pushed into the SQL   ## Debian packages  Sparqlify Debian packages can be obtained by following means: * Via the [Linked Data Stack](http://stack.linkeddata.org) (recommended) * Download from the [Sparqlify website's download section](http://sparqlify.org/downloads/releases). * Directly from source using maven (read down the README)  ### Public repositories  After setting up any of the repositories below, you can install sparqlify with apt using  * apt: `sudo apt-get install sparqlify-cli  #### Linked Data Stack (this is what you want)  Sparqlify is distributed at the [Linked Data Stack](http://stack.linkeddata.org), which offers many great tools done by various contributors of the Semantic Web community.  * The repository is available in the flavors `nightly`, `testing` and `stable` [here](http://stack.linkeddata.org/download/repo.php).  ```bash # !!! Replace stable with nightly or testing as needed !!!  # Download the repository package wget http://stack.linkeddata.org/ldstable-repository.deb  # Install the repository package sudo dpkg -i ldstable-repository.deb  # Update the repository database sudo apt-get update ```   #### Bleeding Edge (Not recommended for production) For the latest development version (built on every commit) perform the following steps  Import the public key with      wget -qO - http://cstadler.aksw.org/repos/apt/conf/packages.precise.gpg.key  | sudo apt-key add -  Add the repository      echo 'deb http://cstadler.aksw.org/repos/apt precise main contrib non-free' | sudo tee -a /etc/apt/sources.list.d/cstadler.aksw.org.list   Note that this also works with distros other than "precise" (ubuntu 12.04) such as ubuntu 14.04 or 16.04.    ## Building Building the repository creates the JAR files providing the `sparqlify-*` tool suite.   ### Debian package Building debian packages from this repo relies on the [Debian Maven Plugin](http://debian-maven.sourceforge.net]) plugin, which requires a debian-compatible environment. If such an environment is present, the rest is simple:      # Install all shell scripts necessary for creating deb packages     sudo apt-get install devscripts      # Execute the follwing from the `<repository-root>/sparqlify-core` folder:     mvn clean install deb:package      # Upon sucessful completion, the debian package is located under `<repository-root>/sparqlify-core/target`     # Install using `dpkg`     sudo dpkg -i sparqlify_<version>.deb      # Uninstall using dpkg or apt:     sudo dpkg -r sparqlify     sudo apt-get remove sparqlify   ### Assembly based Another way to build the project is run the following commands at `<repository-root>`      mvn clean install      cd sparqlify-cli     mvn assembly:assembly   This will generate a single stand-alone jar containing all necessary dependencies. Afterwards, the shell scripts under `sparqlify-core/bin` should work.  ## Tool suite  If Sparqlify was installed from the debian package, the following commands are available system-wide:  * `sparqlify`: This is the main executable for running individual SPARQL queries, creating dumps and starting a stand-alone server. * `sparqlify-csv`: This tool can create RDF dumps from CSV file based on SML view definitions. * `sparqlify-platform`: A stand-alone server component integrating additional projects.  These tools write their output (such as RDF data in the N-TRIPLES format) to STDOUT. Log output goes to STDERR.  ### sparqlify Usage: `sparqlify [options]`  Options are:  * Setup   * -m   SML view definition file  * Database Connectivity Settings   * -h   Hostname of the database (e.g. localhost or localhost:5432)   * -d   Database name   * -u   User name   * -p   Password   * -j   JDBC URI (mutually exclusive with both -h and -d)  * Quality of Service   * -n   Maximum result set size   * -t   Maximum query execution time in seconds (excluding rewriting time)  * Stand-alone Server Configuration   * -P   Server port [default: 7531]  * Run-Once (these options prevent the server from being started and are mutually exclusive with the server configuration)   * -D   Create an N-TRIPLES RDF dump on STDOUT    * -Q   [SPARQL query] Runs a SPARQL query against the configured database and view definitions  #### Example The following command will start the Sparqlify HTTP server on the default port.      sparqlify -h localhost -u postgres -p secret -d mydb -m mydb-mappings.sml -n 1000 -t 30  Agents can now access the SPARQL endpoint at `http://localhost:7531/sparql`  ### sparqlify-csv Usage: `sparqlify-csv [options]`  * Setup   * -m   SML view definition file   * -f   Input data file   * -v   View name (can be omitted if the view definition file only contains a single view)  * CSV Parser Settings   * -d   CSV field delimiter (default is '"')   * -e   CSV field escape delimiter (escapes the field delimiter) (default is '\')   * -s   CSV field separator (default is ',')   * -h   Use first row as headers. This option allows one to reference columns by name additionally to its index.   ### sparqlify-platform (Deprecated; about to be superseded by sparqlify-web-admin) The Sparqlify Platform (under /sparqlify-platform) bundles Sparqlify with the Linked Data wrapper [Pubby](https://github.com/cygri/pubby) and the SPARQL Web interface [Snorql](https://github.com/kurtjx/SNORQL).  Usage: `sparqlify-platform config-dir [port]`   * `config-dir` Path to the configuration directory, e.g. `<repository-root/sparqlify-platform/config/example>` * `port` Port on which to run the platform, default 7531.   For building, at the root of the project (outside of the sparqlify-\* directories), run `mvn compile` to build all modules. Afterwards, lauch the platform using:      cd sparqlify-platform/bin     ./sparqlify-platform <path-to-config> <port>   Assuming the platform runs under `http://localhost:7531`, you can access the following services relative to this base url: * `/sparql` is Sparqlify's SPARQL endpoint * `/snorql` shows the SNORQL web frontend * `/pubby` is the entry point to the Linked Data interface   #### Configuration The configDirectory argument is mandatory and must contain a *sub-directory* for the context-path (i.e. `sparqlify-platform`) in turn contains the files: * `platform.properties` This file contains configuration parameters that can be adjusted, such as the database connection. * `views.sparqlify` The set of Sparqlify view definition to use.  I recommend to first create a copy of the files in `/sparqlify-platform/config/example` under a different location, then adjust the parameters and finally launch the platform with `-DconfigDirectory=...` set appropriately.  The platform *applies autoconfiguration to Pubby and Snorql*: * Snorql: Namespaces are those of the views.sparqlify file. * Pubby: The host name of all resources generated in the Sparqlify views is replaced with the URL of the platform (currently still needs to be configured via `platform.properties`)  Additionally you probably want to make the URIs nice by e.g. configuring an apache reverse proxy:  Enable the apache `proxy_http` module:  	sudo a2enmod proxy_http  Then in your `/etc/apache2/sites-available/default` add lines such as  	ProxyRequest Off 	ProxyPass /resource http://localhost:7531/pubby/bizer/bsbm/v01/ retry=1 	ProxyPassReverse /resource http://localhost:7531/pubby/bizer/bsbm/v01/  These entries will enable requests to `http://localhost/resource/...` rather than `http//localhost:7531/pubby/bizer/bsbm/v01/`.  The `retry=1` means, that apache only waits 1 seconds before retrying again when it encounters an error (e.g. HTTP code 500) from the proxied resource.  *IMPORTANT: ProxyRequests are off by default; DO NOT ENABLE THEM UNLESS YOU KNOW WHAT YOU ARE DOING. Simply enabling them potentially allows anyone to use your computer as a proxy.*   ## SML Mapping Syntax: A Sparqlification Mapping Language (SML) configuration is essentially a set of CREATE VIEW statements, somewhat similar to the CREATE VIEW statement from SQL. Probably the easiest way to learn to syntax is to look at the following resources:  * The [SML documentation](http://sparqlify.org/wiki/SML) * The [SML test suite](https://github.com/AKSW/Sparqlify/tree/master/sparqlify-core/src/test/resources/org/aksw/sml/r2rml_tests) which is derived from the [R2RML test suite](https://github.com/AKSW/Sparqlify/tree/master/sparqlify-core/src/test/resources/org/w3c/r2rml_tests).  Two more examples are from  Additionally, for convenience, prefixes can be declared, which are valid throughout the config file. As comments, you can use //, /\* \*/, and #.   For a first impression, here is a quick example:          /* This is a comment      * /* You can even nest them! */      */     // Prefixes are valid throughout the file     Prefix dbp:<http://dbpedia.org/ontology/>     Prefix ex:<http://ex.org/>      Create View myFirstView As         Construct {             ?s a dbp:Person .             ?s ex:workPage ?w .         }     With         ?s = uri('http://mydomain.org/person', ?id) // Define ?s to be an URI generated from the concatenation of a prefix with mytable's id-column.         ?w = uri(?work_page) // ?w is assigned the URIs in the column 'work_page' of 'mytable'     Constrain         ?w prefix "http://my-organization.org/user/" // Constraints can be used for optimization, e.g. to prune unsatisfiable join conditions     From         mytable; // If you want to use an SQL query, the query (without trailing semicolon) must be enclosed in double square brackets: [[SELECT id, work_page FROM mytable]]   ### Notes for sparqlify-csv For `sparqlify-csv` view definition syntax is almost the same as above; the differences being:  * Instead of `Create View viewname As Construct` start your views with `CREATE VIEW TEMPLATE viewname As Construct` * There is no FROM and CONSTRAINT clause  Colums can be referenced either by name (see the -h option) or by index (1-based).  #### Example      // Assume a CSV file with the following columns (osm stands for OpenStreetMap)     (city\_name, country\_name, osm\_entity\_type, osm\_id, longitude, latitude)      Prefix fn:<http://aksw.org/sparqlify/> //Needed for urlEncode and urlDecode.     Prefix rdfs:<http://www.w3.org/2000/01/rdf-schema#>     Prefix owl:<http://www.w3.org/2002/07/owl#>     Prefix xsd:<http://www.w3.org/2001/XMLSchema#>     Prefix geo:<http://www.w3.org/2003/01/geo/wgs84_pos#>      Create View Template geocode As       Construct {         ?cityUri           owl:sameAs ?lgdUri .          ?lgdUri           rdfs:label ?cityLabel ;           geo:long ?long ;           geo:lat ?lat .       }       With         ?cityUri = uri(concat("http://fp7-pp.publicdata.eu/resource/city/", fn:urlEncode(?2), "-", fn:urlEncode(?1)))         ?cityLabel = plainLiteral(?1)         ?lgdUri = uri(concat("http://linkedgeodata.org/triplify/", ?4, ?5))         ?long = typedLiteral(?6, xsd:float)         ?lat = typedLiteral(?7, xsd:float)
AKSW/jena-sparql-api	## Welcome to the Jena SPARQL API project An advanced Jena-based SPARQL processing stack for building Semantic Web applications.  Highlights: * Fluent SPARQL Query API - Transparently enhance query execution with caching, pagination, rewriting, transformations, and so on, without having to worry about that in your application logic. * Transparent basic (normalized) string caching - Just the usual string based caching as it has been implemented over and over again * Query Transformations * SPARQL sub graph isomorphism checker * Transparent sub graph isomorphy cache - Uses the isomorphism checker for caching - Detects whether prior result sets fit into a current query - regardless of variable naming. * JPA-based Java<->RDF mapper: Run JPA criteria queries over Java classes which are actually backed by SPARQL.    [![Build Status](http://ci.aksw.org/jenkins/job/jena-sparql-api/badge/icon)](http://ci.aksw.org/jenkins/job/jena-sparql-api/)  This library offers several [Jena](http://jena.apache.org/)-compatible ways to *transparently* add delays, caching, pagination, retry and even query transformations before sending off your original SPARQL query. This frees your application layer from the hassle of dealing with those issues. Also, the server module bundles Jena with the [Atmosphere](https://github.com/Atmosphere/atmosphere) framework, giving you a kickstart for REST and websocket implementations.   ### Maven Releases are available on [maven central](http://search.maven.org/#search%7Cga%7C1%7Cjena-sparql-api). Snapshots are presently published in our own archiva:  ```xml <repositories> 	<repository> 	    <id>maven.aksw.snapshots</id> 	    <name>University Leipzig, AKSW Maven2 Repository</name> 	    <url>http://maven.aksw.org/archiva/repository/snapshots</url> 	</repository> </repositories>  <dependencies>         <!-- This is the core artifact; several other ones build on that. --> 	<dependency> 		<groupId>org.aksw.jena-sparql-api</groupId> 		<artifactId>jena-sparql-api-core</artifactId> 		<version>{check available versions with the link below}</version> 	</dependency>	 	... </dependencies> ```  Latest version(s): [jena-sparql-api on maven central](http://search.maven.org/#search%7Cga%7C1%7Cjena-sparql-api)   ### Project structure  This library is composed of the following modules: * `jena-sparql-api-core`: Contains the core interfaces and basic implementations. * `jena-sparql-api-server`: An abstract SPARQL enpdoint class that allows you to easily create your own SPARQL endpoint. For example, the SPARQL-SQL rewriter [Sparqlify](http://github.com/AKSW/Sparqlify) is implemented against these interfaces. * `jena-sparql-api-utils`: Utilities common to all packages. * `jena-sparql-api-example-proxy`: An example how to create a simple SPARQL proxy. You can easily adapt it to add pagination, caching and delays. * `jena-sparql-api-sparql-ext`: SPARQL extensions for processing non-RDF data as part of query evaluation. Most prominently features support for querying JSON documents and unnesting JSON arrays to triples. (We should also add CSV processing for completeness, although covered by the TARQL tool). * `jena-sparql-api-jgrapht`: Provides a JGraphT wrapper for Jena's Graph interface. Yes, we were aware that RDF is not a plain graph, but a labeled directed pseudo graph and implemented it accordingly. Also contains conversions of SPARQL queries to graphs. Enables e.g. subgraph isomorphism analysis. * `jena-sparql-api-mapper`: Powerful module to query RDF data transparently with the Java Persistence API (JPA) criteria queries. I.e. queries and updates are expressed over (annotated) Java classes, and no RDF specifics are exposed to the developer.    ### Usage  Here is a brief summary of what you can do. A complete example is avaible [here](https://github.com/AKSW/jena-sparql-api/blob/master/jena-sparql-api-core/src/main/java/org/aksw/jena_sparql_api/example/Example.java).  Http Query Execution Factory ```Java QueryExecutionFactory qef = new QueryExecutionFactoryHttp("http://dbpedia.org/sparql", "http://dbpedia.org"); ``` Adding a 2000 millisecond delay in order to be nice to the backend ```Java qef = new QueryExecutionFactoryDelay(qef, 2000); ``` Set up a cache  ```Java // Some boilerplace code which may get simpler soon long timeToLive = 24l * 60l * 60l * 1000l;  CacheCoreEx cacheBackend = CacheCoreH2.create("sparql", timeToLive, true); CacheEx cacheFrontend = new CacheExImpl(cacheBackend);  qef = new QueryExecutionFactoryCacheEx(qef, cacheFrontend); ``` Add pagination with (for the sake of demonstration) 900 entries per page (we could have used 1000 as well). Note: Should the pagination abort, such as because you ran out of memory and need to adjust your settings, you can resume from cache! ```Java qef = new QueryExecutionFactoryPaginated(qef, 900); ``` Create and run a query on this fully buffed QueryExecutionFactory ```Java String queryString = "SELECT ?s { ?s a <http://dbpedia.org/ontology/City> } LIMIT 5000"; QueryExecution qe = qef.createQueryExecution(queryString); 		 ResultSet rs = qe.execSelect(); System.out.println(ResultSetFormatter.asText(rs)); ```  ### Proxy Server Example This example demonstrates how you can create your own SPARQL web service. You only have to subclass `SparqlEndpointBase` and override the `createQueryExecution` method. Look at the [Source Code](https://github.com/AKSW/jena-sparql-api/blob/master/jena-sparql-api-example-proxy/src/main/java/org/aksw/jena_sparql_api/example/proxy/SparqlEndpointProxy.java) to see how easy it is.  Running the example: ```bash cd jena-sparql-api-example-proxy mvn jetty:run # This will now start the proxy on part 5522 ``` In your browser or a terminal visit:  [http://localhost:5522/sparql?service-uri=http://dbpedia.org/sparql&query=Select * { ?s ?p ?o } Limit 10](http://localhost:5522/sparql?service-uri=http%3A%2F%2Fdbpedia.org%2Fsparql&query=Select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20Limit%2010)   ## License The source code of this repo is published under the [Apache License Version 2.0](https://github.com/AKSW/jena-sparql-api/blob/master/LICENSE).  This project makes use of several dependencies: When in doubt, please cross-check with the respective projects: * [Apache Jena](https://jena.apache.org/) (Apache License 2.0) * [Atmosphere](https://github.com/Atmosphere/atmosphere) (Apache License 2.0/Partially CDDL License) * [Guava](http://code.google.com/p/guava-libraries/) (Apache License 2.0) * [commons-lang](http://commons.apache.org/proper/commons-lang/) (Apache License 2.0) * [rdf-json-writer](https://github.com/kasabi/rdf-json-writer) (currently copied but also under Apache 2.0 license, will be changed to maven dep)
AKSW/RDFUnit	RDFUnit - RDF Unit Testing Suite ==========  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.aksw.rdfunit/rdfunit-parent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.aksw.rdfunit/rdfunit-parent) [![Build Status](https://travis-ci.org/AKSW/RDFUnit.svg?branch=master)](https://travis-ci.org/AKSW/RDFUnit) [![Coverity Scan Build Status](https://scan.coverity.com/projects/2650/badge.svg?flat=1)](https://scan.coverity.com/projects/2650) [![Coverage Status](https://coveralls.io/repos/AKSW/RDFUnit/badge.svg?branch=master&service=github)](https://coveralls.io/github/AKSW/RDFUnit?branch=master) [![Codacy Badge](https://api.codacy.com/project/badge/grade/02907c27b76141709e5a6e9682fc836c)](https://www.codacy.com/app/jimkont/RDFUnit) [![codebeat badge](https://codebeat.co/badges/fc781acc-0a9f-4796-9d33-28d1ffb3b019)](https://codebeat.co/projects/github-com-aksw-rdfunit) [![Project Stats](https://www.openhub.net/p/RDFUnit/widgets/project_thin_badge.gif)](https://www.ohloh.net/p/RDFUnit)    **Homepage**: http://rdfunit.aksw.org <br/> **Documentation**: https://github.com/AKSW/RDFUnit/wiki  <br/> **Mailing list**: https://groups.google.com/d/forum/rdfunit (rdfunit [at] googlegroups.com)  <br/> **Presentations**: http://www.slideshare.net/jimkont  <br/> **Brief Overview**: https://github.com/AKSW/RDFUnit/wiki/Overview  RDFUnit is implemented on top of the [Test-Driven Data Validation Ontology](http://rdfunit.aksw.org/ns/core#) and designed to read and produce RDF that complies to that ontology only. The main components that RDFUnit reads are [TestCases (manual & automatic), TestSuites](https://github.com/AKSW/RDFUnit/wiki/TestCases), [Patterns & TestAutoGenerators](https://github.com/AKSW/RDFUnit/wiki/Patterns-Generators). RDFUnit also strictly defines the results of a TestSuite execution along with [different levels of result granularity](https://github.com/AKSW/RDFUnit/wiki/Results).  ### Basic usage  See [RDFUnit from Command Line](https://github.com/AKSW/RDFUnit/wiki/CLI) or `bin/rdfunit -h` for (a lot) more options but the simplest setting is as follows:  ```console $ bin/rdfunit -d <local-or-remote-location-URI> ```  What RDFUnit will do is:  1. Get statistics about all properties & classes in the dataset 1. Get the namespaces out of them and try to dereference all that exist in [LOV](http://lov.okfn.org) 1. Run our [Test Generators](https://github.com/AKSW/RDFUnit/wiki/Patterns-Generators) on the schemas and generate RDFUnit Test cases 1. Run the RDFUnit test cases on the dataset 1. You get a results report in html (by default) but you can request it in [RDF](http://rdfunit.aksw.org/ns/core#) or even multiple serializations with e.g.  `-o html,turtle,jsonld`   * The results are by default aggregated with counts, you can request different levels of result details using `-r {status|aggregated|shacl|shacllite|rlog|extended}`. See [here](https://github.com/AKSW/RDFUnit/wiki/Results) for more details.  You can also run: ```console $ bin/rdfunit -d <dataset-uri> -s <schema1,schema2,schema3,...> ```  Where you define your own schemas and we pick up from step 3. You can also use prefixes directly (e.g. `-s foaf,skos`) we can get everything that is defined in [LOV](http://lov.okfn.org).  ### Using Docker  A Dockerfile is provided to create a Docker image of the CLI of RDFUnit.  To create the Docker image:  ```console $ docker build -t rdfunit . ```  It is meant to execute a rdfunit command and then shutdown the container. If the output of rdfunit on stdout is not enough or you want to include files in the container, a directory could be mounted via Docker in order to create the output/result there or include files.  Here an example of usage:  ```console $ docker run --rm -it rdfunit -d https://awesome.url/file -r aggregate ```  This creates a temporary Docker container which runs the command, prints the results on stdout and stops plus removes itself. For further usage of CLI visit https://github.com/AKSW/RDFUnit/wiki/CLI.  ### Supported Schemas  RDFUnit supports the following types of schemas  1. **OWL** (using CWA): We pick the most commons OWL axioms as well as schema.org. (see [[1]](https://github.com/AKSW/RDFUnit/labels/OWL),[[2]](https://github.com/AKSW/RDFUnit/issues/20) for details 1. **SHACL**: SHACL is still in progress but we support [the most stable parts of the language](https://github.com/AKSW/RDFUnit/labels/SHACL). Whatever constructs we support can also run on SPARQL Endpoints (SHACL does not support SPARQL endpoints by design) 1. IBM **Resource Shapes**: The progress is tracked [here](https://github.com/AKSW/RDFUnit/issues/23) bus as soon as SHACL becomes stable we will drop support for RS 1. **DSP** (Dublin Core Set Profiles): The progress is tracked [here](https://github.com/AKSW/RDFUnit/issues/22) bus as soon as SHACL becomes stable we will drop support for RS  Note that you can mix all of these constraints together and RDFUnit will validate the dataset against all of them.  [![Java profiler](http://www.ej-technologies.com/images/product_banners/jprofiler_small.png)](http://www.ej-technologies.com/products/jprofiler/overview.html)
AKSW/SPARQL2NL	SPARQL2NL - the SPARQL to natural language converter  # INSTALL  - Maven Plugin in Eclipse installieren - git clone git@github.com:AKSW/SPARQL2NL.git - Import -> Existing maven project
pyvandenbussche/sparqles	# SPARQL Endpoint Status  ## Introduction  SPARQL Endpoint Status project aims at monitoring SPARQL Endpoints based on 4 aspects:  * **Discoverability** analyses how SPARQL endpoints can be located, what meta-data are available for them, etc. * **Interoperability** identifies which features of SPARQL 1.0 and SPARQL 1.1 standards are supported by an endpoint * **Performance** measures generic performance aspects such as result-streaming, atomic lookups and simple-joins over a HTTP connection. * **Availability** monitors the uptimes of a SPARQL endpoint.  ## Directory structure ``` -node/ (frontend code based on NodeJS technology) -sampleData/ (sample data to populate MongoDB for setting up or testing purposes) -scripts/ (shell scripts used to run global operations such as dumping the data) -src/ (backend Java code used to monitor the SPARQL Endpoints) ```  ## Deploying the application  ### Prerequisite In order to run both backend and frontend of SPARQLES application you need to install the following programs: - Java (tested with version 1.7) - MongoDB (tested with version 2.4.9) - NodeJS (tested with version 0.12.4) - npm  Get the code from GitHub: https://github.com/pyvandenbussche/sparqles  ### Loading sample data For you to test the frontend, you can load the sample data provided in the **sampleData** folder. Use **mongorestore** command to load the unzipped data into a database named **sparqles**.  ### Running the frontend Make sure the **sparqles** database is present in MongoDB and populated. You can now run the frontend by executing the following command: ``` cd node npm install node ./app.js ```  You should see the following message: ``` Express server listening on port 3001 ``` You can then access your application at the following URL: [http://localhost:3001/](http://localhost:3001/)  ## Running the backend  - Git clone the project. - Copy the cloned folder under Eclipse "workspace" and then run "create project" using that path (make sure the folder is in your workspace otherwise Eclipse complains) - Install Maven plugin for Eclipse to handle dependencies http://www.eclipse.org/m2e/index.html  - Once plugin installed, select Configure>Convert to Maven Project - That's it, you should be able to run from command line using these arguments: `SPARQLES -p src/main/resources/sparqles.properties -h`   ## License SPARQLES code and dataset are licensed under a [Creative Commons Attribution 4.0 International License]( https://creativecommons.org/licenses/by/4.0/).
niclashoyer/neo4j-sparql-extension	# neo4j-sparql-extension  [![Build Status](https://api.shippable.com/projects/540f1f2aec1d09a97e66f20d/badge?branchName=master)](https://app.shippable.com/projects/540f1f2aec1d09a97e66f20d/builds/latest) [![Dependency Status](https://www.versioneye.com/user/projects/539018d346c4731b13000040/badge.svg?style=flat)](https://www.versioneye.com/user/projects/539018d346c4731b13000040) [![License](http://img.shields.io/badge/license-GPLv3-lightgrey.svg?style=flat)](LICENSE) [![Releases](http://img.shields.io/badge/release-1.0.0-blue.svg?style=flat)](https://github.com/niclashoyer/neo4j-sparql-extension/releases) [![Neo4j](http://img.shields.io/badge/Neo4j-2.1.5-77CE56.svg?style=flat)](http://www.neo4j.org/) [![Gittip](http://img.shields.io/gratipay/niclashoyer.svg?style=flat)](https://www.gittip.com/niclashoyer/)  Neo4j [unmanaged extension](http://docs.neo4j.org/chunked/stable/server-unmanaged-extensions.html) for [RDF](http://www.w3.org/TR/rdf-primer/) storage and [SPARQL 1.1 query](http://www.w3.org/TR/sparql11-protocol/) features.  ## Installation  Download the latest release from the [releases page](https://github.com/niclashoyer/neo4j-sparql-extension/releases) and place it inside the `/plugins/` directory of the Neo4j server installation.  To enable the extension add it to the `org.neo4j.server.thirdparty_jaxrs_classes` key in the `/conf/neo4j-server.properties` file. For example:  ``` org.neo4j.server.thirdparty_jaxrs_classes=de.unikiel.inf.comsys.neo4j=/rdf ```  The RDF/SPARQL extension is then avaiable as `/rdf` resource on the Neo4j server.  Please note that if there is any data in the database that was not imported using the `/rdf/graph` resource the plugin might crash, because the plugin expects the data to be stored in a special way to support RDF storage in Neo4j.  ### SPARQL Protocol (SPARQL 1.1 Queries)  Base resource: `/rdf/query`  Use this resource to execute [SPARQL queries](http://www.w3.org/TR/sparql11-query/).  ```bash $ curl -v -X POST localhost:7474/rdf/query \        -H "Content-Type: application/sparql-query" \        -H "Accept: application/sparql-results+json" \        -d "SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 5" ```  See [SPARQL 1.1 Protocol "2.1 query operation"](http://www.w3.org/TR/sparql11-protocol/#query-operation).  ### SPARQL Graph Protocol  Base resource: `/rdf/graph`  Use this resource to add, replace or delete RDF data.  ```bash $ curl -v -X PUT \        localhost:7474/rdf/graph \        -H "Content-Type:text/turtle" --data-binary @data.ttl ```  ```bash $ curl -v localhost:7474/rdf/graph ```  See [SPARQL 1.1 Graph Store HTTP Protocol "5 Graph Management Operations"](http://www.w3.org/TR/sparql11-http-rdf-update/#graph-management).  ### SPARQL Protocol (SPARQL 1.1 Update Queries)  Base resource: `/rdf/update`  Use this resource to execute [SPARQL update](http://www.w3.org/TR/sparql11-update/) queries.  ```bash $ curl -v -X POST localhost:7474/rdf/query \        -H "Content-Type: application/sparql-update" \        -d "@prefix dc: <http://purl.org/dc/elements/1.1/> . \            @prefix ns: <http://example.org/ns#> . \            <http://example/book1> ns:price 42 ." ```  See [SPARQL 1.1 Protocol "2.1 update operation"](http://www.w3.org/TR/sparql11-protocol/#update-operation).  ## OWL-2 Inference  The plugin supports (limited) OWL-2 reasoning using query rewriting of SPARQL algebra expressions. For a list of supported axioms, see [the inference wiki page](https://github.com/niclashoyer/neo4j-sparql-extension/wiki/Inference).  To use inference the TBox must be uploaded to the special graph `urn:sparqlextension:tbox`:  ```bash $ curl -v -X PUT \        localhost:7474/rdf/graph\?graph=urn%3Asparqlextension%3Atbox \        -H "Content-Type:text/turtle" --data-binary @tbox.ttl ```  Now it is possible to send SPARQL queries that additionally return inferrend solutions. There are two ways to enable inference:  #### Using a Query Parameter  Just send your SPARQL query to `/rdf/query` and add a query parameter `inference=true`:  ```bash $ curl -v -X POST localhost:7474/rdf/query\?inference=true \        -H "Content-Type: application/sparql-query" \        -H "Accept: application/sparql-results+json" \        -d "SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 5" ```  #### Using the Inference Resource  Send your SPARQL query to `/rdf/query/inference`:  ```bash $ curl -v -X POST localhost:7474/rdf/query/inference \        -H "Content-Type: application/sparql-query" \        -H "Accept: application/sparql-results+json" \        -d "SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 5" ```  ## Chunked Imports  If you want to import a large amount of RDF data, you should enable the chunked import. The import will be split into smaller chunks and each chunk will be committed seperately to the database. To enable a chunked import set the query parameter `chunked=true` when using the `/rdf/graph` resource.  ## Configuration  To change the configuration add a `sparql-extension.properties` file in the `/conf` folder of the Neo4j server installation.  The default configuration is as follows:  ``` de.unikiel.inf.comsys.neo4j.query.timeout = 120 de.unikiel.inf.comsys.neo4j.query.patterns = p,c,pc de.unikiel.inf.comsys.neo4j.inference.graph = urn:sparqlextension:tbox de.unikiel.inf.comsys.neo4j.chunksize = 1000 ```  ## License  [GPLv3](https://github.com/niclashoyer/neo4j-sparql-extension/blob/master/LICENSE)
castagna/jena-examples	Apache Jena - Examples ======================  This is a collection of small and simple examples on how to use Apache Jena to handle RDF data.   How examples are organised --------------------------  Examples are grouped by name: ExampleX_NN.java and are progressively numbered from the simplest to the slightly more advanced ones.  A good learning path is to start from the ExampleIO_NN.java which show how to use Apache Jena to read (i.e. parse) or write (i.e. serialize) RDF data in various format. Apache Jena supports RDF/XML, N3, Turtle, N-Triples, etc. RIOT is a new parsing subsystem for Jena, if you are interested you can look at the ExampleRIOT_NN.java files.  The examples named ExampleAPI_NN.java show how to use the Jena Model APIs to create or manipulate RDF data.  SPARQL is the query language used for data in RDF stores and ARQ is the query engine included in Jena. The examples named ExampleARQ_NN.java show how to run SPARQL queries and iterate through the results. An extension for ARQ to run free text searches is called LARQ (i.e. Lucene + ARQ), if you are interested, you can look at ExampleLARQ_NN.java files.  TDB is the native store included in Apache Jena. The examples named  ExampleTDB_NN.java files show how to load data into TDB and run SPARQL queries.  If you are interested in ontologies and inference look at ExampleONT_NN.java and ExampleINF_NN.java.   Requirements ------------  The only requirements are a Java JDK 1.8 and Apache Maven.  Instructions on how to install Maven are here: http://maven.apache.org/download.html#Installation   If you use Eclipse, it is recommended to set M2_REPO in Eclipse (once):    mvn -Declipse.workspace=/path/to/your/workspace eclipse:add-maven-repo  Once you have Maven installed, verify with:    mvn -version    Then run (once) this to download all the necessary dependencies:    cd jena-example   mvn package  You can import the project in Eclipse via File > Import... > Existing Projects into Workspace  You can run mvn eclipse:eclipse to re-generate Eclipse .project and .classpath files automatically from your pom.xml file.  You can run mvn dependency:copy-dependencies to copy all the *.jar files in the target/dependency/ directory.  You can run mvn dependency:tree to visualize a tree of all the dependency and understand which jar is required by other jar files.      Have fun!
castagna/jena-grande	J e n a    G r a n d e     Jena Grande is a collection of utilities, experiments and examples on    how to use MapReduce, Pig, HBase or Giraph to process data in RDF format.    RDF data model is a labelled directed multigraph. URIs are used to give    globally unique names to nodes and labelled links (a.k.a. properties in   the RDF lingo). A destination node (a.k.a. an object in an RDF statement)    can be an attribute value (a.k.a. literal). Nodes can also be unnamed    (a.k.a. blank nodes).    Apache Jena is a Java library, currently in incubation, which can help    you parsing, storing and querying RDF data.      This is to be considered experimental and work in progress. If you want    to share your experience on processing RDF data with any of the Hadoop    ecosystem's projects, please, do fork this and send your pull requests.    If the graph you need to process isn't RDF, some of the tricks shared   here might be useful to you for other type of graphs.     Requirements   ------------    This is how to get Apache Giraph and install it in your Maven local repo:      svn co https://svn.apache.org/repos/asf/giraph/trunk/ apache-giraph     cd apache-giraph     mvn -P hadoop_cdh4.1.2 install (see GIRAPH-418)    ...     Have fun!      -- Paolo
jprante/elasticsearch-plugin-rdf-jena	![RDF](https://github.com/jprante/elasticsearch-plugin-rdf-jena/raw/master/src/site/resources/jena-logo-large.png)  Image by [Apache Jena](http://jena.apache.org)  # Elasticsearch RDF Jena Plugin  This Elasticsearch plugin stores and retrieves RDF triples by using the Apache Jena API.  [Apache Jena](http://jena.apache.org) is a free and open source Java framework for building semantic web and Linked Data applications. The framework is composed of different APIs interacting together to process RDF data.  Each triple will be stored as a single document, and Jena API uses term filter queries to match triples. Due to the restrictions of such an architecture, do not expect good performance.  ## Versions  | Plugin  | Elasticsearch   | Jena    | Release date | |---------|-----------------|---------|--------------| | 1.4.0.0 | 1.4.0           | 2.12.1  | Dec 28, 2014 |  ## Installation      ./bin/plugin --install rdf-jena --url http://xbib.org/repository/org/xbib/elasticsearch/plugin/elasticsearch-plugin-rdf-jena/1.4.0.0/elasticsearch-plugin-rdf-jena-1.4.0.0.zip  Do not forget to restart the node after installing.  ## Project docs  The Maven project site is available at [Github](http://jprante.github.io/elasticsearch-plugin-rdf-jena)  ## Issue  Feedback and issues are most welcome at [Github](http://github.com/jprante/elasticsearch-plugin-rdf-jena/issues)  # EXPERIMENTAL  The implementation of this plugin has just begun. Currently, only a subset of the Jena API is implemented.  # Example  Loading N-Triples      curl '0:9200/_jena/jena/bsbm' -H 'Content-Type: application/n-triples' --data-binary @/Users/joerg/Projects/github/jprante/elasticsearch-plugin-rdf-jena/src/test/resources/bsbm-generated-dataset.nt      or          curl -XPOST 'http://localhost:9200/_jena/jena/bsbm' -H 'Content-Type: application/n-triples' --data-binary @'src/test/resources/bsbm-generated-dataset.nt'   SPARQL Select      curl '0:9200/_jena/jena/bsbm' --data-urlencode "query=SELECT * WHERE { ?s ?p ?o } LIMIT 10" -H "Accept: application/sparql-results+xml"      or          curl -XPOST 'http://localhost:9200/_jena/jena/bsbm' --data-urlencode "query=SELECT * WHERE { ?s ?p ?o } LIMIT 10" -H "Accept: application/sparql-results+xml"      returns          <?xml version="1.0"?>     <sparql xmlns="http://www.w3.org/2005/sparql-results#">       <head>         <variable name="s"/>         <variable name="p"/>         <variable name="o"/>       </head>       <results>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType1</uri>           </binding>           <binding name="p">             <uri>http://purl.org/dc/elements/1.1/date</uri>           </binding>           <binding name="o">             <literal datatype="http://www.w3.org/2001/XMLSchema#date">2000-07-04</literal>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType2</uri>           </binding>           <binding name="p">             <uri>http://purl.org/dc/elements/1.1/publisher</uri>           </binding>           <binding name="o">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/StandardizationInstitution1</uri>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType3</uri>           </binding>           <binding name="p">             <uri>http://www.w3.org/2000/01/rdf-schema#subClassOf</uri>           </binding>           <binding name="o">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType1</uri>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType4</uri>           </binding>           <binding name="p">             <uri>http://purl.org/dc/elements/1.1/publisher</uri>           </binding>           <binding name="o">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/StandardizationInstitution1</uri>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType5</uri>           </binding>           <binding name="p">             <uri>http://www.w3.org/2000/01/rdf-schema#subClassOf</uri>           </binding>           <binding name="o">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType2</uri>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType6</uri>           </binding>           <binding name="p">             <uri>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</uri>           </binding>           <binding name="o">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/vocabulary/ProductType</uri>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType6</uri>           </binding>           <binding name="p">             <uri>http://purl.org/dc/elements/1.1/date</uri>           </binding>           <binding name="o">             <literal datatype="http://www.w3.org/2001/XMLSchema#date">2000-06-28</literal>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductType7</uri>           </binding>           <binding name="p">             <uri>http://purl.org/dc/elements/1.1/publisher</uri>           </binding>           <binding name="o">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/StandardizationInstitution1</uri>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductFeature1</uri>           </binding>           <binding name="p">             <uri>http://purl.org/dc/elements/1.1/publisher</uri>           </binding>           <binding name="o">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/StandardizationInstitution1</uri>           </binding>         </result>         <result>           <binding name="s">             <uri>http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/ProductFeature2</uri>           </binding>           <binding name="p">             <uri>http://www.w3.org/2000/01/rdf-schema#comment</uri>           </binding>           <binding name="o">             <literal>rearms invasiveness foemen inkstand aircrew bravadoes necking enlivenment discolorations pillaging dispossessed pocketknives upsweeps monosyllables slitted secularized visualizer rescheduled graters sheepish airframes ninepin virulence ramshackle packthreads batiste pastured priors ballades tormented towpaths transfused yahweh admonishments insertions afterwards nontemporally scrawlier luxes</literal>           </binding>         </result>       </results>     </sparql>  SPARQL DESCRIBE      curl "0:9200/_jena/jena/bsbm" --data-urlencode "query=DESCRIBE <http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/dataFromRatingSite1/Reviewer1>"     {       "http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/dataFromRatingSite1/Reviewer1" : {         "http://www.w3.org/1999/02/22-rdf-syntax-ns#type" : [ {           "type" : "uri" ,           "value" : "http://xmlns.com/foaf/0.1/Person"         }          ] ,         "http://purl.org/dc/elements/1.1/publisher" : [ {           "type" : "uri" ,           "value" : "http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/instances/dataFromRatingSite1/RatingSite1"         }          ] ,         "http://xmlns.com/foaf/0.1/mbox_sha1sum" : [ {           "type" : "literal" ,           "value" : "fb3efd92e3c7a8d775a895ba476e11a3e8f3fac"         }          ] ,         "http://xmlns.com/foaf/0.1/name" : [ {           "type" : "literal" ,           "value" : "Ruggiero-Delane"         }          ] ,         "http://purl.org/dc/elements/1.1/date" : [ {           "type" : "literal" ,           "value" : "2008-09-05" ,           "datatype" : "http://www.w3.org/2001/XMLSchema#date"         }          ] ,         "http://www4.wiwiss.fu-berlin.de/bizer/bsbm/v01/vocabulary/country" : [ {           "type" : "uri" ,           "value" : "http://downlode.org/rdf/iso-3166/countries#US"         }          ]       }     }  # Credits  This plugin is heavily based on the work of Andrea Gazzarini's [SolRDF](https://github.com/agazzarini/SolRDF)  # License  Elasticsearch RDF Jena Plugin  Copyright (C) 2014 Jörg Prante  [Follow me on twitter](https://twitter.com/xbib)  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
jsonld-java/jsonld-java	JSONLD-JAVA ===========  This is a Java implementation of the [JSON-LD specification](http://www.w3.org/TR/json-ld/) and the [JSON-LD-API specification](http://www.w3.org/TR/json-ld-api/).  [![Build Status](https://travis-ci.org/jsonld-java/jsonld-java.svg?branch=master)](https://travis-ci.org/jsonld-java/jsonld-java) [![Coverage Status](https://coveralls.io/repos/jsonld-java/jsonld-java/badge.svg?branch=master)](https://coveralls.io/r/jsonld-java/jsonld-java?branch=master)  USAGE =====  From Maven ----------      <dependency>         <groupId>com.github.jsonld-java</groupId>         <artifactId>jsonld-java</artifactId>         <version>0.11.1</version>     </dependency>  Code example ------------ ```java // Open a valid json(-ld) input file InputStream inputStream = new FileInputStream("input.json"); // Read the file into an Object (The type of this object will be a List, Map, String, Boolean, // Number or null depending on the root object in the file). Object jsonObject = JsonUtils.fromInputStream(inputStream); // Create a context JSON map containing prefixes and definitions Map context = new HashMap(); // Customise context... // Create an instance of JsonLdOptions with the standard JSON-LD options JsonLdOptions options = new JsonLdOptions(); // Customise options... // Call whichever JSONLD function you want! (e.g. compact) Object compact = JsonLdProcessor.compact(jsonObject, context, options); // Print out the result (or don't, it's your call!) System.out.println(JsonUtils.toPrettyString(compact)); ``` Processor options -----------------  The Options specified by the [JSON-LD API Specification](http://json-ld.org/spec/latest/json-ld-api/#jsonldoptions) are accessible via the `com.github.jsonldjava.core.JsonLdOptions` class, and each `JsonLdProcessor.*` function has an optional input to take an instance of this class.   Controlling network traffic ---------------------------  Parsing JSON-LD will normally follow any external `@context` declarations. Loading these contexts from the network may in some cases not be desirable, or might require additional proxy configuration or authentication.  JSONLD-Java uses the [Apache HTTPComponents Client](https://hc.apache.org/httpcomponents-client-ga/index.html) for these network connections, based on the [SystemDefaultHttpClient](http://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/impl/client/SystemDefaultHttpClient.html) which reads standard Java properties like `http.proxyHost`.   The default HTTP Client is wrapped with a [CachingHttpClient](https://hc.apache.org/httpcomponents-client-ga/httpclient-cache/apidocs/org/apache/http/impl/client/cache/CachingHttpClient.html) to provide a  small memory-based cache (1000 objects, max 128 kB each) of regularly accessed contexts.   ### Loading contexts from classpath  Your application might be parsing JSONLD documents which always use the same external `@context` IRIs. Although the default HTTP cache (see above) will avoid repeated downloading of the same contexts, your application would still initially be vulnerable to network connectivity.  To bypass this issue, and even facilitate parsing of such documents in an offline state, it is possible to provide a 'warmed' cache populated from the classpath, e.g. loaded from a JAR.  In your application, simply add a resource `jarcache.json` to the root of your classpath together with the JSON-LD contexts to embed. (Note that you might have to recursively embed any nested contexts).  The syntax of `jarcache.json` is best explained by example: ```javascript [   {     "Content-Location": "http://www.example.com/context",     "X-Classpath": "contexts/example.jsonld",     "Content-Type": "application/ld+json"   },   {     "Content-Location": "http://data.example.net/other",     "X-Classpath": "contexts/other.jsonld",     "Content-Type": "application/ld+json"   } ] ``` (See also [core/src/test/resources/jarcache.json](core/src/test/resources/jarcache.json)).  This will mean that any JSON-LD document trying to import the `@context`  `http://www.example.com/context` will instead be given `contexts/example.jsonld` loaded as a classpath resource.   The `X-Classpath` location is an IRI reference resolved relative to the location of the `jarcache.json` - so if you have multiple JARs with a `jarcache.json` each, then the `X-Classpath` will be resolved within the corresponding JAR (minimizing any conflicts).  Additional HTTP headers (such as `Content-Type` above) can be included, although these are generally ignored by JSONLD-Java.   Unless overridden in `jarcache.json`, this `Cache-Control` header is automatically injected together with the current `Date`, meaning that the resource loaded from the JAR will effectively never expire (the real HTTP server will never be consulted by the Apache HTTP client):  ``` Date: Wed, 19 Mar 2014 13:25:08 GMT Cache-Control: max-age=2147483647 ```  The mechanism for loading `jarcache.json` relies on  [Thread.currentThread().getContextClassLoader()](http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#getContextClassLoader%28%29) to locate resources from the classpath - if you are running on a command line, within a framework (e.g. OSGi) or Servlet container (e.g. Tomcat) this should normally be set correctly. If not, try:  ```java ClassLoader oldContextCL = Thread.currentThread().getContextClassLoader(); try {      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());     JsonLdProcessor.expand(input);   // or any other JsonLd operation } finally {      // Restore, in case the current thread was doing something else     // with the context classloader before calling our method     Thread.currentThread().setContextClassLoader(oldContextCL); } ```  To disable all remote document fetching, when using the default DocumentLoader, set the  following Java System Property to "true" using:  ```java System.setProperty("com.github.jsonldjava.disallowRemoteContextLoading", "true"); ```  You can also use the constant provided in DocumentLoader for the same purpose:  ```java System.setProperty(DocumentLoader.DISALLOW_REMOTE_CONTEXT_LOADING, "true"); ```  Note that if you override DocumentLoader you should also support this setting for consistency and security.  ### Loading contexts from a string  Your application might be parsing JSONLD documents which reference external `@context` IRIs that are not available as file URIs on the classpath. In this case, the `jarcache.json` approch will not work. Instead you can inject the literal context file strings through the `JsonLdOptions` object, as follows:  ```java // Inject a context document into the options as a literal string DocumentLoader dl = new DocumentLoader(); JsonLdOptions options = new JsonLdOptions(); // ... the contents of "contexts/example.jsonld" String jsonContext = "{ \"@contxt\": { ... } }"; dl.addInjectedDoc("http://www.example.com/context",  jsonContext); options.setDocumentLoader(dl);  InputStream inputStream = new FileInputStream("input.json"); Object jsonObject = JsonUtils.fromInputStream(inputStream); Map context = new HashMap(); Object compact = JsonLdProcessor.compact(jsonObject, context, options); System.out.println(JsonUtils.toPrettyString(compact)); ```  ### Customizing the Apache HttpClient  To customize the HTTP behaviour (e.g. to disable the cache or provide [authentication credentials)](https://hc.apache.org/httpcomponents-client-ga/tutorial/html/authentication.html), you may want to create and configure your own `CloseableHttpClient` instance, which can be passed to a `DocumentLoader` instance using `setHttpClient()`. This document loader can then be inserted into `JsonLdOptions` using `setDocumentLoader()` and passed as an argument to `JsonLdProcessor` arguments.    Example of inserting a credential provider (e.g. to load a `@context` protected by HTTP Basic Auth):  ```java Object input = JsonUtils.fromInputStream(..); DocumentLoader documentLoader = new DocumentLoader();          CredentialsProvider credsProvider = new BasicCredentialsProvider(); credsProvider.setCredentials(         new AuthScope("localhost", 443),         new UsernamePasswordCredentials("username", "password"));         CacheConfig cacheConfig = CacheConfig.custom().setMaxCacheEntries(1000)         .setMaxObjectSize(1024 * 128).build();  CloseableHttpClient httpClient = CachingHttpClientBuilder         .create()         // allow caching         .setCacheConfig(cacheConfig)         // Wrap the local JarCacheStorage around a BasicHttpCacheStorage         .setHttpCacheStorage(                 new JarCacheStorage(null, cacheConfig, new BasicHttpCacheStorage(                         cacheConfig))).... 		         // Add in the credentials provider         .setDefaultCredentialsProvider(credsProvider);         // When you are finished setting the properties, call build         .build();  documentLoader.setHttpClient(httpClient);          JsonLdOptions options = new JsonLdOptions(); options.setDocumentLoader(documentLoader); // .. and any other options         Object rdf = JsonLdProcessor.toRDF(input, options); ```  PLAYGROUND ----------  The [jsonld-java-tools](https://github.com/jsonld-java/jsonld-java-tools) repository contains a simple application which provides command line access to JSON-LD functions  ### Initial clone and setup  ```bash git clone git@github.com:jsonld-java/jsonld-java-tools.git chmod +x ./jsonldplayground ```  ### Usage  run the following to get usage details:  ```bash ./jsonldplayground --help ```  For Developers --------------  ### Compiling & Packaging  `jsonld-java` uses maven to compile. From the base `jsonld-java` module run `mvn clean install` to install the jar into your local maven repository.  The tests require Java-8 to compile, while the rest of the codebase is still compatible and built using the Java-6 APIs.  ### Running tests  ```bash mvn test ```  or  ```bash mvn test -pl core ```  to run only core package tests  ### Code style  The JSONLD-Java project uses custom Eclipse formatting and cleanup style guides to ensure that Pull Requests are fairly simple to merge.  These guides can be found in the /conf directory and can be installed in Eclipse using "Properties>Java Code Style>Formatter", followed by "Properties>Java Code Style>Clean Up" for each of the modules making up the JSONLD-Java project.  If you don't use Eclipse, then don't worry, your pull requests can be cleaned up by a repository maintainer prior to merging, but it makes the initial check easier if the modified code uses the conventions.  ### Submitting Pull Requests  Once you have made a change to fix a bug or add a new feature, you should commit and push the change to your fork.  Then, you can open a pull request to merge your change into the master branch of the main repository.  Implementation Reports for JSONLD-Java conformance with JSONLD-1.0 ==================================================================  The Implementation Reports documenting the conformance of JSONLD-Java with JSONLD-1.0 are available at:  https://github.com/jsonld-java/jsonld-java/tree/master/core/reports  ### Regenerating Implementation Report  Implementation Reports conforming to the [JSON-LD Implementation Report](http://json-ld.org/test-suite/reports/#instructions-for-submitting-implementation-reports) document can be regenerated using the following command:  ```bash mvn test -pl core -Dtest=JsonLdProcessorTest -Dreport.format=<format> ```  Current possible values for `<format>` include JSON-LD (`application/ld+json` or `jsonld`), NQuads (`text/plain`, `nquads`, `ntriples`, `nq` or `nt`) and Turtle (`text/turtle`, `turtle` or `ttl`). `*` can be used to generate reports in all available formats.  Integration of JSONLD-Java with other Java packages ===================================================  This is the base package for JSONLD-Java. Integration with other Java packages are done in separate repositories.  Existing integrations ---------------------  * [Eclipse RDF4J](https://github.com/eclipse/rdf4j) * [Apache Jena](https://github.com/apache/jena/) * [RDF2GO](https://github.com/jsonld-java/jsonld-java-rdf2go) * [Apache Clerezza](https://github.com/jsonld-java/jsonld-java-clerezza)  Creating an integration module ------------------------------  ### Create a repository for your module  Create a GitHub repository for your module under your user account, or have a JSONLD-Java maintainer create one in the jsonld-java organisation.  Create maven module -------------------  ### Create pom.xml for your module  Here is the basic outline for what your module's pom.xml should look like  ```xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"     xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">    <parent>     <artifactId>jsonld-java-integration</artifactId>     <groupId>com.github.jsonld-java-parent</groupId>     <version>0.11.0-SNAPSHOT</version>   </parent>   <modelVersion>4.0.0</modelVersion>   <artifactId>jsonld-java-{your module}</artifactId>   <name>JSONLD Java :: {your module name}</name>   <description>JSON-LD Java integration module for {RDF Library your module integrates}</description>   <packaging>jar</packaging>    <developers>     <developer>       <name>{YOU}</name>       <email>{YOUR EMAIL ADDRESS}</email>     </developer>   </developers>    <dependencies>     <dependency>       <groupId>${project.groupId}</groupId>       <artifactId>jsonld-java</artifactId>       <version>${project.version}</version>       <type>jar</type>        <scope>compile</scope>      </dependency>     <dependency>       <groupId>${project.groupId}</groupId>       <artifactId>jsonld-java</artifactId>       <version>${project.version}</version>       <type>test-jar</type>       <scope>test</scope>     </dependency>     <dependency>       <groupId>junit</groupId>       <artifactId>junit</artifactId>       <scope>test</scope>     </dependency>     <dependency>       <groupId>org.slf4j</groupId>       <artifactId>slf4j-jdk14</artifactId>       <scope>test</scope>     </dependency>   </dependencies> </project> ```  Make sure you edit the following:  * `project/artifactId` : set this to `jsonld-java-{module id}`, where `{module id}` usually represents the RDF library you're integrating (e.g. `jsonld-java-jena`)  * `project/name` : set this to `JSONLD Java :: {Module Name}`, wher `{module name}` is usually the name of the RDF library you're integrating.  * `project/description`  * `project/developers/developer/...` : Give youself credit by filling in the developer field. At least put your `<name>` in ([see here for all available options](http://maven.apache.org/pom.html#Developers)).  * `project/dependencies/...` : remember to add any dependencies your project needs  ### Import into your favorite editor  For Example: Follow the first few steps in the section above to import the whole `jsonld-java` project or only your new module into eclipse.  Create RDFParser Implementation -------------------------------  The interface `com.github.jsonldjava.core.RDFParser` is used to parse RDF from the library into the JSONLD-Java internal RDF format. See the documentation in [`RDFParser.java`](../core/src/main/java/com/github/jsonldjava/core/RDFParser.java) for details on how to implement this interface.  Create TripleCallback Implementation ------------------------------------  The interface `com.github.jsonldjava.core.JSONLDTripleCallback` is used to generate a representation of the JSON-LD input in the RDF library. See the documentation in [`JSONLDTripleCallback.java`](../core/src/main/java/com/github/jsonldjava/core/JSONLDTripleCallback.java) for details on how to implement this interface.  Using your Implementations --------------------------  ### RDFParser  A JSONLD RDF parser is a class that can parse your frameworks' RDF model and generate JSON-LD.  There are two ways to use your `RDFParser` implementation.  Register your parser with the `JSONLD` class and set `options.format` when you call `fromRDF`  ```java JSONLD.registerRDFParser("format/identifier", new YourRDFParser()); Object jsonld = JSONLD.fromRDF(yourInput, new Options("") {{ format = "format/identifier" }}); ```  or pass an instance of your `RDFParser` into the `fromRDF` function  ```java Object jsonld = JSONLD.fromRDF(yourInput, new YourRDFParser()); ```  ### JSONLDTripleCallback  A JSONLD triple callback is a class that can populate your framework's RDF model from JSON-LD - being called for each triple (technically quad).  Pass an instance of your `TripleCallback` to `JSONLD.toRDF`  ```java Object yourOutput = JSONLD.toRDF(jsonld, new YourTripleCallback()); ```  Integrate with your framework ----------------------------- Your framework might have its own system of readers and writers, where you should register JSON-LD as a supported format. Remember that here the "parse" direction is opposite of above, a 'reader' may be a class  that can parse JSON-LD and populate an RDF Graph.  Write Tests -----------  It's helpful to have a test or two for your implementations to make sure they work and continue to work with future versions.  Write README.md ---------------  Write a `README.md` file with instrutions on how to use your module.  Submit your module ------------------  Once you've `commit`ted your code, and `push`ed it into your github fork you can issue a [Pull Request](https://help.github.com/articles/using-pull-requests) so that we can add a reference to your module in this README file.  Alternatively, we can also host your repository in the jsonld-java organisation to give it more visibility.  CHANGELOG =========  ### 2017-08-26 * Release 0.11.1 * Fix @embed:@always support (Patch by @dr0i)  ### 2017-08-24 * Release 0.11.0  ### 2017-08-22 * Add implicit "flag only" subframe to fix incomplete list recursion (Patch by @christopher-johnson) * Support pruneBlankNodeIdentifiers framing option in 1.1 mode (Patch by @fsteeg and @eroux) * Support new @embed values (Patch by @eroux)  ### 2017-07-11 * Add injection of contexts directly into DocumentLoader (Patch by @ryankenney) * Fix N-Quads content type (Patch by @NicolasRouquette) * Add JsonUtils.fromJsonParser (Patch by @dschulten)  ### 2017-02-16 * Make literals compare consistently (Patch by @stain) * Release 0.10.0  ### 2017-01-09 * Propagate causes for JsonLdError instances where they were caused by other Exceptions * Remove schema.org hack as it appears to work again now... * Remove deprecated and unused APIs * Bump version to 0.10.0-SNAPSHOT per the removed/changed APIs  ### 2016-12-23 * Release 0.9.0 * Fixes schema.org support that is broken with Apache HTTP Client but works with java.net.URL  ### 2016-05-20 * Fix reported NPE in JsonLdApi.removeDependents  ### 2016-05-18 * Release 0.8.3 * Fix @base in remote contexts corrupting the local context  ### 2016-04-23 * Support @default inside of sets for framing  ### 2016-02-29 * Fix ConcurrentModificationException in the implementation of the Framing API  ### 2016-02-17 * Re-release version 0.8.2 with the refactoring work actually in it. 0.8.1 is identical in functionality to 0.8.0 * Release version 0.8.1 * Refactor JSONUtils and DocumentLoader to move most of the static logic into JSONUtils, and deprecate the DocumentLoader versions  ### 2016-02-10 * Release version 0.8.0  ### 2015-11-19 * Replace deprecated HTTPClient code with the new builder pattern * Chain JarCacheStorage to any other HttpCacheStorage to simplify the way local caching is performed * Bump version to 0.8.0-SNAPSHOT as some interface method parameters changed, particularly, DocumentLoader.setHttpClient changed to require CloseableHttpClient that was introduced in HttpClient-4.3  ### 2015-11-16 * Bump dependencies to latest versions, particularly HTTPClient that is seeing more use on 4.5/4.4 than the 4.2 series that we have used so far * Performance improvements for serialisation to N-Quads by replacing string append and replace with StringBuilder * Support setting a system property, com.github.jsonldjava.disallowRemoteContextLoading, to "true" to disable remote context loading.  ### 2015-09-30 * Release 0.7.0  ### 2015-09-27 * Move Tools, Clerezza and RDF2GO modules out to separate repositories. The Tools repository had a circular build dependency with Sesame, while the other modules are best located and managed in separate repositories  ### 2015-08-25 * Remove Sesame-2.7 module in favour of sesame-rio-jsonld for Sesame-2.8 and 4.0 * Fix bug where parsing did not fail if content was present after the end of a full JSON top level element  ### 2015-03-12 * Compact context arrays if they contain a single element during compaction * Bump to Sesame-2.7.15  ### 2015-03-01 * Use jopt-simple for the playground cli to simplify the coding and improve error messages * Allow RDF parsing and writing using all of the available Sesame Rio parsers through the playground cli * Make the httpclient dependency OSGi compliant  ### 2014-12-31 * Fix locale sensitive serialisation of XSD double/decimal typed literals to always be Locale.US * Bump to Sesame-2.7.14 * Bump to Clerezza-0.14  ### 2014-11-14 * Fix identification of integer, boolean, and decimal in RDF-JSONLD with useNativeTypes * Release 0.5.1  ### 2014-10-29 * Add OSGi metadata to Jar files * Bump to Sesame-2.7.13  ### 2014-07-14 * Release version 0.5.0 * Fix Jackson parse exceptions being propagated through Sesame without wrapping as RDFParseExceptions  ### 2014-07-02 * Fix use of Java-7 API so we are still Java-6 compatible * Ensure that Sesame RDFHandler endRDF and startRDF are called in SesameTripleCallback  ### 2014-06-30 * Release version 0.4.2 * Bump to Sesame-2.7.12 * Remove Jena integration module, as it is now maintained by Jena team in their repository  ### 2014-04-22 * Release version 0.4 * Bump to Sesame-2.7.11 * Bump to Jackson-2.3.3 * Bump to Jena-2.11.1  ### 2014-03-26 * Bump RDF2GO to version 5.0.0  ### 2014-03-24 * Allow loading remote @context from bundled JAR cache * Support JSON array in @context with toRDF  * Avoid exception on @context with default @language and unmapped key  ### 2014-02-24 * Javadoc some core classes, JsonLdProcessor, JsonLdApi, and JsonUtils * Rename some core classes for consistency, particularly JSONUtils to JsonUtils and JsonLdTripleCallback * Fix for a Context constructor that wasn't taking base into account  ### 2014-02-20 * Fix JsonLdApi mapping options in framing algorithm (Thanks Scott Blomquist @sblom)  ### 2014-02-06  * Release version 0.3 * Bump to Sesame-2.7.10 * Fix Jena module to use new API  ### 2014-01-29  * Updated to final Recommendation * Namespaces supported by Sesame integration module * Initial implementation of remote document loading * Bump to Jackson-2.3.1  ### 2013-11-22  * updated jena writer  ### 2013-11-07  * Integration packages renamed com.github.jsonldjava.sesame,    com.github.jsonldjava.jena etc. (Issue #76)    ### 2013-10-07  * Matched class names to Spec  - Renamed `JSONLDException` to `JsonLdError`  - Renamed `JSONLDProcessor` to `JsonLdApi`  - Renamed `JSONLD` to `JsonLdProcessor`  - Renamed `ActiveContext` to `Context`  - Renamed `Options` to `JsonLdOptions` * All context related utility functions moved to be members of the `Context` class  ### 2013-09-30 * Fixed JSON-LD to Jena to handle of BNodes  ### 2013-09-02  * Add RDF2Go integration * Bump Sesame and Clerezza dependency versions  ### 2013-06-18  * Bump to version 0.2 * Updated Turtle integration * Added Caching of contexts loaded from URI * Added source formatting eclipse config * Fixed up seasame integration package names * Replaced depreciated Jackson code  ### 2013-05-19  * Added Turtle RDFParser and TripleCallback * Changed Maven groupIds to `com.github.jsonld-java` to match github domain. * Released version 0.1  ### 2013-05-16  * Updated core code to match [JSON-LD 1.0 Processing Algorithms and API / W3C Editor's Draft 14 May 2013](http://json-ld.org/spec/latest/json-ld-api/) * Deprecated JSONLDSerializer in favor of the RDFParser interface to better represent the purpose of the interface and better fit in with the updated core code. * Updated the JSONLDTripleCallback to better fit with the updated code. * Updated the Playground tool to support updated core code.  ### 2013-05-07  * Changed base package names to com.github.jsonldjava * Reverted version to 0.1-SNAPSHOT to allow version incrementing pre 1.0 while allowing a 1.0 release when the json-ld spec is finalised. * Turned JSONLDTripleCallback into an interface.  ### 2013-04-18  * Updated to Sesame 2.7.0, Jena 2.10.0, Jackson 2.1.4 * Fixing a character encoding issue in the JSONLDProcessorTests * Bumping to 1.0.1 to reflect dependency changes  ### 2012-10-30  * Brought the implementation up to date with the reference implementation (minus the normalization stuff) * Changed entry point for the functions to the static functions in the JSONLD class * Changed the JSONLDSerializer to an abstract class, requiring the implementation of a "parse" function. The JSONLDSerializer is now passed to the JSONLD.fromRDF function. * Added JSONLDProcessingError class to handle errors more efficiently   Considerations for 1.0 release / optimisations =========  * The `Context` class is a `Map` and many of the options are stored as values of the map. These could be made into variables, whice should speed things up a bit (the same with the termDefinitions variable inside the Context). * some sort of document loader interface (with a mockup for testing) is required
eclipse/rdf4j	# Welcome to the RDF4J code repository  [![Build Status](https://travis-ci.org/eclipse/rdf4j.svg?branch=master)](https://travis-ci.org/eclipse/rdf4j)  This is the main code repository for the Eclipse RDF4J project. Please see [RDF4J.org](http://rdf4j.org) for detailed information about RDF4J, including user documentation and [downloads of the latest release](http://rdf4j.org/download).  [![Visit our IRC channel](https://kiwiirc.com/buttons/irc.freenode.net/rdf4j.png)](https://kiwiirc.com/client/irc.freenode.net/?nick=rdf4j-user|?#rdf4j)  ## Keen to contribute?  We welcome contributions! To get started, please first read our [Contributor guidelines](https://github.com/eclipse/rdf4j/blob/master/.github/CONTRIBUTING.md).  The short version:  1. Digitally sign the [Eclipse Contributor Agreement (ECA)](https://www.eclipse.org/legal/ECA.php). You can do this by logging into the [Eclipse projects forge](http://www.eclipse.org/contribute/cla); click on "Eclipse Contributor Agreement"; and Complete the form. Be sure to use the same email address when you register for the account that you intend to use on Git commit records. See the [ECA FAQ](https://www.eclipse.org/legal/ecafaq.php) for more info.  2. Create an issue in the [RDF4J GitHub issue tracker](https://github.com/eclipse/rdf4j/issues) that describes your improvement, new feature, or bug fix. 3. Fork the GitHub repository. 4. Create a new branch (starting from master) for your issue.  5. Make your changes on this branch. Apply the [RDF4J code formatting guidelines](https://github.com/eclipse/rdf4j/blob/master/.github/CONTRIBUTING.md#code-formatting). Don't forget to include unit tests. 7. Run `mvn verify` from the project root to make sure all tests succeed (both your own new ones, and existing). 8. Use meaningful commit messages and include the issue number in each commit message. 9. **sign off** every commit (using the `-s` flag). 10. Once your fix is complete, put it up for review by opening a Pull Request against the master branch in the central RDF4J repository.  These steps are explained in more detail in the [Contributor guidelines](https://github.com/eclipse/rdf4j/blob/master/.github/CONTRIBUTING.md).
wikimedia/wikidata-query-rdf	Wikibase RDF Query ==================  Tools for Querying Wikibase instances with RDF.  The modules: * blazegraph - Blazegraph extension to make querying Wikibase instances more efficient   * GPLv2 Licensed * war - Configurations for Blazegraph and the service   * GPLv2 Licensed * tools - Tools for syncing a Wikibase instance with an SPARQL 1.1 compliant triple store   * Apache Licensed * common - Code shared between tools and blazegraph   * Apache Licensed * testTools - Helpers for testing   * Apache Licensed * gui - UI for running queries and displaying results   * Apache Licensed * dist - scripts for running the service   * Apache Licensed  See more in the [User Manual](https://www.mediawiki.org/wiki/Wikidata_query_service/User_Manual).  Development Notes ----------------- ### Eclipse Works well with m2e.  ### Randomized Testing Some tests use RandomizedRunner.  If they fail you'll get a stack trace containing a "seed" that looks like this: ``` 	at __randomizedtesting.SeedInfo.seed([A4D62887A701F9F1:1BF047C091E0A9C2]:0) ``` You can reuse that see by adding @Seed to the test class like this: ```java 	@RunWith(RandomizedRunner.class) 	@Seed("A4D62887A701F9F1:1BF047C091E0A9C2") 	public class MungerUnitTest extends RandomizedTest { ``` Just remember to remove the @Seed annotation before committing the code.  We use RandomizedRunner because its a good way to cover a ton of testing ground with relatively little code.  Its how Lucene consistently finds bugs in the JVM before they're hit in production.  ### Unit and Integration Testing All tests either end in "UnitTest" or "IntegrationTest".  "UnitTest"s are so named because they don't need any external services.  "IntegrationTest"s either need to spin up some service like Blazegraph or they need an Internet connection to wikidata.org or test.wikidata.org.  ### Blazegraph We use Blazegraph for testing SPARQL.  You can start it from the command line by running ```bash 	cd tools && runBlazegraph.sh ``` It is started automatically during integration testing.  ### Maven pom.xml files are sorted according to the usual code convention. The [sortpom-maven-plugin](https://github.com/Ekryd/sortpom/) is used to fail the build if this order is not respected. The pom.xml can be automatically sorted with: ```bash mvn sortpom:sort ```  The application can be started by running the following command in the war submodule: ```bash mvn -pl war jetty:run ```  The `-pl war` argument tells maven to run inside the `war` submodule, this is equivalent to running: ```bash cd war && mvn jetty:run ```  The same target can be used directly from your IDE to run in debug mode and use all the nice IDE integration (automatic class reloading, ...). Check your IDE documentation for details.  Note: `jetty:run` will not automatically detect changes to other modules, but if you run `mvn install` in the root of the project, the changes should be compiled and jetty should auto reload the application.
jbarrasa/neosemantics	# neosemantics  ## Installation   You can either download a prebuilt jar from the [releases area](https://github.com/jbarrasa/neosemantics/releases) or build it from the source. If you prefer to build, check the note below.  1. Copy the  the jar(s) in the <NEO_HOME>/plugins directory of your Neo4j instance. 2. Add the following line to your <NEO_HOME>/conf/neo4j.conf    ```   dbms.unmanaged_extension_classes=semantics.extension=/rdf   ```    3. Restart the server.  4. Check that the installation went well by running `call dbms.procedures()`. The list of procedures should include the ones documented below. You can check that the extension is mounted by running `:GET /rdf/ping`    **Note on build**  When you run   ```   mvn clean package   ``` This will produce two jars :   1. A neosemantics-[...].jar This jar bundles all the dependencies.   2. An original-neosemantics-[...].jar This jar is just the neosemantics bit. So go this way if you want to keep the third party jars separate. In this case you will have to add all third party dependencies (look at the pom.xml).      ## What's in this repository This repository contains a set of stored procedures and extensions to both produce and consume RDF from Neo4j.  ### Stored Procedures  | Stored Proc Name        | params           | Description and example usage  | |:------------- |:-------------|:-----| | semantics.importRDF      | <ul><li>URL of the dataset</li><li>serialization format(*)</li><li>map with zero or more params (see table below)</li></ul> | Imports into Neo4j all the triples in the data set according to [the mapping defined in this post] (https://jesusbarrasa.wordpress.com/2016/06/07/importing-rdf-data-into-neo4j/). <br> **Note** that before running the import procedure an index needs to be created on property uri of Resource nodes. Just run `CREATE INDEX ON :Resource(uri)` on your Neo4j DB. <br>**Examples:**<br>CALL semantics.importRDF("file:///.../myfile.ttl","Turtle", { shortenUrls: false, typesToLabels: true, commitSize: 9000 }) <br> CALL semantics.importRDF("http:///.../donnees.rdf","RDF/XML", { languageFilter: 'fr', commitSize: 5000 , nodeCacheSize: 250000}) | | semantics.previewRDF      | <ul><li>URL of the dataset</li><li>serialization format(*)</li><li>map with zero or more params (see table below)</li></ul> | Parses some RDF and produces a preview in Neo4j browser. Same parameters as data import except for periodic commit, since there is no data written to the DB.<br> Notice that this is adequate for a preliminary visual analysis of a **SMALL dataset**. Think how many nodes you want rendered in your browser.<br> **Examples:**<br>CALL semantics.previewRDF("[https://.../clapton.n3](https://raw.githubusercontent.com/motools/musicontology/master/examples/clapton_perf/clapton.n3)","Turtle", {}) | | semantics.previewRDFSnippet      | <ul><li>An RDF snippet</li><li>serialization format(*)</li><li>map with zero or more params (see table below)</li></ul> | Identical to previewRDF but takes an RDF snippet instead of the url of the dataset.<br> Again, adequate for a preliminary visual analysis of a SMALL dataset. Think how many nodes you want rendered in your browser :)<br> **Examples:**<br>CALL semantics.previewRDFSnippet('[{"@id": "http://indiv#9132", "@type": ... }]', "JSON-LD", { languageFilter: 'en'}) | | semantics.liteOntoImport      | <ul><li>URL of the dataset</li><li>serialization(*)</li></ul> | Imports the basic elements of an OWL or RDFS ontology, i.e. Classes, Properties, Domains, Ranges. Extended description [here](https://jesusbarrasa.wordpress.com/2016/04/06/building-a-semantic-graph-in-neo4j/) <br> **Example:**<br>CALL semantics.liteOntoImport("http://.../myonto.trig","TriG")  |   (*) Valid formats: Turtle, N-Triples, JSON-LD, TriG, RDF/XML  | Param        | values(default)           | Description  | |:------------- |:-------------|:-----| | shortenUrls      | boolean (true) | when set to true, full urls are shortened using generated prefixes for both property names, relationship names and labels | | typesToLabels      | boolean (true) | when set to true, rdf:type statements are imported as node labels in Neo4j | | languageFilter      | ['en','fr','es',...] | when set, only literal properties with this language tag (or untagged ones) are imported  | | commitSize      | integer (25000) | commit a partial transaction every n triples | | nodeCacheSize      | integer (10000) | keep n nodes in cache to minimize reads from DB |   ### Extensions  | Extension        | params           | Description and example usage  | |:------------- |:-------------|:-----| | /rdf/describe/id      | <ul><li><b>nodeid:</b>the id of a node</li><li><b>excludeContext:</b>(optional) if present output will not include connected nodes, just selected one.</li></ul> | Produces an RDF serialization of the selected node. The format will be determined by the **accept** parameter in the header. Default is JSON-LD <br> **Example:**<br>:GET /rdf/describe/id?nodeid=0&excludeContext | | /rdf/describe/uri      | <ul><li><b>nodeuri:</b>the uri of a node</li><li><b>excludeContext:</b>(optional) if present output will not include connected nodes, just selected one.</li></ul> | Produces an RDF serialization of the selected node. It works on a model either imported from an RDF dataset via **semantics.importRDF** or built in a way that nodes are labeled as :Resource and have an uri. This property is the one used by this extension to lookup a node.<br> **Example:**<br>:GET /rdf/describe/uri?nodeuri=http://dataset.com#id_1234  | | /rdf/cypher      | Takes a cypher query in the payload | Produces an RDF serialization of the nodes and relationships returned by the query.<br> **Example:**<br>:POST /rdf/cypher "MATCH (a:Type1)-[r:REL]-(b) RETURN a, r, b"  | | /rdf/cypheronrdf      | Takes a cypher query in the payload | Produces an RDF serialization of the nodes and relationships returned by the query. It works on a model either imported from an RDF dataset via **semantics.importRDF** or built in a way that nodes are labeled as :Resource and have an uri.<br> **Example:**<br>:POST /rdf/cypheronrdf "MATCH (a:Resource {uri:'http://dataset/indiv#153'})-[r]-(b) RETURN a, r, b"  |
nkons/r2rml-parser	# R2RML Parser  An R2RML implementation that can export relational database contents as RDF graphs, based on an [R2RML](http://www.w3.org/TR/r2rml/) mapping document. Contains an R2RML [mapping document](https://github.com/nkons/r2rml-parser/blob/master/dspace/dspace-mapping.rdf) for the [DSpace](http://www.dspace.org/) institutional repository solution.  For more information, please [visit the wiki](https://github.com/nkons/r2rml-parser/wiki).  Please send any feedback/questions by email to [nkons@live.com](mailto:nkons@live.com), by direct message (dm) via twitter to [@nkonstantinou](https://twitter.com/nkonstantinou), or [open a new issue](https://github.com/nkons/r2rml-parser/issues). We'll be happy to discuss how to export your data into RDF.  If you use R2RML Parser, please cite it in your publications as follows: ```bibtex @inproceedings{Konstantinou2014,  author = {Konstantinou, Nikolaos and Kouis, Dimitris and Mitrou, Nikolas},  title = {Incremental Export of Relational Database Contents into RDF Graphs},  booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},  series = {WIMS '14},  year = {2014},  location = {Thessaloniki, Greece},  doi = {10.1145/2611040.2611082},  publisher = {ACM} }  ```  ## Implementation details  R2RML implementation written fully in Java 7, using Apache Jena 2.11, Spring 4.0, JUnit 4.9, and Maven 3.1. Tested against MySQL 5.6, PostgreSQL 9.2 and Oracle 11g.  ## Licence  This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 Unported License.  http://creativecommons.org/licenses/by-nc/4.0/  You are free to use and distribute this work as long as you provide proper reference and respect the license terms.  ## Publications  You can find in the wiki a list of [publications](https://github.com/nkons/r2rml-parser/wiki/Publications) based on the tool.
castagna/freebase2rdf	Freebase 2 RDF   ==============    Freebase, thanks to Google, still publishes a dump of their data,    here: http://download.freebase.com/datadumps/latest/      Freebase2RDF is a small Java program which transform the Freebase    data dump into RDF. The conversion is naive and no attempt is made    to do clever stuff with literals (such as infer data types) nor    extract a schema from the usage of 'properties'. (These are all    possible improvements, contributions welcome!)     Requirements   ------------    The only requirements are a Java JDK 1.6 and Apache Maven.    Instructions on how to install Maven are here:   http://maven.apache.org/download.html#Installation      How to run it   -------------    First, download the Freebase latest data dump:   wget http://download.freebase.com/datadumps/latest/freebase-datadump-quadruples.tsv.bz2      cd freebase2rdf     mvn package     java -cp target/freebase2rdf-0.1-SNAPSHOT-jar-with-dependencies.jar cmd.freebase2rdf </path/to/freebase-datadump-quadruples.tsv.bz2> </path/to/filename.nt.gz>     See also   --------     - http://basekb.com/ and http://code.google.com/p/basekb-tools/    - http://code.google.com/p/freebase-quad-rdfize/    - http://markmail.org/thread/mq6ylzdes6n7sc5o    - http://markmail.org/thread/jegtn6vn7kb62zof     MapReduce and how to use Apache Whirr   -------------------------------------    If you have an Hadoop cluster, here is how you can use    mvn hadoop:pack   hadoop --config ~/.whirr/hadoop jar target/hadoop-deploy/freebase2rdf-hdeploy.jar cmd.freebase2rdf4mr </path/to/freebase-datadump-quadruples.tsv.bz2> </output/path>    If you do not have an Hadoop cluster, here is how to use Apache Whirr:      export KASABI_AWS_ACCESS_KEY_ID=...     export KASABI_AWS_SECRET_ACCESS_KEY=...     cd /opt/     curl -O http://archive.apache.org/dist/whirr/whirr-0.7.1/whirr-0.7.1.tar.gz     tar zxf whirr-0.7.1.tar.gz     ssh-keygen -t rsa -P '' -f ~/.ssh/whirr     export PATH=$PATH:/opt/whirr-0.7.1/bin/     whirr version     whirr launch-cluster --config hadoop-ec2.properties --private-key-file ~/.ssh/whirr     . ~/.whirr/hadoop/hadoop-proxy.sh     # Proxy PAC configuration here: http://apache-hadoop-ec2.s3.amazonaws.com/proxy.pac    To shutdown the cluster:      whirr destroy-cluster --config hadoop-ec2.properties
ontopia/ontopia	To build the current Ontopia distribution run    $ mvn clean install -Pontopia-distribution-tomcat from a terminal. The distribution can then be found in     ontopia-distribution-tomcat/target/ontopia-distribution-tomcat-X.Y.Z-SNAPSHOT/ where X, Y and Z are the current development version numbers.  Once you build the current Ontopia distribution you will probably  want to play with the software.  Everything you want will be inside the distribution you just built; the rest of the subversion checkout serves only to generate that distribution. You can find the documentation within the distribution  under the 'doc' directory.  If you're not already familiar with Ontopia, a good document to start with is the install.html in the doc directory of the build you're using.  If you're just starting out, try starting the Tomcat server as described in section 4.3 of the install.html document, then in your Web browser navigate to http://localhost:8080/ -- the web-based applications listed there will give you plenty to do.
streamreasoning/CSPARQL-engine	CSPARQL-engine ==============  Reasoning over RDF stream made easy. The project contains parent pom in the root and a number of module that inherit from parent pom. To install the csparql-core jar, run mvn install on parent pom.
veraPDF/veraPDF-library	veraPDF-library =============== *Industry Supported PDF/A Validation*  [![Build Status](https://travis-ci.org/veraPDF/veraPDF-library.svg?branch=integration)](https://travis-ci.org/veraPDF/library "Travis-CI") [![Build Status](http://jenkins.openpreservation.org/buildStatus/icon?job=veraPDF-library)](http://jenkins.openpreservation.org/job/veraPDF-library/ "OPF Jenkins Release") [![Build Status](http://jenkins.openpreservation.org/buildStatus/icon?job=veraPDF-library-dev)](http://jenkins.openpreservation.org/job/veraPDF-library-dev/ "OPF Jenkins Development") [![Maven Central](https://img.shields.io/maven-central/v/org.verapdf/verapdf-library.svg)](http://repo1.maven.org/maven2/org/verapdf/verapdf-library/ "Maven central") [![CodeCov Coverage](https://img.shields.io/codecov/c/github/veraPDF/veraPDF-library.svg)](https://codecov.io/gh/veraPDF/veraPDF-library/ "CodeCov coverage") [![Codacy Badge](https://api.codacy.com/project/badge/Grade/cfafc08b44eb49b6aa790d6aaff09cd3)](https://www.codacy.com/app/carlwilson/veraPDF-library?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=veraPDF/veraPDF-library&amp;utm_campaign=Badge_Grade "Codacy grade")  Licensing --------- The veraPDF PDF/A Validation Library is dual-licensed, see:   - [GPLv3+](LICENSE.GPL "GNU General Public License, version 3")  - [MPLv2+](LICENSE.MPL "Mozilla Public License, version 2.0")  Documentation ------------- See the [veraPDF documentation site](http://docs.verapdf.org/).  Quick Start ----------- ### Pre-requisites  In order to build the library you'll need:   * Java 7, which can be downloaded [from Oracle](http://www.oracle.com/technetwork/java/javase/downloads/index.html), or for Linux users [OpenJDK](http://openjdk.java.net/install/index.html).  * [Maven v3+](https://maven.apache.org/)  Life will be easier if you also use [Git](https://git-scm.com/) to obtain and manage the source.  ### Building veraPDF First you'll need to obtain a version of the veraPDF-library source code. You can compile either the latest relase version or the latest development source.  #### Downloading the latest release source Use Git to clone the repository and ensure that the `master` branch is checked out: ``` git clone https://github.com/veraPDF/veraPDF-library git checkout master ``` or download the latest [tar archive](https://github.com/veraPDF/veraPDF-library/archive/master.tar.gz "veraPDF-library latest GitHub tar archive") or [zip archive](https://github.com/veraPDF/veraPDF-library/archive/master.zip "veraPDF-library latest GitHub zip archive") from GitHub.  #### Downloading the latest development source Use Git to clone the repository and ensure that the `integration` branch is checked out:      git clone https://github.com/veraPDF/veraPDF-library     git checkout integration  or download the latest [tar archive](https://github.com/veraPDF/veraPDF-library/archive/integration.tar.gz "veraPDF-library latest GitHub tar archive") or [zip archive](https://github.com/veraPDF/veraPDF-library/archive/integration.zip "veraPDF-library latest GitHub zip archive") from GitHub.  #### Use Maven to compile the source Move to the downloaded project directory and call Maven install:      cd veraPDF-library     mvn clean install
radkovo/Pdf2Dom	Pdf2Dom ========  [![Build Status](https://travis-ci.org/radkovo/Pdf2Dom.png)](https://travis-ci.org/radkovo/Pdf2Dom)  Pdf2Dom is a PDF parser that converts the documents to a HTML DOM representation. The obtained DOM tree may be then serialized to a HTML file or further processed. The inline CSS definitions contained in the resulting document are used for making the HTML page as similar as possible to the PDF input. A command-line utility for converting the PDF documents to HTML is included in the distribution package. Pdf2Dom may be also used as an independent Java library with a standard DOM interface for your DOM-based applications or as an alternative parser for the CSSBox rendering engine in order to add the PDF processing capability to CSSBox.   Pdf2Dom is based on the Apache PDFBox™ library.  See the project page for more information and downloads: http://cssbox.sourceforge.net/pdf2dom
zaro/pdf-metadata-editor	# Pdf Metadata Editor  ## What  A simple GUI editor for PDF Metadata written in java and using Apache PDFBox.  ## Build  It uses maven , so just type:      mvn install  ## Download  Prebuilt installer can be found [here](http://broken-by.me/pdf-metadata-editor/)
tamirhassan/pdfxtk	pdfxtk ======  PDF Extraction Toolkit   = logging please use commons.logging   = troubleshooting  to install a file into local maven repository: mvn install:install-file -DgroupId=com.touchgraph -DartifactId=touchgraph-mod -Dversion=1.0 -Dpackaging=jar -Dfile=${your.file}   # touchgraph TODO: what is it ?    # sonar you need to install sonar locally: http://www.sonarqube.org/downloads/ after that adopt the property 'sonar.host.url' in parent pom.xml  to run use: mvn sonar:sonar
torakiki/pdfsam	PDFsam (PDF Split And Merge) ==============================  Official SCM repository for PDFsam Basic, a free and open source, multi-platform software designed to split, merge and rotate PDF files.     [![Build Status](https://travis-ci.org/torakiki/pdfsam.png)](https://travis-ci.org/torakiki/pdfsam) [![License](http://img.shields.io/badge/license-AGPLv3-blue.svg)](http://www.gnu.org/licenses/agpl-3.0.html) [![Join the chat at https://gitter.im/PDFsam/Lobby](https://badges.gitter.im/PDFsam/Lobby.svg)](https://gitter.im/PDFsam/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  Where ------------------- Official website [pdfsam.org](http://pdfsam.org/ "PDFsam")  License ------------------- PDFsam Version 3 is open source under the [GNU Affero General Public License] v3, previous versions are released under GPLv2.  Requirements ------------------- PDFsam Basic is written using the JavaFX and requires a Java Runtime Environment 8 (or above) with JavaFx to run  Documentation ------------------- Some [documentation](http://www.pdfsam.org/documentation/) and [FAQ](http://www.pdfsam.org/faq/)  Build ------------------- This is a [simple guide](https://github.com/torakiki/pdfsam/wiki/Build-and-run) that can help you building PDFsam Basic  Contribute ------------------ Contributes are more then welcome, just please make sure you first read the [contributing guidelines](CONTRIBUTING.md)     Translations ------------------ If you want to contribute translating PDFsam to your language you can do it [here](https://translations.launchpad.net/pdfsam/pdfsam-v3)  Tips and tweaks   ------------------ A list of properties and arguments that can tweak PDFsam behavior can be found [here](https://github.com/torakiki/pdfsam/wiki/Properties-and-arguments)     [GNU Affero General Public License]: http://www.gnu.org/licenses/agpl-3.0.html
tabulapdf/tabula-java	tabula-java [![Build Status](https://travis-ci.org/tabulapdf/tabula-java.svg?branch=master)](https://travis-ci.org/tabulapdf/tabula-java) [![Join the chat at https://gitter.im/tabulapdf/tabula-java](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/tabulapdf/tabula-java?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) ===========  `tabula-java` is a library for extracting tables from PDF files — it is the table extraction engine that powers [Tabula](http://tabula.technology/) ([repo](http://github.com/tabulapdf/tabula)). You can use `tabula-java` as a command-line tool to programmatically extract tables from PDFs.  (This is the new version of the extraction engine; the previous code can be found at [`tabula-extractor`](http://github.com/tabulapdf/tabula-extractor).)  © 2014-2016 Manuel Aristarán. Available under MIT License. See [`LICENSE`](LICENSE).  ## Download  Download a version of the tabula-java's jar, with all dependencies included, that works on Mac, Windows and Linux from our [releases page](../../releases).  ## Usage Examples  `tabula-java` provides a command line application:  ``` $ java -jar target/tabula-1.0.1-jar-with-dependencies.jar --help usage: tabula [-a <AREA>] [-b <DIRECTORY>] [-c <COLUMNS>] [-d] [-f        <FORMAT>] [-g] [-h] [-i] [-l] [-n] [-o <OUTFILE>] [-p <PAGES>] [-r]        [-s <PASSWORD>] [-t] [-u] [-v]  Tabula helps you extract tables from PDFs   -a,--area <AREA>           Portion of the page to analyze                             (top,left,bottom,right). Example: --area                             269.875,12.75,790.5,561. Default is entire                             page  -b,--batch <DIRECTORY>     Convert all .pdfs in the provided directory.  -c,--columns <COLUMNS>     X coordinates of column boundaries. Example                             --columns 10.1,20.2,30.3  -d,--debug                 Print detected table areas instead of                             processing.  -f,--format <FORMAT>       Output format: (CSV,TSV,JSON). Default: CSV  -g,--guess                 Guess the portion of the page to analyze per                             page.  -h,--help                  Print this help text.  -i,--silent                Suppress all stderr output.  -l,--lattice               Force PDF to be extracted using lattice-mode                             extraction (if there are ruling lines                             separating each cell, as in a PDF of an Excel                             spreadsheet)  -n,--no-spreadsheet        [Deprecated in favor of -t/--stream] Force PDF                             not to be extracted using spreadsheet-style                             extraction (if there are no ruling lines                             separating each cell)  -o,--outfile <OUTFILE>     Write output to <file> instead of STDOUT.                             Default: -  -p,--pages <PAGES>         Comma separated list of ranges, or all.                             Examples: --pages 1-3,5-7, --pages 3 or                             --pages all. Default is --pages 1  -r,--spreadsheet           [Deprecated in favor of -l/--lattice] Force                             PDF to be extracted using spreadsheet-style                             extraction (if there are ruling lines                             separating each cell, as in a PDF of an Excel                             spreadsheet)  -s,--password <PASSWORD>   Password to decrypt document. Default is empty  -t,--stream                Force PDF to be extracted using stream-mode                             extraction (if there are no ruling lines                             separating each cell)  -u,--use-line-returns      Use embedded line returns in cells. (Only in                             spreadsheet mode.)  -v,--version               Print version and exit. ```  It also includes a debugging tool, run `java -cp ./target/tabula-1.0.1-jar-with-dependencies.jar technology.tabula.debug.Debug -h` for the available options.  You can also integrate `tabula-java` with any JVM language. For Java examples, see the [`tests`](src/test/java/technology/tabula/) folder.  JVM start-up time is a lot of the cost of the `tabula` command, so if you're trying to extract many tables from PDFs, you have a few options for speeding it up:   - the [drip](https://github.com/ninjudd/drip) utility  - the [Ruby](http://github.com/tabulapdf/tabula-extractor), [Python](https://github.com/chezou/tabula-py), [R](https://github.com/leeper/tabulizer), and [Node.js](https://github.com/ezodude/tabula-js) bindings  - writing your own program in any JVM language (Java, JRuby, Scala) that imports tabula-java.  - waiting for us to implement an API/server-style system (it's on the [roadmap](https://github.com/tabulapdf/tabula-api))  ## Building from Source  Clone this repo and run:  ``` mvn clean compile assembly:single ```  ## Contributing  Interested in helping out? We'd love to have your help!  You can help by:  - [Reporting a bug](https://github.com/tabulapdf/tabula-java/issues). - Adding or editing documentation. - Contributing code via a Pull Request. - Spreading the word about `tabula-java` to people who might be able to benefit from using it.  ### Backers  You can also support our continued work on `tabula-java` with a one-time or monthly donation [on OpenCollective](https://opencollective.com/tabulapdf#support). Organizations who use `tabula-java` can also [sponsor the project](https://opencollective.com/tabulapdf#support) for acknolwedgement on [our official site](http://tabula.technology/) and this README.  Special thanks to the following users and organizations for generously supporting Tabula with donations and grants:  <a href="https://opencollective.com/tabulapdf/backer/0/website" target="_blank"><img src="https://opencollective.com/tabulapdf/backer/0/avatar"></a> <a href="https://opencollective.com/tabulapdf/backer/1/website" target="_blank"><img src="https://opencollective.com/tabulapdf/backer/1/avatar"></a> <a href="https://opencollective.com/tabulapdf/backer/2/website" target="_blank"><img src="https://opencollective.com/tabulapdf/backer/2/avatar"></a> <a href="https://opencollective.com/tabulapdf/backer/3/website" target="_blank"><img src="https://opencollective.com/tabulapdf/backer/3/avatar"></a> <a href="https://opencollective.com/tabulapdf/backer/4/website" target="_blank"><img src="https://opencollective.com/tabulapdf/backer/4/avatar"></a> <a href="https://opencollective.com/tabulapdf/backer/5/website" target="_blank"><img src="https://opencollective.com/tabulapdf/backer/5/avatar"></a>  <a title="The John S. and James L. Knight Foundation" href="http://www.knightfoundation.org/" target="_blank"><img alt="The John S. and James L. Knight Foundation" src="http://www.knightfoundation.org/media/uploads/media_images/knight-logo-300.jpg"></a> <a title="The Shuttleworth Foundation" href="https://shuttleworthfoundation.org/" target="_blank"><img width="200" alt="The Shuttleworth Foundation" src="https://raw.githubusercontent.com/tabulapdf/tabula/gh-pages/shuttleworth.jpg"></a>
robinhowlett/chart-parser	# Chart Parser Parses horse racing result charts into JSON/CSV/Java...  ## TL;DR  When given an Equibase result chart PDF file e.g.   ![sample-chart-img](https://i.imgur.com/jQtP1Dw.png)  `chart-parser` can turn it into machine-readable formats, like JSON, e.g.  ![sample-json](https://i.imgur.com/hqtJpqb.png)  or CSV, e.g.  ![sample-csv](https://i.imgur.com/ZHIIaJd.png)  or even to be used as code in an SDK:  ![sample-code](https://i.imgur.com/yAWpxgG.png)  ## How it works  PDFs are parsed using [`ChartStripper`](https://github.com/robinhowlett/chart-parser/blob/master/src/main/java/com/robinhowlett/chartparser/charts/text/ChartStripper.java), a customized [`PDFTextStripper`](https://pdfbox.apache.org/docs/2.0.3/javadocs/org/apache/pdfbox/text/PDFTextStripper.html) instance from the [Apache PDFBox](https://pdfbox.apache.org/) library.  It works by extracting out the x-y position, scale, font size, and unicode value of each character within the PDF file into CSV, and determining the relevant field that text would belong to.  https://github.com/robinhowlett/chart-parser/blob/master/src/main/java/com/robinhowlett/chartparser/ChartParser.java#L474  ## Highlights  * The entire PDF is parsed; everything you see in the chart can be used, including race conditions, lengths ahead/behind at each point of call, fractional times, wagering payoffs and pools, footnotes etc.  * Full race card PDFs containing multiple races (including those spread over multiple pages) can be parsed.  * An SDK comes out-of-the-box that supports full serialization to and from a JSON API.  * Textual descriptions of race distances are converted to feet e.g. "Six Furlongs" becomes 3,960.  * Values for lengths ahead/behind are converted to decimal formats.  * The software adds additional features such as attempting to lookup the last-raced track details, displaying the day of the week and of the year that a race took place, outlining each medication and equipment used, providing a normalized "X-to-1" odds determination for all wagering payoffs, and calculating estimated individual fractional and splits at each fraction for each starter in a race.  * Thoroughbred, Quarter Horse, Arabian and Mixed breed races are all supported.  * The software handles edge-case scenarios such as dead-heats, walkovers, non-betting races, disqualifications (including adjusting final winning positions), cancellations, claiming price information etc.  ## How to use  [Handycapper](https://github.com/robinhowlett/handycapper) is provided as a sample application to parse and convert PDF charts to/from JSON.  Parsing a PDF file is simple and can be done in one-line e.g.:  ```java List<RaceResult> raceResults = ChartParser.create().parse(Paths.get("ARP_2016-07-24_race-charts.pdf").toFile()); ```  ## Documentation  * [JSON API Design](http://www.robinhowlett.com/chart-parser/json-design.html) * [JavaDoc](http://www.robinhowlett.com/chart-parser/apidocs/index.html) * [Maven Site](http://www.robinhowlett.com/chart-parser/index.html)  ## Compiling  ***IMPORTANT:*** This project relies on enabling [the Java 8 method parameter reflection feature (`-parameters`)](https://docs.oracle.com/javase/tutorial/reflect/member/methodparameterreflection.html) in your JVM settings e.g.   ![intellij-settings](https://i.imgur.com/8S89Byp.png)  `chart-parser` is a [Maven-based](https://maven.apache.org/) Java open-source project. Running `mvn clean install` will compile the code, run [all tests](https://github.com/robinhowlett/chart-parser/tree/master/src/test/java/com/robinhowlett/chartparser), and install the built artificat to the local repository.  ## Notes  This software is open-source and released under the [MIT License](https://github.com/robinhowlett/chart-parser/blob/master/LICENSE).   This project contains [a single sample Equibase PDF chart](https://github.com/robinhowlett/chart-parser/blob/master/src/test/resources/ARP_2016-07-24_race-charts.pdf) included for testing, educational and demonstration purposes only.  It is recommended users of this software be aware of the conditions on the PDF charts that may apply.
torakiki/sejda	Sejda SDK (http://www.sejda.org) ===== [![Build Status](https://travis-ci.org/torakiki/sejda.png)](https://travis-ci.org/torakiki/sejda) [![License](http://img.shields.io/badge/license-AGPLv3-blue.svg)](http://www.gnu.org/licenses/agpl-3.0.html)  Sejda SDK is an open source, task oriented PDF editor SDK library written in Java.  Sejda SDK offers a set of "ready to go" manipulations implemented using [SAMBox](https://github.com/torakiki/sambox), a PDFBox fork that we maintain, but you can also provide your implementation using iText, iText 5.x, PDFBox or any other PDF engine and configure Sejda to use it.  Quick start: https://github.com/torakiki/sejda/wiki/Getting-Started  A full list of the available tasks: https://github.com/torakiki/sejda/wiki/Tasks  Sejda console ---- `sejda-console` is a command line tool that can be used directly from the terminal or from your own code.  Download latest from https://github.com/torakiki/sejda/releases  Get started: https://github.com/torakiki/sejda/wiki/Getting-Started  Web Interface ---- A web interface is available at http://sejda.com where the most commonly used tasks are available as online services:  [Split PDF by pages](http://sejda.com/split-pdf)  [Merge PDF](http://sejda.com/merge-pdf)  [Split PDF by chapters](http://sejda.com/split-pdf-by-bookmarks)  [Split PDF by size](http://sejda.com/split-pdf-by-size)  [Split PDF by size](http://sejda.com/split-pdf-by-size)  [Alternate/Mix PDF](http://sejda.com/merge-pdf)  [Extract Pages from PDF](http://sejda.com/extract-pdf-pages)  [Apply header/footer or page numbers to PDF pages](http://sejda.com/header-footer-pdf)  [Rotate PDF pages](http://sejda.com/rotate-pdf-pages)  [PDF to JPG Images](http://sejda.com/pdf-to-jpg)   License ----  Sejda 1.x is licensed under Apache License, Version 2.0  Sejda 2.x and 3.x are licensed under GNU Affero Public License 3.0
viltgroup/minium	[![Minium banner](http://minium.vilt.io/images/minium_logo.png)](http://minium.vilt.io/)  What is Minium ==============  [Minium](http://minium.vilt.io/) is a framework developed at [VILT](http://vilt-group.com) that helps you test your web application the same way a human would.  Minium has spawned from the work of Rui Figueira @VILT and that initial code base is now known as Minium Core. It combines jQuery and Selenium for browser testing and tasks automation.  [Minium Developer](https://github.com/viltgroup/minium-tools/) is a console aimed at developers, allowing the creation of automatic tasks in the browser to be used in end-to-end testing of your web applications. Minium is both powerful and simple: tests can be written in Cucumber even by non-technical people, but you can integrate with your base code in any language by exposing a RESTful service. All these features make Minium a great way for you to integrate Behaviour Driven Development practices through out all phases of your project.  The last component of Minium is known as Minium Manager. It's aimed at business solutions, it's a more powerful console that provides useful reports of the ongoing projects and has a strong component in continuous integration.  Documentation =============  For a quick start head to [Minium documentation page](http://minium.vilt.io/docs), where you can also find, among other things, a complete list of all allowed methods and some simple examples.  License -------  Minium is licensed under [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0.html).
FluentLenium/FluentLenium	# What is FluentLenium ?  [![Travis](https://img.shields.io/travis/FluentLenium/FluentLenium.svg)](https://travis-ci.org/FluentLenium/FluentLenium) [![Coveralls](https://img.shields.io/coveralls/FluentLenium/FluentLenium.svg)](https://coveralls.io/github/FluentLenium/FluentLenium) [![codebeat badge](https://codebeat.co/badges/51ecc826-c8e6-407d-a090-12330b2bbe44)](https://codebeat.co/projects/github-com-fluentlenium-fluentlenium-develop) [![Maven Central](https://img.shields.io/maven-central/v/org.fluentlenium/fluentlenium-parent.svg)](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.fluentlenium%22%20AND%20a%3A%22fluentlenium-parent%22) [![Website](https://img.shields.io/website-up-down-green-red/http/fluentlenium.org.svg)](http://fluentlenium.org)  FluentLenium helps you writing readable, reusable, reliable and resilient UI functional tests for the browser.  FluentLenium provides a Java [fluent interface](http://en.wikipedia.org/wiki/Fluent_interface) to [Selenium](http://www.seleniumhq.org/), and brings some magic to avoid common issues faced by Selenium users.  FluentLenium is shipped with adapters for [JUnit](junit.org/), [TestNG](http://testng.org/doc/index.html) and [Cucumber](https://cucumber.io), but it can also be used standalone.  FluentLenium best integrates with [AssertJ](http://joel-costigliola.github.io/assertj/), but you can also choose to use the assertion framework you want.  # Choose the right version  FluentLenium 3.x is still in development and includes latest enhancements and features, but Selenium 3 and Java 8 are required to run it.  Starting from FluentLenium 3.1.0 you can use all sparks of Java 8, including lambdas. It is a nice extension in comparison to Selenium 3 which is still basing on Guava objects. Please take a look on documentation to find `await` lambda usage example.  FluentLenium 1.x is in maintenance state, and no new feature will be added anymore. It requires Selenium 2 and Java 7, but can also be used with Java 8. Selenium 3 is not supported in this version though.  # Quickstart with JUnit and AssertJ  - Add dependencies to your `pom.xml`.  ```xml <dependency>     <groupId>org.fluentlenium</groupId>     <artifactId>fluentlenium-junit</artifactId>     <version>3.4.0</version>     <scope>test</scope> </dependency> <dependency>     <groupId>org.fluentlenium</groupId>     <artifactId>fluentlenium-assertj</artifactId>     <version>3.4.0</version>     <scope>test</scope> </dependency> <dependency>     <groupId>junit</groupId>     <artifactId>junit</artifactId>     <version>4.12</version>     <scope>test</scope> </dependency> <dependency>     <groupId>org.seleniumhq.selenium</groupId>     <artifactId>htmlunit-driver</artifactId>     <version>2.25</version>     <scope>test</scope> </dependency> <dependency>     <groupId>xml-apis</groupId>     <artifactId>xml-apis</artifactId>     <version>1.4.01</version>     <scope>test</scope> </dependency> ```  - Basic FluentLenium test  ```java import org.fluentlenium.adapter.junit.FluentTest; import org.fluentlenium.core.hook.wait.Wait; import org.junit.Test;  import java.util.concurrent.TimeUnit;  import static org.assertj.core.api.Assertions.assertThat;  @Wait public class DuckDuckGoTest extends FluentTest {     @Test     public void titleOfDuckDuckGoShouldContainSearchQueryName() {         goTo("https://duckduckgo.com");         $("#search_form_input_homepage").fill().with("FluentLenium");         $("#search_button_homepage").submit();         await().atMost(5, TimeUnit.SECONDS).until(el("#search_form_homepage")).not().present();         assertThat(window().title()).contains("FluentLenium");     } } ```  - A little bit more advanced version of the same FluentLenium test  ```java import org.fluentlenium.adapter.junit.FluentTest; import org.fluentlenium.core.annotation.Page; import org.junit.Test; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver;  public class DuckDuckGoTest extends FluentTest {     @Page     DuckDuckMainPage duckDuckMainPage;      @Test     public void titleOfDuckDuckGoShouldContainSearchQueryName() {         String searchPhrase = "searchPhrase";          goTo(duckDuckMainPage)                 .typeSearchPhraseIn(searchPhrase)                 .submitSearchForm()                 .assertIsPhrasePresentInTheResults(searchPhrase);     } } ```  ```java import static org.assertj.core.api.Assertions.assertThat;  import java.util.concurrent.TimeUnit;  import org.fluentlenium.core.FluentPage; import org.fluentlenium.core.annotation.PageUrl; import org.fluentlenium.core.domain.FluentWebElement; import org.openqa.selenium.support.FindBy;  @PageUrl("https://duckduckgo.com") public class DuckDuckMainPage extends FluentPage {     private static final String SEARCH_FORM_HOMEPAGE = "#search_form_homepage";      @FindBy(css = "#search_form_input_homepage")     private FluentWebElement searchInput;      @FindBy(css = "#search_button_homepage")     private FluentWebElement searchButton;      public DuckDuckMainPage typeSearchPhraseIn(String searchPhrase) {         searchInput.write(searchPhrase);         return this;     }      public DuckDuckMainPage submitSearchForm() {         searchButton.submit();         awaitUntilSearchFormDisappear();         return this;     }      public void assertIsPhrasePresentInTheResults(String searchPhrase) {         assertThat(window().title()).contains(searchPhrase);     }      private DuckDuckMainPage awaitUntilSearchFormDisappear() {         await().atMost(5, TimeUnit.SECONDS).until(el(SEARCH_FORM_HOMEPAGE)).not().present();         return this;     } } ```  - Run as a JUnit test.  [More FluentLenium examples are available on github](https://github.com/FluentLenium/FluentLenium/tree/master/examples).  ## Documentation  Full documentation is available on [fluentlenium.org](http://fluentlenium.org/docs), or in the [docs sources directory](https://github.com/FluentLenium/FluentLenium/tree/develop/docs).  ## Contact Us If you have any comment, remark or issue, please open an issue on [FluentLenium Issue Tracker](https://github.com/FluentLenium/FluentLenium/issues)
MachinePublishers/jBrowserDriver	# jBrowserDriver A programmable, embeddable web browser driver compatible with the Selenium WebDriver spec -- headless, WebKit-based, pure Java  Licensed under the Apache License v2.0 ([details](https://raw.githubusercontent.com/MachinePublishers/jBrowserDriver/master/LICENSE)).  - - -  ## Download Get a ZIP archive of a [recent release](https://github.com/MachinePublishers/jBrowserDriver/releases/latest).  Or install via Maven: ```xml <dependency>   <groupId>com.machinepublishers</groupId>   <artifactId>jbrowserdriver</artifactId>   <version>0.17.9</version> </dependency> ``` For other install options, see the [Central Repository](http://search.maven.org/#artifactdetails|com.machinepublishers|jbrowserdriver|0.17.9|jar).  ## Prerequisites Java 8 with JavaFX:  * Ubuntu Xenial 16.04 LTS, Debian 8 Jessie ([Backports](https://backports.debian.org/Instructions/#index2h2)), Debian 9 Stretch:<br>&nbsp;&nbsp;&nbsp;&nbsp;`sudo apt-get install openjdk-8-jre openjfx`  * Ubuntu Trusty 14.04 LTS:<br>&nbsp;&nbsp;&nbsp;&nbsp;`sudo add-apt-repository ppa:webupd8team/java && sudo apt-get update && sudo apt-get install oracle-java8-installer libgtk2.0 libxtst6 libxslt1.1 fonts-freefont-ttf libasound2 && sudo update-alternatives --config java`  * Mac, Windows, Linux:<br>&nbsp;&nbsp;&nbsp;&nbsp;[install Oracle Java 8](http://www.oracle.com/technetwork/java/javase/downloads/index.html) *(note: choose either the JRE or JDK but not the "Server JRE" since it doesn't include JavaFX)*  ## Usage For specific details, refer to the [API documentation](http://machinepublishers.github.io/jBrowserDriver/).  Use this library like any other Selenium WebDriver or RemoteWebDriver. It also works with Selenium Server and Selenium Grid (see example below).  You can optionally create a [Settings](http://machinepublishers.github.io/jBrowserDriver/com/machinepublishers/jbrowserdriver/Settings.html) object, [configure it](http://machinepublishers.github.io/jBrowserDriver/com/machinepublishers/jbrowserdriver/Settings.Builder.html), and pass it to the [JBrowserDriver constructor](http://machinepublishers.github.io/jBrowserDriver/com/machinepublishers/jbrowserdriver/JBrowserDriver.html#JBrowserDriver-com.machinepublishers.jbrowserdriver.Settings-) to specify a proxy, request headers, time zone, user agent, or navigator details. By default, the browser mimics the fingerprint of Tor Browser.  Settings can alternately be configured using Java system properties or Selenium Capabilities. See [Settings builder](http://machinepublishers.github.io/jBrowserDriver/com/machinepublishers/jbrowserdriver/Settings.Builder.html) documentation for details.  Each instance of JBrowserDriver is backed by a separate Java process.  #### Example: ```java import org.openqa.selenium.WebDriver; import com.machinepublishers.jbrowserdriver.Timezone; import com.machinepublishers.jbrowserdriver.JBrowserDriver; import com.machinepublishers.jbrowserdriver.Settings;      public class Example {   public static void main(String[] args) {      // You can optionally pass a Settings object here,     // constructed using Settings.Builder     JBrowserDriver driver = new JBrowserDriver(Settings.builder().       timezone(Timezone.AMERICA_NEWYORK).build());      // This will block for the page load and any     // associated AJAX requests     driver.get("http://example.com");      // You can get status code unlike other Selenium drivers.     // It blocks for AJAX requests and page loads after clicks      // and keyboard events.     System.out.println(driver.getStatusCode());      // Returns the page source in its current state, including     // any DOM updates that occurred after page load     System.out.println(driver.getPageSource());          // Close the browser. Allows this thread to terminate.     driver.quit();   } } ```  #### Running from a remote Selenium server:  You can also run JBrowserDriver from a remotely running Selenium server.  First start the remote selenium server(s):   * If you are running using Selenium standalone mode:      Start the standalone server: `java -classpath "selenium-server-standalone-2.53.0.jar:jBrowserDriver-v0.17.0/dist/*" org.openqa.grid.selenium.GridLauncher -browser browserName=jbrowserdriver,version=1,platform=ANY`   * If you are running using Selenium Grid mode:      Start the hub: `java -jar selenium-server-standalone-2.53.0.jar -role hub`      Start the node: `java -classpath "selenium-server-standalone-2.53.0.jar:jBrowserDriver-v0.17.0/dist/*" org.openqa.grid.selenium.GridLauncher -role node http://localhost:4444/grid/register -browser browserName=jbrowserdriver,version=1,platform=ANY`  &nbsp;&nbsp;*On Windows, replace the colon in the classpath with a semi-colon.*  Whether you are using standalone mode or grid mode, you can use this code to call jBrowserDriver remotely:  ```java import java.net.MalformedURLException; import java.net.URL; import org.openqa.selenium.Platform; import org.openqa.selenium.remote.DesiredCapabilities; import org.openqa.selenium.remote.RemoteWebDriver; import com.machinepublishers.jbrowserdriver.Settings; import com.machinepublishers.jbrowserdriver.Timezone;  public class Example {   public static void main(String[] args) throws MalformedURLException {        DesiredCapabilities capabilities =          new DesiredCapabilities("jbrowserdriver", "1", Platform.ANY);          // Optionally customize the settings     capabilities.merge(         Settings.builder().         timezone(Timezone.AMERICA_NEWYORK).         buildCapabilities());          RemoteWebDriver driver = new RemoteWebDriver(         new URL("http://localhost:4444/wd/hub"), capabilities);          driver.get("http://example.com");          System.out.println(driver.getPageSource());          driver.quit();   } } ```  ## Building Install and configure [Maven v3.x](https://maven.apache.org/download.cgi) (which is also available in most Linux package repos) and then from the project root run `mvn clean compile install`. To use in [Eclipse](http://www.eclipse.org/downloads/), either import the existing Java project from the root directory or import the Maven pom.xml file. However, if you merely want to use this as a dependency in a separate project, see the [Download](https://github.com/MachinePublishers/jBrowserDriver#download) section.  ## Contributing Pull requests are welcome, and we ask people contributing code to agree to the [CLA](https://github.com/MachinePublishers/jBrowserDriver/blob/master/CLA-rev2-digital.txt) which is similar to the agreement used by the Selenium Project. Signing the CLA is simply a matter of editing the file to add your digital "signature" and adding it to your pull request.  Feel free to discuss bugs and new features by opening a [new issue](https://github.com/MachinePublishers/jBrowserDriver/issues/new).  - - -  Copyright (C) 2014-2017 jBrowserDriver committers
bonigarcia/webdrivermanager	# WebDriverManager [![][Logo]][GitHub Repository]  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.github.bonigarcia/webdrivermanager/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.github.bonigarcia/webdrivermanager) [![Build Status](https://travis-ci.org/bonigarcia/webdrivermanager.svg?branch=master)](https://travis-ci.org/bonigarcia/webdrivermanager) [![License (LGPL version 2.1)](https://img.shields.io/badge/license-GNU%20LGPL%20version%202.1-brightgreen.svg?style=flat-square)](http://opensource.org/licenses/LGPL-2.1) [![Support badge](https://img.shields.io/badge/support-sof-green.svg)](http://stackoverflow.com/questions/tagged/webdrivermanager-java) [![Twitter Follow](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/boni_gg)  This library is aimed to automate the [Selenium Webdriver] binaries management in runtime for Java.  If you have ever used [Selenium Webdriver], you probably know that in order to use some browsers such as **Chrome**, **Internet Explorer**, **Opera**, **Microsoft Edge**, **PhantomJS**, or **Firefox** you need to download a binary which allows WebDriver to handle the browser. In addition, the absolute path to this binary must be set as Java variables, as follows:  ```java System.setProperty("webdriver.chrome.driver", "/absolute/path/to/binary/chromedriver"); System.setProperty("webdriver.opera.driver", "/absolute/path/to/binary/operadriver"); System.setProperty("webdriver.ie.driver", "C:/absolute/path/to/binary/IEDriverServer.exe"); System.setProperty("webdriver.edge.driver", "C:/absolute/path/to/binary/MicrosoftWebDriver.exe"); System.setProperty("phantomjs.binary.path", "/absolute/path/to/binary/phantomjs"); System.setProperty("webdriver.gecko.driver", "/absolute/path/to/binary/geckodriver"); ```  This is quite annoying since it forces you to link directly this binary in your source code. In addition, you have to check manually when new versions of the binaries are released. This library comes to the rescue, performing in an automated way all this dirty job for you.  WebDriverManager is open source, released under the terms of [LGPL License 2.1].  ## Usage  In order to use WebDriverManager in a Maven project, first add the following dependency to your `pom.xml` (Java 7 or upper required):  ```xml <dependency> 	<groupId>io.github.bonigarcia</groupId> 	<artifactId>webdrivermanager</artifactId> 	<version>1.7.2</version> </dependency> ```  WebDriverManager will be tipically used by tests. In that case, the scope of the dependency should be test (`<scope>test</scope>`).  Once we have included this dependency, you can let WebDriverManager to manage the WebDriver binaries for your application/test. Take a look at this JUnit example which uses Chrome with Selenium WebDriver:  ```java public class ChromeTest {  	private WebDriver driver;  	@BeforeClass 	public static void setupClass() { 		ChromeDriverManager.getInstance().setup(); 	}  	@Before 	public void setupTest() { 		driver = new ChromeDriver(); 	}  	@After 	public void teardown() { 		if (driver != null) { 			driver.quit(); 		} 	}  	@Test 	public void test() { 		// Your test code here 	}  } ```  Notice that simply adding ``ChromeDriverManager.getInstance().setup();`` WebDriverManager does magic for you:  1. It checks for the latest version of the WebDriver binary 2. It downloads the WebDriver binary if it's not present on your system 3. It exports the required WebDriver Java environment variables needed by Selenium  So far, WebDriverManager supports **Chrome**, **Opera**, **Internet Explorer**, **Microsoft Edge**,  **PhantomJS**, and **Firefox**. For that, it provides several *drivers managers* for these browsers, i.e. `ChromeDriverManager`, `FirefoxDriverManager`, `OperaDriverManager`, `PhantomJsDriverManager`, `EdgeDriverManager`, and `InternetExplorerDriverManager`. These *drivers managers* can be used as follows:  ```java ChromeDriverManager.getInstance().setup(); FirefoxDriverManager.getInstance().setup(); OperaDriverManager.getInstance().setup(); PhantomJsDriverManager.getInstance().setup(); EdgeDriverManager.getInstance().setup(); InternetExplorerDriverManager.getInstance().setup(); ```  Moreover, WebDriverManager provides a generic *driver manager* called `WebDriverManager`. This manager which can be parameterized using Selenium driver classes (e.g. `org.openqa.selenium.chrome.ChromeDriver`, `org.openqa.selenium.firefox.FirefoxDriver`, etc), as follows:   ```java import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import io.github.bonigarcia.wdm.WebDriverManager;  // ...  Class<? extends WebDriver> driverClass = ChromeDriver.class; WebDriverManager.getInstance(driverClass).setup(); WebDriver driver = driverClass.newInstance(); ```  ## Examples  Check out [WebDriverManager Examples][WebDriverManager Examples] for some JUnit tests utilizing WebDriverManager.   ## WebDriverManager API  As of version 1.6.0, WebDriverManager exposes its API by means of the **builder pattern**. This means that given a *DriverManger* instance (e.g. ``ChromeDriverManager``, ``FirefoxDriverManager``, and so on), their capabilities can be tuned using different methods, namely:  -  ``version()`` : By default, WebDriverManager tries to download the latest version of a given driver binary. An specific version can be specified using this method.  -  ``forceCache()`` : By default, WebDriverManager connects to the specific driver repository URL to find out what is the latest version of the binary. This can be avoided forcing to use the latest version form the local repository. -  ``forceDownload()`` : By default, after WebDriverManager verifies the latest version of the binary, and then it uses the cached version if exists. This opcion forces to download again the binary even if it has been previously cached. -  ``useBetaVersions()`` : By default, WebDriverManager skip beta versions. With this method, WebDriverManager will download also beta versions. -  ``architecture(Architecture arch)`` : By default, WebDriverManager would try to use the proper binary for the platform running the test case (i.e. 32-bit or 64-bit). This behavior can be changed by forcing a given architecture: 32-bits (``Architecture.x32``) or 64-bits (``Architecture.x64``);    -  ``arch32()`` : Force to use the 32-bit version of a given driver binary. -  ``arch64()`` : Force to use the 64-bit version of a given driver binary. -  ``driverRepositoryUrl(URL url)`` : This method allows to change the repository URL in which the binaries are hosted (see next section for default values). -  ``useTaobaoMirror()`` :  The [npm.taobao.org] site is a mirror which hosts different software assets. Among them, it hosts *chromedriver*, *geckodriver*,  *operadriver*, and *phantomjs* driver. Therefore, this method can be used in ``ChromeDriverManager``, ``FirefoxDriverManager``, ``OperaDriverManager``, and ``PhantomJsDriverManager`` to force to use the taobao.org mirror. -  ``proxy(String proxy)`` : Use a HTTP proxy for the Internet connection. -  ``proxyUser(String username)`` : Specify a username for HTTP proxy. -  ``proxyPass(String password)`` : Specify a password for HTTP proxy.  The following table contains some examples:  | Example                                                                | Description                                                       | |------------------------------------------------------------------------|-------------------------------------------------------------------| | ``ChromeDriverManager.getInstance().version("2.26").setup();``       | Force to use version 2.26 of *chromedriver*                       | | ``FirefoxDriverManager.getInstance().arch32().setup();``             | Force to use the 32-bit version of *geckodriver*                  | | ``OperaDriverManager.getInstance().forceCache().setup();``             | Force to use the cache version of *operadriver*                   | | ``PhantomJsDriverManager.getInstance().useTaobaoMirror().setup();``    | Force to use the taobao.org mirror to download *phantomjs* driver | | ``ChromeDriverManager.getInstance().proxy("server:port").setup();``  | Using proxy *server:port* for the connection                      |   ## Configuration  Configuration parameters for WebDriverManager are set in the ``webdrivermanager.properties`` file:  ```properties wdm.targetPath=~/.m2/repository/webdriver wdm.forceCache=false wdm.override=false wdm.timeout=30 wdm.seekErrorRetries=3  wdm.chromeDriverUrl=https://chromedriver.storage.googleapis.com/ wdm.chromeDriverTaobaoUrl=http://npm.taobao.org/mirrors/chromedriver wdm.chromeDriverExport=webdriver.chrome.driver wdm.chromeDriverVersion=LATEST  wdm.operaDriverUrl=https://api.github.com/repos/operasoftware/operachromiumdriver/releases wdm.operaDriverTaobaoUrl=http://npm.taobao.org/mirrors/operadriver wdm.operaDriverExport=webdriver.opera.driver wdm.operaDriverVersion=LATEST  wdm.internetExplorerDriverUrl=https://selenium-release.storage.googleapis.com/ wdm.internetExplorerExport=webdriver.ie.driver wdm.internetExplorerVersion=LATEST  wdm.edgeDriverUrl=https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/ wdm.edgeExport=webdriver.edge.driver wdm.edgeVersion=LATEST  wdm.phantomjsDriverUrl=https://bitbucket.org/ariya/phantomjs/downloads/ wdm.phantomjsDriverTaobaoUrl=http://npm.taobao.org/mirrors/phantomjs wdm.phantomjsDriverExport=phantomjs.binary.path wdm.phantomjsDriverVersion=LATEST  wdm.geckoDriverUrl=https://api.github.com/repos/mozilla/geckodriver/releases wdm.geckoDriverTaobaoUrl=http://npm.taobao.org/mirrors/geckodriver wdm.geckoDriverExport=webdriver.gecko.driver wdm.geckoDriverVersion=LATEST  wdm.gitHubTokenName= wdm.gitHubTokenSecret=  wdm.architecture= ```  The variable ``wdm.targetPath`` is the default folder in which WebDriver binaries are going to be stored. Notice that by default the path of the Maven local repository is used. The URLs to check the latest version of Chrome, Opera, Internet Explorer, Edge, PhantomJS, and Firefox are set using the variables ``wdm.chromeDriverUrl``, ``wdm.operaDriverExport``, ``wdm.operaDriverUrl``, ``wdm.edgeDriverUrl``, ``wdm.phantomjsDriverUrl``, and ``wdm.geckoDriverUrl``.  These properties can be overwritten by Java system properties, for example:  ```java System.setProperty("wdm.targetPath", "/my/custom/path/to/driver/binaries"); ```  ... or by command line, for example:  ```properties -Dwdm.override=true ```  By default, WebDriverManager downloads the latest version of the WebDriver binary. But concrete versions of WebDriver binaries can be forced by changing the value of the variables ``wdm.chromeDriverVersion``, ``wdm.operaDriverVersion``,  ``wdm.internetExplorerVersion``, or  ``wdm.edgeVersion`` from its default value (``LATEST``) to a concrete version. For instance:  ```properties -Dwdm.chromeDriverVersion=2.25 -Dwdm.internetExplorerVersion=2.46 -Dwdm.operaDriverVersion=0.2.0 -Dwdm.edgeVersion=3.14366 -Dwdm.phantomjsDriverVersion=2.1.1 -Dwdm.geckoDriverVersion=0.11.1 ```  If no version is specified, WebDriverManager sends a request to the server hosting the binary. In order to avoid this request and check if any binary has been previously downloaded, the key `wdm.forceCache` can be used.  ### HTTP Proxy  If you use an HTTP Proxy in your Internet connection, you can configure your settings by exporting the Java environment variable ``HTTPS_PROXY`` using the following notation: ``my.http.proxy:1234`` or ``username:password@my.http.proxy:1234``. Also you can configure username and password using environment variables (``HTTPS_PROXY_USER`` and ``HTTPS_PROXY_PASS``).  ### Known Issues  Some of the binaries (for Opera and Firefox) are hosted on GitHub. When several consecutive requests are made by WebDriverManager, GitHub servers return an **HTTP 403 error** response as follows:  ``` Caused by: java.io.IOException: Server returned HTTP response code: 403 for URL: https://api.github.com/repos/operasoftware/operachromiumdriver/releases 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1840) 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:254) 	at io.github.bonigarcia.wdm.BrowserManager.openGitHubConnection(BrowserManager.java:463) 	at io.github.bonigarcia.wdm.OperaDriverManager.getDrivers(OperaDriverManager.java:55) 	at io.github.bonigarcia.wdm.BrowserManager.manage(BrowserManager.java:168) ```  ``` Caused by: java.io.IOException: Server returned HTTP response code: 403 for URL: https://api.github.com/repos/mozilla/geckodriver/releases 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1840) 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:254) 	at io.github.bonigarcia.wdm.FirefoxDriverManager.getDrivers(FirefoxDriverManager.java:61) 	at io.github.bonigarcia.wdm.BrowserManager.manage(BrowserManager.java:163) ```  In order to avoid this problem, [authenticated requests] should be done. The procedure is the following:  1. Create a token/secret pair in your [GitHub account] 2. Tell WebDriverManager the value of this pair token/secret. To do that you should use the configuration keys ``wdm.gitHubTokenName`` and ``wdm.gitHubTokenSecret``. You can pass them as command line Java parameters as follows:  ```properties -Dwdm.gitHubTokenName=<your-token-name> -Dwdm.gitHubTokenSecret=<your-token-secret> ```  ## Help  If you have questions on how to use WebDriverManager properly with a special configuration or suchlike, please consider asking a question on [stackoverflow](https://stackoverflow.com/questions/tagged/webdrivermanager-java) and tag it with  *webdrivermanager-java*.  ## About  WebDriverManager (Copyright &copy; 2015-2017) is a personal project of [Boni Garcia] licensed under [LGPL License 2.1]. Comments, questions and suggestions are always very [welcome][WebDriverManager issues]!  [Logo]: http://bonigarcia.github.io/img/webdrivermanager.png [Selenium Webdriver]: http://docs.seleniumhq.org/projects/webdriver/ [LGPL License 2.1]: http://www.gnu.org/licenses/lgpl-2.1.html [Boni Garcia]: http://bonigarcia.github.io/ [GitHub Repository]: https://github.com/bonigarcia/webdrivermanager [authenticated requests]: https://developer.github.com/v3/#rate-limiting [GitHub account]: https://github.com/settings/tokens [WebDriverManager issues]: https://github.com/bonigarcia/webdrivermanager/issues [WebDriverManager Examples]: https://github.com/bonigarcia/webdrivermanager-examples [npm.taobao.org]: http://npm.taobao.org/mirrors/
vmi/selenese-runner-java	Selenese Runner Java ====================  This is selenese script interpreter implemented by Java.  It supports test-case and test-suite which are Selenium IDE's native format.  Note: Supported Java version is 8 or later.  [![Build Status](https://travis-ci.org/vmi/selenese-runner-java.svg?branch=master)](https://travis-ci.org/vmi/selenese-runner-java)  Download --------  You can download the executable jar from:  https://github.com/vmi/selenese-runner-java/releases  Features --------  * Run test-case and test-suite generated by Selenium IDE from command line. * Support commands of Selenium IDE: Flow Control.   https://github.com/davehunt/selenium-ide-flowcontrol * Log URL/title/Cookies at all commands. * Take screenshots at all commands. (optional) * Override base URL. * Override directory of screenshot path. * Generate JUnit XML result. * Generate HTML result.  Usage -----      java -jar selenese-runner.jar <option> ... <test-case|test-suite> ...           --config (-c) <file>                    : load option information from file.      --driver (-d) <driver>                  : firefox (default) | chrome | ie | safari | htmlunit | phantomjs | remote | appium | FQCN-of-WebDriverFactory      --profile (-p) <name>                   : profile name (Firefox only *1)      --profile-dir (-P) <dir>                : profile directory (Firefox only *1)      --chrome-experimental-options <file>    : path to json file specify experimental options for chrome (Chrome only *1)      --chrome-extension <file>               : chrome extension file (multiple, Chrome only *1)      --proxy <proxy>                         : proxy host and port (HOST:PORT) (excepting IE)      --proxy-user <user>                     : proxy username (HtmlUnit only *2)      --proxy-password <password>             : proxy password (HtmlUnit only *2)      --no-proxy <hosts>                      : no-proxy hosts      --cli-args <arg>                        : add command line arguments at starting up driver (multiple)      --remote-url <url>                      : Remote test runner URL (Remote only)      --remote-platform <platform>            : Desired remote platform (Remote only)      --remote-browser <browser>              : Desired remote browser (Remote only)      --remote-version <browser-version>      : Desired remote browser version (Remote only)      --highlight (-H)                        : highlight locator always.      --interactive (-i)                      : interactive mode.      --screenshot-dir (-s) <dir>             : override captureEntirePageScreenshot directory.      --screenshot-all (-S) <dir>             : take screenshot at all commands to specified directory.      --screenshot-on-fail <dir>              : take screenshot on fail commands to specified directory.      --ignore-screenshot-command             : ignore captureEntirePageScreenshot command.      --baseurl (-b) <baseURL>                : override base URL set in selenese.      --firefox <path>                        : path to 'firefox' binary. (implies '--driver firefox')      --geckodriver <path>                    : path to 'geckodriver' binary. (implies '--driver firefox')      --chromedriver <path>                   : path to 'chromedriver' binary. (implies '--driver chrome')      --iedriver <path>                       : path to 'IEDriverServer' binary. (implies '--driver ie')      --phantomjs <path>                      : path to 'phantomjs' binary. (implies '--driver phantomjs')      --xml-result <dir>                      : output XML JUnit results to specified directory.      --html-result <dir>                     : output HTML results to specified directory.      --timeout (-t) <timeout>                : set timeout (ms) for waiting. (default: 30000 ms)      --set-speed <speed>                     : same as executing setSpeed(ms) command first.      --height <height>                       : set initial height. (excluding mobile)      --width <width>                         : set initial width. (excluding mobile)      --define (-D) <key>[:<type>][+]=<value> : define parameters for capabilities. <type> is a value type: str (default), int or bool (multiple)      --var (-V) <var-name>=<json-value>      : set JSON value to variable with a specified name. (multiple)      --rollup <file>                         : define rollup rule by JavaScript. (multiple)      --cookie-filter <+RE|-RE>               : filter cookies to log by RE matching the name. ("+" is passing, "-" is ignoring)      --command-factory <FQCN>                : register user defined command factory. (See Note *3)      --no-exit                               : don't call System.exit at end.      --strict-exit-code                      : return strict exit code, reflected by selenese command results at end. (See Note *4)      --max-time <max-time>                   : Maximum time in seconds that you allow the entire operation to take.      --help (-h)                             : show this message.          [Note]     *1 It is available if using "--driver remote --remote-browser firefox".          *2 If you want to use basic and/or proxy authentication on Firefox, then create new profile, install AutoAuth plugin, configure all settings, access test site with the profile, and specify the profile by --profile option.          *3 Use "java -cp ...:selenese-runner.jar Main --command-factory ...".        Because "java" command ignores all class path settings, when using "-jar" option.          *4 The list of strict exit code is follows:        - 0: SUCCESS        - 2: WARNING        - 3: FAILURE        - 4: ERROR        - 5: UNEXECUTED        - 6: MAX_TIME_EXCEEDED  Requirements ------------  * Java 8 or later. * Apache Maven 2.x or later to build.  Release Note ------------  The release note is moved to [RELEASENOTE.md](RELEASENOTE.md) file.  Building the Application ------------------------  * Install Apache Maven. * clone this repository * run build script 	`mvn -P package`  That will create the *selenese-runner.jar* file within the 'target' directory.  Options -------  ### Configuration file (1.8.0 or later)  You can read option information from the following configuration file by using "--config" option.  You can overwrite the information by additional command line options.      # configuration file format.          driver: DRIVER_NAME     profile: PROFILE_NAME     profile-dir: /PATH/TO/PROFILE/DIRECTORY     proxy: PROXY_HOST     proxy-user: PROXY_USER     proxy-password: PROXY_PASSWORD     no-proxy: NO_PROXY_HOSTS     cli-args: DRIVER_CLI_ARG1       DRIVER_CLI_ARG2       DRIVER_CLI_ARG3     remote-url: http://remote.example.com:4444/wd/hub     remote-platform: REMOTE_PLATFORM     remote-browser: REMOTE_BROWSER     remote-version: REMOTE_VERSION     # "highlight" parameter is "true" or "false".     highlight: true     screenshot-dir: /PATH/TO/SCREENSHOT/DIRECTORY     screenshot-all: /PATH/TO/SCREENSHOT/DIRECTORY/ALL     screenshot-on-fail: /PATH/TO/SCREENSHOT/DIRECTORY/ON/FAIL     # "ignore-screenshot-command" parameter is "true" or "false".     ignore-screenshot-command: true     baseurl: http://baseurl.example.com/     firefox: /PATH/TO/FIREFOX/BINARY     chromedriver: /PATH/TO/CHROMEDRIVER/BINARY     iedriver: /PATH/TO/IEDRIVER/BINARY     phantomjs: /PATH/TO/PHANTOMJS/BINARY     xml-result: /PATH/TO/XML/RESULT/DIRECTORY     html-result: /PATH/TO/HTML/RESULT/DIRECTORY     # The unit of "timeout" parameter is millisecounds.     timeout: 30000     # The unit of "set-speed" parameter is millisecounds.     set-speed: 100     # The unit of "height" parameter is pixcels.     height: 1024     # The unit of "width" parameter is pixcels.     width: 768     define: CAPABILITY_KEY1=CAPABILITY_VALUE1        CAPABILITY_KEY2=CAPABILITY_VALUE2        CAPABILITY_KEY3+=CAPABILITY_VALUE31        CAPABILITY_KEY3+=CAPABILITY_VALUE32        CAPABILITY_KEY3+=CAPABILITY_VALUE33     rollup: /PATH/TO/ROLLUP/FILE     cookie-filter: COOKIE_FILTER_REGEXP     command-factory: full.qualify.class.Name  ### Firefox, Chrome and PhantomJS driver  If you want to add command line options to above driver's binary, add following options:      java -jar selenese-runner.jar --driver DRIVER_NAME \       --cli-args ARG1 \       --cli-args ARG2 \       ...  Example:  * Firefox          java -jar selenese-runner.jar --driver firefox \           --cli-args -jsconsole \           ...  * Chrome          java -jar selenese-runner.jar --driver chrome \           --cli-args --incognito \           --cli-args --ignore-certificate-errors \           ...  * PhantomJS          java -jar selenese-runner.jar --driver phantomjs \           --cli-args --ssl-certificates-path=/PATH/TO/CERTS-DIR/ \           ...  ### Rollup  "--rollup" option and "rollup" command are used for a definition and execution of a user-defined command.  Refer to the following for how to write "rollup" script:  * Using the rollup feature of Selenium   http://sanjitmohanty.wordpress.com/2012/07/06/using-the-rollup-feature-of-selenium/ * Selenium Tutorial : Testing Strategies https://thenewcircle.com/static/bookshelf/selenium_tutorial/testing_strategies.html  However, this feature has the following limitations:  * supported properties of rollup rule: ** name ** args ** expandedCommands or getExpandedCommans * cannot access any browser object.  ### Cookie filter  You can filter cookies to log by the regular expression matching the name.  Example:  * logging the cookie whose name ends with "ID":          java -jar selenese-runner.jar --cookie-filter +'ID$' ...  * don't logging the cookie whose name contains "__utm":          java -jar selenese-runner.jar --cookie-filter -__utm ...  ### User defined command factory  You can register user defined command factory:      java -cp YOUR_CLASS_PATH:selenese-runner.jar Main \       --command-factory your.command.factory.ClassName ...  Note:  * Use the above command line instead of "java -jar ...". Because "java" command ignores all class path settings, when using "-jar" option.  * Top-level Main class is contained ONLY in stand-alone "selenese-runner.jar", and is not contained in "selenese-runner-java-X.Y.Z.jar" in maven repository. Please use "jp.vmi.selenium.selenese.Main" instead of "Main" if you want to use this feature with the jar in maven repository.  Original Commands -----------------  ### `include`  Usage: `include` FILENAME  This command include and execute FILENAME test-case.  You can use variables in FILENAME.  See [the test-case example](src/test/resources/selenese/testcase_include.html).  License -------  The Apache License, Version 2.0.  see "LICENSE" file.
sayems/java.webdriver	Selenium WebDriver =======  This repository contains WebDriver code examples, exercises and tutorials for developers. Over time, more and more test example will be uploaded here. All tests examples in this repository is to be considered public domain unless stated otherwise.    ## Demo website to practice WebDriver: Here is a list of websites where you can practice Selenium webdriver. You will find the list incredibly useful as these will cover many of your real-time web automation use case scenario. Some of the common examples includes are like testing of a login page, online registration forms, and automating flight booking.    - [The Internet by Elemental Selenium](http://the-internet.herokuapp.com/) - [jQuery UI Demos](http://jqueryui.com/demos/) - [PHPTRAVELS](http://phptravels.com/demo/) - [Mercury Tours](http://newtours.demoaut.com/) - [PHPTRAVELS](http://phptravels.com/demo/) - [Way2Automation](http://www.way2automation.com/demo.html) - [Automation Practice](http://automationpractice.com/index.php) - [DemoQA](http://demoqa.com/) - [OrangeHRM Enterprise](http://enterprise.demo.orangehrmlive.com/symfony/web/index.php/auth/login)  ## Examples with:  - Mouse hover - Drag & Drop - Draggable - Selectable - Sortable - Actionable - Radio Button - Checkbox - Datepicker  ## Pre-requisites  You'll need to install  * Java 8. * Maven. * Firefox.  ###Contributions  If you have any code examples you would like to contribute to this repository, please feel free to open a pull request.  ##Feedback  Contributors to this repo would be very grateful to receive feedback! If you would like to praise or comment on any test examples, or the repo as a whole, please do so in the issue tracker. I'd love to hear what you think, so please take a moment to let me know.   ##Contact  If you have any questions about this repo, or need some help to contribute, please do not hesitate to contact me.
xebia/Xebium	Xebium [![Build Status](https://travis-ci.org/xebia/Xebium.png?branch=master)](https://travis-ci.org/xebia/Xebium) ====== [Xebium](http://xebia.github.com/Xebium/) combines the powers of FitNesse and Selenium. Visit the [xebium home page](http://xebia.github.com/Xebium/) for more details and examples.  Features --------  * Full Selenium-IDE - FitNesse roundtrip with your web tests * Create data-driven tests. * Tests are executed using the modern WebDriver interfaces and Selenium Server. * Tests are run from FitNesse using the SLIM engine.  Getting Started ---------------  Execute the following command:  	$ mvn -Pfitnesse test  and open a browser, pointing at http://localhost:8000.  Click the Xebium link in order to get to the Xebium section and read on in the Getting Started page.  Have fun!  <hr/> PS. For those of you who import Xebium as an Eclipse project, run `mvn eclipse:eclipse` to get your classpath setup right.
conductor-framework/conductor	Conductor === [See the site](http://conductor.ddavison.io)  [![star](http://githubbadges.com/star.svg?user=conductor-framework&repo=conductor)](http://github.com/conductor-framework/conductor) [![fork](http://githubbadges.com/fork.svg?user=conductor-framework&repo=conductor)](http://github.com/conductor-framework/conductor/fork)  # Getting Started Using maven, include it as a dependency: ```xml <dependency>   <groupId>io.ddavison</groupId>   <artifactId>conductor</artifactId>   <version>3.0.1</version> </dependency> ```  Create a Java Class, and extend it from `io.ddavison.conductor.Locomotive`  ### Drivers Drivers should be put in the root of your project, and be named like this:  #### Mac chromedriver.mac  #### Windows chromedriver.exe  #### Linux chromedriver.linux  So as an example, your project structure could be: ``` Project | src |   main |     java |       TestClass.java | pom.xml | chromedriver.mac | chromedriver.exe | chromedriver.linux ```  Currently, six browsers are supported and they are Firefox, HTMLUnit, Chrome, Internet Explorer, Safari, and PhantomJS   # Goals The primary goals of this project are to... - Take advantage of method chaining, to create a fluent interface. - Abstract the programmer from bloated scripts resulting from using too many css selectors, and too much code. - Provide a quick and easy framework in Selenium 2 using Java, to get started writing scripts. - Provide a free to use framework for any starting enterprise, or individual programmer. - Utilize the power of CSS!  # Actions You can perform any action that you could possibly do, using the inline actions. - ```click(By)``` - ```setText(By, text)``` - ```getText(By)``` - ```hoverOver(By)``` - ```check(By)``` - ```uncheck(By)``` - ```navigateTo(url)``` - ```goBack()``` - ```isPresent(By)``` - ```getAttribute(By, attribute)``` - etc.  # In-line validations This is one of the most important features that I want to _*accentuate*_. - ```validateText``` - ```validateTextNot``` - ```validateChecked``` - ```validateUnchecked``` - ```validatePresent``` - ```validateNotPresent``` - ```validateTextPresent``` - ```validateTextNotPresent```  All of these methods are able to be called in-line, and fluently without ever having to break your tests.  # Switching Windows Another nice feature that is offered, is the simplicity of window switching in Selenium.  - ```switchToWindow(regex)``` - ```waitForWindow(regex)``` - ```closeWindow(regex)```  All of these functions take a regular expression argument, and match either the url or title of the window that you want to interact with.  # Switching Frames - ```switchToFrame(idOrName)``` - ```switchToDefaultContent()```  # Implicit Waiting In addition to the Selenium 2 implicit waiting, the ```AutomationTest``` class extends on this concept by implenting a sort of ```waitFor``` functionality which ensures that an object appears before interacting with it.  This rids of most ```ElementNotFound``` exceptions that Selenium will cough up.   [See a working example](https://github.com/ddavison/conductor/blob/master/src/test/java/io/ddavison/conductor/FrameworkTest.java) of what a test script written using this framework might look like.  # Pull requests If you have an idea for the framework, fork it and submit a pull-request!
jenkinsci/acceptance-test-harness	# Acceptance tests for Jenkins  End to end test suite for Jenkins automation server and its plugins.  The scenarios are described ain form of tests controlling Jenkins under test (JUT) through UI / REST API. Clean instance is started for individual tests to isolate the tests. The harness provides convenient docker support so integration tests can be written easily.  ## Getting Started  The simplest way to start the harness is calling `BROWSER=firefox JENKINS_VERSION=latest mvn test`. Complete test suite takes hours to run due to the number of covered components/use-cases, the cost of Jenkins setup and selenium interactions. That can be avoided by selecting a subset of tests to be run - smoke tests for instance.  ## Further Reading  ### Running tests  The harness provides a variety of ways to configure the execution including:  * [Selecting web browser](docs/BROWSER.md) * [Specifying test(s) to run](docs/SINGLE-TEST.md) * [Managing the versions of Jenkins and plugins](docs/SUT-VERSIONS.md) * [Using a http proxy](docs/USING-A-HTTP-PROXY.md) * [Prelaunching Jenkins](docs/PRELAUNCH.md) * [Selecting how to launch Jenkins](docs/CONTROLLER.md) * [Obtaining a report of plugins that were exercised](docs/EXERCISEDPLUGINSREPORTER.md) * [Running tests in container](docs/DOCKER.md) * Selecting tests based on plugins they cover (TODO)  ### Writing tests  Given how long it takes for the suite to run, test authors are advised to focus on the most popular plugins and use-cases to maximize the value of the test suite. Tests that can or already are written as a part of core/plugin tests should be avoided here as well as tests unlikely to catch future regressions (reproducers for individual bugs, boundary condition testing, etc.). Individual maintainers are expected to update their tests reflecting core/plugin changes as well as ensuring the tests does not produce false positives. Tests identified to violate this guideline might be removed without author's notice for the sake of suite reliability.  Areas where acceptance-tests-harness is more suitable then jenkins-test-harness are:  - Installing plugins for cross-plugin integration - Running tests in realistic classloader environment - Verifying UI behaviour in actual web browser  * [Docker fixtures](docs/FIXTURES.md) * [Page objects](docs/PAGE-OBJECTS.md)     * [Mix-ins](docs/MIXIN.md) * [Guice is our glue](docs/GUICE.md) * Writing tests     * [Video tutorial](https://www.youtube.com/watch?v=ZHAiywgMG-M) by Kohsuke on how to write tests     * [Writing JUnit test](docs/JUNIT.md) * [Testing slaves](docs/SLAVE.md) * [Testing emails](docs/EMAIL.md) * [Hamcrest matchers](docs/MATCHERS.md) * [How to use this from your own module](docs/EXTERNAL.md) * [EC2 provider configuration](docs/EC2-CONFIG.md) * [Investigation](docs/INVESTIGATION.md)
arquillian/arquillian-graphene	= Arquillian Graphene 2 :asciidoctor-source: https://raw.githubusercontent.com/arquillian/arquillian-graphene/master/docs :numbered: :sectlink: :sectanchors: :sectid: :source-language: java :source-highlighter: coderay :sectnums: :icons: font :toc: left  image:https://travis-ci.org/arquillian/arquillian-graphene.svg?branch=master["Build Status", link="https://travis-ci.org/arquillian/arquillian-graphene"]  ifndef::generated-doc[] To read complete documentation visit http://arquillian.org/arquillian-graphene/ endif::generated-doc[]  *'`Robust Functional Tests leveraging WebDriver with flavour of neat AJAX-ready API`'*  ifdef::generated-doc[] include::{asciidoctor-source}/graphene-introduction.adoc[] include::{asciidoctor-source}/getting-started.adoc[] include::{asciidoctor-source}/graphene-configuration.adoc[] include::{asciidoctor-source}/graphene-utility-class.adoc[] include::{asciidoctor-source}/waiting-api.adoc[Waiting API] include::{asciidoctor-source}/request-guards.adoc[] include::{asciidoctor-source}/page-abstractions.adoc[] include::{asciidoctor-source}/location-strategies.adoc[] include::{asciidoctor-source}/dependency-injection.adoc[] include::{asciidoctor-source}/parallel-browsers.adoc[] include::{asciidoctor-source}/javascript-interface.adoc[] include::{asciidoctor-source}/tips-and-tricks.adoc[] include::{asciidoctor-source}/advanced-techniques.adoc[] endif::generated-doc[]  [[project-info]] == Project Info  [cols="1,1"] |=== |Type: |Maven  |Source Code: |https://github.com/arquillian/arquillian-graphene  |Issue tracking: |https://issues.jboss.org/browse/ARQGRA  |Forums: |http://community.jboss.org/en/arquillian?view=discussions  |License: |LGPL v2.1 or ASL v2.0 (dual-licensed) |===  [[getting-started]] === Getting Started Guide  Apart from the documentation available here, there is also a comprehensive guide http://arquillian.org/guides/functional_testing_using_graphene/[Functional Testing using Drone and Graphene] to help you leverage the benefits of Graphene for writing robust functional tests.  [[contributing]] == Contributing  Community contributions are essential for maintaining the vitality of the Arquillian project.  Contributing to the project helps to deliver functionality you need and allows you to share your code with other contributors and users.  We want to keep it as easy as possible to contribute your changes and we will work hard to deliver your contributions in an upcoming release.  Please refer to https://github.com/arquillian/arquillian-graphene/blob/master/CONTRIBUTING.md[How to Contribute] to find related instructions.  [[building-the-project]] == Building the Project  [[prerequisities]] === Prerequisites  * JDK 1.6+ * Maven 3.0.3+  [[how-to-build-project]] === How-to build Project  .... mvn clean install ....  [[running-integration-tests]] === Running Integration Tests  .... mvn clean install  cd ftest/ mvn clean verify -Dbrowser=firefox mvn clean verify -Dbrowser=chrome mvn clean verify -Dbrowser=phantomjs ....
tarun3kumar/seleniumtestsframework	# SeleniumTestsFramework  [![Join the chat at https://gitter.im/tarun3kumar/seleniumtestsframework](https://badges.gitter.im/tarun3kumar/seleniumtestsframework.svg)](https://gitter.im/tarun3kumar/seleniumtestsframework?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Javadocs](http://www.javadoc.io/badge/com.seleniumtests/seleniumtestsframework.svg)](http://www.javadoc.io/doc/com.seleniumtests/seleniumtestsframework) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.seleniumtests/seleniumtestsframework/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.seleniumtests/seleniumtestsframework)  ## Introduction Selenium Tests Framework (referred as STF from here on) is a test automation framework for automated testing of Desktop web, mobile site and mobile apps. STF is based on WebDriver, Appium, TestNG and Maven.   * [STF Features](http://www.seleniumtests.com/2013/10/announcing-selenium-tests-automation.html)    * [Awesome STF Reporting](http://www.seleniumtests.com/2013/12/stf-test-report-snapshots.html)  * [5 minutes guide](http://www.seleniumtests.com/2013/12/5-minutes-guides-to-using-selenium.html)  * [Installing STF](http://www.seleniumtests.com/2013/10/installing-selenium-tests-framework.html)  * [Parameters used in STF](http://www.seleniumtests.com/2014/04/parameters-used-in-selenium-tests-framework.html)  * [How to setup your own cutom driver](http://www.seleniumtests.com/2013/01/set-up-driver-in-selenium-tests.html)  * [STF and mobile device automation](http://www.seleniumtests.com/2015/07/stf-and-android-test-automation.html)  ## Is there a test project which uses STF? If you are looking for a sample project which uses STF then you should clone [seleniumtests](https://github.com/TestingForum/seleniumtests) project and not this project. [seleniumtests](https://github.com/TestingForum/seleniumtests) uses STF and demonstrates its features.  ## I have a question Have a question or feature request? post it in [Testing Forum](http://www.seleniumtests.com/p/testing-forum.html) :-)
dgageot/simplelenium	# Simplelenium  A simple and robust layer on top of [Selenium](http://docs.seleniumhq.org/projects/webdriver/) and [PhantomJS](http://phantomjs.org/).  It also supports Chrome and Firefox. SauceLabs support is ongoing.  ## Goal  Testing web pages with Selenium can prove difficult. I've seen a lot of projects with an unstable build because of Selenium. To be fair, it's more because of the way Selenium is used. Although experience showed me that using Selenium properly is harder that one might think.  In fact I think that proper usage of Selenium must be left out of tester hands and baked into a small, effective library. Simplelenium is my attempt to do so and it served me well.  Simplelenium deals properly and out-of-the-box with timing issues and `StaleElementReferenceExceptions`. It supports running tests in parallel without you thinking about it. It doesn't open annoying windows since it's default behaviour is to use [PhantomJS](http://phantomjs.org/), a headless browser.  Give it a try and you'll be surprised at how Selenium testing can be fun again (was it ever?).  ## Setup (Maven)  Add Simplelenium as a maven test dependency to your project and you are all set to go. **Simplelenium requires java 8**.  ```xml <dependency>   <groupId>net.code-story</groupId>   <artifactId>simplelenium</artifactId>   <version>2.2</version>   <scope>test</scope> </dependency> ```  The first time you run a test, it will download [PhantomJS](http://phantomjs.org/) automatically for you so that nothing has to be installed on the machine. `mvn clean install` is all one should need!  ## Build status  [![Build Status](https://api.travis-ci.org/dgageot/simplelenium.png)](https://travis-ci.org/dgageot/simplelenium)  ## Quick Start  ```java import net.codestory.simplelenium.SeleniumTest; import org.junit.Test;  public class QuickStartTest extends SeleniumTest {   @Test   public void web_driver_site() {     goTo("http://docs.seleniumhq.org/projects/webdriver/");      find("#q").fill("StaleElementReferenceExceptions");     find("#submit").click();      find("a.gs-title")       .should()       .haveMoreItemsThan(5)       .contain("Issue 1887 - selenium - Element not found in the cache")       .not().contain("Selenium Rocks!");   } } ```  Notice the fluent api that doesn't rely on static imports. This will make your life easier.  Lots of finders, actions and verifications are supported. Notice that no timing information is provided. The default settings should be ok the vast majority of times.  ## Finders  Finding elements start with either a [find("cssSelector")](src/main/java/net/codestory/simplelenium/DomElementFinder.java) or a [find(org.openqa.selenium.By)](src/main/java/net/codestory/simplelenium/DomElementFinder.java) . There's no other choice. That's simple. You can use the full power of cssSelector, which should be enough most of the time, or use standard Selenium `org.openqa.selenium.By` sub-classes.  Searching is not done until a verification is made on the elements. Simplelenium is both lazy and tolerant to slow pages and ongoing refreshes. You don't have to worry about it. Just write what the page should look like and if it happens within a sound period of time, the next verification is made.  We'll dig into more details in the last section.  ## Verifications  The most simple verification is to check that elements are found:  ```java find(".name").should().exist(); ```  Of course more complex verifications are supported:  ```java find(".name").should().contain("a word", "anything"); find(".name").should().match(Pattern.compile("regexp")); find(".name").should().beEmpty(); find(".name").should().beEnabled(); find(".name").should().beDisplayed(); find(".name").should().beSelected(); find(".name").should().haveMoreItemsThan(min); find(".name").should().haveSize(10); find(".name").should().haveLessItemsThan(max); find(".name").should().haveDimension(width, height); find(".name").should().beAtLocation(x, y); ```  Verifications can be inverted:  ```java find(".name").should().not().contain("a word"); ```  Verifications can be chained:  ```java find(".name")   .should()   .contain("a word")   .contain("anything")   .beSelected()   .not().beDisplayed(); ```  The way Simplelenium deals with timing issue is basic, yet efficient:   + It tries to make the search  + Then the verification  + If it passes, then we're cool  + If not, it tries again immediately with a new search to avoid Staled elements  + It does so for at least 5 seconds  The "magic" comes from:   + Not searching until you need to check something  + Searching again if the check fails  + Doing a lot of retries as quickly as possible  + Using the fact that you tell what the page should look like and consider all    the failures as false negatives. That is until the maximum delay is reached.  Default timeout can be set using this syntax:  ```java find(".name").should().within(1, MINUTE).contain("a word"); ```  ## Narrowing search  Sometimes, searching elements is more difficult than using a simple css selector. Simplelenium supports narrowing searches with additional filters, like those:  ```java find("...").withText().beingEmpty().should()...; find("...").withText().containing("text").should()...; find("...").withName().startingWith("text").should()...; find("...").withId().equalTo("text").should()...; find("...").with("name").matching(Pattern.compile(".*value")).should()...; find("...").withTagName().equalTo("h1").should()...; find("...").withClass().containingWord("blue").should()...; find("...").withCss("color").not().endingWith("grey").should()...; find("...").withText().startsWith("Prefix").endingWith("Suffix").should()...; ... ```  Also multiple results can be filtered out this way:  ```java find("...").first(); find("...").second(); find("...").third(); find("...").fourth(); find("...").nth(5); find("...").limit(10); find("...").skip(3); find("...").skip(5).limit(20); find("...").last(); ```  ## Actions  Often, you have to interact with the page, not just make verifications. Simplelenium supports a lot of actions. Here are some of them:  ```java find("...").fill("name"); find("...").submit(); find("...").click(); find("...").click(x, y); find("...").pressReturn(); find("...").sendKeys("A", "B", "C"); find("...").clear(); find("...").doubleClick(); find("...").clickAndHold(); find("...").contextClick(); find("...").release();  find("...").select("text"); find("...").deselect(); find("...").deselectByValue("value"); find("...").deselectByVisibleText("text"); find("...").deselectByIndex(index); find("...").selectByIndex(index); find("...").selectByValue("value"); ```  If that's not enough, three generic methods give you access to the Selenium Api underneath but in a managed fashion:  To do anything with the underlying `WebElement`:  ```java find("...").execute(Consumer<? super WebElement> action); ```  To execute `actions` on the element:  ```java find("...").executeActions(String description, BiConsumer<WebElement, Actions> actionsOnElement); ```  To execute `selections` on the element:  ```java find("...").executeSelect(String description, Consumer<Select> selectOnElement); ```  Those three methods should hopefully not be used often but it's great to know that the full power of Selenium is there underneath.  ## Advanced topics  Let's say you are not impressed, what else can Simplelenium do to make writing tests easier?  ### Page Objects and Section Objects  Using Page Objects and Section Objects, one can encapsulate both the extraction of web elements and the verification, in a more domain oriented fashion. This also removes a lot of boilerplate code and decreases code duplication.  Let's take a look at a small example:  ```java import net.codestory.simplelenium.DomElement; import net.codestory.simplelenium.PageObject; import net.codestory.simplelenium.SeleniumTest; import org.junit.Test;  public class QuickStartTest extends SeleniumTest {   Home home;    @Override   protected String getDefaultBaseUrl() {     return "http://localhost:8080/base/";   }    @Test   public void check_page() {     goTo(home);      home.shouldDisplayHello();     home.shouldLinkToOtherPages();   }    static class Home implements PageObject {     DomElement title;     DomElement greeting;     DomElement links = find("a.sections");      @Override     public String url() {       return "/home";     }      void shouldDisplayHello() {       title.should().contain("Home page");       greeting.should().contain("Hello");     }      void shouldLinkToOtherPages() {       links.should().haveSize(5).and().contain("Section1", "Section5");     }   } } ```  How cool is that? All you have to do is implement `PageObject`. Page Objects are automatically injected into tests. So are `DomElement`s present as fields into Page Objects. By default elements a searched by name or id but one can use standard `find(...)` methods to override this behaviour. Same as usual.  If you make the additional effort to return `this` in Page Objects methods, you than have a nice fluent api.  ```java @Test public void check_page() {   home     .goTo()     .shouldDisplayHello()     .shouldLinkToOtherPages(); }  static class Home implements PageObject {   @Override   public String url() {     return "/home";   }     Home goTo() {     goTo(url());     return this;   }     Home shouldDisplayHello() {     ...     return this;   }     Home shouldLinkToOtherPages() {     ...     return this;   } } ```  Page Objects represent a Page with a url. For sections of pages, you can implement `SectionObject` instead. It makes it easy to split a page into multiple reusable parts that carry their own finders and verifications.  Sections are injected automatically into tests, page objects and other sections.  ### Running tests in parallel  Simplelenium is good at running tests in parallel. In fact without you doing anything on the code side, it should just work.  Simplelenium keeps a distinct WebDriver for each thread. You don't have to think about it. Let's say you configure surefire to run tests in parallel at class or method level. Easy! You don't have to copy this configuration, with a different syntax, into your test framework. It will just work.  Running tests in parallel with multiple JVMs also works well. We use a lock on the filesystem when we download PhantomJS. I told you, you don't have to think about it.  ### Tests without JUnit  Sometimes, running the tests with JUnit is not what you want. You'd like to do your own threading and own test lifecycle. You can then use the `FluentTest` class:  ```java import org.junit.Test;  import static java.util.stream.IntStream.range;  public class FluentTestTest {   @Test   public void parallel() {     String baseUrl = ...;      range(0, 20).parallel().forEach(index -> {       new FluentTest(baseUrl)         .goTo("/")         .find("h1").should().contain("Hello World").and().not().contain("Unknown")         .find("h2").should().contain("SubTitle")         .find(".age").should().contain("42")         .goTo("/list")         .find("li").should().contain("Bob").and().contain("Joe");     });   } } ``` How cool is that?   ### Run tests with Chrome or Firefox  Even if Simplelenium supports PhantomJs out of the box and by default, tests can be run on Chrome or Firefox.  Run tests with '-Dbrowser=chrome',  '-Dbrowser=firefox', '-Dbrowser=phantom_js' to choose which browser you want to use.  If you choose `chrome`, Simplelenium will download [chromedriver](https://code.google.com/p/selenium/wiki/ChromeDriver) automatically for you.  If you don't choose `phantomjs`, then you have to manually install Firefox or Chrome. Make sure you install them where FirefoxDriver and ChromeDriver expect them to be. If you used [homewbrew](http://brew.sh/) to install Chrome, like I do, ChromeDriver will figure this out.  If you need to set the port used by ChromeDriver, use the `chromedriver.port` system property. The default is to use a random free port.  ### Custom Download URL  If you can't access default download url from where you are (thank you corporate IT proxy). You can override them by providing the following System properties :  * `phantomjs.url` : url where to download phantomjs ie: https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-1.9.8-windows.zip  * `phantomjs.exe` : relative path where the executable is from the compressed archive. ie: phantomjs-1.9.8-windows/phantomjs.exe  ## What Simplelenium doesn't do  ### Reading properties from web elements  Sometimes, you want to read a property of a web element and use your own assertions framework to verify if it's ok. That's not how Simplelenium works. You should be able to expect what the element will look like and tell Simplelenium to check. Otherwise you might extract a value a bit too soon and there you are, back into timing hell, with false negative tests. You don't want that. Trust me.  Here's the Simplelenium way of doing this:  ```java find("...").should().match(element -> /* Test something on every element found /*); ```  ## Release  ```bash mvn release:clean release:prepare release:perform ```
Ardesco/selenium-standalone-server-plugin	Selenium driver-binary-downloader-maven-plugin =================================  [![Join the chat at https://gitter.im/Ardesco/selenium-standalone-server-plugin](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Ardesco/selenium-standalone-server-plugin?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build Status](https://travis-ci.org/Ardesco/selenium-standalone-server-plugin.svg?branch=master)](https://travis-ci.org/Ardesco/selenium-standalone-server-plugin) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.lazerycode.selenium/driver-binary-downloader-maven-plugin/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.lazerycode.selenium/driver-binary-downloader-maven-plugin) [![Javadoc](https://javadoc-emblem.rhcloud.com/doc/com.lazerycode.selenium/driver-binary-downloader-maven-plugin/badge.svg)](http://www.javadoc.io/doc/com.lazerycode.selenium/driver-binary-downloader-maven-plugin)   A Maven plugin that will download the WebDriver stand alone server binaries for use in your mavenised Selenium project.  What's changed?  See the [Changelog](https://github.com/Ardesco/selenium-standalone-server-plugin/blob/master/CHANGELOG.md).  **This plugin now requires Java 8!**  Default Usage -----      <plugins>         <plugin>             <groupId>com.lazerycode.selenium</groupId>             <artifactId>driver-binary-downloader-maven-plugin</artifactId>             <version>1.0.14</version>             <configuration>                 <!-- root directory that downloaded driver binaries will be stored in -->                 <rootStandaloneServerDirectory>/my/location/binaries</rootStandaloneServerDirectory>                 <!-- Where you want to store downloaded zip files -->                 <downloadedZipFileDirectory>/my/location/zips</downloadedZipFileDirectory>             </configuration>             <executions>                 <execution>                     <goals>                         <goal>selenium</goal>                     </goals>                 </execution>             </executions>         </plugin>     </plugins>  By default the plugin will download the most recent binary specified in the RepositoryMap.xml for every driver/os/bitrate. If you want to filter out the ones you don't need have a look at the advanced usage options below.  Advanced Usage -----      <build>         <plugins>             <plugin>                 <groupId>com.lazerycode.selenium</groupId>                 <artifactId>driver-binary-downloader-maven-plugin</artifactId>                 <version>1.0.14</version>                 <configuration>                     <!-- root directory that downloaded driver binaries will be stored in -->                     <rootStandaloneServerDirectory>/tmp/binaries</rootStandaloneServerDirectory>                     <!-- Where you want to store downloaded zip files -->                     <downloadedZipFileDirectory>/tmp/zips</downloadedZipFileDirectory>                     <!-- Location of a custom repository map -->                     <customRepositoryMap>/tmp/repo.xml</customRepositoryMap>                     <!-- This will ensure that the plugin only downloads binaries for the current OS, this will override anything specified in the <operatingSystems> configuration -->                     <onlyGetDriversForHostOperatingSystem>false</onlyGetDriversForHostOperatingSystem>                     <!-- Operating systems you want to download binaries for (Only valid options are: windows, linux, osx) -->                     <operatingSystems>                         <windows>true</windows>                         <linux>true</linux>                         <mac>true</mac>                     </operatingSystems>                     <!-- Download 32bit binaries -->                     <thirtyTwoBitBinaries>true</thirtyTwoBitBinaries>                     <!-- Download 64bit binaries -->                     <sixtyFourBitBinaries>true</sixtyFourBitBinaries>                     <!-- If set to false will download every version available (Other filters will be taken into account -->                     <onlyGetLatestVersions>false</onlyGetLatestVersions>                     <!-- Provide a list of drivers and binary versions to download (this is a map so only one version can be specified per driver) -->                     <getSpecificExecutableVersions>                         <googlechrome>18</googlechrome>                     </getSpecificExecutableVersions>                     <!-- Throw an exception if any specified binary versions that the plugin tries to download do not exist -->                     <throwExceptionIfSpecifiedVersionIsNotFound>false</throwExceptionIfSpecifiedVersionIsNotFound>                     <!-- Number of times to attempt to download each file -->                     <fileDownloadRetryAttempts>2</fileDownloadRetryAttempts>                     <!-- Number of ms to wait before timing out when trying to connect to remote server to download file -->                     <fileDownloadConnectTimeout>20000</fileDownloadConnectTimeout>                     <!-- Number of ms to wait before timing out when trying to read file from remote server -->                     <fileDownloadReadTimeout>10000</fileDownloadReadTimeout>                     <!-- Overwrite any existing binaries that have been downloaded and extracted -->                     <overwriteFilesThatExist>true</overwriteFilesThatExist>                     <!-- Check file hashes of downloaded files.  Default: true -->                     <checkFileHashes>true</checkFileHashes>                     <!-- auto detect system proxy to use when downloading files -->                     <!-- To specify an explicit proxy set the environment variables http.proxyHost and http.proxyPort -->                     <useSystemProxy>true</useSystemProxy>                 </configuration>                 <executions>                     <execution>                         <goals>                             <goal>selenium</goal>                         </goals>                     </execution>                 </executions>             </plugin>         </plugins>     </build>  Custom RepositoryMap.xml -----  You __should__ supply your own RepositoryMap.xml file, if you don't supply one a default one (which will most likely be out of date) will be used instead.  Your RepositoryMap.xml must match the schema available at [Here](https://github.com/Ardesco/selenium-standalone-server-plugin/blob/master/src/main/resources/RepositoryMap.xsd).  How do I get the SHA1/MD5 hases for the binaries I ant to download?  The people providing the binaries should be publishing MD5/SHA1 hashes as well so that you can check that the file you have downloaded is not corrupt.  It seems that recently this does not seem to be happening as a matter of course.  If you can't find a published SHA1/MD5 hash you can always download the file yourself, check that it is not corrupt and then generate the hash using the following commands:  On a *nix system it's pretty easy. perform the following command:      openssl md5 <filename>     openssl sha1 <filename>  On windows you can do the following (according to https://support.microsoft.com/en-us/kb/889768):      FCIV -sha1 <filename>     FCIV -md5 <filename>  ___Below is an example RepositoryMap.xml that I will endeavour to keep up to date so that you can just copy/paste the contents into your own file.___      <?xml version="1.0" encoding="utf-8" standalone="yes"?>     <root>         <windows>             <driver id="internetexplorer">                 <version id="3.4.0">                     <bitrate sixtyfourbit="true">                         <filelocation>http://selenium-release.storage.googleapis.com/3.4/IEDriverServer_x64_3.4.0.zip</filelocation>                         <hash>3e79d5cf2acdd91d0ddab9f2044ce51057ff77fe</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                     <bitrate thirtytwobit="true">                         <filelocation>http://selenium-release.storage.googleapis.com/3.4/IEDriverServer_Win32_3.4.0.zip</filelocation>                         <hash>8657fb63344f6e26198b860bc089a7a651e43441</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="edge">                 <version id="4.15063">                     <bitrate sixtyfourbit="true" thirtytwobit="true">                         <filelocation>https://download.microsoft.com/download/3/4/2/342316D7-EBE0-4F10-ABA2-AE8E0CDF36DD/MicrosoftWebDriver.exe</filelocation>                         <hash>31e80ae4fd88ed3097a1c4d3fe763539ee88e560</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="googlechrome">                 <version id="2.29">                     <bitrate thirtytwobit="true" sixtyfourbit="true">                         <filelocation>https://chromedriver.storage.googleapis.com/2.29/chromedriver_win32.zip</filelocation>                         <hash>2f02f28d3ff1b8f2a63cb3bc32c26ade60ac4737</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="operachromium">                 <version id="0.2.2">                     <bitrate sixtyfourbit="true">                         <filelocation>https://github.com/operasoftware/operachromiumdriver/releases/download/v0.2.2/operadriver_win64.zip</filelocation>                         <hash>8b84d334ca6dc5e30c168d8df080c1827e4c6fdb</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                     <bitrate thirtytwobit="true">                         <filelocation>https://github.com/operasoftware/operachromiumdriver/releases/download/v0.2.2/operadriver_win32.zip</filelocation>                         <hash>daa9ba52eeca5ea3cb8a9020b85642ec047e9890</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="phantomjs">                 <version id="2.1.1">                     <bitrate thirtytwobit="true" sixtyfourbit="true">                         <filelocation>https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-windows.zip</filelocation>                         <hash>eb61e6dc49832a3d60f708a92fa7299c57cad7db</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>                 <version id="1.9.8">                     <bitrate thirtytwobit="true" sixtyfourbit="true">                         <filelocation>https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-1.9.8-windows.zip</filelocation>                         <hash>4531bd64df101a689ac7ac7f3e11bb7e77af8eff</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="marionette">                 <version id="0.16.1">                     <bitrate sixtyfourbit="true">                         <filelocation>https://github.com/mozilla/geckodriver/releases/download/v0.16.1/geckodriver-v0.16.1-win64.zip</filelocation>                         <hash>f49aa6de084ce82b546868992b646e57e3bf3de8</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                     <bitrate thirtytwobit="true">                         <filelocation>https://github.com/mozilla/geckodriver/releases/download/v0.16.1/geckodriver-v0.16.1-win32.zip</filelocation>                         <hash>37b75297a9c316226bf3814d9908b044e21b43a3</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>         </windows>         <linux>             <driver id="googlechrome">                 <version id="2.29">                     <bitrate sixtyfourbit="true">                         <filelocation>https://chromedriver.storage.googleapis.com/2.29/chromedriver_linux64.zip</filelocation>                         <hash>025a098cde0a6ad8aef53d6734979c9845bf49b5</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                     <bitrate thirtytwobit="true">                         <filelocation>https://chromedriver.storage.googleapis.com/2.29/chromedriver_linux32.zip</filelocation>                         <hash>36d4082a6fb3b3cbb31b013a08b1900baf13743d</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="operachromium">                 <version id="0.2.2">                     <bitrate thirtytwobit="true">                         <filelocation>https://github.com/operasoftware/operachromiumdriver/releases/download/v0.2.2/operadriver_linux32.zip</filelocation>                         <hash>8b0b92a870a1b4ba619eb0b85ec587caa2942e5b</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                     <bitrate sixtyfourbit="true">                         <filelocation>https://github.com/operasoftware/operachromiumdriver/releases/download/v0.2.2/operadriver_linux64.zip</filelocation>                         <hash>c207c6916e20ecbbc7157e3bdeb4737f14f15fe3</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="phantomjs">                 <version id="2.1.1">                     <bitrate sixtyfourbit="true">                         <filelocation>https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2</filelocation>                         <hash>f8afc8a24eec34c2badccc93812879a3d6f2caf3</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                     <bitrate thirtytwobit="true">                         <filelocation>https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-i686.tar.bz2</filelocation>                         <hash>9870663f5c2826501508972b8a201d9210d27b59</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>                 <version id="1.9.8">                     <bitrate sixtyfourbit="true">                         <filelocation>https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-1.9.8-linux-x86_64.tar.bz2</filelocation>                         <hash>d29487b2701bcbe3c0a52bc176247ceda4d09d2d</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                     <bitrate thirtytwobit="true">                         <filelocation>https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-1.9.8-linux-i686.tar.bz2</filelocation>                         <hash>efac5ae5b84a4b2b3fa845e8390fca39e6e637f2</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="marionette">                 <version id="0.16.1">                     <bitrate sixtyfourbit="true">                         <filelocation>https://github.com/mozilla/geckodriver/releases/download/v0.16.1/geckodriver-v0.16.1-linux64.tar.gz</filelocation>                         <hash>95276285b65987b6e94e57be55fdc60b95423016</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                     <bitrate arm="true">                         <filelocation>https://github.com/mozilla/geckodriver/releases/download/v0.16.1/geckodriver-v0.16.1-arm7hf.tar.gz</filelocation>                         <hash>6f013de184c8f3dba3ec39ea49de2a166cb18efb</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>         </linux>         <osx>             <driver id="googlechrome">                 <version id="2.29">                     <bitrate sixtyfourbit="true">                         <filelocation>https://chromedriver.storage.googleapis.com/2.29/chromedriver_mac64.zip</filelocation>                         <hash>cec18df4ef736d6712593faf91b462352217214a</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="operachromium">                 <version id="0.2.2">                     <bitrate sixtyfourbit="true">                         <filelocation>https://github.com/operasoftware/operachromiumdriver/releases/download/v0.2.2/operadriver_mac64.zip</filelocation>                         <hash>d58a3b676dda7ede5c38e5df218e7c619495c4ed</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="phantomjs">                 <version id="2.1.1">                     <bitrate thirtytwobit="true" sixtyfourbit="true">                         <filelocation>https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-macosx.zip</filelocation>                         <hash>c6e1a16bb9e89ce1e392a4768e99177797c93350</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>                 <version id="1.9.8">                     <bitrate thirtytwobit="true" sixtyfourbit="true">                         <filelocation>https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-1.9.8-macosx.zip</filelocation>                         <hash>d70bbefd857f21104c5961b9dd081781cb4d999a</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>             <driver id="marionette">                 <version id="0.15.0">                     <bitrate thirtytwobit="true" sixtyfourbit="true">                         <filelocation>https://github.com/mozilla/geckodriver/releases/download/v0.16.1/geckodriver-v0.16.1-macos.tar.gz</filelocation>                         <hash>f687a69521b22640080ea87920b358c7498bdf31</hash>                         <hashtype>sha1</hashtype>                     </bitrate>                 </version>             </driver>         </osx>     </root>
webdriverextensions/webdriverextensions	[![Travis Build Status](https://travis-ci.org/webdriverextensions/webdriverextensions.svg?branch=master)](https://travis-ci.org/webdriverextensions/webdriverextensions) [![Maven Central](https://img.shields.io/maven-central/v/com.github.webdriverextensions/webdriverextensions.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3Acom.github.webdriverextensions)  WebDriver Extensions ===================  WebDriver Extensions is designed to simplify Java based Selenium/WebDriver tests. It's built on top of Selenium/WebDriver to make your tests more readable, reusabable and maintainable by combining the [Page Object Pattern](https://code.google.com/p/selenium/wiki/PageObjects) and [Bot Pattern](https://code.google.com/p/selenium/wiki/BotStyleTests).  Available through the [Maven Central Repository](http://mvnrepository.com/search?q=webdriverextensions)! Latest release is version 3.6.1 which includes selenium-java 3.5.3 as a transitive dependency.   <br>  ### What's included in this framework? - A [Maven Plugin](https://github.com/webdriverextensions/webdriverextensions-maven-plugin#webdriver-extensions-maven-plugin) to manage, download and install drivers - [Annotation based JUnit Runner](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/WebDriverRunner.html) for running Selenium/WebDriver tests locally or remotely against multiple browsers - New classes for modelling your website e.g. [WebComponent](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebComponent.html) (an extendable WebElement), [WebPage](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html), [WebSite](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebSite.html) and [WebRepository](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebRepository.html) - A [Bot](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/Bot.html) providing static methods for interacting, asserting and checking conditions of WebElements, WebComponents, WebPages and WebSites - A WebSite and WebRepository [generators](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/generator/package-summary.html) that enables adding WebComponents, WebPages, WebSites and WebRepositories by [annotations](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/generator/annotations/package-summary.html) - A [Maven Archetype](https://github.com/webdriverextensions/webdriverextensions-archetype-quickstart#webdriver-extension-archetype-quickstart) for creating new projects  <br>  ### Want to Contribute? If you find a bug or have a feature request please [create a new GitHub issue](https://github.com/webdriverextensions/webdriverextensions/issues/new) or even better clone this repository, commit your changes and make a [Pull Request](https://help.github.com/articles/using-pull-requests/).  <br>  ### Have any Questions? If you have question you can [ask them in a GitHub issue](https://github.com/webdriverextensions/webdriverextensions/issues/new).  <br>  # Content - [Hello World Example](#hello-world-example)     - [With WebDriver Extensions](#with-webdriver-extensions)     - [Without WebDriver Extensions](#without-webdriver-extensions)     - [Further increased readability with Groovy](#further-increased-readability-with-groovy) - [Getting Started](#getting-started)     - [Requirements](#requirements)     - [Use Maven to add WebDriver Extensions](#use-maven-to-add-webdriver-extensions)     - [Download and manage your drivers with the Maven Plugin](#download-and-manage-your-drivers-with-the-maven-plugin)     - [Speed up your tests by running them in parallel](#speed-up-your-tests-by-running-them-in-parallel)     - [Cross Browser test your website with the JUnitRunner](#cross-browser-test-your-website-with-the-junitrunner)     - [Model your website with the Page Object Pattern](#model-your-website-with-the-page-object-pattern)     - [Model your page components with the WebComponent](#model-your-page-components-with-the-webcomponent)     - [Make your test readable as instructions with the Bot Pattern](#make-your-test-readable-as-instructions-with-the-bot-pattern)     - [Create new projects with the Maven Archetype](#create-new-projects-with-the-maven-archetype) - [Javadoc](#javadoc) - [Changelog](#changelog) - [Contributors](#contributors) - [License](#license)    <br>  # Hello World Example Here is an example of how a cross browser test looks like with and without the WebDriver Extensions Framework. The test will run on Firefox, Chrome and Internet Explorer. It will google for "Hello World" and assert that the search result contains the searched text "Hello World".    ### With WebDriver Extensions ```java @RunWith(WebDriverRunner.class) @Firefox @Chrome @InternetExplorer public class WebDriverExtensionsExampleTest {      // Model     @FindBy(name = "q")     WebElement queryInput;     @FindBy(name = "btnG")     WebElement searchButton;     @FindBy(id = "search")     WebElement searchResult;      @Test     public void searchGoogleForHelloWorldTest() {         open("http://www.google.com");         assertCurrentUrlContains("google");          type("Hello World", queryInput);         click(searchButton);          waitFor(3, SECONDS);         assertTextContains("Hello World", searchResult);     } } ``` _<sub>Imports are hidden for the sake of simplicity, for imports and instructions on how to run this example see this [gist](https://gist.github.com/andidev/ad006a454edfd9f0e9e5)</sub>_    <br>  ### Without WebDriver Extensions ```java @RunWith(Parameterized.class) public class WebDriverExampleTest {     WebDriver driver;     @Parameters     public static Collection<Object[]> data() {         return Arrays.asList(new Object[][]{             {"Firefox"}, {"Chrome"}, {"InternetExplorer"}         });     }      public WebDriverTest(String browserName) {         if (browserName.equals("Firefox")) {             driver = new FirefoxDriver();         } else if (browserName.equals("Chrome")) {             driver = new ChromeDriver();         } else if (browserName.equals("InternetExplorer")) {             driver = new InternetExplorerDriver();         }         PageFactory.initElements(driver, this);     }      @After     public void tearDown() {         driver.quit();     }      // Model     @FindBy(name = "q")     WebElement queryInput;     @FindBy(name = "btnG")     WebElement searchButton;     @FindBy(id = "search")     WebElement searchResult;      @Test     public void searchGoogleForHelloWorldTest() throws InterruptedException {         driver.get("http://www.google.com");         assert driver.getCurrentUrl().contains("google");          queryInput.sendKeys("Hello World");         searchButton.click();          SECONDS.sleep(3);         assert searchResult.getText().contains("Hello World");     } } ``` _<sub>Imports are hidden for the sake of simplicity, for imports and instructions on how to run this example see this [gist](https://gist.github.com/andidev/6c5dc8033c019e4c069d)</sub>_   As you can see WebDriver Extensions Framework made the test almost readable as instructions you would give to someone who needs to manually perform this test. This is one of the main points of this framework. It also removed a lot of verbose boilerplate configuration code.  For the sake of simplicity this example does not demonstrate the [Page Object Pattern](https://code.google.com/p/selenium/wiki/PageObjects). Please keep on reading the [Getting Started](#getting-started) section to read more about how to create and use Page Objects.    <br>  ### Further increased readability with Groovy If wanted one could further increase readability by using the Groovy language instead of Java. Then the Hello World example would look like this  ```groovy @Grab(group='com.github.webdriverextensions', module='webdriverextensions', version='3.6.1') @RunWith(WebDriverRunner) @Firefox @Chrome @InternetExplorer class WebDriverExtensionsGroovyExampleTest {      // Model     @FindBy(name = "q")     WebElement queryInput;     @FindBy(name = "btnG")     WebElement searchButton;     @FindBy(id = "search")     WebElement searchResult;      @Test     void searchGoogleForHelloWorldTest() {         open "http://www.google.com"         assertCurrentUrlContains "google"          type "Hello World", queryInput         click searchButton          waitFor 3, SECONDS         assertTextContains "Hello World", searchResult     } } ```  _<sub>Imports are hidden for the sake of simplicity, for imports and instructions on how to run this example see this [gist](https://gist.github.com/andidev/b182c59a92d5ad66b035)</sub>_  Note that Groovy examples will not be covered by this document.    <br>  # Getting Started  ### Requirements The Selenium project is compiled with Java 8 since version 3.0.0. Therefore WebDriver Extensions also requires you to use Java 3 in version 3.0.0 and above. Maven is not a requirement but is preferred and referred to in this document.  - Java 8 or above - Maven 3 or higher    <br>  ### Use Maven to add WebDriver Extensions Add ```xml <dependency> 	<groupId>com.github.webdriverextensions</groupId> 	<artifactId>webdriverextensions</artifactId> 	<version>3.6.1</version> </dependency> ``` ...as a dependency in your [pom.xml](https://gist.github.com/andidev/ad006a454edfd9f0e9e5#file-pom-xml) file.   <br>  ### Download and manage your drivers with the [Maven Plugin](https://github.com/webdriverextensions/webdriverextensions-maven-plugin#webdriver-extensions-maven-plugin) There is no need to download any drivers manually. Instead use the [WebDriver Extensions Maven Plugin GitHub](https://github.com/webdriverextensions/webdriverextensions-maven-plugin) to download and manage your drivers by adding ```xml <plugin>     <groupId>com.github.webdriverextensions</groupId>     <artifactId>webdriverextensions-maven-plugin</artifactId>     <version>3.1.1</version>     <executions>         <execution>             <goals>                 <goal>install-drivers</goal>             </goals>         </execution>     </executions>     <configuration>         <drivers>             <driver>                 <name>edgedriver</name>                 <version>4.15063</version>             </driver>             <driver>                 <name>internetexplorerdriver</name>                 <version>3.4.0</version>             </driver>             <driver>                 <name>chromedriver</name>                 <version>2.29</version>             </driver>             <driver>                 <name>geckodriver</name>                 <version>0.16.0</version>             </driver>             <driver>                 <name>phantomjs</name>                 <version>2.1.1</version>             </driver>         </drivers>     </configuration> </plugin> ``` ...as a plugin in your [pom.xml](https://gist.github.com/andidev/ad006a454edfd9f0e9e5#file-pom-xml) file. Then simply just update the version tag of the driver when a new driver is available and re-run your tests with the `mvn test` command or your preferred IDE.  The plugin will download the most suitable driver for  your OS. The bit of the driver will be 32bit with the exception of running the tests from a linux 64bit OS. If you would like to specify the OS and bit of the drivers to download you can provide them with a `<platform>` and `<bit>`-tag inside each `<driver>`-tag. Platform can be set to `windows`, `mac` or `linux` while the bit can be set to `32` or `64`.  The drivers will placed in a folder called `drivers` in the project root. If you will use the provided [WebDriverRunner](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/WebDriverRunner.html) there is no need for passing driver paths as System Properties since the framework will take care of the for you. If you won't be using it make sure to point the drivers out manually.  If you have configured a proxy in the settings.xml file the first encountered active proxy will be used. To specify a specific proxy to use you can provide the proxy id in the configuration.  If you run your tests from eclipse make sure you've allowed the webdriverextensions-maven-plugin to run the install-drivers goal. You can do this by adding the following to your pom.xml ```xml <pluginManagement>     <plugins>         <!--Eclipse m2e settings needed to install drivers with the webdriverextensions-maven-plugin -->         <plugin>             <groupId>org.eclipse.m2e</groupId>             <artifactId>lifecycle-mapping</artifactId>             <version>1.0.0</version>             <configuration>                 <lifecycleMappingMetadata>                     <pluginExecutions>                         <pluginExecution>                             <pluginExecutionFilter>                                 <groupId>com.github.webdriverextensions</groupId>                                 <artifactId>webdriverextensions-maven-plugin</artifactId>                                 <versionRange>[1.0,)</versionRange>                                 <goals>                                     <goal>install-drivers</goal>                                 </goals>                             </pluginExecutionFilter>                             <action>                                 <execute>                                     <runOnIncremental>true</runOnIncremental>                                 </execute>                             </action>                         </pluginExecution>                     </pluginExecutions>                 </lifecycleMappingMetadata>             </configuration>         </plugin>     </plugins> </pluginManagement> ```  For more information on configuring the driver please visit the [WebDriver Extensions Maven Plugin GitHub page](https://github.com/webdriverextensions/webdriverextensions-maven-plugin). If the latest drivers are not available yet please create an issue [here](https://github.com/webdriverextensions/webdriverextensions-maven-plugin/issues/new).    <br>  ### Speed up your tests by running them in parallel Run your tests in parallel by adding ```xml <plugin>     <groupId>org.apache.maven.plugins</groupId>     <artifactId>maven-surefire-plugin</artifactId>     <version>2.18.1</version>     <configuration>         <parallel>all</parallel>         <threadCount>10</threadCount>         <perCoreThreadCount>false</perCoreThreadCount>     </configuration> </plugin> ``` ...to your pom.xml file.  This configuration will run maximum 10 tests in parallel. For more information about the configuration please see section [Fork Options and Parallel Test Execution](http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html) in the documentation of the [Maven Surefire Plugin](http://maven.apache.org/surefire/maven-surefire-plugin/index.html).  Try not to use non final static variables within your tests if you run your tests in parallel. If you really have to use static variables that are not defined as final make sure to wrap them in [InheritableThreadLocal](http://docs.oracle.com/javase/7/docs/api/java/lang/InheritableThreadLocal.html) objects. In this way they will be static within the current thread and child threads (i.e. the current test).  Also before configuring to run your tests in parallel check that your website allows it. For example problems could occur when logging in with the same user at the same time (if your website supports a login functionality). There could also be other reasons not to run tests in parallel.    <br>  ### Cross Browser test your website with the JUnitRunner  Run your tests locally by using the [WebDriverRunner](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/WebDriverRunner.html)  ```java import com.github.webdriverextensions.junitrunner.WebDriverRunner; import com.github.webdriverextensions.junitrunner.annotations.*;  @RunWith(WebDriverRunner.class) @Firefox @Chrome @InternetExplorer @Edge @PhantomJS public class CrossBrowserTest {      // Add WebElements, WebPages and other supported web models to use in tests      @Test     public void test1() {         // Configure browsers to test by annotating the class     }      @Test     @Safari     public void test2() {         // ...or by annotating methods     }      @Test     @IgnoreInternetExplorer     public void test3() {         // ...and use the ignore annotations to ignore specific browsers     }      ...  } ```  ...or remotely by adding the [@RemoteAddress](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/RemoteAddress.html) annotaion  ```java @RunWith(WebDriverRunner.class) @RemoteAddress("http://your-remote-url") @Firefox @Chrome @InternetExplorer @Edge @PhantomJS public class CrossBrowserTest { 	... } ```  To run your test headless without starting a browser, use the [@HtmlUnit](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/HtmlUnit.html) annotation. If wanted you can also run your tests against the Safari browser with the [@Safari](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/Safari.html) annotation (just make sure the chromedriver is installed). Note that there is currently a [WebDriver issue](https://code.google.com/p/selenium/issues/detail?id=7933) with running the SafariDriver on some OSX/Safari versions.  Browser `version` and `platform` settings can be passed as annotation parameters e.g. `@Firefox(version = "35.0", platform = Platform.MAC)`.  The desired capabilities can either be provided in JSON format as a string e.g. `@Chrome(desiredCapabilities = "{ chromeOptions: { args: [''--start-maximized'] }")` or by creating a new class that extends the WebDriver's `DesiredCapabilities` class ```java public class StartMaximized extends DesiredCapabilities {     public StartMaximized() {         ChromeOptions options = new ChromeOptions();         options.addArguments("--start-maximized");         setCapability(ChromeOptions.CAPABILITY, options);     } } ``` ...and passing that to the annotation e.g. `@Chrome(desiredCapabilitiesClass = StartMaximized.class)`.  If you want set a custom browser name this can be done by using the [@Browser](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/Browser.html) annotation e.g. `Browser(browserName = "foo")`.  For larger and more complex test grids the [@Browsers](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/Browsers.html) annotation can be used. For example to test the Firefox browser on Windows, Mac and Linux ```java @Browsers(firefox = {     @Firefox(platform = Platform.WINDOWS),     @Firefox(platform = Platform.MAC),     @Firefox(platform = Platform.LINUX) }) ```  If you would like to use a custom driver path annotate the test with the  [@DriverPaths](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/DriverPaths.html) annotation, e.g.  ```java @DriverPaths(chrome="path/to/chromedriver", internetExplorer ="path/to/internetexplorerdriver") ```  If you want to run your test against 64bit Internet Explorer versions you can specify the path to the 64 bit driver with the [@DriverPaths](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/DriverPaths.html) annotation like this ```java @DriverPaths(internetExplorer ="drivers/internetexplorerdriver-windows-64bit.exe") ``` another way to do it is to set the `webdriverextensions.ie.driver.use64Bit` to `true`, e.g. when running the tests with maven: `mvn test -Dwebdriverextensions.ie.driver.use64Bit=true`.  To take screenshots on test failure annotate the test class with the [@TakeScreenshotOnFailure](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/TakeScreenshotOnFailure.html). The screenshots will be saved into a directory named `screenshots` located in the project root. The path to the screenshots directory can be configured either by annotating the test class with the  [@ScreenshotsPath](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/ScreenshotsPath.html) annotation or by setting the `webdriverextensions.screenshotspath` property. E.g.  ```java @RunWith(WebDriverRunner.class) @Firefox @TakeScreenshotOnFailure @ScreenshotsPath("path/to/screenshots") public class SomeTest { 	... } ```  The implicitly wait for tests can be set by annotating test classes or methods with the [@ImplicitlyWait](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/annotations/ImplicitlyWait.html) annotation. E.g.  ```java @RunWith(WebDriverRunner.class) @Firefox @ImplicitlyWait(1) public class SomeTest {     @Test     public void somethingToTest() {         // Implicittly wait is set to one second     }     @Test     @ImplicitlyWait(value = 1, unit = MINUTES)     public void somethingElseToTest() {         // Implicittly wait is set to one minute     } } ```  To set other driver specific setting use the JUnit @Before annotation. The driver can be retreived by using the [driver()](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/Bot.html#driver--) method in the [Bot](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/Bot.html) class. E.g.  ```java @RunWith(WebDriverRunner.class) @Firefox public class SomeTest {     @Before     public void configure() {         driver().manage().timeouts().pageLoadTimeout(10, SECONDS);     }     ... } ```  <br>  ### Model your website with the [Page Object Pattern](https://code.google.com/p/selenium/wiki/PageObjects)  Model your website pages, e.g. a login page  ```html <html>     <head>         <title>Login Page</title>     </head>     <body>         <form>             <label>Username</label> <input name="username">             <label>Password</label> <input name="password">             <input type="checkbox" name="remember-me"> Remember me             <button id="login-button">Login</button>         </form>     </body> </html> ```  ...by extending the [WebPage](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html) class  ```java import com.github.webdriverextensions.WebPage;  public class LoginPage extends WebPage {      @FindBy(name = "username")     public WebElement usernameInput;     @FindBy(name = "password")     public WebElement passwordInput;     @FindBy(name = "remember-me")     public WebElement rememberMeCheckbox;     @FindBy(id = "login-buttom")     public WebElement loginButton;      @Override     public void open(Object... arguments) {         // Define how to open this page, e.g.         open("https://www.your-website-url.com/login");         assertIsOpen();     }      @Override     public void assertIsOpen(Object... arguments) {         // Define how to assert that this page is open, e.g.         assertTitleEquals("Login Page");         assertIsDisplayed(usernameInput);         assertIsDisplayed(passwordInput);         assertIsDisplayed(rememberMeCheckbox);         assertIsDisplayed(loginButton);     } } ``` ...and then add and use it in your tests  ```java @RunWith(WebDriverRunner.class) @Firefox public class LoginPageTest {      // Add models to inject into test     LoginPage loginPage;      @Test     public void loginTest() {         open(loginPage); // Calls the open method defined in LoginPage         type("foo", loginPage.username);         type("bar", loginPage.password);         click(loginButtom);         assertIsNotOpen(loginPage); // Calls the assertIsNotOpen method in the abstract WebPage class which inverts the assertIsOpen method defined in LoginPage     }      ...  } ```  Since the [WebPage](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html) class only implements a part of the the [Openable](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/internal/Openable.html) interface you have to implement the [open(Object... arguments)](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html#open-java.lang.Object...-) and [assertIsOpen(Object... arguments)](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html#assertIsOpen-java.lang.Object...-) methods yourself. As soon as this is done you can also call the [isOpen(Object... arguments)](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html#isOpen-java.lang.Object...-), [isNotOpen(Object... arguments)](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html#isNotOpen-java.lang.Object...-) and the [assertIsNotOpen(Object... arguments)](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html#assertIsNotOpen-java.lang.Object...-) methods inherited from the [WebPage](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html) class.  The [open(Object... arguments)](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html#open-java.lang.Object...-) and [assertIsOpen(Object... arguments)](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html#assertIsOpen-java.lang.Object...-) methods can take any number of arguments and therefore it is possible to pass entity ids or other required data needed to load the page. E.g. a page showing a specific order  ```java public class OrderPage {      @FindBy(id = "order-number")     public WebElement orderNumber;     ...      @Override     public void open(Object... arguments) {         int orderNumberToOpen = (int) arguments[0];         System.err.println("https://www.your-website-url.com/order?orderid=" + orderNumberToOpen);         assertIsOpen(orderNumberToOpen);     }      @Override     public void assertIsOpen(Object... arguments) {         int orderNumberToAssert = (int) arguments[0];         assertTextEquals(orderNumberToAssert, orderNumber);         ...     } } ```  ...and then use it in your test  ```java open(orderPage, 134523); // Calls the open method defined in OrderPage with the order number 134523 as an argument ```  There is also a [WebSite](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebSite.html) class which can be used if you would want to create a Site Object i.e. a model of the complete website. It is actually no difference between the [WebPage](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html) and the [WebSite](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebSite.html) class except the name.  An alternative to using the [WebPage](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html) class is using the [WebRepository](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebRepository.html) class. The only difference is that it does not implement the [Openable](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/internal/Openable.html) interface and therefore there is no need to override and implement the `open(Object... arguments)` and `assertIsOpen(Object... arguments)` methods.  Note that any class extending the [WebPage](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebPage.html), [WebSite](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebSite.html) or [WebRepository](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebRepository.html) class that are added as fields in the test will automatically be injected/instantiated if the [WebDriverRunner](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/WebDriverRunner.html) is used. If you won't run your tests with the [WebDriverRunner](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/WebDriverRunner.html) you can call the Selenium WebDriver `PageFactory.initElements` method and pass the [WebDriverExtensionFieldDecorator](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebDriverExtensionFieldDecorator.html) before running the test, e.g.  ```java PageFactory.initElements(new WebDriverExtensionFieldDecorator(yourDriver), this); ```    <br>  ### Model your page components with the [WebComponent](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebComponent.html)  Model repeating html content, e.g. table rows  ```html <table id="playlist">      <tr>           <td class="track">Hey Joe</td>           <td class="artist">Jimi Hendrix</td>           <td class="time">3:30</td>           <td class="album">Are You Experienced</td>      </tr>      <tr>           <td class="track">Play with Fire</td>           <td class="artist">The Rolling Stones</td>           <td class="time">2:14</td>           <td class="album">The Last time</td>      </tr>       ...  </table> ```  ...by extending the [WebComponent](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebComponent.html)  ```java import com.github.webdriverextensions.WebComponent;  public class PlaylistRow extends WebComponent {      @FindBy(className = "track")     public WebElement track;     @FindBy(className = "artist")     public WebElement artist;     @FindBy(className = "time")     public WebElement time;     @FindBy(className = "album")     public WebElement album; } ```  ...and then include it as you include a WebElement  ```java @FindBy(css = "#playlist tr") public List<PlaylistRow> playlist; ```  ...and then start using it  ```java assertTextEquals("Hey Joe", playlist.get(0).track); // Use WebElements in WebComponents click(playlist.get(0));                             // Use WebComponents as WebElements ```  Note that `@FindBy` annotation locators used inside a WebComponent have the WebComponent's html content as the search context. To locate html tags outside the WebComponent you could reset the search context by adding the [@ResetSearchContext](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/annotations/ResetSearchContext.html) annotation.  If you wish to delegate the method calls of a WebComponent to an underlying WebElement you can do so by annotating a WebElement inside the WebComponent with the [@Delegate](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/annotations/Delegate.html) annotation.  If you won't run your tests with the [WebDriverRunner](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/WebDriverRunner.html) you must call the Selenium WebDriver `PageFactory.initElements` method and pass the [WebDriverExtensionFieldDecorator](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebDriverExtensionFieldDecorator.html) before running the test, e.g. ```java PageFactory.initElements(new WebDriverExtensionFieldDecorator(yourDriver), this); ```   <br>  ### Make your test readable as instructions with the [Bot Pattern](https://code.google.com/p/selenium/wiki/BotStyleTests)  Simply import the static [Bot](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/Bot.html) where you want to use it  ```java import static com.github.webdriverextensions.Bot.*; ```  ...and start interacting with your web models  ```java open("https://www.your-website-url.com");   // Open urls type("testuser", usernameInput);            // Type into WebElements referencing text input tags type("ai78cGsT", passwordInput); uncheck(rememberMeCheckbox);                // Check and uncheck WebElements referencing checkbox input tags click(loginButton);                         // Click at WebElements  open(settingsPage)                          // Open WebPages selectOption("Swedish", languageSelectBox); // Select options in WebElements referencing select tags ```  ...and write your asserts  ```java assertIsOpen(homePage);                                        // Assert WebPages are open assertTextEquals("testuser", currentUser);                     // Assert text in WebElements equals assertTitleStartsWith("Wikipedia - ");                         // Assert title starts with assertCurrentUrlMatches("http://[a-z]{2,3}.wikipedia.org/.*"); // Assert current url matches regex assertHasClass("selected", homeTab);                           // Assert WebElement tags has class // ...type assert then bring up the list of all supported asserts with your IDE's autocompletion ```  ...and conditional statements  ```java if (hasClass("selected", homeTab)) { // Check if WebElement tags has class     // ...do something } if (browserIsInternetExplorer()) {   // Check if browser is Internet Explorer     // ...handle cross browser difference } ```  ...and wait for specific time and conditions  ```java waitFor(3, MINUTES);                                         // Wait for specific time waitForElementToDisplay(downloadCompletePopup, 30, SECONDS); // Wait for WebElements to display within specific time ```  ...and use the driver  ```java System.out.println(driver().getPageSource()); ```  ...and take screenshots  ```java takeScreenshots("screenshotfilename") // Save a screenshot to the screenshots directory in the project root ```  For a list of provided [Bot](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/Bot.html) methods take a look at the [javadoc for the Bot class](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/Bot.html) or use the autocompletion tool of your IDE (usally with Ctrl + Space and then start typing).  If you feel that some [Bot](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/Bot.html) methods are missing please describe them in a [new GitHub issue](https://github.com/webdriverextensions/webdriverextensions/issues/new) or even better clone this repository, commit the new methods and create a [Pull Request](https://help.github.com/articles/using-pull-requests/).  If you won't run your tests with the [WebDriverRunner](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/junitrunner/WebDriverRunner.html) make sure you set the driver in the [WebDriverExtensionsContext](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/WebDriverExtensionsContext.html) before using the [Bot](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/3.6.1/com/github/webdriverextensions/Bot.html)  ```java WebDriverExtensionsContext.setDriver(yourDriver); ```  There is now also a VaadinBot that can be used if testing an application using the [Vaadin Framework](https://vaadin.com/home)  <br>  ### Create new projects with the [Maven Archetype](https://github.com/webdriverextensions/webdriverextensions-archetype-quickstart#webdriver-extension-archetype-quickstart) Open your terminal and run ```sh mvn archetype:generate -DarchetypeGroupId=com.github.webdriverextensions -DarchetypeArtifactId=webdriverextensions-archetype-quickstart ```  ...and answer the questions to generate ``` projectname ├── drivers ├── pom.xml └── src     ├── main     │   └── java     │       └── com     │           └── companyname     │               ├── SiteNameSite.java     │               ├── SiteNameSiteTest.java     │               ├── component     │               │   └── ExampleWebComponent.java     │               └── page     │                   └── MainPage.java     └── test         ├── java         │   └── com         │       └── companyname         │           └── MainPageTest.java         └── resources             └── logback-test.xml ```  No need to add any drivers since the webdriverextensions-maven-plugin is configured to download them for you!  Simply just run the generated template test by executing ```sh cd projectname mvn test ```   <br>  # Javadoc The Javadoc of this project is available online hosted by javadoc.io. You can find the latest documentation over  [here](http://www.javadoc.io/doc/com.github.webdriverextensions/webdriverextensions). Please note that at the moment the documentation of the java classes and methods are limited (except for this documentation). I will try to add the Javadoc as soon as possible.   <br>  # Changelog #### 3.6.1 (2017 September 12) - SELENIUM UPDATE Updated selenium version to 3.5.3  #### 3.6.0 (2017 August 17) - SELENIUM UPDATE Updated selenium version to 3.5.1  #### 3.5.2 (2017 July 6) - ENHANCEMENT Make subclassing WebDriverRunner easier thanks to [@Snipx](https://github.com/Snipx) - BUGFIX Override getDescription() so that only filtered methods are added to the Description thanks to [@Snipx](https://github.com/Snipx)  #### 3.5.1 (2017 May 18) - BUGFIX Fixed Edge driver can't find file /drivers/edgedriver-windows-64bit issue  #### 3.5.0 (2017 April 26) - SELENIUM UPDATE Updated selenium version to 3.4.0  #### 3.4.0 (2017 Mars 15) - SELENIUM UPDATE Updated selenium version to 3.3.1  #### 3.3.0 (2017 February 26) - SELENIUM UPDATE Updated selenium version to 3.2.0  #### 3.2.0 (2017 February 22) - SELENIUM UPDATE Updated selenium version to 3.1.0  #### 3.1.0 (2017 February 15) - ENHANCEMENT Added possibility to disable/ignore specific browsers tests by system property "webdriverextensions.disabledbrowsers" E.g. ```bash mvn install -Dwebdriverextensions.disabledbrowsers=firefox,chrome,safari ``` thanks to [@alexnb](https://github.com/alexnb)) and [@dve](https://github.com/dve)  #### 3.0.1 (2016 September 20) - BUGFIX Fixed issue with Marionette/Geckodriver not loading pages (caused by WebDriverRunner using an incorrect property)  #### 3.0.0 (2016 September 19) - SELENIUM UPDATE Selenium 3.0.1 support - ENHANCEMENT Added support for Marionette/Geckodriver  #### 2.9.2 (2016 September 30) - BUGFIX Added missing htmlunit dependency - BUGFIX Normalized Bot.textIn(WebElement webElement) method to trim spaces so that Chrome and PhantomJs drivers behaves as other drivers according to WebElement docs    #### 2.9.1 (2016 September 28) - SELENIUM UPDATE Updated to latest htmlunit-driver version  #### 2.9.0 (2016 September 28) - ENHANCEMENT Added support to run test against PhantomJS locally - ENHANCEMENT Implemented Filterable support for WebDriverRunner (makes it possible to run single test methods from IntelliJ)  #### 2.8.2 (2016 September 27) - SELENIUM UPDATE Updated selenium version to 2.53.1  #### 2.8.1 (2016 September 16) - BUGFIX Fixes invalid screenshot filename on Windows platform ([PR 68](https://github.com/webdriverextensions/webdriverextensions/pull/68) thanks to [@consulbit](https://github.com/consulbit))  #### 2.8.0 (2016 July 2) - ENHANCEMENT Added support for [@Edge](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/2.8.0/com/github/webdriverextensions/junitrunner/annotations/Edge.html) browser  #### 2.7.0 (2016 May 24) - ENHANCEMENT Added doubleClick(WebElement webElement), waitUntil(Predicate<WebDriver> perdicate) and waitUntil(Predicate<WebDriver> perdicate, long secondsToWait) to Bot (thanks to [@dve](https://github.com/dve)) - ENHANCEMENT Added VaadinBot that can be used if testing an application using the [Vaadin Framework](https://vaadin.com/home) (thanks to [@dve](https://github.com/dve))  #### 2.6.2 (2016 April 9) - ENHANCEMENT Added ISO date and time to screenshot filename  #### 2.6.1 (2016 April 9) - Made private constructor public again for Bot, BotUtils and WebDriverExtensionsContext classes [fix issue #63](https://github.com/webdriverextensions/webdriverextensions/issues/63)  #### 2.6.0 (2016 Mars 17) - SELENIUM UPDATE Updated selenium version to 2.53.0 - ENHANCEMENT SonarQube improvements (thanks to [@faisal-hameed](https://github.com/faisal-hameed) at [@DevFactory](https://github.com/DevFactory))  #### 2.5.0 (2016 February 14) - SELENIUM UPDATE Updated selenium version to 2.52.0 - BUGFIX WebPage.assertIsNotOpen(Openable, Object...) is now working as it should ([PR 57](https://github.com/webdriverextensions/webdriverextensions/pull/57) thanks to [@cplaetzinger](https://github.com/dve))  #### 2.4.0 (2016 February 4) - SELENIUM UPDATE Updated selenium version to 2.50.1  #### 2.3.0 (2016 January 20) - SELENIUM UPDATE Updated selenium version to 2.49.0  #### 2.2.0 (2015 November 6) - JUNIT UPDATE Updated JUnit version to 4.12 - BUGFIX Fixes issue with wrong test description when using JUnit 4.12 and above ([PR 56](https://github.com/webdriverextensions/webdriverextensions/pull/56) thanks to [@cplaetzinger](https://github.com/cplaetzinger)) - BUGFIX Corrected incorrect screenshot file extension from .jpg to .png ([PR 55](https://github.com/webdriverextensions/webdriverextensions/pull/55) thanks to [@cplaetzinger](https://github.com/cplaetzinger))  #### 2.1.2 (2015 November 4) - BUGFIX Fixed that Chrome Driver path is not set to blank if only Internet Explorer Driver path is set with @DriverPath annoation  #### 2.1.1 (2015 October 14) - SELENIUM UPDATE Updated selenium version to 2.48.2  #### 2.1.0 (2015 October 9) - SELENIUM UPDATE Updated selenium version to 2.48.1 - BUGFIX Fixed bug when @TakeScreenshotOnFailure takes more than one screenshot  of a failing test when more then one test fails in the same test class ([PR 52](https://github.com/webdriverextensions/webdriverextensions/pull/52) thanks to [@gunnee](https://github.com/gunnee))  #### 2.0.1 (2015 September 30) - SELENIUM UPDATE Updated selenium version to 2.47.2  #### 2.0.0 (2015 September 20) - JAVA 7 REQUIREMENT Now compiled with java 7 as target since selenium already does that since the 2.47.0 version  #### 1.7.0 (2015 August 11) - SELENIUM UPDATE Updated selenium version to 2.47.1  #### 1.6.0 (2015 June 9) - SELENIUM UPDATE Updated selenium version to 2.46.0  #### 1.5.0 (2015 May 12) - FEATURE Added support for passing WebComponents as generic arguments to other WebComponents, WebPages and WebRepositories [fixes issue #50](https://github.com/webdriverextensions/webdriverextensions/issues/50). E.g.  ```java public class TableComponent<T extends WebComponent> extends WebComponent {     @FindBy(...)     public List<T> rowList; }  public class ASearchResultType extends WebComponent {     // the model for the search result row }  @FindBy(...) TableComponent<ASearchResultType> resultTable; ```  - ENHANCEMENT Added descriptive messages to general field instantiation exceptions thrown by WebDriverExtensions - BUGFIX Removed driver from ThreadLocal when test finished running or failed  #### 1.4.0 (2015 Mars 23) - FEATURE Added Bot method waitForElementToDisplay with TimeUnit as parameter - FEATURE Added @ImplicitlyWait annotation to WebDriverRunner - FEATURE Added @TakeScreenshotOnFailure and @ScreenshotsPath annotations to WebDriverRunner - FEATURE Added takeScreenshot method to Bot - BUGFIX Made @DriverPath and @RemoteAddress annotations only applicable as class annotations  #### 1.3.0 (2015 Mars 12) - SELENIUM UPDATE Updated selenium version to 2.45.0  #### 1.2.1 (2014 December 3) - ENHANCEMENT Added descriptive messages to instantiation exceptions thrown by WebDriverExtensions when WebPage, WebSite and WebRepository class is either abstract, has no no args constructor or has no accessible constructor  #### 1.2.0 (2014 October 29) - SELENIUM UPDATE Updated selenium version to 2.44.0 - BUGFIX Using JUnit @Test timeouts will no longer cause WebDriverExtensionsContext.getDriver() method to throw an exception  #### 1.1.0 (2014 September 15) - SELENIUM UPDATE Updated selenium version to 2.43.1 - FEATURE Added waitForElementsToDisplay - FEATURE Added ignore case to text bot methods - BUGFIX @Delegate annotated field is now allowed to be private  #### 1.0.1 (2014 September 4) - BUGFIX Swallowing 'No runnable methods' error from BlockJUnit4ClassRunner so empty tests are allowed - BUGFIX Made created abstract open method in WebPage and SitePage so eclipse wont complain about @Override annotation   #### 1.0.0 (2014 September 2) - Initial release!    <br>  # Contributors - Thanks [Sauce Labs](https://saucelabs.com/) and [TestingBot](http://testingbot.com) for supporting this project with a free account for testing the remote setting for the [WebDriverRunner](http://static.javadoc.io/com.github.webdriverextensions/webdriverextensions/1.2.1/com/github/webdriverextensions/junitrunner/WebDriverRunner.html) - Thanks [Eniro](http://www.eniro.se/) for helping me develop and test this this framework     <br>  # License  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this work except in compliance with the License. You may obtain a copy of the License in the LICENSE file, or at:     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
LinuxSuRen/phoenix.webui.framework	[![Build Status](https://travis-ci.org/LinuxSuRen/phoenix.webui.framework.svg?branch=master)](https://travis-ci.org/LinuxSuRen/phoenix.webui.framework)  [![License](https://img.shields.io/github/license/LinuxSuRen/phoenix.webui.framework.svg)](https://github.com/LinuxSuRen/phoenix.webui.framework/master/LICENSE)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.surenpi.autotest/autotest.web.framework/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.surenpi.autotest/autotest.web.framework)  [![Join the chat at https://gitter.im/phoenix-webui-framework/Lobby](https://badges.gitter.im/phoenix-webui-framework/Lobby.svg)](https://gitter.im/phoenix-webui-framework/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    # PhoenixAutoTest  本项目是一个基于Selenium的Web自动测试框架，通过该框架可以简化测试人员的学习难度，只要编写少量的Java代码即可，大多数的工作都是编写页面元素的描述文件以及对应的数据源。以下是本框架的特色：  - 支持多种元素选择策略（优先级、循环、区域）  - 支持多种数据源（xml、excel）  - 支持数据源、URL、元素定位信息的参数化  - 支持密文数据，javascript、groovy、freemarker等动态脚本数据  - 支持动态生成日期、身份证号码、手机号、邮编等数据  - 支持操作日志生成GIF动态图片  - 支持主流的浏览器（ie、firefox、chrome、opera、safari）  - 支持移动自动化（Android）  - 支持Eclipse插件生成代码  - 支持纯XML编写完成自动化测试功能  - 自带Windows版本的driver驱动    # 测试报告  ![Excel格式的测试报告](http://surenpi.com/wp-content/uploads/2017/06/autotest_report_excel_1.png)  ![Excel格式的测试报告](http://surenpi.com/wp-content/uploads/2017/06/autotest_report_excel_2.png)  ![数据库格式的测试报告](http://surenpi.com/wp-content/uploads/2017/06/report_database.png)    为了方便喜欢PhoenixAutotest框架的朋友们能尽快上手，这里提供了[一系列教程](http://surenpi.com/2016/07/18/phoenix_autotest_tutorial)。    # 备注  由于本项目没有提交任何工程、IDE相关的文件（这样，您就可以任选Eclipse、IntelliJ IDEA或者是NetBeans作为您的开发工具了），所以check出来以后还需要一些步骤。    这里给出在Eclipse中使用Maven的教程。      QQ交流群：52492046    加群后请及时修改备注为：城市-昵称
testIT-WebTester/webtester-core	[![License](https://img.shields.io/badge/License-Apache%20License%202.0-brightgreen.svg)](http://www.apache.org/licenses/LICENSE-2.0.txt) [![GitHub version](https://badge.fury.io/gh/testIT-WebTester%2Fwebtester-core.svg)](https://badge.fury.io/gh/testIT-WebTester%2Fwebtester-core) [![Build Status](https://travis-ci.org/testIT-WebTester/webtester-core.svg?branch=master)](https://travis-ci.org/testIT-WebTester/webtester-core)  ![testIT WebTester](documentation/images/logo-650x157.png)  > This is the Java 6/7 optimized version of WebTester (v1.x), for the Java 8 version see [2.x](https://github.com/testIT-WebTester/webtester2-core).  testIT WebTester is a web-application UI test automation framework based on Selenium (http://www.seleniumhq.org).  It is the product of years of consulting experience in various projects and aims at providing a very intuitive, declarative and extendable API for writing programmatic UI tests in Java.  ### Features - Page Object Pattern as it's main focus - Functional page elements instead of generic WebElement API - Script style testing support using Ad-Hoc element identification API - Event System for traceability and custom action - Option to highlight used elements as a visual debugging support - Support modules for the integration with frameworks like: assertj, hamcrest, junit, spring - If you must, Selenium is always just a method call away.  ### Other Versions This repository contains the Java 6/7 branch of WebTester. For the Java 8 version of WebTester take a look [here](https://github.com/testIT-WebTester/webtester2-core).  ### Get in touch You can contact us via [e-mail](mailto:webtester@novatec-gmbh.de) or create an [issue](https://github.com/testIT-WebTester/webtester-core/issues).  ### Documentation The [user documentation](documentation/README.md) is part of the repository and provides in-depth documentation on all the features. If you have further questions please get in touch with us.  ### Contribute If you want to contribute to WebTester, fork the repository, make your additions and changes and create a pull request. Things you need to know are documented [here](https://github.com/testIT-WebTester/webtester-core/wiki/Contribution).  ### Issues If you experience any issues please use GitHub's [issue](https://github.com/testIT-WebTester/webtester-core/issues) system to tell us about it!  ### Licensing testIT WebTester is licensed under [The Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0.txt).  ### Sponsoring testIT WebTester is mainly developed by [NovaTec Consulting GmbH](http://www.novatec-gmbh.de/), a German consultancy firm that drives quality in software development projects.
MarkusBernhardt/robotframework-selenium2library-java	Java port of the Selenium 2 (WebDriver) Python library for Robot Framework ==========================================================================  Repository moved ------------ Maintainer of the library is changed, and new repository is at https://github.com/Hi-Fi/robotframework-seleniumlibrary-java.  Introduction ------------  Selenium2Library is a web testing library for Robot Framework that leverages the [Selenium 2 (WebDriver)](http://docs.seleniumhq.org/docs/03_webdriver.jsp) libraries from the [Selenium](http://docs.seleniumhq.org) project. It is modeled after (and forked from) the Robot Framework [SeleniumLibrary](http://code.google.com/p/robotframework-seleniumlibrary/) library, but re-implemented to use Selenium 2 and WebDriver technologies. * More information about this library can be found in the   [Keyword Documentation](http://search.maven.org/remotecontent?filepath=com/github/markusbernhardt/robotframework-selenium2library-java/1.4.0.8/robotframework-selenium2library-java-1.4.0.8-libdoc.html). * For keyword completion in RIDE you can download this   [Library Specs](http://search.maven.org/remotecontent?filepath=com/github/markusbernhardt/robotframework-selenium2library-java/1.4.0.8/robotframework-selenium2library-java-1.4.0.8-libdoc.xml)   and place it in your PYTHONPATH.  This Java port of the existing Selenium2Library was created to enable the usage of a Selenium 2 library with Jython. * Python Selenium2Library needs Python 2.6 upwards * The latests stable release of Jython is 2.5.3 * Jython 2.7b2 is a beta version * There seems to be only slow development of a stable Jython 2.7  Usage -----  This library is a direct replacement to the Python Selenium2Library. There are almost no changes necesssary to the existing code. You  can execute the same testcases and keywords with Python and Jython.  If you are using the robotframework-maven-plugin you can use this library by adding the following dependency to  your pom.xml:      <dependency>         <groupId>com.github.markusbernhardt</groupId>         <artifactId>robotframework-selenium2library-java</artifactId>         <version>1.4.0.8</version>         <scope>test</scope>     </dependency>  If you cannot use the robotframework-maven-plugin you can use the [jar-with-dependencies](http://search.maven.org/remotecontent?filepath=com/github/markusbernhardt/robotframework-selenium2library-java/1.4.0.8/robotframework-selenium2library-java-1.4.0.8-jar-with-dependencies.jar), which contains all required libraries.  If you want more control and feel adventurous you could you use this [jar](http://search.maven.org/remotecontent?filepath=com/github/markusbernhardt/robotframework-selenium2library-java/1.4.0.8/robotframework-selenium2library-java-1.4.0.8.jar) and provide all required libraries from this [list](DEPENDENCIES.md) on your own.  Differences -----------  * Selenium Speed not implemented    Setting the Selenium Speed is deprecated several years and not   implemented in WebDriver. The Python Selenium2Library tries to   emulate the old behavior. I have not implemented this emulation   for the following reasons.      * As far as I understand the emulation is broken and only works     with RemoteWebDriver   * I do not know how to implement that in a correct way with Java    * There is a reason, why this is not implemented in WebDriver.     It's a bad idea.  Demo ----  This is a maven project. If you have firefox installed, you can execute the unit tests with:      mvn integration-test  Getting Help ------------  The [user group for Robot Framework](https://groups.google.com/forum/#!forum/robotframework-users) is the best place to get help. Consider including in the post: * Full description of what you are trying to do and expected outcome * Version number of Selenium2Library, Robot Framework, and Selenium * StackTraces or other debug output containing error information
fhoeben/hsac-fitnesse-fixtures	# hsac-fitnesse-fixtures [![Build Status](https://travis-ci.org/fhoeben/hsac-fitnesse-fixtures.svg?branch=master)](https://travis-ci.org/fhoeben/hsac-fitnesse-fixtures) [![Maven Central](https://img.shields.io/maven-central/v/nl.hsac/hsac-fitnesse-fixtures.svg?maxAge=86400)](https://mvnrepository.com/artifact/nl.hsac/hsac-fitnesse-fixtures)  This project assists in testing (SOAP) web services and web applications by providing an application to define and run tests. To this end it contains a baseline installation of FitNesse (an acceptance testing wiki framework) and some FitNesse fixture (base) classes.  The fixtures provided aim to assist in testing (SOAP) web services and web applications (using Selenium) minimizing the amount of (custom) Java code needed to define tests.  The baseline FitNesse installation offers the following features: * Ability to easily create a standalone (no JDK or Maven required) FitNesse environment. * Run FitNesse tests on a build server, reporting the results in both JUnit XML format and HTML. * FitNesse installation for test/fixture developers containing:     - the fixture base classes (and Selenium drivers for _Chrome_, _Internet Explorer_, _Edge_ and _Firefox_),     - Maven classpath plugin (such that tests can use all dependencies from `pom.xml`),     - HSAC's fitnesse-plugin to add additional Wiki features (random values, calculating relative dates,       Slim scenarios without need to specify all parameters, Slim scripts that take a screenshot after each step),     - easy fixture debugging,  The fastest way to get started: just download the 'standalone.zip' from the  [Releases](https://github.com/fhoeben/hsac-fitnesse-fixtures/releases/latest), extract, run it (you'll just need a Java runtime) and explore the example tests and maybe add a couple of your own.  ## To create your own test project When you want to use the project's baseline to create and maintain your own test suite, we recommend creating your own Maven project based on the project. This will allow you to run and maintain your test set, in version control, without the need to keep your own copies of dependencies (neither Selenium WebDrivers nor Java libraries).  To create a Maven project to run your own tests, based on this project's baseline (assuming you already have a working [Maven installation](https://maven.apache.org/guides/getting-started/maven-in-five-minutes.html):  * Go to the directory below which you want to create the project.  * Use the [archetype, `nl.hsac:fitnesse-project`,](https://github.com/fhoeben/fitnesse-project-archetype) to generate the project:     * (Maven should find the latest version of the archetype automatically, but it doesn't always.) Check the latest version number on the [releases page](https://github.com/fhoeben/fitnesse-project-archetype/releases/latest), and use this instead of `<latest-version>` in the next command.    * On the commandline execute: `mvn archetype:generate -DarchetypeGroupId=nl.hsac -DarchetypeArtifactId=fitnesse-project -DarchetypeVersion=<latest-version>` (or use your IDE's ability to create a project from an archetype).    * Answer the prompts for 'groupId', 'artifactId' and 'package' for the project.  * _Alternatively_ you could also manually copy and update a sample project:     * Clone or download the [sample project](https://github.com/fhoeben/sample-fitnesse-project) to a new directory. (This is the approach used in the [Installation Guide](https://github.com/fhoeben/hsac-fitnesse-fixtures/wiki/Installation-Guide).)    * Update the 'groupId' and 'artifactId' in the [`pom.xml`](https://github.com/fhoeben/sample-fitnesse-project/blob/master/pom.xml) downloaded as part of the sample to reflect your own organisation and project name.    * Move the file for Java class [`FixtureDebugTest`](https://github.com/fhoeben/sample-fitnesse-project/blob/master/src/test/java/nl/hsac/fitnesse/sample_project/FixtureDebugTest.java) to a package/directory of your choice.  * Start the wiki, as described in the generated project's [`README.md`](https://github.com/fhoeben/fitnesse-project-archetype/blob/master/src/main/resources/archetype-resources/README.md#running-locally).  * Start writing tests (and custom fixtures if needed)...  ## To create the standalone FitNesse installation: Execute `mvn clean package -DskipTests`, the standalone installation is present in the wiki directory and as `...-standalone.zip` file in the target directory. It can be distributed by just copying the wiki directory or by copying and extracting the zip file to a location without spaces in its own name, or in its parent's names). This standalone installation can be started using `java -jar fitnesse-standalone.jar` from the wiki directory (or directory where the _standalone.zip_ was extracted).  A zip file containing released versions of this project can be downloaded from the [Releases](https://github.com/fhoeben/hsac-fitnesse-fixtures/releases/latest) or [Maven Central](https://repository.sonatype.org/service/local/artifact/maven/redirect?r=central-proxy&g=nl.hsac&a=hsac-fitnesse-fixtures&c=standalone&p=zip&v=RELEASE). A similar zip file containing the latest *snapshot* (i.e. not released but based on the most recent code) version is published as part of the automated build of this project at https://fhoeben.github.io/hsac-fitnesse-fixtures-test-results/hsac-fitnesse-fixtures-snapshot-standalone.zip.  ## To run the tests on a build server: Have the build server checkout the project and execute `mvn clean test-compile failsafe:integration-test`. Append `failsafe:verify` to the command if you want the build to fail in case of test failures. The result in JUnit XML results can be found in: `target/failsafe-reports` (most build servers will pick these up automatically) The HTML results can be found in: `target/fitnesse-results/index.html`  The FitNesse suite to run can be specified by changing the value of the `@Suite` annotation in `nl.hsac.fitnesse.fixture.FixtureDebugTest`, or (preferably) by adding a system property, called `fitnesseSuiteToRun`, specifying the suite to run to the build server's mvn execution. By using the `@SuiteFilter` and `@ExcludeSuiteFilter` annotations, or (preferably) by adding `suiteFilter` and/or `excludeSuiteFilter` system properties, one can provide tags to in- or exclude and further filter the tests to run within the specified suite. Provide multiple tags by comma-separating them.  The Selenium configuration (e.g. what browser on what platform) to use when testing websites can be overridden by using system properties (i.e. `seleniumGridUrl` and either `seleniumBrowser` or `seleniumCapabilities`). This allows different configurations on the build server to test with different browsers, without requiring different Wiki content, but only requiring a different build configuration.  ### Reports Example reports for Windows using a Sauce Labs Selenium driver (https://fhoeben.github.io/hsac-fitnesse-fixtures-test-results/examples-results/) and Linux with Chrome Headless (https://fhoeben.github.io/hsac-fitnesse-fixtures-test-results/acceptance-test-results/) are generated in the automated build process of this project.  ## Fixture developer installation: Import this project in your favorite Java IDE (with Maven support).  To start FitNesse: have the IDE execute `mvn compile exec:exec`. The port used by FitNesse can be controlled by changing the `fitnesse.port` property's value in pom.xml. FitNesse will be available at `http://localhost:<fitnesse.port>/`, example usage of the symbols and fixtures can be seen in `http://localhost:<fitnesse.port>/HsacExamples`.  To debug a fixture used in a FitNesse page: change the `@Suite` annotation's value to contain page name in `nl.hsac.fitnesse.fixture.FixtureDebugTest`, then just debug this test.  ## Documentation More information about this project can be found on its [GitHub Wiki](https://github.com/fhoeben/hsac-fitnesse-fixtures/wiki)
ameizi/solrj-example	# solrj-example  solrj+webmagic+selenium示例  使用webmagic和selenium-java爬取京东商品信息并入库MySQL。采用solr dataimport创建索引，采用solrj检索商品信息。  # solr单机模式  https://coding.net/u/aimeizi/p/solr/git  基于solr4.10.4集成IKAnalyzer、mmseg4j、ansj中文分词及Dataimport功能  # solr集群模式  https://coding.net/u/aimeizi/p/SolrCloud/git  solr集群模式。基于solr4.10.4集成IKAnalyzer、mmseg4j、ansj中文分词及Dataimport等功能  # 运行  JDProductProcessor 是JD商品采集的入口，main方法直接运行。  CcdiPageProcessor 是纪检委网站采集的入口，main方法直接运行。  启动solr服务，运行爬虫采集程序，启动搜索服务，完成搜索。  # Screenshots  webmagic jmx监控 ![](Screenshots/jconsole-1.png)  webmagic jmx监控查看总抓取页数 ![](Screenshots/jconsole-2.png)  webmagic爬取JD商品数据入库数据 ![](Screenshots/jddata.png)  solrdataimport数据导入 ![](Screenshots/solrdataimport.png)  solr搜索 查询`所有商品`按`评论降序`排列，以`表格`的方式展现 ![](Screenshots/search-1.png)  solr搜索 查询`名称`为`手机`的商品按`价格降序`排列，以`表格`的方式展现 ![](Screenshots/search-2.png)  solr搜索 查询`名称`为`手机`且过滤`产品类别为手机`的商品信息按`价格降序`，以`列表`的方式展现 ![](Screenshots/search-3.png)  solr搜索 查询名称为`洗衣机`且过滤`产品类别为洗衣机`的商品信息按`价格降序`，以`表格`的方式展现 ![](Screenshots/search-4.png)  solr搜索 查询名称为`iPhone`的商品信息按`价格降序`，以`表格`的方式展现 ![](Screenshots/search-5.png)  # 参考文档  http://webmagic.io/docs/zh/
seleniumQuery/seleniumQuery	# [seleniumQuery](http://seleniumquery.github.io)  [![Maven Central](https://img.shields.io/maven-central/v/io.github.seleniumquery/seleniumquery.svg)](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22io.github.seleniumquery%22%20AND%20a%3A%22seleniumquery%22)  [![Codacy Badge](https://api.codacy.com/project/badge/grade/6f25f5fe245746a4a7a53f426e0e1288)](https://www.codacy.com/app/acdcjunior/seleniumQuery) [![codecov.io](https://codecov.io/gh/seleniumQuery/seleniumQuery/branch/master/graph/badge.svg)](https://codecov.io/gh/seleniumQuery/seleniumQuery) [![Dependency Status](https://www.versioneye.com/user/projects/56861ab2eb4f47003c000e43/badge.svg?style=flat)](https://www.versioneye.com/user/projects/56861ab2eb4f47003c000e43) [![GitHub license](https://img.shields.io/badge/license-Apache%202-blue.svg)](https://raw.githubusercontent.com/seleniumQuery/seleniumQuery/master/LICENSE.txt) [![Join the chat at https://gitter.im/seleniumQuery/seleniumQuery](https://badges.gitter.im/seleniumQuery/seleniumQuery.svg)](https://gitter.im/seleniumQuery/seleniumQuery?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [![Linux Build Status](https://img.shields.io/travis/seleniumQuery/seleniumQuery/master.svg?label=Linux+Build)](https://travis-ci.org/seleniumQuery/seleniumQuery) [![Windows Build Status](https://img.shields.io/appveyor/ci/acdcjunior/seleniumQuery/master.svg?label=Windows+Build)](https://ci.appveyor.com/project/acdcjunior/seleniumQuery/branch/master) [![Build status](https://codeship.com/projects/7b37d0c0-d5b4-0133-1efe-62329e93051f/status?branch=master)](https://codeship.com/projects/142644) [![wercker status](https://app.wercker.com/status/b772beb5c952865d659e548bf7d64f48/s "wercker status")](https://app.wercker.com/project/bykey/b772beb5c952865d659e548bf7d64f48) [![Circle CI](https://circleci.com/gh/seleniumQuery/seleniumQuery.svg?style=svg)](https://circleci.com/gh/seleniumQuery/seleniumQuery)  [![Sauce Test Status](https://saucelabs.com/open_sauce/build_matrix/acdcjunior.svg)](https://saucelabs.com/u/acdcjunior)  ### *Cross-Driver* jQuery-like Java interface for Selenium WebDriver  seleniumQuery is a Java library/framework that brings a ***cross-driver*** **jQuery-like** interface for [Selenium WebDriver](http://docs.seleniumhq.org/projects/webdriver/).  ### Example snippet:  ```java // getting the value String oldStreet = $("input.street").val(); // setting the value $("input.street").val("4th St!"); ```  ### No special configuration needed - use it in your project right now:  On a regular `WebElement`...  ```java // an existing WebElement... WebElement existingWebElement = driver.findElement(By.id("myId")); // call jQuery functions String elementVal = $(existingWebElement).val(); boolean isButton = $(existingWebElement).is(":button"); // enhanced selector! for (WebElement child: $(existingWebElement).children()) {   System.out.println("That element's child: "+child); } ```  Or an existing `WebDriver`...  ```java // an existing WebDriver... WebDriver driver = new FirefoxDriver(); // set it up $.driver().use(driver); // and use all the goods for (WebElement e: $(".myClass:contains('My Text!'):not(:button)")) {   System.out.println("That element: " + e); } ```  ## What can you do with it?  Allows querying elements by:  - **CSS3 Selectors** - `$(".myClass")`, `$("#table tr:nth-child(3n+1)")`; - **jQuery/Sizzle enhancements** - `$(":text:eq(3)")`, `$(".myClass:contains('My Text!')")`; - **XPath** - `$("//div/*/label/preceding::*")`; - and even some own **seleniumQuery selectors**: `$("#myOldDiv").is(":not(:present)")`.  Built using Selenium WebDriver's native capabilities **only**:  - No `jQuery.js` is embedded at the page, no side-effects are generated;     - Doesn't matter if the page uses jQuery or not (or even if the JavaScript global variable `$` is other library like `Prototype.js`). - Capable of handling/testing JavaScript-disabled pages     - Test pages that use [Unobtrusive JavaScript](http://en.wikipedia.org/wiki/Unobtrusive_JavaScript).     - Most functions don't even require the browser/driver to have JavaScript enabled!  ## Quickstart: A running example  Try it out now with the running example below:  ```java import static io.github.seleniumquery.SeleniumQuery.$; // this will allow the short syntax  public class SeleniumQueryExample {   public static void main(String[] args) {     // sets Firefox as the driver (this is optional, if omitted, will default to HtmlUnit)     $.driver().useFirefox(); // The WebDriver will be instantiated only when first used      // or use ("decorate") any previously existing driver     $.driver().use(new FirefoxDriver());      // starts the driver (if not started already) and opens the URL     $.url("http://www.google.com/?hl=en");      // interact with the page     $(":text[name='q']").val("selenium"); // the keys are actually typed!     $(":button[name=btnG]").click(); // simulates a real user click (not just the JS event)      // Besides the short syntax and the jQuery behavior you already know,     // other very useful function in seleniumQuery is .waitUntil(),     // handy for dealing with user-waiting actions (specially in Ajax enabled pages):     String resultsText = $("#resultStats").waitUntil().is(":visible").then().text();      System.out.println(resultsText);     // should print something like: About 24,900,000 results (0.37 seconds)      $.quit(); // quits the currently used driver (firefox)   } } ```  ## Download and execute the [seleniumQuery showcase project](https://github.com/acdcjunior/seleniumQuery-showcase)  ...and see it in action [right now](https://github.com/acdcjunior/seleniumQuery-showcase).  To get the latest version of seleniumQuery, add to your **`pom.xml`**:  ```xml <dependency>     <groupId>io.github.seleniumquery</groupId>     <artifactId>seleniumquery</artifactId>     <version>0.16.0</version> </dependency> ```  <br>  # Features  seleniumQuery aims to implement all relevant jQuery functions, as well as adding some of our own. Our main goal is to make emulating user actions and reading the state of pages easier than ever, with a consistent behavior across drivers.   ### Readable jQuery syntax you already know  Make your code/tests more readable and easier to maintain. Leverage your knowledge of jQuery.  ```java // Instead of regular Selenium code: WebElement element = driver.findElement(By.id("mySelect")); new Select(element).selectByValue("ford");  // You can have the same effect writing just: $("#mySelect").val("ford"); ```  Get to know what jQuery functions seleniumQuery supports and what else it brings to the table on our [seleniumQuery API wiki page](https://github.com/seleniumQuery/seleniumQuery/wiki/seleniumQuery-API).  ### Powerful selector system  Let the tool do the hard work and find elements easily:  - CSS3 Selectors - `$(".myClass")`, `$("#table tr:nth-child(3n+1)")` - jQuery/Sizzle enhancements - `$(".claz:eq(3)")`, `$(".claz:contains('My Text!')")` - XPath - `$("//div/*/label/preceding::*")` - and even some own seleniumQuery selectors: `$("#myOldDiv").is(":not(:present)")`.  You pick your style. Whatever is more interesting at the moment. Mixing is OK:  ```java $("#tab tr:nth-child(3n+1)").find("/img[@alt='calendar']/preceding::input").val("2014-11-12") ``` Find more about them in [seleniumQuery Selectors wiki page.](https://github.com/seleniumQuery/seleniumQuery/wiki/seleniumQuery-Selectors)  <br>  ### Waiting capabilities for improved Ajax testing  Other important feature is the leverage of `WebDriver`'s `FluentWait` capabilities **directly** in the element (no boilerplate code!) through the use of the `.waitUntil()` function:  ```java // WebDriver cannot natively detect the end of an Ajax call. // To test your application's behavior, you can and should always work with the // Ajax's expected effects, visible for the end user. // Below is an example of a <div> that should be hidden as effect of an Ajax call. // The code will hold until the modal is gone. If it is never gone, seleniumQuery // will throw a timeout exception. $("#modalDiv :button:contains('OK')").click(); $("#modalDiv :button:contains('OK')").waitUntil().is(":not(:visible)");  // Or, fluently: $("#modalDivOkButton").click().waitUntil().is(":not(:visible)"); ```  And, that's right, the `.is()` function above is your old-time friend that takes a selector as argument!  Check out what else `.waitUntil()` can do in the [seleniumQuery API wiki page](https://github.com/seleniumQuery/seleniumQuery/wiki/seleniumQuery-API).  <br>  ### Plugin System  seleniumQuery supports plugins through the `.as(PLUGIN)` function, such as:  ```java $("div").as(YOURPLUGIN).someMethodFromYourPlugin(); ```  There are some default plugins. To check them out, call `.as()` without arguments. Example:  ```java // the .select() plugin $("#citiesSelect").as().select().selectByVisibleText("New York"); // picks an <option> in the <select> based in the <option>'s visible text ```  For an example of how to create your own plugin, check the [seleniumQuery Plugin wiki page](https://github.com/seleniumQuery/seleniumQuery/wiki/seleniumQuery-Plugin-Support---.as()-function).   <br>  ### Flexible WebDriver builder system  How to setup the `WebDriver`? Simply use our builder. The driver will be instantiated only when first used.  ##### Firefox  ```java $.driver().useFirefox(); // Will set up firefox as driver $.url("http://seleniumquery.github.io"); //the driver will be instantiated when this executes ```  ##### Firefox driver with disabled JavaScript  Want `FirefoxDriver` without JavaScript? Just: ```java $.driver().useFirefox().withoutJavaScript(); // when started, Firefox will have JS OFF ```  ##### Chrome, InternetExplorer, PhantomJS drivers  All you have to do is download [their executables](https://github.com/seleniumQuery/seleniumQuery-demos/tree/master/src/main/resources) before. Setting them up in seleniumQuery is all too easy:  ```java // Using Chrome $.driver().useChrome(); // will look for chromedriver/exe to you, including in the classpath! // Or if you want to set the path yourself $.driver().useChrome().withPathToChromeDriver("path/to/chromedriver.exe")  // InternetExplorerDriver $.driver().useInternetExplorer(); // we search IEDriverServer.exe for you // Or you set the path yourself $.driver().useInternetExplorer().withPathToIEDriverServerExe("C:\\IEDriverServer.exe");  // PhantomJS (GhostDriver) $.driver().usePhantomJS(); // again, we'll find phantomjs[.exe] to you // Or you may set the path yourself $.driver().usePhantomJS().withPathToPhantomJS("path/to/phantomjs.exe"); ````  ##### HtmlUnit  So many possibilities to set up `HtmlUnitDriver`... If only there was a simple way to use them. Oh, wait:  ```java // HtmlUnit default (Chrome/JavaScript ON) $.driver().useHtmlUnit(); // Want disabled JavaScript, just call .withoutJavaScript() $.driver().useHtmlUnit().withoutJavaScript();  // HtmlUnit emulating Chrome $.driver().useHtmlUnit().emulatingChrome(); $.driver().useHtmlUnit().emulatingChrome().withoutJavaScript(); // HtmlUnit emulating Firefox $.driver().useHtmlUnit().emulatingFirefox(); // could disable JS here as well // And IE $.driver().useHtmlUnit().emulatingInternetExplorer11(); // JS is disableable as well $.driver().useHtmlUnit().emulatingInternetExplorer(); // will pick latest IE ````  #### But there is more  Explore the auto-complete. There are additional options to every driver, such as `.withCapabilities(DesiredCapabilities)` or some specific, such as `.withProfile(FirefoxProfile)` or `.withOptions(ChromeOptions)`.  Finally, if you want to create the `WebDriver` yourself:  ```java WebDriver myDriver = ...; $.driver().use(myDriver); ```  <br>  ## seleniumQuery still is Selenium - with "just" a jQuery interface  That's why it can work with disabled JavaScript!  But there is a more important aspect to it: Although our functions yield the same result as if you were using jQuery, remember we always execute them from the user perspective. In other words, when you call: ```java $(":input[name='email']").val("seleniumQuery@example.com"); ```  We don't change  the `value` attribute directly like jQuery does. We actually do as a user would: We **clear** the input and **type, key by key**, the string provided as argument!  And we go the *extra mile* whenever possible: - Our **`$().val()` even works on `contenteditable` elements AND `documentMode=on <iframe>`s**: They don't have `value`, but we type the text in them, again, key by key, as an user would; - If it is an `<input type="file">` we select the file; - When the element is a `<select>`, we choose the `<option>` by the value given (same as `$("selector").as().select().selectByValue("123")`).  ### Always from the user perspective  On the same tone, when selecting/checking `<option>`s or checkboxes or radios, try not to use `$().prop("selected", true)` directly to them (which to work, of course, would need JS to be enabled on the driver). Do as an user would: call `.click()`! Or, better yet, use seleniumQuery's `.as().select()` functions: `$().as().select().selectByVisibleText("My Option")` or `$().as().select().selectByValue("123")`.  <br><br>  ### Alternate symbols  If the dollar symbol, `$`, gives you the yikes -- we know, it is used for internal class names --, it is important to notice that the `$` symbol in seleniumQuery is not a class name, but a `static` method (and field). Still, if you don't feel like using it, you can resort to `sQ()` or good ol' `jQuery()` and benefit from all the same goodies:  ```java import static io.github.seleniumquery.SeleniumQuery.sQ; import static io.github.seleniumquery.SeleniumQuery.jQuery; ... String oldStreet = sQ("input.street").val(); sQ("input.street").val("4th St!");  String oldStreetz = jQuery("input.street").val(); jQuery("input.street").val("5th St!"); ```  <br>  # Using multiple browsers/drivers simultaneously  Typically, the `$` is a static variable, thus every command you issue only affects the one same instance of WebDriver.  But... what if you want/need to use two WebDrivers at the same time?  We've got your back, see the [example](src/test/java/endtoend/browser/SeleniumQueryBrowserTest.java):  ```java public class SeleniumQueryBrowserTest {      // using two drivers (chrome and firefox) at the same time     private SeleniumQueryBrowser chrome = new SeleniumQueryBrowser();     private SeleniumQueryBrowser firefox = new SeleniumQueryBrowser();      @Test     public void multiple_browser_instances_should_work_OK() {         chrome.$.driver().useHtmlUnit().emulatingChrome();         chrome.$.url("http://google.com");          firefox.$.driver().useHtmlUnit().emulatingFirefox();         firefox.$.url("http://google.com");          // assuming, of course, that such #agent elements exist         assertThat(chrome.$("#agent").text(), containsString("Chrome"));         assertThat(firefox.$("#agent").text(), containsString("Firefox"));     }      @After     public void tearDown() {         chrome.$.quit();         firefox.$.quit();     }  } ```  <br>  # More  Find more on our [wiki](https://github.com/seleniumQuery/seleniumQuery/wiki).  <br>  # Changelog/Roadmap  See [releases](https://github.com/seleniumQuery/seleniumQuery/releases).  # Contributing  The tool quite simple, so there's a lot of room for improvement. Some of its main functionalities were just created (didn't exist in jQuery) for our specific needs, like the `.waitUntil()`, the `.as()` plugins, the driver builder and so on. So if you come up with an idea of something that could be useful, tell us, or, even better, do it yourself and join the team!  ## Goals and non-goals  Goals: - Have a uniform behavior thoughout targeted WebDriver implementations     - A given code should behave as similar as possible in all WebDrivers.         - Selenium itself takes care of that, but it does leave some room for improvement         - This is important to our functions as well, they should behave the same regardless of WebDriver implementation (browser) used - Mimic jQuery's interface and behavior, but...     - Do it all, when possible, from the user's perspective         - e.g. `$().val("")` types content instead of setting the `value` attribute.     - Improve it a little (e.g. throw exception when invalid selectors, such as `"div:file"` are used) - Add functions that tackle common problems when dealing with web (testing) automation, such as waiting (`$().waitUntil()`) - Add quick commands for common usage patterns (such as driver builder does) - Simplify overall usage with convention over configuration  Non-goals: - Add all jQuery's functions - Replace WebDriver   ## History - What went bad?     - Since the selector system supports not only pure CSS (it allows the extended CSS supported by jQuery- and implemented by Sizzle), its implementation is a challenge by itself.         - The first version used regexes, didn't work so well and never made it into a release         - The second version (released as 0.9.0) converts every CSS selector into a XPath expression and executes it.             - The advantage is that this makes Selenium bring every element the user wanted already, without the need to iterate over them or anything.             - The problem with this approach is that not every CSS can be translated into an equivalent XPath expression (e.g. `:selected` or `:visible`)         - The third version (currently under development, called "secondgen") will parse the selector and...             - If the selector is plain CSS or XPath, use it directly             - If the selector is an extended CSS that can be translated fully to an XPath expression, than translate it and use it             - Otherwise, translate the CSS to the XPath expression that brings the smallest numbers of element possible and then iteratively filter the results before returning  ## What else?  Feel free to [request, suggest](https://github.com/seleniumQuery/seleniumQuery/issues/new), create pull requests. As said, any opinions/help are more than welcome!
LinuxSuRen/phoenix.platform	[![Build Status](https://travis-ci.org/LinuxSuRen/phoenix.platform.svg?branch=master)](https://travis-ci.org/LinuxSuRen/phoenix.platform)    # autotest.platform  这是一款无需编码即可实现WebUI自动化测试的平台。为了方便新手尽快熟悉平台，每一页都有帮助向导！    # 演示地址  http://phoenix.surenpi.com/    默认的用户名和密码都是demo    # 兼容性  JRE1.8+    Tomcat6    Tomcat7    Tomcat8    Jetty7.6.21    # 浏览器  IE8    Chrome55    # 操作系统  Windows_XP_32    Windows7_64    # Docker  平台的Docker镜像信息请访问https://hub.docker.com/r/surenpi/autotest.platform/    # 交流  QQ群：52492046  群里不定期会有公开课，两周至少会有一次，每期都有录屏。敬请关注！
seleniumkit/selenograph	# Selenograph [![Maven Central](https://maven-badges.herokuapp.com/maven-central/ru.qatools.seleniumkit/selenograph/badge.svg?style=flat)](https://maven-badges.herokuapp.com/maven-central/ru.qatools.seleniumkit/selenograph)  [![Build status](http://ci.qatools.ru/job/pessimistic-mongodb_master-deploy/badge/icon)](http://ci.qatools.ru/job/selenograph_master-deploy/)   **Selenograph** is a powered version of [Selenium Grid Router](https://github.com/seleniumkit/gridrouter)  providing more information about currently running Selenium sessions and hubs state. With this information Selenograph finds available browser faster and provides data for the real time status and statistics.  ![Screenshot](screen.png)  ## Requirements  Unlike the original Grid Router, Selenograph has the shared state between nodes, which is stored in database. We use [MongoDB](https://www.mongodb.org/) (v. 3.2+) as a database, because it provides high availability, fault tolerance and schema-less approach. Although it's recommended to use a replica set (at least 3 nodes of MongoDB), you can run just a single instance to play with Selenograph.  ## Features  * Web interface displaying all available quotas, browsers and versions with the corresponding number of currently running sessions * REST API showing information about currently running sessions for each quota and hub * Ability to export to Graphite e.g. in order to visualize quotas and browsers usage statistics  ## Installation  For Ubuntu users we provide deb packages. Please note that yandex-selenograph package conflicts with  yandex-grid-router, so if you have previously installed Selenium Grid Router, you'll need to uninstall it first.   Also ensure that you have Java 8 installed: ``` $ sudo add-apt-repository ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get install oracle-java8-installer ```  To install Selenograph itself: ``` $ sudo add-apt-repository ppa:yandex-qatools/selenograph $ sudo apt-get update $ sudo apt-get install yandex-selenograph $ sudo service yandex-selenograph start ```  You can also run Selenograph in Docker container (not yet available - coming soon):  ``` $ sudo docker run --net host \                  --log-opt max-size=1g \                 --log-opt max-file=2 \                 -v /etc/grid-router:/etc/grid-router:ro \                 -v /var/log/grid-router:/var/log/grid-router \                 -d qatools/selenograph:latest  ```  ## Configuration  Most of configuration options duplicate the original [Selenium Grid Router](https://github.com/seleniumkit/gridrouter#configuration) ones. To configure the MongoDB connection and other Selenograph-specific options, you should edit `/etc/grid-router/selenograph.properties`:  ``` ##### Main MongoDB options:  ##### # Specifies the mongodb hosts camelot.mongodb.replicaset=localhost:27017  # Set the database name for selenograph camelot.mongodb.dbname=selenograph  # Set the username for selenograph user camelot.mongodb.username=  # Set the password for selenograph user camelot.mongodb.password=   ##### Graphite export options:  ##### # Specifies the graphite api host (uncomment to enable) # graphite.host=127.0.0.1  # Specifies the graphite api port # graphite.port=42000  # Specifies the prefix for metrics # selenograph.gridrouter.graphite.prefix=selenograph   ##### Advanced MongoDB options:  ##### # The following options are not recommended to change # Edit them at your own risk only if you want to configure the connection options # camelot.mongodb.connections.per.host=30 # camelot.mongodb.threads.connection.mult=40 # camelot.mongodb.connect.timeout=15000 # camelot.mongodb.heartbeat.timeout=15000 # camelot.mongodb.heartbeat.frequency=1000 # camelot.mongodb.heartbeat.socket.timeout=10000 # camelot.mongodb.readpreference=PRIMARY_PREFERRED # camelot.mongodb.socket.timeout=60000 # camelot.mongodb.waitForLockSec=120 # camelot.mongodb.lockPollMaxIntervalMs=7 ```   ## REST API  ### /quota Requires Basic HTTP authorization with quota credentials.  Shows information about browsers available in current quota. Output example: ``` {      "firefox:33.0":135,    "chrome:opera-28.0":10,    "chrome:opera-29.0":10,    "firefox:37.0":55,    "firefox:36.0":30,    "firefox:39.0":10,    "firefox:38.0":110,    "chrome:48.0":100,    "chrome:43.0":10,    "internet explorer:9":20,    "chrome:42.0":130,    "internet explorer:8":20,    "chrome:45.0":20,    "chrome:44.0":10,    "chrome:opera-30.0":10,    "MicrosoftEdge:12.1":10,    "firefox:40.0":20,    "internet explorer:11":20,    "firefox:41.0":20,    "iOS:7.1":2,    "internet explorer:10":20,    "chrome:yandex-browser":25,    "chrome:41.0":55,    "chrome:40.0":5,    "iOS:8.4":2,    "android:6.0":5,    "opera:12.16":20 } ``` ### /stats Requires Basic HTTP authorization with quota credentials.  Returns usage statistics for each browser version for quota. `max` and `avg` numbers displays the maximum and average concurrent sessions during current minute. `raw` and `current` displays the currently running sessions count. Output example:  ``` {      "selenium:firefox:36.0":{         "max":0,       "avg":0,       "raw":0,       "current":0    },    "selenium:chrome:41.0":{         "max":0,       "avg":0,       "raw":0,       "current":0    },    "selenium:firefox:37.0":{         "max":0,       "avg":0,       "raw":0,       "current":0    },    "selenium:firefox:38.0":{         "max":0,       "avg":0,       "raw":0,       "current":0    } } ``` ### /ping A ping API for load balancers. Returns 200 when service is functioning properly.  ### /api/selenograph/strategy Shows a list of all available hubs with percentage of free browsers (100% for completely free hub).  This information is used while selecting the next hub to route the session to. Output example: ``` {      "lastUpdated":"Feb,26 12:39:13.061",    "hubs":{         "firefox33-1.selenium.net:4445":100,       "firefox33-2.selenium.net:4445":75,       "firefox42-1.selenium.net:4445":50,       "firefox42-2.selenium.net:4445":100,       "chrome45-1.selenium.net:4444":80,       "firefox38-1.selenium.net:4445":100,    } } ``` ### /api/selenograph/quotas Shows information about all available quotas: which browser versions exist and how many browsers are available for each version. This info is actually the mirror of the configured Gridrouter quotas enriched with currently running sessions count. ``` {      "all":[         {            "versions":[               {                  "version":"33.0",                "running":211,                "max":500             }          ],          "name":"firefox",          "running":211,          "max":500       },       {            "versions":[               {                  "version":"11.0",                "running": 280,                "max": 584,                "occupied":0             },          ],          "name":"internet explorer",          "running":0,          "max":584,          "occupied":0       },    ],    "nick-ie11":[         {          "versions":[               {                  "version":"11.0",                "running": 280,                "max": 584             },          ],          "name":"internet explorer",          "running":0,          "max": 584       },    ],    "john-firefox":[         {          "versions":[               {                  "version":"33.0",                "running": 211,                "max": 500             },          ],          "name":"firefox",          "running":211,          "max": 500       },    ] } ```  ### /api/selenograph/quota/:name  The same as `/api/selenograph/quotas`, but for the single quota name.  ```
Cognifide/bobcat	![Cognifide logo](http://cognifide.github.io/images/cognifide-logo.png)    [![Build Status](https://travis-ci.org/Cognifide/bobcat.svg?branch=master)](https://travis-ci.org/Cognifide/bobcat)  [![Maven Central](https://img.shields.io/maven-central/v/com.cognifide.qa.bb/bobcat.svg?label=Maven%20Central)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.cognifide.qa.bb%22%20AND%20a%3A%22bobcat%22)  [![Apache License, Version 2.0, January 2004](https://img.shields.io/github/license/cognifide/bobcat.svg?label=License)](http://www.apache.org/licenses/)    # Bobcat    <p align="center">    <img src="assets/bobcat-384-384.png" alt="Bobcat Logo"/>  </p>    Bobcat is an automated testing framework for functional testing of web applications. It wraps Selenium Browser Automation with a handy set of tools (accelerators). Since using Selenium (Webdriver), it aims to mimic the behaviour of a real user, and as such interacts with the HTML of the application.     Bobcat is implemented in Java and allows test development in pure JUnit and in Gherkin for BDD approaches. Bobcat also provides set of good practices that accelerates testing process.    ## What's philosophy behind Bobcat?    #### We have identified major properties that Bobcat should follow:    * Maintainable      * written in Java with Guice,      * provides set of libraries and helpers,      * enforces Page Object pattern,      * supports both JUnit and Cucumber.    * Stable      * provides set of Archetypes for project setup,      * unaffectedly supports Continous Integration environments,      * comes together with good practices.    * Scalable      * allows parallel execution both Junit and Cucumber tests,      * supports testing on different levels (Integration, Staging, SIT...),      * integrates with majority of test clouds (Sauce Labs, Cross Browser Testing, Browser Stack...).        ## What tools does Bobcat consist of?    #### In Bobcat we combine many tools:  * Selenium to enable testing on web browsers,  * Appium to enable testing on mobile devices,  * Cucumber JVM to simplify test automation in BDD,  * ChromeDriver to enable testing on Chrome browser,  * IEDriver to enable testing on IE browser.    #### Bobcat uses set of libraries that supports development:  * Google Guice to let dependency management be more effective,  * JUnit as a test runner,  * Maven as a project managing tool.    ## AEM Support    Bobcat accelerates test development, especially when it comes to AEM authoring. Bobcat provides set of libraries which allows you to test almost every action on the AEM author side. This includes:  - Site Admin,  - Dialogs,  - Parsyses,  - Components,  - Component fields,  - Crx.    #### Bobcat supports AEM authoring in newest version - AEM 6.2 and compatible features in older versions.     ## License    **Bobcat** is licensed under [Apache License, Version 2.0 (the "License")](https://www.apache.org/licenses/LICENSE-2.0.txt)    ## Dependencies    - org.seleniumhq.selenium  - io.appium  - net.lightbody.bmp  - com.google.inject  - info.cukes.cucumber  - org.asserj  - org.apache.jackrabbit    ## Developer setup guide    To work with **Bobcat** the following tools are required:    - JDK 8 (from _065)  - Maven 3  - Chrome Driver - if tests will be executed on chrome    ## Roadmap    - AEM Touch UI testing support,  - Gradle,  - Solr testing support,  - Model based testing - http://graphwalker.github.io/    ## Documentation  * [Bobcat Wiki](https://github.com/Cognifide/bobcat/wiki)  * [Bobcat 1.2.1 APIdocs](https://cognifide.github.io/bobcat/apidocs/1-2-1/)
jacum/icefaces-archetype	Why you should use this project as startup:  - set of key technologies integrated and adjusted to play nice with each other - ICEFaces as the best JSF implementation that works just out of the box - Spring beans seamlessly integrated with JSF managed beans - Spring Security integrated with log on/off and redirects for protected resources - MyFaces instead of default Mojarra - for better error reporting (also custom LoggingExceptionHandler) - Jetty to start up as development server - Selenium for black-box functional testing - JRebel for zero-turnaround redeployment (this one comes with commercial license but believe me it worth its price!)   To build and launch:  $ mvn jetty:run-exploded  Open http://localhost:8080/ in the browser.  ***  To run Selenium integration test (runs its own Jetty, watch out for port conflicts):  $ mvn integration-test  Selenium tests require Firefox to be locally installed. On headless CI Linux servers, install xvfb to enable testing.  ***  For production build (replaces a placeholder in web.xml):  $ mvn -Pproduction package
selenium-cucumber/selenium-cucumber-java	selenium-cucumber-java =================  selenium-cucumber : Automation Testing Using Java  selenium-cucumber is a behavior driven development (BDD) approach to write automation test script to test Web. It enables you to write and execute automated acceptance/unit tests. It is cross-platform, open source and free. Automate your test cases with minimal coding. [More Details](http://seleniumcucumber.info/)  Documentation ------------- * [Installation](doc/installation.md) * [Predefined steps](doc/canned_steps.md)  Download a Framework -------------- * Maven - https://github.com/selenium-cucumber/selenium-cucumber-java-maven-example  Writing a test --------------  The cucumber features goes in the `features` library and should have the ".feature" extension.  You can start out by looking at `features/my_first.feature`. You can extend this feature or make your own features using some of the [predefined steps](doc/canned_steps.md) that comes with selenium-cucumber.   Predefined steps ----------------- By using predefined steps you can automate your test cases more quickly, more efficiently and without much coding.  The predefined steps are located [here](doc/canned_steps.md)  Running test --------------  Go to your project directory from terminal and hit following commands * `mvn test (defualt will run on local firefox browser)` * `mvn test "-Dbrowser=chrome" (to use any other browser)` * `mvn test "-Dcloud_config=saucelab_windows_chrome52" (to run test on cloud test platforms)`  Using canned tests in your project ----------------------------------  In your TestRunner class add a glue option:  ``` package stepDefintions;  import org.junit.runner.RunWith;  import cucumber.api.CucumberOptions; import cucumber.api.junit.Cucumber;  @RunWith(Cucumber.class) @CucumberOptions( 	plugin = {"html:target/cucumberHtmlReport"}, 	features = "classpath:features", 	glue = {"info.seleniumcucumber.stepdefinitions"} )  public class RunCukeTest { } ```  Maven/Gradle Dependency -----------------------  See https://jitpack.io/#selenium-cucumber/selenium-cucumber-java .  License -------  (The MIT License)  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
11039850/monalisa-orm	# Features  * Using the database takes only 1 line of code * Generic ORM functions(CRUD) * Auto-Generate DTOs * Object fields * Reload SQL dynamically * Sharding support  * Write multi-line strings easily  5 minutes video: [Youtube](http://www.youtube.com/watch?v=3qpr0J7D7cQ) / [YouKu](http://v.youku.com/v_show/id_XMTU0ODk1MzA2MA==.html)   [Example Project](https://github.com/11039850/monalisa-example) |  [Eclipse Plugin](https://github.com/11039850/monalisa-orm/wiki/Code-Generator#eclipse-plugin)   # Usage   ## Using Database  ![image](https://github.com/11039850/monalisa-orm/raw/master/doc/images/db.gif)  ```java   	@DB(url="jdbc:mysql://127.0.0.1:3306/test" ,username="root", password="root") 	public interface TestDB{ 		public static DBConfig DB=DBConfig.fromClass(TestDB.class);   	} ```  ```java 	new User().setName("zzg.zhou").setStatus(1).save(); ```	    ## Auto-Generate DTOs  ![image](https://github.com/11039850/monalisa-orm/raw/master/doc/images/select.gif)  ```java 	public class UserBlogDao { 		@Select(name="test.result.UserBlogs")      // <--- Auto create/update: test.result.UserBlogs 		public List  selectUserBlogs(int user_id){ // <--- Auto replace List to List<UserBlogs> 			Query q=TestDB.DB.createQuery(); 			             			q.add(""/**~{ 				SELECT a.id, a.name, b.title, b.content, b.create_time 					FROM user a, blog b    					WHERE a.id=b.user_id AND a.id=?		 			}*/, user_id); 			  			return q.getList();               // <--- Auto replace getList() to getList<UserBlogs> 		}  	} ```   ## Database Service  Direct Database Access by HTTP, see: [monalisa-service](https://github.com/11039850/monalisa-service) ```   curl http://localhost:8080/your_web_app/dbs/testdb/your_table_name ```  ## Query Example  ### Insert  ```java 	//insert 	new User().setName("zzg.zhou").setStatus(1).save(); 	 	//parse data from type: Map, json/xml string, JsonObject(Gson), HttpServletRequest, JavaBean 	new User().parse("{'name':'oschina','status':0}").save(); 	new User().parse("<data> <name>china01</name><status>1</status> </data>").save(); 	new User().parse(request).save(); 	 	//Object field 	Address address=new Address("guangdong","shenzhen"); 	user.setAddress(address).save(); 	 	//File field 	String detail_save_path="path/001.txt"; 	String content="This is a big text."; 	user.setDetail(path,content.getBytes()).save();  	user.getDetailAsString(); ```   ### Delete  ```java 	//delete user by primary key or unique key 	user.delete(); 	 	//SQL: DELETE FROM `user` WHERE `name`='china01' 	User.WHERE().name.eq("china01").delete(); 	 	User.DELETE().deleteAll(); 	User.DELETE().truncate();      ```   ### Update  ```java 	//update by primary key 	User user=User.SELECT().selectOne("name=?", "zzg.zhou"); 	user.setStatus(3).update(); 	 		 	//SQL: UPDATE user SET name='tsc9526' WHERE name like 'zzg%'	 	User updateTo=new User().setName("tsc9526"); 	User.WHERE().name.like("zzg%").update(updateTo); ```    ### Select  ```java 	//select by primary key 	User.SELECT().selectByPrimaryKey(1); 	 	//SQL: SELECT * FROM `user` WHERE `name` = 'zzg.zhou' 	User.SELECT().selectOne("name=?", "zzg.zhou"); 	 	//SQL: SELECT `name`, `status` FROM `user` 	User.SELECT().include("name","status").select();    	//SQL: SELECT * FROM `user` WHERE (`name` like 'zzg%' AND `status` >= 0)  	//                             OR (`name` = 'zzg' AND `status` > 1) ORDER BY `status` ASC  	for(User x:User.WHERE() 			.name.like("zzg%").status.ge(0) 			.OR() 			.name.eq("zzg").status.gt(1) 			.status.asc() 			.SELECT().select()){ //SELECT / delete / update 		System.out.println(x); 	} 	  	//Page 	Page<User> page=User.WHERE() 		.name.like("zzg%") 		.status.in(1,2,3) 		.SELECT().selectPage(10,0);	 	 ```  ### Query  ```java 	TestDB.DB.select("SELECT * FROM user WHERE name like ?","zzg%"); 	 	TestDB.DB.createQuery() 		.add("SELECT * FROM user WHERE name like ?","zzg%") 		.getList(User.class); 	  ```  ### DataTable	  ```java 	Query q=new Query(TestDB.DB); 	DataTable<DataMap> rs=q.add("SELECT * FROM user WHERE name like ?","zzg%") 	 .add(" AND status ").in(1,2,3) 	 .getList(); 	  	//Query inside DataTable 	//SQL: SELECT name, count(*) as cnt FROM _THIS_TABLE WHERE status>=0 GROUP BY name ORDER BY name ASC 	DataTable<DataMap> newTable=rs.select("name, count(*) as cnt","status>=0","name ASC","GROUP BY name");	 ```  ### Transaction  ```java 	//transaction 	Tx.execute(new Tx.Atom() { 		public int execute() { 			new User().setName("name001").setStatus(1).save(); 			new User().setName("name002").setStatus(2).save(); 			//... other database operation 			return 0; 		} 	}); ```  ### Record  ```java 	//Dynamic model: Record 	Record r=new Record("user").use(TestDB.DB); 	r.set("name", "jjyy").set("status",1) 	 .save(); 		 	//SQL: SELECT * FROM `user` WHERE (`name` like 'jjyy%' AND `status` >= 0) 	//                             OR (`name` = 'zzg' AND `status` > 1) ORDER BY `status` ASC  	for(Record x:r.WHERE() 			.field("name").like("jjyy%").field("status").ge(0) 			.OR() 			.field("name").eq("zzg").field("status").gt(1) 			.field("status").asc() 			.SELECT().select()){ 		System.out.println(x); 	}  		 	//SQL: DELETE FROM `user` WHERE `name` like 'jjyy%' AND `status` >= 0 	r.WHERE() 	 .field("name").like("jjyy%").field("status").ge(0) 	 .delete(); ```		    ### Sharding  ```java 	public class ShardingUser extends User{ 		//Override 		public Table table(){ 			String tableName= "user_"+( getId()%10 ); 			return ModelMeta.createTable(tableName); 		} 		 		//Override 		public DBConfig db(){ 			return getId()<10 ? TestDB.DB1 : TestDB.DB2; 		} 	} 	 	ShardingUser user1=new ShardingUser(1); 	user1.save(); //Will be saved to table: user_1, database: TestDB.DB1 	 	ShardingUser user2=new ShardingUser(15); 	user2.save(); //Will be saved to table: user_5, database: TestDB.DB2 ```	   ## Multi-line strings  see [Multiple-line-syntax](https://github.com/11039850/monalisa-orm/wiki/Multiple-line-syntax)  ```java 	public static void main(String[] args) { 		String name="zzg"; 		 		String lines = ""/**~!{ 			SELECT *  				FROM user 				WHERE name="$name" 		}*/; 		 		System.out.println(lines); 	} ```  Output will be:  ```sql 	SELECT *  		FROM user 		WHERE name="zzg" ```   [Details](https://github.com/11039850/monalisa-orm/wiki)  # Maven:  ```xml	 	<dependency> 		<groupId>com.tsc9526</groupId> 		<artifactId>monalisa-orm</artifactId> 		<version>2.0.0</version> 	</dependency> ```        # TODO list  * Other database's dialect * Automatic refresh the query cache in the background * ...   If you have any ideas or you want to help with the development just write me a message.  **zzg zhou**, 11039850@qq.com
GeeQuery/ef-orm	# GeeQuery ----  A Simple OR-Mapping framework on multiple databases.   * [使用手册(中文)](./manual/)<br>  * 使用示例工程   [Tutorial](./orm-tutorial/)  EF-ORM是一个轻量，便捷的Java ORM框架。并且具备若干企业级的应用特性，如分库分表、JTA事务等。  * 代码生成插件for eclipse（请在eclipse中Help/Install new software后输入地址并安装）   http://geequery.github.io/plugins/1.3.x/  ---- ## 特点  ###1. 零配置，少编码  EF的设计的一个主要目的是提高开发效率，减少编码工作，让开发者“零配置”“少编码”的操作数据库大部分功能。  例如：数据库查询条件的传入问题是所有ORM框架都不能回避的一个问题，所以我经常在想——既然我们可以用向DAO传入一个Entity来实现插入操作，为什么就不能用同样的方法来描述一个不以主键为条件的update/select/delete操作？为什么DAO的接口参数老是变来变去？为什么很多应用中，自行设计开发类来描述各种业务查询条件才能传入DAO？为什么我们不能在数据访问层上花费更少的时间和精力?  　　JPA1.0和早期的H框架，其思想是将关系型数据库抽象为对象池，这极大的限制了本来非常灵活的SQL语句的发挥空间。而本质上，当我们调用某H框架的session.get、session.load、session.delete时，我们是想传递一个以对象形式表达的数据库操作请求。只不过某H框架要求（并且限制）我们将其视作纯粹的“单个”对象而已。JPA 2.0为了弥补JPA1.0的不足，才将这种Query的思想引入为框架中的另一套查询体系——Criteria API。事实上针对单个对象的get/load/persist/save/update/merge/saveOrUpdate API和Criteria API本来就为一体，只不过是历史的原因被人为割裂成为两套数据库操作API罢了。  　　因此，对于关系型数据库而言——Entity和Query是一体两面的事物，所谓Query，可以包含各种复杂的查询条件，甚至可以作为一个完整的SQL操作请求的描述。为此，EF彻底将Entity和Query绑在了一起。这种思想，使得——  1. 开发人员需要编写的类更少。开发人员无需编写其他类来描述复杂的SQL查询条件。也无需编写代码将这些查询条件转换为SQL/HQL/JPQL。DAO层也不会有老要改来改去的接口和API，几乎可以做到零编码。  2. 对单个对象进行CRUD的操作API现在和Criteria API合并在一起。Session对象可以直接提供原本要Criteria API才能提供实现的功能。API大大简化。  3. IQueryableEntity允许你将一个实体直接变化为一个查询（Query），在很多时候可以用来完成复杂条件下的数据查询。比如 ‘in (?,?,?)’， ‘Between 1 and 10’之类的条件。   xxQL有着拼装语句可读性差、编译器无法检查、变更维护困难等问题，但是却广受开发人员欢迎。这多少有历史原因，也有Criteria API设计上过于复杂的因素。两者一方是极端灵活但维护困难，一方是严谨强大而学习和编写繁琐，两边都是极端。事实上JPA的几种数据查询方式存在青黄不接的问题。选择查询语言xxQL，项目面临后续维护困难，跨数据库移植性差；选择Criteria API，代码臃肿，操作繁琐，很多人望而却步。EF的设计思想是使人早日摆脱拼装SQL/HQL/JPQL的困扰，而是用（更精简易用的）Criteria API来操作数据库。  4. 基于轻量级Criteria API的操作方式，使得对数据库的变更和重构变得非常轻松，解决了SQL语句多对软件维护和移植造成产生的不利影响。  * **阅读推荐：第3、4章**   ###2. 将SQL的使用发挥到极致，解决SQL拼凑问题、数据库移植问题 大部分OLTP应用系统到最后都不免要使用SQL/JPQL，然而没有一个很好的方法解决SQL在多种数据库下兼容性的问题。 EF-ORM中采用了独特的SQL解析和改写技术，能够主动检查并确保SQL语句或者SQL片段在各个数据库上的兼容性。  EF中除了Criteria API以外，可以直接使用“SQL语句”或者“SQL片段”。但是这些SQL语句并不是直接传送给JDBC驱动的，而是 有着一个数据库方言层，经过方言层处理的SQL语句，就具备了在当前数据库上正确操作的能力。这相当于提供了一种能跨数据库操作的SQL语言。(E-SQL)  E-SQL不但解决了异构数据库的语法问题、函数问题、特殊的写法问题，还解决了动态SQL问题、绑定变量扩展等特性。  对于各种常用SQL函数和运算符，都可以自动转换为当前数据库支持的方言来操作。其函数支持也要多于HQL支持的函数。  * **阅读推荐：第7、8章**   ###3. 可能是业界最快的ORM框架. 得益于ASM的动态代码生成技术，部分耗时操作通过动态代码固化为硬编码实现，EF-ORM的大部分操作性能要超过已知的其他框架。      实际性能测试表明，EF的大部分操作都要快于Hiberante和MyBatis， 部分操作速度甚至数十倍于上述框架。 EF在极限插入模式下，甚至刷新了每秒10万条写入的记录。远远超过了其他框架。  一个初步的性能测试：<br> 测试代码：http://geequery.github.io/ef-orm/manual/performance-test.rar 测试报告：http://geequery.github.io/ef-orm/manual/performance-compare.docx  * **阅读推荐：第9、17章**   ###4. 分库分表 开发过程中参照了Hibernate Shards、Alibaba TDDL、Cobar等框架，也是基于词法分析器来提取SQL参数，并计算路由。    能支持分库维度含糊等场景下的分库分表。以及包括多库多表下的 order by , distinct, group by, having等操作。  * **阅读推荐：第10章**  ###5. 常用DDL操作的封装 从数据库元数据访问，到建表，创建约束，创建sequence等各种DDL操作进行了封装，用户无需编写各种SQL，可以直接通过API操作数据库结构。 尤其是ALTER TABLE等修改数据库的语句，各种不同的RDBMS都有较大语法差异。  ###6. 解决各种跨RDBMS的移植问题 1、DML操作、自增值处理与返回、查询这些不同数据库操作差异很大的东西，都了统一的封装。 2、DDL操作、建表、删表、trunacte，Sequence创建和TABLE模拟Sequence等，都做了支持。 3、对SQL语法操作和函数的改写与支持。  ----- ##其他特性  ### 轻量 该框架对应用环境、连接池、 是否为J2EE应用等没有特殊要求。可以和EJB集成，也可与Spring集成，也可以单独使用。整个框架只有两个JAR包，模块和功能都较为轻量。  ### 依赖少 整个框架只有三个jar库。间接依赖仅有commons-lang, slf4j等7个通用库，作为一个ORM框架，对第三方依赖极小。  ### 简单直接的API 框架的API设计直接面向数据库操作，不绕弯子，开发者只需要数据库基本知识，不必学习大量新的操作概念即可使用API完成各种DDL/DML操作。 最大限度利用编译器减少编码错误的可能性	API设计和元数据模型（meta-model）的使用，使得常规的数据库查询都可以直接通过Criteria API来完成，无需使用任何JPQL/HQL/SQL。可以让避免用户犯一些语法、拼写等错误。  ### JPA2规范兼容 使用JPA 2.0规范的标准注解方式来定义和操作对象。（但整个ORM不是完整的JPA兼容实现）  ### 更高的性能 依赖于ASM等静态字节码技术而不是CGlib，使得改善了代理性能；依赖于动态反射框架，内部数据处理上的开销几乎可以忽略。操作性能接近JDBC水平。对比某H开头的框架，在写入操作上大约领先30%，在大量数据读取上领先50%以上。  ### 更多的性能调优手段 debug模式下提供了大量性能日志，帮您分析性能瓶颈所在。同时每个查询都可以针对batch、fetchSize、maxResult、缓存、级联操作类型等进行调整和开关，可以将性能调到最优。  ### 可在主流数据库之间任意切换 支持Oracle、MySQL、Postgres、MSSQL、GBase、SQLite、HSQL、Derby等数据库。除了API方式下的操作能兼容各个数据库之外，就连SQL的本地化查询也能使之兼容。  ### JMX动态调节 可以用JMX查看框架运行统计。框架的debug开关和其他参数都可以使用JMX动态调整。  ### 动态表支持 表结构元数据的API也向用户开放，同时支持在使用过程中，灵活调整映射关系，因此用户可以用API动态的创建表结构的模型，从而实现各种动态类型和表的映射（例如POJO中包含一个Map，用于映射各种动态扩展的字段）  ### 企业级特性支持 SQL分析，性能统计，分库分表，Oracle RAC支持，读写分离支持
jeffdcamp/dbtools-android	DBTools for Android =================  DBTools for Android is an Android ORM library that makes it easy to work with SQLite Databases.  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.dbtools/dbtools-android/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.dbtools/dbtools-android)  **Features**  * Clean layout of of domain classes, domain manager classes, database definitions * Mature/Stable library: Used in many apps for years, and large apps with millions of users * Fast: Benchmark as one of the fastest ORMs for both reading and writing small, medium, and large sets of data * Code generator generates all of the boiler plate code. * Table change listeners (both Rx and non-Rx), to know when changes happen to specific tables * First class testing support:  All JUnit tests can run against JDBC Sqlite in the JVM * Rx support: Full support for Rx java Observables * Injection/Dagger support * Kotlin support: Generator can generate Kotlin and KotlinRx classes * Support Android Sqlite, sqlite.org sqlite, SQLCipher * SQLBuilder library to simplify building of SQL queries * Multiple database support: Easily work with many databases at the same time, and even merge databases   Usage =====  The following are some examples DBTools can be used:    * Insert          Individual individual = new Individual();         individual.setName("Jeff Campbell");         individual.setPhone("801-555-1234");         individual.setIndividualType(IndividualType.HEAD);          individualManager.save(individual);    * Update          Individual individual = individualManager.findByRowId(1);         individual.setPhone("801-555-0000");         individualManager.save(individual);    * Delete          // Delete using record object         Individual individual = individualManager.findByRowId(1);         individualManager.delete(individual);                  // Delete using the primary key id               individualManager.delete(1);                   // Delete all individuals who has "555" in their phone number         individualManager.delete(IndividualConst.C_PHONE + " LIKE ?, new String[]{"555"});             * Transactions          // Start transaction (all record managers share transactions)         individualManager.beginTransaction();           boolean success = true;          individualManager.save(individual1);         individualManager.save(individual2);         individualManager.save(individual3);         individualManager.save(individual4);         individualManager.save(individual5);          // End transaction.  (if success false, transaction is reverted)         individualManager.endTransaction(success);     DBTools Manager classes have many built-in methods that make working with tables even easier.  Here is a few examples:    * Find records          // Individual Record by primary key         Individual individual = individualManager.findByRowId(1);                  // Find FIRST individual who has "555" in their phone number         Individual individual = individualManager.findBySelection(IndividualConst.C_PHONE + " LIKE ?", new String[]{"555"});           // All Records         List<Individual> allIndividuals = individualManager.findAll();                  // All Records, ordered by the "NAME" column         List<Individual> allOrderedIndividuals = individualManager.findAllOrderBy(IndividualConst.C_NAME);                  // ALL Records who have "555" in their phone number         List<Individual> specificIndividuals = individualManager.findAllBySelection(IndividualConst.C_PHONE + " LIKE ?, new String[]{"555"});     * Using cursors          // Find all, order by NAME column         Cursor cursor = individualManager.findCursorBySelection(null, null, IndividualConst.C_NAME);                   // Find cursor of those who have "555" in their phone number         Cursor cursor = individualManager.findCursorBySelection(IndividualConst.C_PHONE + " LIKE ?, new String[]{"555"});             * Access data from Cursor          Cursor cursor;         // populate all items from cursor into Individual         Individual individual = new Individual(cursor);                   // Get data from a single field, from a cursor.  (for use in places such as Adapters, etc) Examples:         String name = Individual.getName(cursor);         IndividualType type = Individual.getType(cursor);         Date birthDate = Individual.getBirthDate(cursor);    * Count number of items in the database          // Find count of ALL records in a table         int count = individualManager.findCount();                  // Find count of ALL records who have "555" in their phone number         int count = individualManager.findCountBySelection(IndividualConst.C_PHONE + " LIKE ?, new String[]{"555"});     Table Change Listeners    * Add Listener            // Listener         individualManager.addTableChangeListener(new DBToolsTableChangeListener() {             @Override             public void onTableChange(DatabaseTableChange event) {                 onTableChange(event);             }         });  RxJava ======    * Setup      Tell DBTools to support RxJava:           dbtools {            rxJavaSupport true // support RxJava        }           Include the RxJava dependecy:          // RxJava       compile 'io.reactivex:rxandroid:<latest version>'       compile 'io.reactivex:rxjava:<latest version>'          * Usage      Managers      Use xxxRx(...) method calls on the manager classes to return an Observable            individualManager.findAllRx()               .subscribe(individual -> Log.i(TAG, "Individual: " + individual.getFirstName()));    Subscribe to table changes (following example will output 2 table changes)            public void changeNames() {             // Subscribe to TABLE changes             Subscription tableChangeSubscription = individualManager.tableChanges()                     .subscribe(changeType -> handleTableChange(changeType));                          // Make some changes             Individual individual = individualManager.findAll().get(0);             if (individual != null) {                 // change name                 individual.setFirstName("Bobby");                 individualManager.save(individual);                              // change name (again)                 individual.setFirstName("John");                 individualManager.save(individual);             }                      // Unsubscribe             tableChangeSubscription.unsubscribe();         }                  public void handleTableChange(DatabaseTableChange change) {             Log.e(TAG, "Individual Table Changed: [" + change.hasChange() + "]");         }   Setup =====  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.dbtools/dbtools-android/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.dbtools/dbtools-android)  *For a working implementation of DBTools for Android see the Android-Template application (https://github.com/jeffdcamp/android-template)    1. Add DBTools Gradle Plugin and dbtools-android dependency to build.gradle file          buildscript {             repositories {                 mavenCentral()             }             dependencies {                 classpath 'org.dbtools:gradle-dbtools-plugin:<latest-version>'             }         }          apply plugin: 'dbtools'          dependencies {             compile 'org.dbtools:dbtools-android:<latest dbtools-android version>'                          // optional dependencies             compile 'com.google.code.findbugs:jsr305:3.0.1'             androidTestCompile 'com.google.code.findbugs:jsr305:3.0.1' // fix conflicting issue with jsr305 annotations (espresso-core)         }          dbtools {             type 'ANDROID' // or 'ANDROID-KOTLIN'              basePackageName 'org.company.project.domain'             outputSrcDir 'src/main/java/org/company/project/domain'              // optional items             injectionSupport false // support for @Inject (using Dagger, Guice, JEE, etc)             jsr305Support true // support for @Notnull / @Nullable etc             includeDatabaseNameInPackage true // place each set of domain objects into a package named after its database             dateType 'DATE' // DATE, JSR-310, JODA             rxJavaSupport false // support RxJava         }    2. For new projects, create initial schema.xml files (Default: new files will be created in src/main/database)          ./gradlew dbtools-init          ... or ...                  ./gradle app:dbtools-init                  ... or ...          From Android Studio:  DOUBLE-CLICK on "dbtools-init" task from the "Gradle" Tools Window    3. Define your database: Add schema.xml file (after executing the "dbtools-init" task (from above) an XSD definition file will be created (this may help writing the XML file in some IDE's)), to the /src/main/database directory.  This file contains a list of all of the databases and tables in each database.  The following is a sample of this file:          <?xml version="1.0" encoding="UTF-8" ?>         <dbSchema xmlns='https://github.com/jeffdcamp/dbtools-gen'                   xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance'                   xsi:schemaLocation='https://github.com/jeffdcamp/dbtools-gen dbschema.xsd'>             <database name="main">                 <table name="INDIVIDUAL_TYPE" className="IndividualType" enumerations="HEAD,SPOUSE,CHILD">                     <field name="_id" jdbcDataType="BIGINT" increment="true" primaryKey="true" notNull="true"/>                     <field name="NAME" jdbcDataType="VARCHAR" size="255" notNull="true" unique="true"/>                 </table>                  <table name="INDIVIDUAL">                     <field name="_id" jdbcDataType="BIGINT" increment="true" primaryKey="true" notNull="true"/>                     <field name="INDIVIDUAL_TYPE_ID" jdbcDataType="INTEGER" varName="individualType" foreignKeyTable="INDIVIDUAL_TYPE" foreignKeyField="_id" foreignKeyType="ENUM" enumerationDefault="HEAD"/>                     <field name="NAME" jdbcDataType="VARCHAR" size="255" notNull="true"/>                     <field name="BIRTH_DATE" jdbcDataType="TIMESTAMP"/>                     <field name="PHONE" jdbcDataType="VARCHAR" size="255"/>                     <field name="EMAIL" jdbcDataType="VARCHAR" size="255"/>                 </table>             </database>         </dbSchema>    4. Use DBTools Generator to generate DatabaseManager and all domain classes.  Execute gradle task:          ./gradlew dbtools-genclasses                  ... or ...                  ./gradle app:dbtools-genclasses          ... or ...          From Android Studio:  DOUBLE-CLICK on "dbtools-genclasses" task from the "Gradle" Tools Window    DBTools Generator will create the following files to manage all database connections and create/update database tables:         DatabaseManager.java (extends DatabaseBaseManager and is used for developer customizations.  Contains CONST versions of the databases) (NEVER overwritten by generator)        DatabaseBaseManager.java (contains boiler-plate code creating all tables and views for all databases defined in the schema.xml) (this file is ALWAYS overwritten by generator)        DatabaseManagerConst.java (contains constant values of the database names) (this file is ALWAYS overwritten by generator)        AppDatabaseConfig.java (DatabaseManager configuration) (NEVER overwritten by generator)    DBTools Generator will create the following files for each table (example for the Individual table):          individual/                Individual.java (extends IndividualBaseRecord and is used for developer customizations) (NEVER overwritten by generator)                IndividualConst.java (static fields/methods for table and columns) (this file is ALWAYS overwritten by generator)                IndividualBaseRecord.java (contains boiler-plate code for doing CRUD operations and contains CONST names of the table and all columns (used to help writing queries)) (this file is ALWAYS overwritten by generator)                 IndividualManager.java (extends IndividualBaseManager and is used for developer customizations (such as adding new findByXXX(...) methods) (NEVER overwritten by generator)                IndividualBaseManager.java (contains boiler-plate code for doing CRUD operations) (this file is ALWAYS overwritten by generator)                    5. Use DBTools             IndividualManager individualManager = MainDatabaseManagers.getIndividualManager();                  Individual individual1 = new Individual();         individual1.setFirstName("Bob");         individual1.setPhone("555-555-1234");          individualManager.save(individual1);              6. Refer to sample app using DBTools: https://github.com/jeffdcamp/android-template  Documentation =============  DBTools-Android Javadoc: http://jeffdcamp.github.io/dbtools-android/javadoc/  Proguard Rules ==============      # DBTools     -dontwarn org.dbtools.query.**     -dontwarn org.sqlite.**     -dontwarn net.sqlcipher.**     -dontwarn com.squareup.otto.**      # SQLCipher (if using SQLCipher)     -keep public class net.sqlcipher.** { *; }     -keep public class net.sqlcipher.database.** { *; }      # SQLite.org (if using sqlite from sqlite.org)     -keep public class org.sqlite.** { *; }     -keep public class org.sqlite.database.** { *; }      # Threetenbp     -dontwarn org.threeten.**  Upgrade ======= Migration guide (https://github.com/jeffdcamp/dbtools-android/blob/master/MIGRATION.md)  Other Projects ============== DBTools Query - https://github.com/jeffdcamp/dbtools-query  Android Template - https://github.com/jeffdcamp/android-template  License =======      Copyright 2014 Jeff Campbell      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
florent37/RxAndroidOrm	# RxAndroidOrm  <a href="https://play.google.com/store/apps/details?id=com.github.florent37.florent.champigny">   <img alt="Android app on Google Play" src="https://developer.android.com/images/brand/en_app_rgb_wo_45.png" /> </a>  # Download  <a href='https://ko-fi.com/A160LCC' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi1.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>  [ ![Download](https://api.bintray.com/packages/florent37/maven/rxandroidorm-compiler/images/download.svg) ](https://bintray.com/florent37/maven/rxandroidorm-compiler/_latestVersion) ```java dependencies {     compile 'com.github.florent37:rxandroidorm:1.0.1'     provided 'com.github.florent37:rxandroidorm-annotations:1.0.1'     annotationProcessor 'com.github.florent37:rxandroidorm-compiler:1.0.1' } ```  A simple & fluent Android ORM, how can it be easier ? And it's compatible with RxJava2 !  ```java Observable.fromArray(                 new Computer(Computer.WINDOWS, "MasterRace", Arrays.asList(new Software("Photoshop"))),                 new Computer(Computer.WINDOWS, "Gamerz"),                 new Computer(Computer.LINUX, "MasterRace", Arrays.asList(new Software("Gimp"))))                 .flatMap(computerDb::add)                 .subscribe();  Observable.just(new Computer(Computer.MAC, "Mac Mini"))                 .flatMap(computerDb::add)                 .doOnNext(computer -> computer.getSoftwares().add(new Software("Photoshop")))                 .flatMap(computerDb::update)                 .subscribe();  computerDb.select()                 .label().equalsTo("MasterRace")                 .or()                 .softwares(SoftwareDatabase.where().name().equalsTo("Photoshop"))                  .asObservable()                 .subscribe(computers -> Log.d(TAG, computers.toString())); ```  # First, initialize !  Don't forget to initialise RxAndroidOrm in your applicarion:  ```java public class MyApplicarion extends Applicarion {      @Override public void onCreate() {         super.onCreate();         RxAndroidOrm.onCreate(this);     }  } ```  # Second, annotate your models  Use Annotations to mark classes to be persisted:  ```java @Model public class Computer {      @Id long id;     String name;         List<Software> softwares; } ```  ## Logging  You can log all SQL queries from entities managers:  ```java computerDb.logQueries((query, datas) -> Log.d(TAG, query) } ```  # TODO  - Enum support  # Credits  Author: Florent Champigny [http://www.florentchampigny.com/](http://www.florentchampigny.com/)  Blog : [http://www.tutos-android-france.com/](http://www.www.tutos-android-france.com/)  <a href="https://play.google.com/store/apps/details?id=com.github.florent37.florent.champigny">   <img alt="Android app on Google Play" src="https://developer.android.com/images/brand/en_app_rgb_wo_45.png" /> </a> <a href="https://plus.google.com/+florentchampigny">   <img alt="Follow me on Google+"        src="https://raw.githubusercontent.com/florent37/DaVinci/master/mobile/src/main/res/drawable-hdpi/gplus.png" /> </a> <a href="https://twitter.com/florent_champ">   <img alt="Follow me on Twitter"        src="https://raw.githubusercontent.com/florent37/DaVinci/master/mobile/src/main/res/drawable-hdpi/twitter.png" /> </a> <a href="https://www.linkedin.com/in/florentchampigny">   <img alt="Follow me on LinkedIn"        src="https://raw.githubusercontent.com/florent37/DaVinci/master/mobile/src/main/res/drawable-hdpi/linkedin.png" /> </a>   License --------      Copyright 2017 Florent37, Inc.      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
j256/ormlite-jdbc	This package provides the JDBC specific functionality.  You will also need to download the ormlite-core package as well.  Android users should download the ormlite-android package instead of this JDBC one.  For more information, see the online documentation on the home page:     http://ormlite.com/  Sources can be found online via Github:     https://github.com/j256/ormlite-jdbc  Enjoy, Gray Watson
goldmansachs/reladomo	# Reladomo  [![][maven img]][maven]  [![][ChangeLog img]][ChangeLog]  [![][docs img]][docs]  [![][contrib img]][contrib]  [![][license-apache img]][license-apache]  [![][travis img]][travis]  [![][release img]][release]    ## What is it?   Reladomo is an object-relational mapping (ORM) framework for Java with the following enterprise features:    * Strongly typed compile-time checked query language   * Bi-temporal chaining   * Transparent multi-schema support   * Full support for unit-testable code   * See the [documentation](https://goldmansachs.github.io/reladomo/) for more detail.    ## What can I do with it?   * Model data as objects with meaningful relationships between them   * Define classes and relationships using simple XML files   * Traverse, query, fetch, and update graphs of objects in an idiomatic object-oriented way   * Manage bi-temporal data using built-in methods   * Define, create, query, and  update data that has both business date and processing date axes   * Maintain complete and accurate audit history of changes efficiently   * Answer as-of questions such as "what did this object look like at the end of last quarter"   * Build applications as diverse as interactive web-apps to batch-processing   * Leverage transactions and batch operations to support high-performance throughput   * Detach objects to allow users to change data off-line   * Write database vendor-independent code       ## Detailed feature list   * Strongly typed compile-time checked query language   * Audit-only, Business time-series only, and Bi-temporal chaining   * Transparent multi-schema support (partition data across many databases)   * Object-oriented batch operations   * Flexible object relationship inflation   * Detached objects (allow data to be changed independently (a.k.a. delayed edit functionality) of the DB and then pushed (or reset) as and when required) - useful when users are editing data in a GUI form  * Multi-Threaded matcher Loader (MTLoader) is a high-performance pattern for merging changes from another source (file, feed, other DB, etc.) to your existing DB data. By design it is flexible/customizable and re-runnable   * Tunable caching by object type - partial, full, full off-heap  * Available meta-data - enables higher-level programming paradigms  * Multi-tier operation - obviates the need for direct DB access from client-side apps, enables better connection sharing, with no code changes required  * Full support for unit-testable code   * Databases supported include: Sybase (ASE & IQ), DB2, Oracle, Postgres, MS-SQL, H2, Derby, "generic" ...    ## Sample Project  To help getting started with Reladomo, a simple project is available with maven and gradle build set-up.    Prerequisite: install maven or gradle.    ```  git clone https://github.com/goldmansachs/reladomo.git  cd samples/reladomo-sample-simple  ```    #### Maven  ```  mvn clean install  ```    #### Gradle  ```  gradle clean build  ```    Once build is successful, run `src/main/java/sample/HelloReladomoApp` to see how it behaves.      ## Documentation    Documentation is available [online](https://goldmansachs.github.io/reladomo/)  and also included within the [Reladomo Javadoc jar file](http://search.maven.org/remotecontent?filepath=com/goldmansachs/reladomo/reladomo/16.0.0/reladomo-16.0.0-javadoc.jar).  Extract the jar file and refer to the docs below.    | Reference        | Description           | File Path  |  | ------------- |-------------| -----|  | Tutorial| This tutorial demonstrates the necessary steps to get your Reladomo project started. | userguide/ReladomoTutorial.html |  | FAQ      | Reladomo FAQ      |   mithrafaq/ReladomoFaq.html |  | Reladomo Test Resource | This document explains the steps required to use Reladomo objects in unit tests.     |   mithraTestResource/ReladomoTestResource.html |  | Reladomo Notification | When you have multiple JVMs connecting to a DB via Reladomo, you need to keep each JVM up-to-date with changes made by any of the other JVMs. Reladomo Notification is the primary mechanism for achieving this and keeping each JVMs Reladomo cache fresh.     |   notification/Notification.html |  | Reladomo Primary Key Generator | Primary key generator is an optional feature in Reladomo that allows Reladomo objects to declare how the primary key is going to be generated. | primaryKeyGenerator/PrimaryKeyGenerator.html |  | Reladomo Database Definition Generators | Database definition language (DDL) file generation is an optional feature in Reladomo that allows users to generate scripts to create tables, indices and foreign keys from the Reladomo object definition XML files. | mithraddl/ReladomoDdlGenerator.html |  | Reladomo Object XML Generator | To expedite the creation of object XML files from existing schema, an object XML file generator has been created. It connects directly to a database, retrieving a list of the existing tables and generating object XML files that appropriately map to these tables. | objectxmlgenerator/Generator.html |  | Visualize Domain Model Using Reladomo Metadata | When a persistent set of objects is specified in Reladomo metadata, the objects can be visualized. The output can be used as documentation, or simply browsed through to gain understanding of the domain. | visualization/ReladomoVisualization.html |  | Reladomo Architecture | Reladomo internal architecture. | architecture/ReladomoInternalArchitecture.html |  | Presentations | Reladomo presentation materials. | presentations |      ## Acquiring Reladomo    * [Versions](https://github.com/goldmansachs/reladomo/releases)  * [Maven Central](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.goldmansachs.reladomo%22)    [travis]:https://travis-ci.org/goldmansachs/reladomo  [travis img]:https://travis-ci.org/goldmansachs/reladomo.svg?branch=master    [license-apache]:LICENSE.txt  [license-apache img]:https://img.shields.io/badge/License-Apache%202-blue.svg    [maven]:http://search.maven.org/#search|gav|1|g:"com.goldmansachs.reladomo"  [maven img]:https://maven-badges.herokuapp.com/maven-central/com.goldmansachs.reladomo/reladomo/badge.svg    [release]:https://github.com/goldmansachs/reladomo/releases  [release img]:https://img.shields.io/github/release/goldmansachs/reladomo.svg    [docs]:https://goldmansachs.github.io/reladomo/  [docs img]:https://img.shields.io/badge/Documentation-online-brightgreen.svg    [ChangeLog]:CHANGELOG.md  [ChangeLog img]:https://img.shields.io/badge/Change%20log-markdown-blue.svg    [contrib]:CONTRIBUTING.md  [contrib img]:https://img.shields.io/badge/Contribution-guide-blue.svg
arnaudroger/SimpleFlatMapper	[![License](https://img.shields.io/github/license/arnaudroger/simpleFlatMapper.svg)](https://raw.githubusercontent.com/arnaudroger/SimpleFlatMapper/master/LICENSE) [![Build Status](https://img.shields.io/travis/arnaudroger/SimpleFlatMapper.svg)](https://travis-ci.org/arnaudroger/SimpleFlatMapper) [![Coverage Status](https://img.shields.io/coveralls/arnaudroger/SimpleFlatMapper.svg)](https://coveralls.io/r/arnaudroger/SimpleFlatMapper)  [![Java 6](https://img.shields.io/badge/java-6-orange.svg)](#java-6)[![Java 7](https://img.shields.io/badge/java-7-green.svg)](#java-7)[![Java 8](https://img.shields.io/badge/java-8-brightgreen.svg)](#java-8)[![Java 9-ea](https://img.shields.io/badge/java-9-brightgreen.svg)](#java-8)  # [Simple Flat Mapper](http://simpleflatmapper.org/)   ## [Release Notes](http://simpleflatmapper.org/10-news.html)  ## [Getting Started](http://simpleflatmapper.org/01-getting-started.html)   ## [Docs](http://simpleflatmapper.org/02-docs.html)  ## Building it  The build is using [Maven](http://maven.apache.org/).  ``` git clone https://github.com/arnaudroger/SimpleFlatMapper.git cd SimpleFlatMapper mvn install ```
florent37/Freezer	# Freezer  [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Freezer-brightgreen.svg?style=flat)](http://android-arsenal.com/details/1/3080) [![CircleCI](https://circleci.com/gh/florent37/Freezer.svg?style=svg)](https://circleci.com/gh/florent37/Freezer)  <a href="https://play.google.com/store/apps/details?id=com.github.florent37.florent.champigny">   <img alt="Android app on Google Play" src="https://developer.android.com/images/brand/en_app_rgb_wo_45.png" /> </a>  A simple & fluent Android ORM, how can it be easier ? And it's compatible with RxJava2 !  ```java UserEntityManager userEntityManager = new UserEntityManager();  userEntityManager.add(new User("Florent", 6)); userEntityManager.add(new User("Florian", 3)); userEntityManager.add(new User("Bastien", 3));  List<User> allUsers = userEntityManager.select()                              .name().startsWith("Flo")                              .asList();  userEntityManager.select()        .age().equals(3)        .asObservable()         .subscribeOn(Schedulers.newThread())        .observeOn(AndroidSchedulers.mainThread())        .subscribe(users ->           //display the users        ); ```  # First, initialize !  Don't forget to initialise Freezer in your application:  ```java public class MyApplication extends Application {      @Override public void onCreate() {         super.onCreate();         Freezer.onCreate(this);     }  } ```  # Second, annotate your models  Use Annotations to mark classes to be persisted:  ```java @Model public class User {     int age;     String name;     Cat cat;     List<Cat> pets; } ```  ```java @Model public class Cat {     @Id long id;     String name; } ```  # Now, play with the managers !  ## Persist datas  Persist your data easily:  ```java UserEntityManager userEntityManager = new UserEntityManager();  User user = ... // Create a new object userEntityManager.add(user); ```  ## Querying  Freezer query engine uses a fluent interface to construct multi-clause queries.  ### Simple  To find all users: ```java   List<User> allUsers = userEntityManager.select()                              .asList(); ```                                                    To find the first user who is 3 years old:              ```java                               User user3 = userEntityManager.select()                     .age().equalsTo(3)                     .first(); ```  ### Complex  To find all users  - with `name` "Florent" - or who own a pet with `named` "Java"       you would write:              ```java   List<User> allUsers = userEntityManager.select()                                 .name().equalsTo("Florent")                              .or()                                 .cat(CatEntityManager.where().name().equalsTo("Java"))                              .or()                                 .pets(CatEntityManager.where().name().equalsTo("Sasha"))                              .asList(); ```  ### Selectors  ```java //strings      .name().equalsTo("florent")      .name().notEqualsTo("kevin")      .name().contains("flo")      .name().in("flo","alex","logan") //numbers      .age().equalsTo(10)      .age().notEqualsTo(30)      .age().greatherThan(5)      .age().between(10,20)      .age().in(10,13,16) //booleans      .hacker().equalsTo(true)      .hacker().isTrue()      .hacker().isFalse() //dates      .myDate().equalsTo(OTHER_DATE)      .myDate().notEqualsTo(OTHER_DATE)      .myDate().before(OTHER_DATE)      .myDate().after(OTHER_DATE) ```  ### Aggregation  The `QueryBuilder` offers various aggregation methods:  ```java float agesSum      = userEntityManager.select().sum(UserColumns.age); float agesAverage  = userEntityManager.select().average(UserColumns.age); float ageMin       = userEntityManager.select().min(UserColumns.age); float ageMax       = userEntityManager.select().max(UserColumns.age); int count          = userEntityManager.select().count(); ```  ### Limit  The `QueryBuilder` offers a limitation method, for example, getting 10 users, starting from the 5th:  ```java List<User> someUsers = userEntityManager.select()                                 .limit(5, 10) //start, count                                 .asList(); ```  ## Asynchronous  Freezer offers various asynchronous methods:  ### Add / Delete / Update  ```java userEntityManager                 .addAsync(users)                 .async(new SimpleCallback<List<User>>() {                     @Override                     public void onSuccess(List<User> data) {                      }                 }); ```  ### Querying  ```java userEntityManager                 .select()                 ...                 .async(new SimpleCallback<List<User>>() {                     @Override                     public void onSuccess(List<User> data) {                      }                 }); ```  ### Observables  [With RxJava](https://github.com/ReactiveX/RxJava)  ```java userEntityManager                 .select()                 ...                 .asObservable()                 ... //rx operations                 .subscribe(new Action1<List<User>>() {                     @Override                     public void call(List<User> users) {                                          }                 }); ```  ## Entities  Freezer makes it possible, yes you can design your entities as your wish:  ```java @Model public class MyEntity {      // primitives     [ int / float / boolean / String / long / double ] field;          //dates     Date myDate;      // arrays     [ int[] / float[] / boolean[] / String[] / long[] / double ] array;           // collections     [ List<Integer> / List<Float> / List<Boolean> / List<String> / List<Long> / List<Double> ] collection;          // One To One     MySecondEntity child;          // One To Many     List<MySecondEntity> childs; } ```  ## Update  You can update a model:  ```java user.setName("laurent"); userEntityManager.update(user); ```  ## Id  You can optionnaly set a field as an identifier:  ```java @Model public class MyEntity {     @Id long id; } ``` The identifier must be a `long`  ## Ignore  You can ignore a field:  ```java @Model public class MyEntity {     @Ignore     int field;     } ```   ## Logging  You can log all SQL queries from entities managers:  ```java userEntityManager.logQueries((query, datas) -> Log.d(TAG, query) } ```  ## Migration  To handle schema migration, just add `@Migration(newVersion)` in a static method, then describe the modifications:  ```java public class DatabaseMigration {      @Migration(2)     public static void migrateTo2(Migrator migrator) {         migrator.update("User")                 .removeField("age")                 .renameTo("Man");     }      @Migration(3)     public static void migrateTo3(Migrator migrator) {         migrator.update("Man")                 .addField("birth", ColumnType.Primitive.Int);     }          @Migration(4)     public static void migrateTo4(Migrator migrator) {         migrator.addTable(migrator.createModel("Woman")                 .field("name", ColumnType.Primitive.String)                 .build());     } } ```  Migration isn't yet capable of: - changing type of field - adding/modifying One To One - adding/modifying One To Many - handling collections/arrays  # Download  <a href='https://ko-fi.com/A160LCC' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi1.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>  [ ![Download](https://api.bintray.com/packages/florent37/maven/freezer-compiler/images/download.svg) ](https://bintray.com/florent37/maven/freezer-compiler/_latestVersion) ```java buildscript {   dependencies {     classpath 'com.neenbedankt.gradle.plugins:android-apt:1.8'   } }  apply plugin: 'com.neenbedankt.android-apt'  dependencies {   compile 'fr.xebia.android.freezer:freezer:2.0.6'   provided 'fr.xebia.android.freezer:freezer-annotations:2.0.6'   apt 'fr.xebia.android.freezer:freezer-compiler:2.0.6' } ```  # Changelog  ## 1.0.1  Introduced Migration Engine.  ## 1.0.2  - Support long & double - Support arrays - Improved QueryBuilder - Refactored cursors helpers  ## 1.0.3  - Support dates - Added unit tests - Fixed one to many  ## 1.0.4  - Added @Id & @Ignore  ## 1.0.5  - Model update  ## 2.0.0  - Async API - Support Observables - Added @DatabaseName  ## 2.0.1  - Limit  ## 2.0.2  - Added query.in(...values...)  ## 2.0.3  - Freezer.onCreate is no longer dynamic  ## 2.0.5  - Improved performace for batch add & update (thanks to graphee-gabriel)  ## 2.0.6  - Add or update object if same `@Id on `add, addAll`  ## 2.1.0  - Added RxJava2 support  # A project initiated by Xebia  This project was first developed by Xebia and has been open-sourced since. We will continue working on it. We encourage the community to contribute to the project by opening tickets and/or pull requests.  [![logo xebia](https://raw.githubusercontent.com/florent37/Freezer/master/logo_xebia.jpg)](http://www.xebia.fr/)  <a href="https://play.google.com/store/apps/details?id=com.github.florent37.florent.champigny">   <img alt="Android app on Google Play" src="https://developer.android.com/images/brand/en_app_rgb_wo_45.png" /> </a>  License --------      Copyright 2015 Xebia, Inc.      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
maskarade/Android-Orma	# Android Orma [![Circle CI](https://circleci.com/gh/maskarade/Android-Orma/tree/master.svg?style=svg)](https://circleci.com/gh/maskarade/Android-Orma/tree/master) [ ![Download](https://api.bintray.com/packages/gfx/maven/orma/images/download.svg) ](https://bintray.com/gfx/maven/orma/) [![Gitter](http://badges.gitter.im/Android-Orma.svg)](https://gitter.im/Android-Orma/Lobby)  <p align="center"> <img src="Orma.png" width="256" height="256"  alt="Android Orma" /> </p>  Orma is a ORM (Object-Relation Mapper) for [Android SQLiteDatabase](http://developer.android.com/reference/android/database/sqlite/SQLiteDatabase.html). Because it generates helper classes at compile time with **annotation processing**, its query builders are type-safe.  The interface of Orma is simple and easy to use, as the author respects the Larry Wall's wisdom:  > Easy things should be easy, and hard things should be possible -- [Larry Wall](http://www.amazon.com/gp/feature.html?ie=UTF8&docId=7137)  ## Table of Contents  <!-- TOC depthFrom:2 anchorMode:github.com -->  - [Table of Contents](#table-of-contents) - [Motivation](#motivation) - [Requirements](#requirements) - [Getting Started](#getting-started) - [Synopsis](#synopsis) - [The Components](#the-components)     - [Database Handles](#database-handles)     - [Models](#models)     - [Schema Helpers](#schema-helpers)     - [Relation Helpers](#relation-helpers)     - [Selector Helpers](#selector-helpers)     - [Updater Helpers](#updater-helpers)     - [Deleter Helpers](#deleter-helpers)     - [Query Helper Methods](#query-helper-methods)         - [List of Query Helper Methods](#list-of-query-helper-methods)         - [How to Control Generation of Query Helpers](#how-to-control-generation-of-query-helpers)     - [The Inserter Helpers](#the-inserter-helpers) - [Details of Database Handles](#details-of-database-handles)     - [Configuration of Database Handles](#configuration-of-database-handles)     - [Database Handle Builders](#database-handle-builders)     - [In-Memory Database](#in-memory-database) - [Details of Models](#details-of-models)     - [Setters and Getters](#setters-and-getters)     - [Immutable Models](#immutable-models)     - [Composite Indexes](#composite-indexes)     - [Reserved Names](#reserved-names) - [RxJava Integration](#rxjava-integration) - [Associations](#associations)     - [Has-One Associations with `SingleAssociation<T>`](#has-one-associations-with-singleassociationt)     - [Direct Associations](#direct-associations)     - [Has-Many Associations with `SingleAssociation<T>`](#has-many-associations-with-singleassociationt)     - [Has-Many Associations with Direct Associations](#has-many-associations-with-direct-associations)     - [Limitations in Associations](#limitations-in-associations) - [Type Adapters](#type-adapters)     - [How Serialized Types Used](#how-serialized-types-used)     - [`@StaticTypeAdapters` for Multiple Serializers at Once](#statictypeadapters-for-multiple-serializers-at-once)     - [Built-In Type Adapters](#built-in-type-adapters)     - [Generic Type Adapters](#generic-type-adapters) - [Pagination](#pagination)     - [limit and offset](#limit-and-offset)     - [page and per](#page-and-per) - [Raw Queries](#raw-queries) - [Migration](#migration) - [DataSet Changed Events](#dataset-changed-events) - [Cooperation with Serialization Libraries](#cooperation-with-serialization-libraries) - [Encryption](#encryption) - [Example](#example) - [Benchmark](#benchmark) - [Method Count](#method-count) - [FAQ](#faq)     - [Can't build my project.](#cant-build-my-project)     - [How can I enable debug logging on release build?](#how-can-i-enable-debug-logging-on-release-build)     - [How can see the generated Java files?](#how-can-see-the-generated-java-files)     - [Does Orma work with Kotlin?](#does-orma-work-with-kotlin)     - [Does Orma work with the Jack compiler?](#does-orma-work-with-the-jack-compiler)     - [When the database handle is opened and closed?](#when-the-database-handle-is-opened-and-closed)     - [Who uses Orma?](#who-uses-orma) - [Support](#support) - [Licenses in Runtime Dependencies](#licenses-in-runtime-dependencies) - [Contribution](#contribution) - [Release Engineering for Maintainers](#release-engineering-for-maintainers) - [See Also](#see-also) - [Authors and Contributors](#authors-and-contributors) - [License](#license)  <!-- /TOC -->  ## Motivation  There are already [a lot of ORMs for Android](https://github.com/search?q=topic%3Aandroid+topic%3Aorm). Why I have to add another wheel?  The answer is that I need an ORM that has *all* the following features:  * Fast as hand-written code * POJO models   * Model classes should have no restriction   * Might implement `Parcelable` and/or extend any classes   * They should be passed to another thread * A database handle must be an object instance   * Not a static-method based class   * Even though it is designed to be used as a singleton scope * Easy migration   * Some `ALTER TABLE`, e.g. `add column` and `drop column`, should be detected and processed   * There is a wheel in Perl: [SQL::Translator::Diff](https://metacpan.org/pod/SQL::Translator::Diff) * Type safe and code completion friendly   * `db.selectFromModel()` is better than `new Select(Model.class)`   * `todos.idEq(id).toList()` is better than `todos.equalTo("id", id)` * Custom raw queries are sometimes inevitable   * `GROUP BY ... HAVING ...`   * `SELECT max(value), min(value), avg(value), count(value) FROM ...`  And now they are exactly what Orma has.  ## Requirements  * JDK 8 (1.8.0_66 or later) to build * Android API level 15 to use  ## Getting Started  Declare dependencies to use Orma and its annotation processor.  ```gradle:build.gradle dependencies {     annotationProcessor 'com.github.gfx.android.orma:orma-processor:4.2.5'     compile 'com.github.gfx.android.orma:orma:4.2.5' } ```  NOTE: if you use Android Gradle Plugin before 2.2.0, you must use [android-apt](https://bitbucket.org/hvisser/android-apt) plugin instead of `annotationProcessor` configuration.  ## Synopsis  First, define model classes annotated with `@Table`, `@Column`, and `@PrimaryKey` and run the **Build APK** command to generate helper classes.  ```java package com.github.gfx.android.orma.example;  import com.github.gfx.android.orma.annotation.Column; import com.github.gfx.android.orma.annotation.PrimaryKey; import com.github.gfx.android.orma.annotation.Table;  import android.support.annotation.Nullable;  @Table public class Todo {      @PrimaryKey(autoincrement = true)     public long id;      @Column(indexed = true)     public String title;      @Column     @Nullable // allows NULL (default: NOT NULL)     public String content;      @Column     public long createdTimeMillis; } ```  Second, instantiate a database handle `OrmaDatabase`, which is generated by `orma-processor`.  Here is an example to configure `OrmaDatabase`:  ```java // See OrmaDatabaseBuilderBase for other options. OrmaDatabase orma = OrmaDatabase.builder(context)     .name("main.db") // default: "${applicationId}.orma.db"     .build(); ```  Then, you can create, read, update and delete models via `OrmaDatabase`:  ```java Todo todo = ...;  // create orma.insertIntoTodo(todo);  // prepared statements with transaction orma.transactionSync( -> { // or transactionAsync() to execute tasks in background     Inserter<Todo> inserter = orma.prepareInsertIntoTodo();     inserter.execute(todo); });  // read orma.selectFromTodo()   .titleEq("foo") // equivalent to `where("title = ?", "foo")`   .executeAsObservable() // first-class RxJava interface   .subscribe(...);  // update orma.updateTodo()   .titleEq("foo")   .content("a new content") // to setup what are updated   .execute();  // delete orma.deleteFromTodo()   .titleEq("foo")   .execute(); ```  ## The Components  ### Database Handles  A database handle, named `OrmaDatabase` by default, is generated by `orma-processor`, which is an entry point of all the high-level database operations.  This is typically used as a singleton instance and you don't need to manage its lifecycle. That is, you don't need to explicitly close it.  ### Models  A **model** in Orma is a Java class that is annotated with `@Table`, which has at least one column, a field annotated with `@Column` or `@PrimaryKey`.  `orma-processor` generates helper classes for each model: `Schema`, `Relation`, `Selector`, `Updater`, and `Deleter`.  Because these helper classes are generated at the compile time, you can use Orma as a type-safe ORM.  ### Schema Helpers  A Schema helper, e.g. `Todo_Schema`, has metadata for the corresponding model.  This is an internal helper class and not intended to be employed by users.  ### Relation Helpers  A Relation helper, e.g. `Todo_Relation`, is an entry point of table operations.  This is created by a database handle:  ```java public static Todo_Relation relation() {   return orma.relationOfTodo(); } ```  And is able to create `Selector`, `Updater`, `Deleter`, and `Inserter` for the target model.  ```java Todo_Relation todos = orma.relationOfTodo();  todos.selector().toList(); // Todo_Selector todos.updater().content("foo").execute(); // Todo_Updater todos.inserter().execute(todo); // Inserter<Todo> todos.deleter().execute(); // Todo_Deleter ```  This can be a subset of a table which has `ORDER BY` clauses and `WHERE` clauses with some `List`-like methods:  ```java Todo_Relation todos = orma.relationOfTodo()   .doneEq(false) // can have conditions   .orderByCreatedTimeMillis(); // can have orders  // List-like features: int count = todos.count(); Todo todo = todos.get(0);  // Convenience utilities int position = todos.indexOf(todo); todos.deleteWithTransactionAsObservable()   .subscribeOn(Schedulers.io())   .observeOn(AndroidSchedulers.mainThread())   .subscribe(position -> {     notifyItemRemoved(position); // assumes Adapter#notifyItemRemoved()   }) todos.truncateWithTransactionAsObservable()   .subscribeOn(Schedulers.io())   .subscribe();  // Todo_Relation implements Iterable<Todo> for (Todo todo : todos) {   // ... } ```  And has convenience `#upsert()` to "save it anyway", returning a new model:  ```java Todo_Relation todos = orma.relationOfTodo()  Todo newTodo = todos.upsert(todo); // INSERT if it's not persistent; UPDATE Otherwise ```  Unlike `INSERT` with `OnConflict.REPLACE`, `#upsert()` doesn't break associations.  NOTE: if you use a model after `#upsert()`, you must use the returned `newModel`. This is because Orma does not change the model's primary key on `INSERT`.  ### Selector Helpers  A `Selector` helper, e.g. `Todo_Selector`, is created by a `Relation`:  ```java Todo_Selector selector = relation().selector(); // or orma.selectFromTodo(); ```  This is a query builder for `SELECT ... FROM *` statements.  ### Updater Helpers  An `Updater` helper, e.g. `Todo_Updater`, is created by a `Relation`:  ```java Todo_Updater updater = relation().updater(); // or orma.updateTodo(); ```  This is a query builder for `UPDATE *` statements.  ### Deleter Helpers  A `Deleter` helper, e.g. `Todo_Deleter`, is created by a `Relation`:  ```java Todo_Deleter deleter = relation().deleter(); // or orma.deleteFromTodo(); ```  This is a query builder for `DELETE FROM *` statements.  ### Query Helper Methods  There are **Query Helpers** which are generated to query conditions and orders in a type-safe way.  For example, `titleEq()` shown in the synopsis section, are generated to help make `WHERE` and `ORDER BY` clauses, for `Relation`, `Selector`, `Deleter`, and `Updater`.  Most of them are generated for columns with `indexed = true`, and some are for `@PrimaryKey` columns.  #### List of Query Helper Methods  Here is a list of Query Helpers that are generated for **all** the `indexed` columns, where `*` is a column name pladeholder:  | Method           | SQL                 | |:----------------:|:-------------------:| | `*Eq(value)`     | `* = value`         | | `*NotEq(value)`  | `* <> value`        | | `*In(values)`    | `* IN (values)`     | | `*NotIn(values)` | `* NOT IN (values)` |  The following are generated for `@Nullable` columns.  | Method         | SQL             | |:--------------:|:---------------:| | `*IsNull()`    | `* IS NULL`     | | `*IsNotNull()` | `* IS NOT NULL` |  The following are generated for numeric columns  (i.e. `byte`, `short`, `int`, `long`, `float`, `double`, and their corresponding box types)  | Method           | SQL                 | |:----------------:|:-------------------:| | `*Lt(value)`     | `* < value`         | | `*Le(values)`    | `* <= value`        | | `*Gt(value)`     | `* > value`         | | `*Ge(value)`     | `* >= value`        | | `*Between(a, b)` | `* BETWEEN a AND b` |  The following are generated for `TEXT` and not `PRIMARY KEY` columns.  | Method              | SQL                  | |:-------------------:|:--------------------:| | `*Glob(pattern)`    | `* GLOB pattern`     | | `*NotGlob(pattern)` | `* NOT GLOB pattern` | | `*Like(pattern)`    | `* LIKE pattern`     | | `*NotLike(pattern)` | `* NOT LIKE pattern` |  And `ORDER BY` helpers:  | Method           | SQL               | |:----------------:|:-----------------:| | `orderBy*Asc()`  | `ORDER BY * ASC`  | | `orderBy*Desc()` | `ORDER BY * DESC` |  #### How to Control Generation of Query Helpers  **This is an advanced setting for those who know what they do.**  You can control which Query Helpers are generated for a column by `@Column(helpers = ...)` attribute:  ```java @Column(     helpers = Column.Helpers.AUTO // default to AUTO ) ```  Here are the definition of options defined in [Column.java](annotations/src/main/java/com/github/gfx/android/orma/annotation/Column.java):  ```java long AUTO = -1; // the default, a smart way long NONE = 0;  long CONDITION_EQ = 0b01; long CONDITION_NOT_EQ = CONDITION_EQ << 1; long CONDITION_IS_NULL = CONDITION_NOT_EQ << 1; long CONDITION_IS_NOT_NULL = CONDITION_IS_NULL << 1; long CONDITION_IN = CONDITION_IS_NOT_NULL << 1; long CONDITION_NOT_IN = CONDITION_IN << 1;  long CONDITION_LT = CONDITION_NOT_IN << 1; long CONDITION_LE = CONDITION_LT << 1; long CONDITION_GT = CONDITION_LE << 1; long CONDITION_GE = CONDITION_GT << 1; long CONDITION_BETWEEN = CONDITION_GE << 1;  long CONDITIONS = CONDITION_EQ | CONDITION_NOT_EQ | CONDITION_IS_NULL | CONDITION_IS_NOT_NULL         | CONDITION_IN | CONDITION_NOT_IN         | CONDITION_LT | CONDITION_LE | CONDITION_GT | CONDITION_GE | CONDITION_BETWEEN;  long ORDER_IN_ASC = CONDITION_BETWEEN << 1; long ORDER_IN_DESC = ORDER_IN_ASC << 1;  long ORDERS = ORDER_IN_ASC | ORDER_IN_DESC;  long ALL = CONDITIONS | ORDERS; ```  ### The Inserter Helpers  This is a prepared statement for `INSERT INTO ...` for bulk insertions.  ```java Inserter<Todo> inserter = relation().inserter(); // or orma.insertIntoTodo()  inserter.execute(todo); inserter.executeAll(todos); ```  ## Details of Database Handles  The section describes the details of database handles.  ### Configuration of Database Handles  The database class is configured by the [`@Database`](https://github.com/gfx/Android-Orma/blob/master/annotations/src/main/java/com/github/gfx/android/orma/annotation/Database.java) annotation:  ```java @Database(     databaseClassName = "OrmaDatabase", // default to "OrmaDatabase"     includes = { /* ... */ } // Give model classes to handle     excludes = { /* ... */ } // Give model classes not to handle ) public class DatabaseConfiguration { } ```  The annotated class is not used for now, but the package is used to place the OrmaDatabase class.  ### Database Handle Builders  `OrmaDatabase.builder(Context)` returns a builder isntance, which has configure the database handle instance:  | Method                 | Description                 | Default             | |:----------------------:|:---------------------------:|:-------------------:| | `name(String)`         | The filename of SQLite DB   | `"${package}.orma.db"` | | `migrationEngine(MigrationEngine)`| Custom migration engine | `OrmaMigration`  | | `writeAheadLogging(boolean)`  | SQLite WAL flag      | `true`              | | `foreignKeys(boolean)` | SQLite FOREIGN_KEYS flag    | `true`              | | `migrationStep(int, ManualStepMigration.Step)` | A migration step | none   | | `trace(boolean)`       | Output executed queries to logcat if true | dynamic (*1) | | `readOnMainThread(AccessThreadConstraint)`  | Check read operation on main thread | dynamic (*2) | | `writeOnMainThread(AccessThreadConstraint)` | Check write operation on main thread | dynaimc (*3) |  * **\*1** `BuildConfig.DEBUG ? true : false` * **\*2** `BuildConfig.DEBUG ? WARN : NONE` * **\*3** `BuildConfig.DEBUG ? FATAL : NONE`  Note that **Orma aborts if writing occurs on main thread** in debug build.  Use background threads, e.g. via `AsyncTask` for writing, or RxJava interfaces with `Schedulers.io()`.  Otherwise you can disable this behavior:  ```java OrmaDatabase orma = OrmaDatabase.builder(context)     .writeOnMainThread(AccessThreadConstraint.NONE)     .build(); ```  ### In-Memory Database  You can create in-memory databases by passing `null` to `OrmaDatabase.Builder#name()`.  This is useful for testing.  ## Details of Models  The section describes the details of model definition.  ### Setters and Getters  Orma can use getters and setters if columns have corresponding methods.  You can also connect getters and setters with `@Getter` and `@Setter` respectively, which tells `orma-processor` to use accessors.  Each accessor name can have a column name in SQLite databases, which is inferred from its method name if omitted.  ```java @Table public class KeyValuePair {      static final String kKey = "Key";      @Column(kKey) // specifies the name     private String key;      @Column // omits the name     private String value;      @Getter(kKey)     public String getKey() {         return key;     }      @Setter(kKey)     public void setKey(String key) {         this.key = key;     }      // used as a getter for the "value" column     // @Getter is optional in this case     public String getValue() {         return value;     }      // used as a setter for the "value" column     // @Setter is optional in this case     public void setValue(String value) {         this.value = value;     } } ```  ### Immutable Models  Immutable models, where all the fields are declared with `final`, are supported by annotating a constructor with `@Setter`.  ```java @Table public class KeyValuePair {      @Column     public final String key;      @Column     public final String value;      @Setter     KeyValuePair(String key, String value) {         this.key = key;         this.value = value;     } } ```  It can be declared with custom names:  ```java @Table public class KeyValuePair {     static final String kKey = "Key";     static final String kValue = "Value";      @Column(kKey)     public final String key;      @Column(kValue)     public final String value;      KeyValuePair(@Setter(kKey) String key, @Setter(kValue) String value) {         this.key = key;         this.value = value;     } } ```  ### Composite Indexes  There is the `indexes` parameter that `@Table` takes in order to create composite indexes (a.k.a. multi-column indexes).  ```java // for CREATE INDEX: @Table(indexes = @Index(value = {"resourceType", "resourceId"})) public class Entry {      @PrimaryKey     public long id;      @Column     public String resourceType;      @Column     public long resourceId; } ```   ```java // for CREATE UNIQUE INDEX: @Table(     indexes = @Index(                 value = {"resourceType", "resourceId"},                 unique = true         ) ) public class Entry {      @PrimaryKey     public long id;      @Column     public String resourceType;      @Column     public long resourceId; } ```  Composite indexes generate query helper methods only for `==` and `ORDER BY` for helper classes like the following:  * `Selector#resourceTypeAndResourceIdEq(String, long)` * `Selector#orderByResourceTypeAndResourceIdAsc()` * `Selector#orderByResourceTypeAndResourceIdDesc()`  You can control generated helpers with the `helpers` parameter.  See also [Query Helper Methods](#query-helper-methods).  ### Reserved Names  Column names starting `$` are reserved for metadata.  Other names, including SQLite keywords, are allowed.  ## RxJava Integration  RxJava integration provides a set of powerful API to transform, filter, and combine DB rows.  For example, there is a model named `Book` with `@Column(unique = true) String title` and you'd like to get a `Map<String, Book>` where the key is `Book#title`:  ```java Map<String, Book> map = db.selectFromBook()     .executeAsObservable()         .toMap(new Function<Book, String>() {             @Override             public String apply(Book book) throws Exception {                 return book.title;             }         }).blockingGet(); ```  ## Associations  Two or more Orma models can be associated with **association** mechanism.  There are two type of associations: **has-one** and **has-many**.  In addition, there are another two kind of association supports: indirect associations with `SingleAssociation<T>` and direct associations.  ### Has-One Associations with `SingleAssociation<T>`  There is `SingleAssociation<T>` to support has-one associations, which is retrieved on demand; this is useful if the associations are not always used.  For example, a book has a publisher:  ```java @Table class Publisher {   @PrimaryKey   public long id; }  @Table class Book {      @Column     public SingleAssociation<Publisher> publisher; } ```  To save this a book:  ```java Book book = new Book(); Publisher publisher = new Publisher();  long publisherId = db.insertIntoPublisher()     .execute(publisher);  // if publisher has a valid primary key, `just(publisher)` is also okay. book.publisher = SingleAssociation.just(publisherId);  db.insertIntoBook()     .execute(book) ```  To get a publisher from books:  ```java db.selectFromBook()     .forEach((book) -> {         // blocking:         Publisher publisher = book.publisher.get();          // with RxJava Single<Publisher>:         book.publisher.single()             .subscribe((publisher) -> {                 // use publisher             })     }); ```  ### Direct Associations  There are _direct associations_, where an Orma model has another Orma model directly.  Given a `has-one` association, `Book has-one Publisher`:  ```java @Table class Publisher {   @PrimaryKey   public long id;    @Column   public String name; }  @Table class Book {      @PrimaryKey     public long id;      @column     public String title;      @Column     public Publisher publisher; } ```  The corresponding table definition is something like this:  ```sql CREATE TABLE `Publisher` (   `id` INTEGER PRIMARY KEY,   `name` TEXT NOT NULL ) CREATE TABLE `Book` (   `id` INTEGER PRIMARY KEY,   `title` TEXT NOT NULL,   `publisher` INTEGER NOT NULL     REFERENCES `Publisher`(`id`) ON UPDATE CASCADE ON DELETE CASCADE ) ```  In SQL, `Book#publisher` refers `Publisher#id`, indicating the two tables should be joined in `SELECT` statements.  In Java, `Book#publisher` is a `Publisher` instance, which is retrieved in each `SELECT` operations. There is no lazy loading in direct associations.  ### Has-Many Associations with `SingleAssociation<T>`  Has-many associations are not directly supported but you can define a method to get associated objects:  ```java @Table class Publisher {     @PrimaryKey     public long id;      // NOTE: If OrmaDatabase is a singleton, no parameter is required!     public Book_Relation getBooks(OrmaDatabase db) {         return db.relationOfBook().publisherEq(this);     } }  @Table class Book {      @Column(indexed = true)     public SingleAssociation<Publisher> publisher;  } ```  ### Has-Many Associations with Direct Associations  As `SingleAssociation` is, you can define a helper method to get has-many associations:  ```java @Table class Publisher {     @PrimaryKey     public long id;      // NOTE: If OrmaDatabase is a singleton, no parameter is required!     public Book_Relation getBooks(OrmaDatabase db) {         return db.relationOfBook().publisherEq(this);     } }  @Table class Book {      @Column(indexed = true)     public Publisher publisher;  } ```  ### Limitations in Associations  * There are no methods to query associated models  These issues will be fixed in a future.  ## Type Adapters  Orma models are able to have embedded objects with **type adapters**, a.k.a. static type adapters, by defining classes with `@StaticTypeAdapter` annotation.  For example, if you want to embed [LatLng](https://developers.google.com/android/reference/com/google/android/gms/maps/model/LatLng) in your Orma model, you can define a type adapter like this:  ```java @StaticTypeAdapter(     targetType = LatLng.class, // required     serializedType = String.class // required ) public class LatLngAdapter {      // SerializedType serialize(TargetType source)     @NonNull     public static String serialize(@NonNull LatLng source) {         return source.latitude + "," + source.longitude     }      // TargetType deserialize(SerializedType serialized)     @NonNull     public static LatLng deserialize(@NonNull String serialized) {         String[] values = serialized.split(",");         return new LatLng(             Double.parseDouble(values[0]),             Double.parseDouble(values[1]));     } } ```  `@StaticTypeAdapter` requires `targetType` and `serializedType` options and two static methods `SerializedType serialize(TargetType)` and `TargetType deserialize(SerializedType)`.  ### How Serialized Types Used  A `@StaticTypeAdapter#serializedType` is bound to an SQLite storage type. Thus it must be one of the "Java Type" listed the table below, where each "Java Type" has a corresponding "SQLite Type":  | Java Type | SQLite Type | |:---------:|:-----------:| | int       | INTEGER     | | short     | INTEGER     | | long      | INTEGER     | | boolean   | INTEGER     | | float     | REAL        | | double    | REAL        | | String    | TEXT        | | byte[]    | BLOB        |  ### `@StaticTypeAdapters` for Multiple Serializers at Once  You can also define multiple type serializers to single class with `@StaticTypeAdapters` annotation containers:  ```java @StaticTypeAdapters({     @StaticTypeAdapter(         targetType = MutableInt.class,         serializedType = int.class,         serializer = "serializeMutableInt",         deserializer = "deserializeMutableInt"     ),     @StaticTypeAdapter(         targetType = MutableLong.class,         serializedType = long.class,         serializer = "serializeMutableLong",         deserializer = "deserializeMutableLong"     ) }) public class TypeAdapters {      public static int serializeMutableInt(@NonNull MutableInt target) {         return target.value;     }      @NonNull     public static MutableInt deserializeMutableInt(int deserialized) {         return new MutableInt(deserialized);     }      public static long serializeMutableLong(@NonNull MutableLong target) {         return target.value;     }      @NonNull     public static MutableLong deserializeMutableLong(long deserialized) {         return new MutableLong(deserialized);     } } ```  ### Built-In Type Adapters  There are built-in type adapters for typically used value objects and collections:  * `java.math.BigDecimal` * `java.math.BigInteger` * `java.nio.ByteBuffer` * `java.util.Currency` * `java.util.Date` * `java.sql.Date` * `java.sql.Time` * `java.sql.Timestamp` * `java.util.UUID` * `java.util.List<String>` * `java.util.ArrayList<String>` * `java.util.Set<String>` * `java.util.HashSet<String>` * `android.net.Uri`  ### Generic Type Adapters  If your `deserialize()` takes a `Class<T>` parameter, the type serializer is _generic_, handling classes with the common base classe.  For example, if you have some enums that implement `EnumDescription`, e.g. `T extends Enum<T> & EnumDescription`, you can handle it with a generic type adapter.  Given an interface `EnumDescription`:  ```java public interface EnumDescription {      long getValue(); } ```  And here is its type adapter:   ```java @StaticTypeAdapter(         targetType = EnumDescription.class,         serializedType = long.class ) public class EnumTypeAdapter {      public static <T extends Enum<T> & EnumDescription> long serialize(@NonNull T value) {         return value.getValue();     }      @NonNull     public static <T extends Enum<T> & EnumDescription> T deserialize(long serialized, @NonNull Class<T> type) {          for (T enumValue : type.getEnumConstants()) {             if (enumValue.getValue() == serialized) {                 return enumValue;             }         }          throw new RuntimeException("Unknown id: " + serialized + " for " + type);     } } ```  Now `deserialize()` uses the type information for the conclete target class.  ## Pagination  There are two style pagination. You can use either, but not mixed.  ### limit and offset  SQL style pagination:  ```java for (Todo todo : orma.selectFromTodo().titleEq("buy").offset(0).limit(10)) {     // ... } ```  ### page and per  "paging" style pagination inspired from Ruby's [kaminari](https://github.com/kaminari/kaminari).  ```java for (Todo todo : orma.selectFromTodo().titleEq("buy").page(1).per(10)) {     // ... } ```  Note that `page` starts from 1.  ## Raw Queries  For low-level operations, e.g. executing a raw query, you can use `OrmaDatabase#getConnection()`, which returns `OrmaConnection`.  For example:  ```java Cursor cursor = db.getConnection().rawQuery("SELECT max(bookId) as max_id, min(bookId) as min_id FROM Book"); cursor.moveToFirst(); // get data from cursor cursor.close(); ```  NOTE: Don't use `rawQuery()` for performance because Orma query builders are fast enough.  ## Migration  There is a pluggable migration mechanism via the `MigrationEngine` interface.  The default migration engine is `SchemaDiffMigration`, which handles schema changes by making diff with old and new DDL stored in `sqlite_master`. That is, you don't need migration steps for the following cases:  * Adding tables * Adding columns (In this case, you need to add `defaultExpr` or `@Nullable` to new columns for auto-migration to work) * Changing column types * Changing column constraints (`NOT NULL`, `UNIQUE`, and etc.)  Of course, you can define migration steps for each schema version (or `BuildConfig.VERSION`). If you add   Here is an example to define migration steps:  ```java int VERSION_2; // a past version of VERSION_CODE  OrmaDatabase orma = OrmaDatabase.builder(this)         .migrationStep(VERSION_2, new ManualStepMigration.ChangeStep() {             @Override             public void change(@NonNull ManualStepMigration.Helper helper) {               Log.(TAG, helper.upgrade ? "upgrade" : "downgrade");               helper.execSQL("DROP TABLE foo");               helper.execSQL("DROP TABLE bar");             }         })         // ... other configurations         .build(); ```  See [migration/README.md](migration/README.md) for details.  ## DataSet Changed Events  NOTE: **This is experimental in v4.2.5: its existence, signature or behavior might change without warning from one release to the next.**  `Relation#createQueryObservable()` can create a event stream to observe data-set changed events for the relation.  This likes [SQLBrite](https://github.com/square/sqlbrite)'s' "Query Observable", whereas Orma's does not notify the initial event.  ```java // NOTE: Keep the observable instance. If it's released, the observable is disposed.  // create a query observable, which is a hot observable Observable<Author_Selector> observable = db.relationOfAuthor()         .createQueryObservable();  // subscribe the events observable.flatMap(new Function<Author_Selector, Observable<Author>>() {     @Override     public Observable<Author> apply(Author_Selector selector) throws Exception {         Log.d(TAG, "Author has been changed!");         return selector.executeAsObservable();     } })         .map(new Function<Author, String>() {             @Override             public String apply(Author author) throws Exception {                 return author.name;             }         })         .subscribe(new Consumer<String>() {             @Override             public void accept(String name) throws Exception {                 Log.d(TAG, "name: " + name);             }         }); ```  See `OrmaListAdapter` and `OrmaRecyclerViewAdapter`, which use Query Observables to trigger `#notifyDataSetChanged()`.  * [OrmaListAdapter](https://github.com/gfx/Android-Orma/blob/master/library/src/main/java/com/github/gfx/android/orma/widget/OrmaListAdapter.java) * [OrmaRecyclerViewAdapter](https://github.com/gfx/Android-Orma/blob/master/library/src/main/java/com/github/gfx/android/orma/widget/OrmaRecyclerViewAdapter.java)  ## Cooperation with Serialization Libraries  Beause Orma reuqires nothing to do to models, serializers, e.g. Android Parcels or GSON, can serialize Orma models.  ## Encryption  There's an encryption extension as `orma-encryption` since Orma v5.0.0-rc1:  ```build.gradle dependencies {     compile 'com.github.gfx.android.orma:orma-encryption:5.0.0-rc1' } ```  That provies `EncryptedDatabase`:  ```java String password = "..."; OrmaDatabase orma = OrmaDatabase.builder(context)     .provider(new EncryptedDatabase.Provider(password))     // ...     .build(); ```  Encrypted database are managed by [SQLCipher](https://github.com/sqlcipher/android-database-sqlcipher), so the database files are not compatible with non-encrypted ones.  Note that with this extension the database handle throws `net.sqlcipher.database.SQLException` instead of `android.database.SQLException` as runtime exceptions, so it might not be 100% compatible with the default database.  ## Example  There is [an example app](example/) to demonstrate:  * Migration * Orma with `RecyclerView` / `ListView` * Benchmark (see below)  ## Benchmark  There is a simple benchmark with [Realm](https://github.com/realm/realm-java) and hand-written SQLiteDatabase code:  [example/BenchmarkFragment](example/src/main/java/com/github/gfx/android/orma/example/fragment/BenchmarkFragment.java)  Here is a result performed on Android 6.0.0 / Xperia Z4 as of Orma v4.2.5 and Realm 2.3.0, processing 10 items x 100 times:  <img src="benchmark.png" alt="" width="420"/>  I welcome benchmark in another condition and/or another code.  ## Method Count  Orma runtime is very lightweight: [Method Count for v4.2.5](http://www.methodscount.com/?lib=com.github.gfx.android.orma:orma:4.2.5)  ## FAQ  ### Can't build my project.  Check your toolchain. FYI here are my toolchain versions:  * JDK 1.8.0_66 * Android SDK Tools 25 or later * Android SDK Platform Tools 24 or later * Android SDK Build Tools 24 or later * Android Gradle Plugin 2.2.0 or later  ### How can I enable debug logging on release build?  Call `OrmaDatabase.Builder#trace(boolean)` with `true`:  ```java OrmaDatabase orma = OrmaDatabase.builder(context)     .trace(true)     .create(); ```  This option also enables logging in the default migration engine.  If you give a custom migration engine to the orma builder, you have to enable `trace` flag to its constructor:  ```java boolean trace = true; SchemaDiffMigration migration = new SchemaDiffMigration(context, trace); ```  ### How can see the generated Java files?  As other annotation processors do, Orma save files to `$modle/build/generated/source/apt/`.  You can see [generated files for example models](example/build/generated/source/apt/debug/com/github/gfx/android/orma/example/orma).  ### Does Orma work with Kotlin?  Yes, but it's _experimental_. Here is an example to use Orma with Kotlin:  https://github.com/gfx/OrmaWithKotlin  NOTE: Kotlin APT support, a.k.a. _kapt_, is **really unstable**. Don't ask me how to solve kapt problems.  ### Does Orma work with the Jack compiler?  Yes. As of Android Gradle Plugin 2.2.2, Orma should work with Jack.  ```gradle:build.gradle dependencies {     annotationProcessor 'com.github.gfx.android.orma:orma-processor:4.2.5'     compile 'com.github.gfx.android.orma:orma:4.2.5' } ```  See https://github.com/gfx/OrmaWithJack for a working example.  ### When the database handle is opened and closed?  Orma opens the database handle in instantiating `OrmaDatabase`, and you don't need to close it.  In other word, you can define the database handle as a singleton instance in your application scope, and forget `close`.  ### Who uses Orma?  Here is a list of open-source Androdi apps using Orma which are released to Google Play:  * [gfx/Android-Helium](https://github.com/gfx/Android-Helium) * [DroidKaigi/conference-app-2017](https://github.com/DroidKaigi/conference-app-2017)  Or you can search use cases by [GitHub search](https://github.com/search?o=desc&q=%22com.github.gfx.android.orma%22+filename%3Abuild.gradle&ref=searchresults&s=indexed&type=Code&utf8=%E2%9C%93).  Also, here is a list of apps using Orma which are proprietary:  * [Cookpad (ja)](https://play.google.com/store/apps/details?id=com.cookpad.android.activities) * [Abema TV (ja)](https://play.google.com/store/apps/details?id=tv.abema)  Tell me if your projects use Orma!  ## Support  * Use [GitHub issues](https://github.com/gfx/Android-Orma/issues) for the issue tracker * Feel free to ask for questions to the author [@\_\_gfx\_\_](https://twitter.com/__gfx__) * Gitter Rooms:     * https://gitter.im/Android-Orma/Lobby (en)     * https://gitter.im/Android-Orma/Ja (ja)  ## Licenses in Runtime Dependencies  * https://github.com/ReactiveX/RxJava - Apache Software License 2.0 * [bkiers/sqlite-parser](https://github.com/bkiers/sqlite-parser) - The MIT License   * The original code of [SQLite.g4](sqliteparser/src/main/antlr/com/github/gfx/android/orma/sqliteparser/g/SQLite.g4)  ## Contribution  Patches are welcome!  ## Release Engineering for Maintainers  ```shell ./gradlew bumpMajor # or bumpMinor / bumpPatch git add -va make publish # run tests, build artifacts, publish to jcenter, and make a tag ```  Visual Studio Code (a.k.a. vscode) is recommended to edit README.md and CHANGELOG.md. Especially the ToC section is managed by [AlanWalk/Markdown-TOC](https://github.com/AlanWalk/Markdown-TOC).  ## See Also  * [SQLite](http://sqlite.org/) * [SQLiteDatabase](http://developer.android.com/reference/android/database/sqlite/SQLiteDatabase.html) * [Version of SQLite used in Android? - Stack Overflow](http://stackoverflow.com/questions/2421189/version-of-sqlite-used-in-android)  ## Authors and Contributors  FUJI Goro ([gfx](https://github.com/gfx)).  And contributors are listed here: [Contributors](https://github.com/gfx/Android-Orma/graphs/contributors)  ## License  Copyright (c) 2015 FUJI Goro (gfx).  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
ebean-orm/ebean	# Need help? Post questions or issues to the Ebean google group - https://groups.google.com/forum/#!forum/ebean  # Documentation Goto [https://ebean-orm.github.io/](http://ebean-orm.github.io/ "Ebean ORM's Website")   ## Maven cental links: [Maven central - ebean](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22io.ebean%22%20AND%20a%3A%22ebean%22 "maven central ebean")  [Maven central - all related projects](http://search.maven.org/#search%7Cga%7C1%7Cebean "maven central all related projects")  ## Current versions * [![Maven Central : ebean](https://maven-badges.herokuapp.com/maven-central/io.ebean/ebean/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.avaje.ebean/ebean) - ebean * [![Maven Central : ebean-agent](https://maven-badges.herokuapp.com/maven-central/io.ebean/ebean-agent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.avaje.ebean/ebean-agent) - ebean-agent * [![Maven Central : ebean-maven-plugin](https://maven-badges.herokuapp.com/maven-central/io.ebean/ebean-maven-plugin/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.avaje.ebean/ebean-maven-plugin) - ebean-maven-plugin
speedment/speedment	Speedment is a Java 8 Stream ORM Toolkit and Runtime ====================================================  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.speedment/runtime/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.speedment/runtime) [![Javadoc](https://javadoc-emblem.rhcloud.com/doc/com.speedment/runtime-deploy/badge.svg)](http://www.javadoc.io/doc/com.speedment/runtime-deploy) [![Build Status](https://travis-ci.org/speedment/speedment.svg?branch=develop-3.0)](https://travis-ci.org/speedment/speedment) [![Hex.pm](https://img.shields.io/hexpm/l/plug.svg?maxAge=2592000)](https://raw.githubusercontent.com/speedment/speedment/master/LICENSE) [![Join the chat at https://gitter.im/speedment/speedment](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/speedment/speedment?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  <img src="https://raw.githubusercontent.com/speedment/speedment-resources/master/src/main/resources/wiki/frontpage/Forest.png" alt="Spire the Hare" title="Spire" align="right" width="240px" />  The toolkit analyzes the metadata of an existing legacy SQL database  and creates a Java representation of the data model which together with  the Speedment runtime allows the user to create scalable and efficient  Java applications using **standard Java 8** streams without any specific query language or any new API.   ### One-liner Search for a long film (of length greater than 120 minutes): ```java // Searches are optimized in the background! Optional<Film> longFilm = films.stream()     .filter(Film.LENGTH.greaterThan(120))     .findAny(); ```   Results in the following SQL query: ```sql SELECT      `film_id`,`title`,`description`,`release_year`,     `language_id`,`original_language_id`,`rental_duration`,`rental_rate`,     `length`,`replacement_cost`,`rating`,`special_features`,     `last_update`  FROM       FROM `sakila`.`film WHERE     (`length` > 120) ```  No need for manually writing SQL-queies any more. Remain in a pure Java world!  ### Expressing SQL as Java 8 Streams When we started the open-source project Speedment, the main objective was to remove the polyglot requirement for Java database application developers. After all, we all love Java and why should we need to know SQL when, instead, we could derive the same semantics directly from Java streams? When one takes a closer look at this objective, it turns out that there is a remarkable resemblance between Java streams and SQL as summarized in this simplified table:  | SQL         | Java 8 Stream Equivalent          | | :---------- | :-------------------------------- | | `FROM`       | `stream()`   | | `SELECT`     | `map()`      | | `WHERE`      | `filter()` (before collecting) | | `HAVING`     | `filter()` (after collecting) | | `JOIN`       | `flatMap()`  | | `DISTINCT`   | `distinct()` | | `UNION`      | `concat(s0, s1).distinct()` | | `ORDER BY`   | `sorted()`   | | `OFFSET`     | `skip()`     | | `LIMIT`      | `limit()`    | | `GROUP BY`   | `collect(groupingBy())` | | `COUNT`      | `count()`    |   ## Documentation  You can read the online [Speedment User's Guide here](https://speedment.github.io/speedment-doc/introduction.html)!  ## Tutorials  The tutorials are divided into three sections. The basics are covered in the first section without any expected prior knowledge of Speedment. This builds a foundation of knowledge needed to fully benefit from the following tutorials.  ### Basics * [Tutorial 1 - Set up the IDE](https://github.com/speedment/speedment/wiki/Tutorial:-Set-up-the-IDE) * [Tutorial 2 - Get started with the UI](https://github.com/speedment/speedment/wiki/Tutorial:-Get-started-with-the-UI) * [Tutorial 3 - Hello Speedment](https://github.com/speedment/speedment/wiki/Tutorial:-Hello-Speedment) * [Tutorial 4 - A First Stream from Speedment](https://github.com/speedment/speedment/wiki/Tutorial:-A-First-Stream-from-Speedment)  ### Sample applications * [Tutorial 5 - Speedment Spring Boot Integration; REST assured - it is easy](https://github.com/speedment/speedment/wiki/Tutorial:-Speedment-Spring-Boot-Integration) * [Tutorial 6 - Speedment filters based on Json Web Tokens](https://github.com/speedment/speedment/wiki/Tutorial:-Speedment-Stream-Filters-Using-JWT-Data) * [Tutorial 7 - Build a Social Network](https://github.com/speedment/speedment/wiki/Tutorial:-Build-a-Social-Network) * [Tutorial 8 - Log errors in a database](https://github.com/speedment/speedment/wiki/Tutorial:-Log-errors-in-a-database) * [Tutorial 9 - Use Speedment with Java EE](https://github.com/speedment/speedment/wiki/Tutorial:-Use-Speedment-with-Java-EE) * [Tutorial 10 - Create Event Sourced Systems](https://github.com/speedment/speedment/wiki/Tutorial:-Create-an-Event-Sourced-System)  ### Extending Speedment * [Tutorial 11 - Writing your own extensions](https://github.com/speedment/speedment/wiki/Tutorial:-Writing-your-own-extensions) * [Tutorial 12 - Plug-in a Custom TypeMapper](https://github.com/speedment/speedment/wiki/Tutorial:-Plug-in-a-Custom-TypeMapper)  Quick Start ----------- Assuming you have Maven installed and a relational database available, you can try out Speedment in a minute either by setting up a POM file for your project or launching a Maven archetype template.  #### Setup a POM file Use the Speedment [Initializer](https://www.speedment.com/initializer/) to get a POM template for your project.  #### Launch Archtype Directly Running the following from a command-line:  ###### MySQL ``` mvn archetype:generate -DgroupId=com.company -DartifactId=speedment-demo -DarchetypeArtifactId=speedment-archetype-mysql -DarchetypeGroupId=com.speedment.archetypes -DinteractiveMode=false -DarchetypeVersion=3.0.11 && cd speedment-demo && mvn speedment:tool ```  ###### PostgreSQL ``` mvn archetype:generate -DgroupId=com.company -DartifactId=speedment-demo -DarchetypeArtifactId=speedment-archetype-postgresql -DarchetypeGroupId=com.speedment.archetypes -DinteractiveMode=false -DarchetypeVersion=3.0.11 && cd speedment-demo && mvn speedment:tool ```  ###### MariaDB ``` mvn archetype:generate -DgroupId=com.company -DartifactId=speedment-demo -DarchetypeArtifactId=speedment-archetype-mariadb -DarchetypeGroupId=com.speedment.archetypes -DinteractiveMode=false -DarchetypeVersion=3.0.11 && cd speedment-demo && mvn speedment:tool ```  A graphical dialog will prompt for database connection details.  1. Enter database name and credentials and press **Connect**. 2. Press the **Generate** button and then quit the tool.   Now you have a demo project set up with generated application code in the directory `speedment-demo`. To learn more about how to leverage the generated Speedment classes and the Speedment runtime in your project, please see the following tutorials and guides.  Examples -------- Here are a few examples of how you could use Speedment from your code assuming that you have an exemplary MySQL database called "Sakila" avaiable. Sakila can be downloaded directly form Oracle [here](https://dev.mysql.com/doc/index-other.html)    ### Query with Optimised Stream Predicate Short-Circuit Search for a long film (of length greater than 120 minutes): ```java // Searches are optimized in the background! Optional<Film> longFilm = films.stream()     .filter(Film.LENGTH.greaterThan(120))     .findAny(); ```   Results in the following SQL query: ```sql SELECT      `film_id`,`title`,`description`,`release_year`,     `language_id`,`original_language_id`,`rental_duration`,`rental_rate`,     `length`,`replacement_cost`,`rating`,`special_features`,     `last_update`  FROM       FROM `sakila`.`film WHERE     (`length` > 120) ```  ### Query with Optimised Paging Show page 3 of PG-13 rated films sorted by length: ```java private static final long PAGE_SIZE = 50;  // Even complex streams can be optimized! long page = 3; List<Film> stream = films.stream();     .filter(Film.RATING.equal("PG-13"));     .sorted(Film.LENGTH.comparator())     .skip(page * PAGE_SIZE)     .limit(PAGE_SIZE);     .collect(toList()); ```   Results in the following SQL query: ```sql SELECT      `film_id`,`title`,`description`,`release_year`,     `language_id`,`original_language_id`,`rental_duration`,`rental_rate`,     `length`,`replacement_cost`,`rating`,`special_features`,     `last_update`  FROM      `sakila`.`film`  WHERE      (`rating`  = 'PG-13' COLLATE utf8_bin)  ORDER BY      `sakila`.`film`.`length` ASC  LIMIT 50 OFFSET 150; ```  ### Classification Create a Map with film ratings and the corresponding films: ``` Map<String, List<Film>> map = films.stream()     .collect(         Collectors.groupingBy(             // Apply this classifier             Film.RATING.getter()         )     ); ``` This will produce a Map like this: ``` Rating PG-13 maps to 223 films  Rating R     maps to 195 films  Rating NC-17 maps to 210 films  Rating G     maps to 178 films  Rating PG    maps to 194 films  ```  ### Joins Construct a Map with all film languages and the corresponding films: ```java Map<Language, List<Film>> languageFilmMap = films.stream()     .collect(         // Apply this foreign key classifier         groupingBy(languages.finderBy(Film.LANGUAGE_ID))     ); ```  ### Many-to-many Construct a Map with all Actors and the corresponding Films they have acted in: ```java Map<Actor, List<Film>> filmographies = filmActors.stream()     .collect(         groupingBy(actors.finderBy(FilmActor.ACTOR_ID), // Applies the FilmActor to ACTOR classifier             mapping(                 films.finderBy(FilmActor.FILM_ID), // Applies the FilmActor to Film finder                 toList()                           // Use a List collector for downstream aggregation.             )         )     ); ``` Note: FilmActor is an entity with foreign keys to both the Film and the Actor table.  ### Entities are Linked No need for complicated joins! ```java // Find any film where english is spoken Optional<Film> anyFilmInEnglish = languages.stream()     .filter(Language.NAME.equal("English"))     .flatMap(films.finderBackwardsBy(Film.LANGUAGE_ID))     .findAny();  // Find the language of the film with id 42 Optional<Language> languageOfFilmWithId42 = films.stream()     .filter(Film.FILM_ID.equal(42))     .map(languages.finderBy(Film.LANGUAGE_ID))     .findAny(); ```  ### Easy Initialization The `SakilaApplication`, `SakilaApplicationBuilder` and `FilmManager` classes are generated automatically from the database. ```java final SakilaApplication app = new SakilaApplicationBuilder()     .withPassword("myPwd729")     .build();      final FilmManager      films      = app.getOrThrow(FilmManager.class); final LanguageManager  languages  = app.getOrThrow(CarrotManager.class); final ActorManager     actors     = app.getOrThrow(ActorManager.class); final FilmActorManager filmActors = app.getOrThrow(FilmActorManager.class); ```  ### Easy Persistence Entities can easily be persisted in a database. ```java Film newFilm = new FilmImpl();  // Creates a new empty Film newFilm.setTitle("Police Academy 13"); newFilm.setRating("G"); newFilm.setLength(123); ...  // Auto-Increment-fields have been set by the database Film persistedFilm = films.persist(newFilm);  ```  ### Update ```java films.stream()     .filter(Film.ID.equal(42))   // Filters out all Films with ID = 42 (just one)     .map(Film.LENGTH.setTo(143)) // Applies a setter that sets the length to 143     .forEach(films.updater());   // Applies the updater function ``` or another example ```java films.stream()     .filter(Film.ID.between(48, 102))   // Filters out all Films with ID between 48 and 102     .map(f -> f.setRentalDuration(f.getRentalDuration() + 1)) // Applies a lambda that increases their rental duration by one     .forEach(films.updater());          // Applies the updater function to the selected films ```  ### Remove ```java films.stream()     .filter(Film.ID.equal(71))  // Filters out all Films with ID = 71 (just one)     .forEach(films.remover());  // Applies the remover function ```   ### Full Transparency By appending a logger to the builder, you can follow exactly what happens behind the scenes. ```java SakilaApplication app = new SakilaApplicationBuilder()     .withPassword("myPwd729")     .withLogging(ApplicationBuilder.LogType.STREAM)     .withLogging(ApplicationBuilder.LogType.PERSIST)     .withLogging(ApplicationBuilder.LogType.UPDATE)     .withLogging(ApplicationBuilder.LogType.REMOVE)     .build(); ```  ### Integration with Spring Boot It is easy to integrate Speedment with Spring Boot. Here is an example of a Configuration file for Spring: ```java @Configuration public class AppConfig {     private @Value("${dbms.username}") String username;     private @Value("${dbms.password}") String password;     private @Value("${dbms.schema}") String schema;      @Bean     public SakilaApplication getSakilaApplication() {         return new SakilaApplicationBuilder()             .withUsername(username)             .withPassword(password)             .withSchema(schema)             .build();     }      // Individual managers     @Bean     public FilmManager getFilmManager(SakilaApplication app) {         return app.getOrThrow(FilmManager.class);     } } ```  So when we need to use a manager in a SpringMVC Controller, we can now simply autowire it: ```java     private @Autowired FilmManager films; ```   Features -------- Here are some of the many features packed into the Speedment framework!  ### Database Centric Speedment is using the database as the source-of-truth, both when it comes to the domain model and the actual data itself. Perfect if you are tired of configuring and debuging complex ORMs. After all, your data is more important than programming tools, is it not?  ### Code Generation Speedment inspects your database and can automatically generate code that reflects the latest state of your database. Nice if you have changed the data structure (like columns or tables) in your database. Optionally, you can change the way code is generated [using an intuitive UI](https://github.com/speedment/speedment/wiki/Tutorial:-Get-started-with-the-UI) or programatically using your own code.  ### Modular Design Speedment is built with the ambition to be completely modular! If you don't like the current implementation of a certain function, plug in you own! Do you have a suggestion for an alternative way of solving a complex problem? Share it with the community!  ### Type Safety When the database structure changes during development of a software there is always a risk that bugs sneak into the application. Thats why type-safety is such a big deal! With Speedment, you will notice if something is wrong seconds after you generate your code instead of weeks into the testing phase.  ### Null Protection Ever seen a `NullPointerException` suddenly casted out of nowhere? Null-pointers have been called the billion-dollar-mistake of java, but at the same time they are used in almost every software project out there. To minimize the production risks of using null values, Speedment analyzes if null values are allowed by a column in the database and wraps the values as appropriate in Java 8 Optionals.   Using Maven ----------- The easiest way to get started with Speedment and Maven is to use one of [the existing archetypes](https://github.com/speedment/speedment-archetypes). An archetype is similar to a template project. When you start a new project, it will add all the dependencies you need to your `pom.xml`-file so that you can begin program immediately.  If you do not want to use an archetype, for an example if you already have a project you want to use Speedment with, you can always write your `pom.xml`-file manually. Just add the following lines (between the ... marker lines) to your project's `pom.xml` file. Make sure to use the latest `${speedment.version}` available  #### MySQL ```xml <build>     <plugins>                  <plugin>             <groupId>com.speedment</groupId>             <artifactId>speedment-maven-plugin</artifactId>             <version>${speedment.version}</version>         </plugin>              </plugins> </build> <dependencies>          <dependency>         <groupId>com.speedment</groupId>         <artifactId>runtime</artifactId>         <version>${speedment.version}</version>         <type>pom</type>     </dependency>     <dependency>         <groupId>mysql</groupId>         <artifactId>mysql-connector-java</artifactId>         <version>5.1.42</version>         <scope>runtime</scope>     </dependency>      </dependencies> ```   #### PostgreSQL ```xml  <build>     <plugins>                  <plugin>             <groupId>com.speedment</groupId>             <artifactId>speedment-maven-plugin</artifactId>             <version>${speedment.version}</version>         </plugin>              </plugins> </build> <dependencies>          <dependency>         <groupId>com.speedment</groupId>         <artifactId>runtime</artifactId>         <version>${speedment.version}</version>         <type>pom</type>     </dependency>     <dependency>         <groupId>org.postgresql</groupId>         <artifactId>postgresql</artifactId>         <version>42.0.0</version>         <scope>runtime</scope>     </dependency>      </dependencies>  ```  #### MariaDB ```xml <build>     <plugins>                  <plugin>             <groupId>com.speedment</groupId>             <artifactId>speedment-maven-plugin</artifactId>             <version>${speedment.version}</version>         </plugin>              </plugins> </build> <dependencies>          <dependency>         <groupId>com.speedment</groupId>         <artifactId>runtime</artifactId>         <version>${speedment.version}</version>         <type>pom</type>     </dependency>     <dependency>         <groupId>org.mariadb.jdbc</groupId>         <artifactId>mariadb-java-client</artifactId>         <version>2.0.1</version>         <scope>runtime</scope>     </dependency>      </dependencies>  ```  Again, make sure that you use the latest `${speedment.version}` available.  ### Requirements Speedment comes with support for the following databases out-of-the-box: * MySQL * MariaDB * PostgreSQL   This site covers the **Speedment Open Source** project available under the  [Apache 2 license](http://www.apache.org/licenses/LICENSE-2.0). The  enterprise product with support for commercial  databases (i.e. Oracle, MS SQL Server, DB2, AS400) and in-JVM-memory acceleration can be found at  [www.speedment.com](http://speedment.com/).  Speedment requires `Java 8` or later. Make sure your IDE configured to use JDK 8 (version 1.8.0_40 or newer).  License -------  Speedment is available under the [Apache 2 License](http://www.apache.org/licenses/LICENSE-2.0).   #### Copyright  Copyright (c) 2014-2017, Speedment, Inc. All Rights Reserved. Visit [www.speedment.com](http://www.speedment.com/) for more info.  [![Analytics](https://ga-beacon.appspot.com/UA-64937309-1/speedment/main)](https://github.com/igrigorik/ga-beacon)  [![Beacon](http://stat.speedment.com:8081/Beacon?site=GitHub&path=main)](https://some-site.com)  [Github activity visualized](https://www.youtube.com/watch?v=Rmc_3lLZQpM)
hibernate/hibernate-orm	<img src="http://static.jboss.org/hibernate/images/hibernate_logo_whitebkg_200px.png" />   Hibernate ORM is a component/library providing Object/Relational Mapping (ORM) support to applications and other components/libraries.  It is also provides an implementation of the JPA specification, which is the standardized Java specification for ORM.  See  [Hibernate.org](http://hibernate.org/orm/) for additional information.   [![Build Status](http://ci.hibernate.org/job/hibernate-orm-master-h2-main/badge/icon)](http://ci.hibernate.org/job/hibernate-orm-master-h2-main/)   Resources =========  The build requires a Java 8 JDK as JAVA_HOME.  You will need http://git-scm.com/[git] to obtain the http://github.com/hibernate/hibernate-orm/[source].  Hibernate uses [Gradle](http://gradle.org) as its build tool.  See the _Gradle Primer_ section below if you are new to Gradle.  Contributors should read the [Contributing Guide](CONTRIBUTING.md)  See the guides for setting up [IntelliJ](https://developer.jboss.org/wiki/ContributingToHibernateUsingIntelliJ) or [Eclipse](https://developer.jboss.org/wiki/ContributingToHibernateUsingEclipse) as your development environment.  Check out the _Getting Started_ section in CONTRIBUTING.md for getting started working on Hibernate source.   CI Builds =========  Hibernate makes use of [Jenkins](http://jenkins-ci.org) for its CI needs.  The project is built continuous on each  push to the upstream repository.   Overall there are a few different jobs, all of which can be seen at  [http://ci.hibernate.org/view/ORM/](http://ci.hibernate.org/view/ORM/)   Gradle primer =========  This section describes some of the basics developers and contributors new to Gradle might  need to know to get productive quickly.  The Gradle documentation is very well done; 2 in  particular that are indispensable:  * [Gradle User Guide](https://docs.gradle.org/current/userguide/userguide_single.html) is a typical user guide in that it follows a topical approach to describing all of the capabilities of Gradle. * [Gradle DSL Guide](https://docs.gradle.org/current/dsl/index.html) is quite unique and excellent in quickly getting up to speed on certain aspects of Gradle.   Using the Gradle Wrapper ------------------------  For contributors who do not otherwise use Gradle and do not want to install it, Gradle offers a very cool features called the wrapper.  It lets you run Gradle builds without a previously installed Gradle distro in  a zero-conf manner.  Hibernate configures the Gradle wrapper for you.  If you would rather use the wrapper and  not install Gradle (or to make sure you use the version of Gradle intended for older builds) you would just use the command `gradlew` (or `gradlew.bat`) rather than `gradle` (or `gradle.bat`) in the following discussions.   Note that `gradlew` is only available in the project's root dir, so depending on your `pwd` you may need to adjust  the path to `gradlew` as well.  Examples use the `gradle` syntax, but just swap `gradlew` (properly relative) for `gradle` if you wish to use  the wrapper.  _Note that another reason to use `gradlew` is that it uses the exact version of Gradle that the build is  defined to work with.   Executing Tasks ------------------------  Gradle uses the concept of build tasks (equivalent to Ant targets or Maven phases/goals). You can get a list of available tasks via       gradle tasks  To execute a task across all modules, simply perform that task from the root directory.  Gradle will visit each sub-project and execute that task if the sub-project defines it.  To execute a task in a specific module you can  either:  1. `cd` into that module directory and execute the task 2. name the "task path".  For example, in order to run the tests for the _hibernate-core_ module from the root directory you could say `gradle hibernate-core:test`  Common Java related tasks ------------------------  * _build_ - Assembles (jars) and tests this project * _buildDependents_ - Assembles and tests this project and all projects that depend on it.  So think of running this in hibernate-core, Gradle would assemble and test hibernate-core as well as hibernate-envers (because envers depends on core) * _classes_ - Compiles the main classes * _testClasses_ - Compiles the test classes * _compile_ (Hibernate addition) - Performs all compilation tasks including staging resources from both main and test * _jar_ - Generates a jar archive with all the compiled classes * _test_ - Runs the tests * _publish_ - Think Maven deploy * _publishToMavenLocal_ - Installs the project jar to your local maven cache (aka ~/.m2/repository).  Note that Gradle  never uses this, but it can be useful for testing your build with other local Maven-based builds. * _eclipse_ - Generates an Eclipse project * _idea_ - Generates an IntelliJ/IDEA project (although the preferred approach is to use IntelliJ's Gradle import). * _clean_ - Cleans the build directory   Testing and databases =====================  Testing against a specific database can be achieved in 2 different ways:   Using the "Matrix Testing Plugin" for Gradle. ---------------------------------------------  Coming soon...   Using "profiles" ------------------------  The Hibernate build defines a number of database testing "profiles" in `databases.gradle`.  These profiles can be activated by name using the `db` build property which can be passed either as a JVM system prop (`-D`) or as a Gradle project property (`-P`).  Examples below use the Gradle project property approach.      gradle clean build -Pdb=pgsql  To run a test from your IDE, you need to ensure the property expansions happen. Use the following command:      gradle clean compile -Pdb=pgsql  _*NOTE : If you are running tests against a JDBC driver that is not available via Maven central (generally due to license nonsense - Oracle, DB2, etc) be sure to add these drivers to your local Maven repo cache (~/.m2/repository) or (better) add it to a personal Maven repo server*_
Raizlabs/DBFlow	![Image](https://github.com/agrosner/DBFlow/blob/develop/dbflow_banner.png?raw=true)  [![JitPack.io](https://img.shields.io/badge/JitPack.io-4.1.1-red.svg?style=flat)](https://jitpack.io/#Raizlabs/DBFlow) [![Android Weekly](http://img.shields.io/badge/Android%20Weekly-%23129-2CB3E5.svg?style=flat)](http://androidweekly.net/issues/issue-129) [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-DBFlow-brightgreen.svg?style=flat)](https://android-arsenal.com/details/1/1134)  A robust, powerful, and very simple ORM android database library with **annotation processing**.  The library is built on speed, performance, and approachability. It not only eliminates most boiler-plate code for dealing with databases, but also provides a powerful and simple API to manage interactions.  Let DBFlow make SQL code _flow_ like a _steady_ stream so you can focus on writing amazing apps.  # Why Use DBFlow DBFlow is built from a collection of the best features of many database libraries in the most efficient way possible. Also, it is built to not only make it _significantly_ easier to deal with databases on Android, but also to provide extensibility. Don't let an ORM or library get in your way, let the code you write in your applications be the best as possible. - **Extensibility**: No restrictions on inheritance of your table classes. They can be plain POJOs, no subclass required, but as a convenience we recommend using `BaseModel`. You can extend non-`Model` classes in different packages and use them as your DB tables. Also you can subclass other tables to join the `@Column` together, and again they can be in different packages. - **Speed**: Built with java's annotation processing code generation, there's almost zero runtime performance hit by using this library (only reflection is creation of the main, generated database module's constructor). This library saves hours of boilerplate code and maintenance by generating the code for you. With powerful model caching (multiple primary key `Model` too), you can surpass the speed of SQLite by reusing where possible. We have support for lazy-loading relationships on-demand such as `@ForeignKey` or `@OneToMany` that make queries happen super-fast. - **SQLite Query Flow**: The queries in this library adhere as closely as possible to SQLite native queries. `select(name, screenSize).from(Android.class).where(name.is("Nexus 5x")).and(version.is(6.0)).querySingle()` - **Open Source**: This library is fully open source and contributions are not only welcomed, but encouraged. - **Robust**: We support `Trigger`, `ModelView`, `Index`, `Migration`, built-in ways to manage database access, and many more features. SQLCipher, RXJava, and more! - **Multiple Databases, Multiple Modules**: we seamlessly support multiple database files, database modules using DBFlow in other dependencies, simultaneously. - **Built On SQLite**: SQLite is the most widely used database engine in world and using it as your base, you are not tied to a limited set of platforms or libraries.  # Changelog  Changes exist in the [releases tab](https://github.com/Raizlabs/DBFlow/releases).  # Usage Docs For more detailed usage, check out it out [here](https://agrosner.gitbooks.io/dbflow/content/)  # Including in your project  ```groovy  allProjects {   repositories {     // required to find the project's artifacts     maven { url "https://www.jitpack.io" }   } } ```  Add the library to the project-level build.gradle, using the apt plugin to enable Annotation Processing:  ```groovy    apply plugin: 'kotlin-kapt' // required for kotlin.    def dbflow_version = "4.1.1"   // or dbflow_version = "develop-SNAPSHOT" for grabbing latest dependency in your project on the develop branch   // or 10-digit short-hash of a specific commit. (Useful for bugs fixed in develop, but not in a release yet)    dependencies {      // if Java use this. If using Kotlin do NOT use this.     annotationProcessor "com.github.Raizlabs.DBFlow:dbflow-processor:${dbflow_version}"      // Use if Kotlin user.     kapt "com.github.Raizlabs.DBFlow:dbflow-processor:${dbflow_version}"      compile "com.github.Raizlabs.DBFlow:dbflow-core:${dbflow_version}"     compile "com.github.Raizlabs.DBFlow:dbflow:${dbflow_version}"      // sql-cipher database encryption (optional)     compile "com.github.Raizlabs.DBFlow:dbflow-sqlcipher:${dbflow_version}"     compile "net.zetetic:android-database-sqlcipher:${sqlcipher_version}@aar"      // kotlin extensions     compile "com.github.Raizlabs.DBFlow:dbflow-kotlinextensions:${dbflow_version}"      // RXJava 1 support     compile "com.github.Raizlabs.DBFlow:dbflow-rx:${dbflow_version}"      // RXJava 1 Kotlin Extensions Support     compile "com.github.Raizlabs.DBFlow:dbflow-rx-kotlinextensions:${dbflow_version}"      // RXJava 2 support     compile "com.github.Raizlabs.DBFlow:dbflow-rx2:${dbflow_version}"      // RXJava 2 Kotlin Extensions Support     compile "com.github.Raizlabs.DBFlow:dbflow-rx2-kotlinextensions:${dbflow_version}"    }  ```  # Pull Requests I welcome and encourage all pull requests. It usually will take me within 24-48 hours to respond to any issue or request. Here are some basic rules to follow to ensure timely addition of your request:   1. Match coding style (braces, spacing, etc.) This is best achieved using **Reformat Code** shortcut, <kbd>command</kbd>+<kbd>option</kbd>+<kbd>L</kbd> on Mac and <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>L</kbd> on Windows, with Android Studio defaults.   2. If its a feature, bugfix, or anything please only change code to what you specify.   3. Please keep PR titles easy to read and descriptive of changes, this will make them easier to merge :)   4. Pull requests _must_ be made against `develop` branch. Any other branch (unless specified by the maintainers) will get rejected.   5. Have fun!  # Maintained By [agrosner](https://github.com/agrosner) ([@agrosner](https://www.twitter.com/agrosner))
benas/todolist-mvc	## Todolist MVC  Todolist MVC is like [TodoMVC][] but for Java Web Frameworks instead of Javascript Frameworks.  The goal is to implement the same application using different technologies and compare them  in order to help you choose the right framework for your next Java web application.  Two types of frameworks are being compared:  #### 1. Request/Action based frameworks  * Servlets/JSPs  * Spring MVC * Struts * JSR 371 Java EE 8 MVC 1.0 (WIP)  #### 2. Component based frameworks  * Tapestry * JSF  Other frameworks will be added progressively (JSR 371, Wicket, Grails, Play, etc..), you are welcome to contribute with any Java web framework you are comfortable with.  Performance comparison is not addressed here. There are many excellent benchmarks on the web like [TechEmpower's Framework Benchmarks][].  The goal is to focus on features set and developer productivity using each framework:  * Form handling * Request parameters binding * Session handling * Ajax support * I18N support * etc  ## About Todolist MVC  Todolist MVC is the classic CRUD web application to manage todo list online.  Some features like user and session management go beyond CRUD operations.  The application is small enough to be easy to implement, and big enough to cover most of the features of each framework.  Here is the list of features:   #### User management  * Register a new account * View account details * Update account * Delete account  #### Session management  * Login * Logout  #### Todolist management  * CRUD operations on todos * Search todo list  Todolist MVC uses [Twitter Bootstrap][] for the user interface, here are some screenshots:  ![Index page](https://github.com/benas/todolist-mvc/raw/master/src/site/screenshots/todolist-index.png)  ![Sign-in page](https://github.com/benas/todolist-mvc/raw/master/src/site/screenshots/todolist-signin.png)  ![Home page](https://github.com/benas/todolist-mvc/raw/master/src/site/screenshots/todolist-home.png)  ![Search page](https://github.com/benas/todolist-mvc/raw/master/src/site/screenshots/todolist-search.png)  ## Architecture  The application's backend is developed using Spring and JPA/Hibernate. Data is persisted in an in-memory HSQL database to make testing/running the application relatively easy. The module named `todolist-core` is common to all web layers and is a good use case to see how web frameworks integrate with Spring.  Common web utilities (JSTL tags, Filters, Backing beans, etc) are packaged in a separate common web module named `todolist-web-common`.  For each web framework, a separate war module is created to implement ONLY the web layer of the application.  View technology may vary for each framework. Here, JSP views are (re)used for most of current implementations. Thymeleaf is also planned to be used as alternative to JSPs.  URLs under `/user/*` and `/todos/*` must be accessible to only logged users.  This requirement should be implemented using a servlet filter or equivalent (Struts interceptor, Spring MVC interceptor, etc)  Note that security is not addressed since not all frameworks provide security support.  Form validation should be done using Bean Validation API (JSR303). Form backing beans are already defined with validation constraints in the `todolist-web-common` module. All you have to do is to integrate the validation logic with the framework in use.  Finally, the static html template located in `src/site/template` can be used as a starting point to implement the application with a new framework.  ## Build and run Todolist MVC  1.  Check out the project source code from github : `git clone https://github.com/benas/todolist-mvc.git` 2.  Open a terminal and run the following command from root directory : `mvn install` 3.  Choose a web framework to test and run it. For example : `cd todolist-web-springmvc && mvn tomcat7:run` 4.  Browse the following URL : `localhost:8080/` 5.  You can register a new account or login using the following credentials : foo@bar.org / foobar  ## Contributions  In order to compare these frameworks objectively, the best implementation should be provided for each framework.  If you are an expert in some of these technologies, please don't hesitate to fix/improve every single aspect of the current implementation.  The goal of Todolist MVC is to be community driven, so every single contribution is welcome!  ## Todo  - [ ] Add Spring boot support - [ ] Add JSR 371 (Ozark) implementation - [ ] Add Thymeleaf support - [ ] Update static resources (Bootstrap, FontAwesome) to last versions - [ ] Update dependencies to last versions - [ ] Add comparsion table  ## License Todolist MVC is released under the [MIT License][].  [TodoMVC]: http://todomvc.com/ [TechEmpower's Framework Benchmarks]: https://github.com/TechEmpower/FrameworkBenchmarks [Twitter Bootstrap]: http://getbootstrap.com/ [MIT License]: http://opensource.org/licenses/mit-license.php/
spring-petclinic/spring-framework-petclinic	# Spring PetClinic Sample Application [![Build Status](https://travis-ci.org/spring-petclinic/spring-framework-petclinic.svg?branch=master)](https://travis-ci.org/spring-petclinic/spring-framework-petclinic/)  Approved by the Spring team, this repo is a fork of the [spring-projects/spring-petclinic](https://github.com/spring-projects/spring-petclinic). It allows the Spring community to maintain a Petclinic version with a plain old **Spring Framework configuration** and with a **3-layer architecture** (i.e. presentation --> service --> repository). The "canonical" implementation is now based on Spring Boot, Thymeleaf and [aggregate-oriented domain]([https://github.com/spring-projects/spring-petclinic/pull/200).    ## Understanding the Spring Petclinic application with a few diagrams [See the presentation here](http://fr.slideshare.net/AntoineRey/spring-framework-petclinic-sample-application) (2017 update)  ## Running petclinic locally ``` 	git clone https://github.com/spring-petclinic/spring-framework-petclinic.git 	cd spring-framework-petclinic 	./mvnw tomcat7:run-war ```  You can then access petclinic here: http://localhost:9966/petclinic/  <img width="1042" alt="petclinic-screenshot" src="https://cloud.githubusercontent.com/assets/838318/19727082/2aee6d6c-9b8e-11e6-81fe-e889a5ddfded.png">  ## In case you find a bug/suggested improvement for Spring Petclinic Our issue tracker is available here: https://github.com/spring-petclinic/spring-framework-petclinic/issues   ## Database configuration  In its default configuration, Petclinic uses an in-memory database (HSQLDB) which gets populated at startup with data. A similar setups is provided for MySql and PostgreSQL in case a persistent database configuration is needed. To run petclinic locally using persistent database, it is needed to run with profile defined in main pom.xml file.  For MySQL database, it is needed to run with 'MySQL' profile defined in main pom.xml file.  ``` ./mvnw tomcat7:run-war -P MySQL ```  Before do this, would be good to check properties defined in MySQL profile inside pom.xml file.  ``` <properties>     <jpa.database>MYSQL</jpa.database>     <jdbc.driverClassName>com.mysql.jdbc.Driver</jdbc.driverClassName>     <jdbc.url>jdbc:mysql://localhost:3306/petclinic?useUnicode=true</jdbc.url>     <jdbc.username>root</jdbc.username>     <jdbc.password>petclinic</jdbc.password> </properties> ```        You may also start a MySql database with docker:  ``` docker run --name mysql-petclinic -e MYSQL_ROOT_PASSWORD=petclinic -e MYSQL_DATABASE=petclinic -p 3306:3306 mysql:5.7.8 ```  For PostgreSQL database, it is needed to run with 'PostgreSQL' profile defined in main pom.xml file.  ``` ./mvnw tomcat7:run-war -P PostgreSQL ```  Before do this, would be good to check properties defined in PostgreSQL profile inside pom.xml file.  ``` <properties>     <jpa.database>POSTGRESQL</jpa.database>     <jdbc.driverClassName>org.postgresql.Driver</jdbc.driverClassName>     <jdbc.url>jdbc:postgresql://localhost:5432/petclinic</jdbc.url>     <jdbc.username>postgres</jdbc.username>     <jdbc.password>petclinic</jdbc.password> </properties> ``` You may also start a Postgres database with docker:  ``` docker run --name postgres-petclinic -e POSTGRES_PASSWORD=petclinic -e POSTGRES_DB=petclinic -p 5432:5432 -d postgres:9.6.0 ```  ## Working with Petclinic in Eclipse/STS  ### prerequisites The following items should be installed in your system: * Maven 3 (http://www.sonatype.com/books/mvnref-book/reference/installation.html) * git command line tool (https://help.github.com/articles/set-up-git) * Eclipse with the m2e plugin (m2e is installed by default when using the STS (http://www.springsource.org/sts) distribution of Eclipse)  Note: when m2e is available, there is an m2 icon in Help -> About dialog. If m2e is not there, just follow the install process here: http://eclipse.org/m2e/download/   ### Steps:  1) In the command line ``` git clone https://github.com/spring-petclinic/spring-framework-petclinic.git ``` 2) Inside Eclipse ``` File -> Import -> Maven -> Existing Maven project ```   ## Looking for something in particular?  | Java Config |   | |-------------|---| | Java config branch | Petclinic uses XML configuration by default. In case you'd like to use Java Config instead, there is a Java Config branch available [here](https://github.com/spring-petclinic/spring-framework-petclinic/tree/javaconfig) |  | Inside the 'Web' layer | Files | |------------------------|-------| | Spring MVC - XML integration | [mvc-view-config.xml](src/main/resources/spring/mvc-view-config.xml)  | | Spring MVC - ContentNegotiatingViewResolver| [mvc-view-config.xml](src/main/resources/spring/mvc-view-config.xml) | | JSP custom tags | [WEB-INF/tags](src/main/webapp/WEB-INF/tags), [createOrUpdateOwnerForm.jsp](src/main/webapp/WEB-INF/jsp/owners/createOrUpdateOwnerForm.jsp)| | JavaScript dependencies | [JavaScript libraries are declared as webjars in the pom.xml](pom.xml) | | Static resources config | [Resource mapping in Spring configuration](/src/main/resources/spring/mvc-core-config.xml#L30) | | Static resources usage | [staticFiles.jsp](src/main/webapp/WEB-INF/jsp/fragments/staticFiles.jsp#L12) | | Thymeleaf | In the late 2016, the original [Spring Petclinic](https://github.com/spring-projects/spring-petclinic) has moved from JSP to Thymeleaf. |  | 'Service' and 'Repository' layers | Files | |-----------------------------------|-------| | Transactions | [business-config.xml](src/main/resources/spring/business-config.xml), [ClinicServiceImpl.java](src/main/java/org/springframework/samples/petclinic/service/ClinicServiceImpl.java) | | Cache | [tools-config.xml](src/main/resources/spring/tools-config.xml), [ClinicServiceImpl.java](src/main/java/org/springframework/samples/petclinic/service/ClinicServiceImpl.java) | | Bean Profiles | [business-config.xml](src/main/resources/spring/business-config.xml), [ClinicServiceJdbcTests.java](src/test/java/org/springframework/samples/petclinic/service/ClinicServiceJdbcTests.java), [PetclinicInitializer.java](src/main/java/org/springframework/samples/petclinic/PetclinicInitializer.java) | | JDBC | [business-config.xml](src/main/resources/spring/business-config.xml), [jdbc folder](src/main/java/org/springframework/samples/petclinic/repository/jdb) | | JPA | [business-config.xml](src/main/resources/spring/business-config.xml), [jpa folder](src/main/java/org/springframework/samples/petclinic/repository/jpa) | | Spring Data JPA | [business-config.xml](src/main/resources/spring/business-config.xml), [springdatajpa folder](src/main/java/org/springframework/samples/petclinic/repository/springdatajpa) |   ## Interaction with other open source projects  One of the best parts about working on the Spring Petclinic application is that we have the opportunity to work in direct contact with many Open Source projects. We found some bugs/suggested improvements on various topics such as Spring, Spring Data, Bean Validation and even Eclipse! In many cases, they've been fixed/implemented in just a few days. Here is a list of them:  | Name | Issue | |------|-------| | Spring JDBC: simplify usage of NamedParameterJdbcTemplate | [SPR-10256](https://jira.springsource.org/browse/SPR-10256) and [SPR-10257](https://jira.springsource.org/browse/SPR-10257) | | Bean Validation / Hibernate Validator: simplify Maven dependencies and backward compatibility |[HV-790](https://hibernate.atlassian.net/browse/HV-790) and [HV-792](https://hibernate.atlassian.net/browse/HV-792) | | Spring Data: provide more flexibility when working with JPQL queries | [DATAJPA-292](https://jira.springsource.org/browse/DATAJPA-292) | | Dandelion: improves the DandelionFilter for Jetty support | [113](https://github.com/dandelion/dandelion/issues/113) |   # Contributing  The [issue tracker](/issues) is the preferred channel for bug reports, features requests and submitting pull requests.  For pull requests, editor preferences are available in the [editor config](.editorconfig) for easy use in common text editors. Read more and download plugins at <http://editorconfig.org>.
ldcsaa/JessMA	JessMA ======  Powerful Java MVC &amp; REST Full-Stack Framework  	Maven usage:  	1) JessMA core  		<dependency> 			<groupId>org.jessma</groupId> 			<artifactId>jessma-core</artifactId> 			<version>3.5.1</version> 		</dependency>  	2) JessMA RESTful extension  		<dependency> 			<groupId>org.jessma</groupId> 			<artifactId>jessma-ext-rest</artifactId> 			<version>3.5.1</version> 		</dependency>  	3) JessMA Guice extension 	 		<dependency> 			<groupId>org.jessma</groupId> 			<artifactId>jessma-ext-guice</artifactId> 			<version>3.5.1</version> 		</dependency>  	4) JessMA Spring extension 	 		<dependency> 			<groupId>org.jessma</groupId> 			<artifactId>jessma-ext-spring</artifactId> 			<version>3.5.1</version> 		</dependency>
ykameshrao/spring-mvc-angular-js-hibernate-bootstrap-java-single-page-jwt-auth-rest-api-webapp-framework	# Spring MVC AngularJS Hibernate Bootstrap JWT Token Authenticated REST API Single Page Java Webapp Framework  ![Home Page](https://github.com/ykameshrao/spring-mvc-angular-js-hibernate-bootstrap-java-single-page-jwt-auth-rest-api-webapp-framework/blob/master/screenshots/HomePage.png)  ### Blog Post ### http://www.orangeapple.org/post/134959159212/spring-mvc-angularjs-hibernate-bootstrap-java  ### Introduction ### I am showcasing a webapp template project framework that can be used AS-IS to begin developing a modern AngularJS Single Page Web App backed by Spring and Java. The projects brings together Java, Spring 4, Spring MVC 4, Spring JPA, Hibernate, Angular JS, Twitter Bootstrap, JWT Token Authentication technology in a clean easy to use fashion. Download and start building your next big thing in Java. Free your mind of scalability and other web app development scares.  The stack here consists of Spring MVC, AngularJS, Hibernate, Spring JPA, Twitter Bootstrap, JWT Token Authentication, RESTful API. All of these are nicely brought togather with a bunch of Java and JS boilerplate code meant to give a template framework allowing the flow of control in conventional way. Consider this as a pre-configured Java Template Project Framework promoting the Convention over Configuration idea of Design.  ### Whats in the Project? ### The project includes all the code required to create and authenticate a new user including backend models, entities, repositories, service, controller code and frontend forms and relevant javascript validations. Refer the screenshots for getting some idea of the functionality. Also implemented is a ready to use form validation setup in javascript, along with URL redirection based on authenticated and un-authenticated sections/endpoints of the website. Also implemented is the simple JWT token based authentication of the REST API endpoints.  The project organises the code in the following sections-  1. Framework - java package housing the common data, dao, service, common code classes to generalize most of the boilerplate code using Java generics and other similar rich features.  2. Model - java package housing the code related to creating database entities and repository classes defining code to access these entities from database using Hibernate and Spring JPA. 3. Service - java package housing the code defining the various methods specific to your business logic to manipulate the data accordingly. 4. Interceptor - Spring AOP in action here to remove redundant boilerplate code to handle and respond to exceptions in a standard way and also collect performance metrics of the API and service. Refer: WebAppMetricsInterceptor.java, WebAppExceptionAdvice.java 5. Auth - package housing the filter code to process and handle JWT Token based authentication of the REST API requests. 6. Core - package housing the code associated with running a scheduled set of background jobs. The idea with this code piece is to enable offload asynchronous processing like sending mails, uploading files to S3, etc. It needs more work to better setup the thread pools, etc. The code present here today shows a small coding problem I did to create a priority based job runner prioritized using Category they belong to. This also shows a job queue that can be persisted to database for execution of jobs without missing. 7. Categories - The category entity shows a self link entity object that lets you create categories and sub categories of categories using a single table. 8. AngularJS App - the entire AngularJS app is placed in the webapp directory and further organizes the JavaScript code into services, controllers, views and models giving a convention-over-configuration setup to start building your AngularJS powered Web UI. The entire purpose of this code peice is to get you bootstrapped and running ASAP. The primary single page app (SPA) is loaded in index.html.     ### Executing ### Download the code and build the project the same way as you would do for any maven project. I have currently plugged in the test-jdbc xml file in the configuration that creates and runs a InMemory H2 database. I also included a standard MySQL Spring JDBC xml file. Use that one by changing the context file in web.xml and after changing the parameters there according to your local MySQL setup.  ###Technologies###   -  Java   -  Spring   -  Spring MVC   -  Spring AOP   -  Hibernate     -  Angular JS   -  Twitter Bootstrap   -  JWT Token Auth   -  Domain Driven Model   -  Maven  ### References ### I have heavily referenced many external links and books while creating this project apart from my own experience. I am listing a few of them here. If I missed something, please accept my deepest apologies. I also spent a long time trying to implement Spring Security based authentication but ultimately failed. My whole approach here was to bring everything together in a simple and easy and ready to reuse package. I have tried to avoid any direct copying of code though, since most of it required some kind of changes. Most of the spring security code is copied from elsewhere but you would find it in the unused package since it was never really used in the functioning website.  1. AngularJS In Action 2. https://github.com/jwtk/jjwt  ### ScreenShots ### ![Home Page](https://github.com/ykameshrao/spring-mvc-angular-js-hibernate-bootstrap-java-single-page-jwt-auth-rest-api-webapp-framework/blob/master/screenshots/HomePage.png)  ![Registration Page w/ Validation](https://github.com/ykameshrao/spring-mvc-angular-js-hibernate-bootstrap-java-single-page-jwt-auth-rest-api-webapp-framework/blob/master/screenshots/RegisterWithValidationFail.png)  ![Registration Page](https://github.com/ykameshrao/spring-mvc-angular-js-hibernate-bootstrap-java-single-page-jwt-auth-rest-api-webapp-framework/blob/master/screenshots/RegisterWithValidationSuccess.png)  ![Dashboard Page](https://github.com/ykameshrao/spring-mvc-angular-js-hibernate-bootstrap-java-single-page-jwt-auth-rest-api-webapp-framework/blob/master/screenshots/Dashboard.png)  ![API Response Structure](https://github.com/ykameshrao/spring-mvc-angular-js-hibernate-bootstrap-java-single-page-jwt-auth-rest-api-webapp-framework/blob/master/screenshots/APIResponse.png) ### Enjoy!! ###   GitHub Repo for the project: https://github.com/ykameshrao/spring-mvc-angular-js-hibernate-bootstrap-java-single-page-jwt-auth-rest-api-webapp-framework  ### Blog Post: http://www.orangeapple.org/post/134959159212/spring-mvc-angularjs-hibernate-bootstrap-java ###
sps/mustache-spring-view	[Mustache.js](http://mustache.github.com/mustache.5.html) View for [Spring Web MVC](http://static.springsource.org/spring/docs/4.0.x/spring-framework-reference/html/mvc.html) ============================================================================ Supports both [jmustache](https://github.com/samskivert/jmustache) and [mustache.java](https://github.com/spullara/mustache.java)  [![Build Status](https://travis-ci.org/sps/mustache-spring-view.png?branch=master)](https://travis-ci.org/sps/mustache-spring-view) [![Coverage Status](https://coveralls.io/repos/sps/mustache-spring-view/badge.png?branch=master)](https://coveralls.io/r/sps/mustache-spring-view?branch=master)  Maven Dependency -----------------      <dependency>         <groupId>com.github.sps.mustache</groupId>         <artifactId>mustache-spring-view</artifactId>         <version>1.3</version>     </dependency>      <!-- jmustache -->     <dependency>         <groupId>com.samskivert</groupId>         <artifactId>jmustache</artifactId>         <version>${jmustache.version}</version>     </dependency>      <!-- mustache.java -->     <dependency> 		<groupId>com.github.spullara.mustache.java</groupId>         <artifactId>compiler</artifactId>         <version>${mustache.java.version}</version>     </dependency>    Spring Configuration -------------     <!-- jmustache -->     <bean id="viewResolver" class="org.springframework.web.servlet.view.mustache.MustacheViewResolver">         <property name="suffix" value=""/>         <property name="cache" value="${TEMPLATE_CACHE_ENABLED}" />         <property name="templateFactory">             <bean class="org.springframework.web.servlet.view.mustache.jmustache.JMustacheTemplateFactory">                 <property name="escapeHTML" value="true"/>                 <property name="standardsMode" value="false"/>                 <property name="templateLoader">                     <bean class="org.springframework.web.servlet.view.mustache.jmustache.JMustacheTemplateLoader"/>                                                 </property>             </bean>         </property>     </bean>  	<!-- mustache.java -->     <bean id="viewResolver" class="org.springframework.web.servlet.view.mustache.MustacheViewResolver">         <property name="suffix" value=""/>         <property name="cache" value="${TEMPLATE_CACHE_ENABLED}"/>         <property name="templateFactory">             <bean class="org.springframework.web.servlet.view.mustache.java.MustacheJTemplateFactory" />         </property>     </bean>        Localization Support --------------- 	<bean id="messageSource" .../>          <!-- using mustache.java -->     <bean id="i18nMessageInterceptor"           class="org.springframework.web.servlet.view.mustache.java.LocalizationMessageInterceptor">         <property name="localeResolver" ref="..." />     </bean>  	<!-- using jmustache --> 	<bean id="i18nMessageInterceptor" class="org.springframework.web.servlet.view.mustache.jmustache.LocalizationMessageInterceptor">         <property name="localeResolver" ref="..." />     </bean>       Thanks --------------- Thanks to [Eric White](https://github.com/ericdwhite) for [forking](https://github.com/ericdwhite/mustache.java-spring-webmvc/) this code base and providing the mustache.java implementation.
livrospringmvc/lojacasadocodigo	#Agradecimento Obrigado por ter comprado o livro ou por estar interessado no projeto criado a partir do conteúdo do mesmo :). Fique a vontade para navegar, realizar o download e fazer as experimentações que achar necessárias.  ##Informações importantes sobre o projeto	   	* Caso precise, faça o download do zip do projeto pronto, basta seguir este link -> https://github.com/livrospringmvc/lojacasadocodigo/archive/versao_download.zip 	* O projeto pronto para download, aponta para um banco mysql chamado "casadocodigo", fique atento a isso. 	* A maioria dos commits foram feitos pensando nos capítulos do livro, então fique a vontade para clonar o projeto e navegar entre eles.
hzxie/voj	![Verwandlung Online Judge](https://raw.githubusercontent.com/hzxie/voj/master/web/src/main/webapp/assets/img/logo.png)  Version: 0.1.0  [![Build Status](https://travis-ci.org/hzxie/voj.png?branch=master)](https://travis-ci.org/hzxie/voj) [![Build status](https://ci.appveyor.com/api/projects/status/j62ns9p8whttittm?svg=true)](https://ci.appveyor.com/project/hzxie/voj) [![Coverage Status](https://coveralls.io/repos/hzxie/voj/badge.svg?branch=master&service=github)](https://coveralls.io/github/hzxie/voj?branch=master)  [**Official Website**](#) |  [**Tech Support**](http://haozhexie.com) | [**Change Log**](#)  ---  ## Introduction  It is a cross-platform online judge system based on [Spring MVC Framework](http://spring.io).  The application used following open-source projects:   - [Spring MVC](http://spring.io) famework  - [MyBatis](https://mybatis.github.io/mybatis-3/index.html) persistence framework  - [Apache ActiveMQ](http://activemq.apache.org/) message queue  - [Druid](https://github.com/alibaba/druid/) database connection pool  - [Flat UI](http://flat-ui.com)  - [jQuery](http://jquery.com)  - [FontAwesome](http://fontawesome.io)  - [CodeMirror](http://codemirror.net)  - [Highlight.js](https://highlightjs.org/)  ### The Origin of Verwandlung  In 2011, the LinkedIn Inc. published a message queue product called [Kafka](http://kafka.apache.org/). It's implemented in Scala and open-sourced.  In 2012, Alibaba Inc. published a message queue product called [MetaQ](https://github.com/killme2008/Metamorphosis), which is based on Kafka. It's implemented in Java.  MetaQ stands for *Metamorphosis*, which is a famous literature written by the author Franz Kafka.  As the message queue is one of the important components in the application, so I named the application *Verwandlung*, which is the German name of *Metamorphosis*.  ### Architecture  The application contains two modules:  - Web Application - Cross-Platform Judger  which can be described as following:  ![Software-Architecture](https://infinitescript.com/wordpress/wp-content/uploads/2015/04/Software-Architecture.png)  As you see, the Online Judge System can contain multiple judgers. The judgers communicate with the web application through a message queue.  ---  ## Getting Started  ### System Requirements  #### Hardware Requirements  - **CPU**: 2.0 GHz or faster 32-bit (x86) or 64-bit (x64) processor  For Web Application (including Database and Message Queue):  - **RAM**: 2.0 GB RAM on Windows, 1.0 GB RAM on Linux.  For Judger:  - **RAM**: 1.0 GB RAM on Windows, 512 MB RAM on Linux.  #### Software Requirements  For Web Application (including Database and Message Queue):  - **Operating System**: Windows, Linux or Mac - **Database**: [MySQL](http://www.mysql.com) 5.5+ or [MariaDB](https://mariadb.org/) 5.5+ - **Java Runtime**: [JRE](http://java.oracle.com) 1.8+ or JDK 1.8+ - **Message Queue**: [ActiveMQ](http://activemq.apache.org) 5.11+ - **Web Server**: [Tomcat](http://tomcat.apache.org) 8.5+  For Judger:  - **Operating System**: Windows or Linux - **Java Runtime**: [JRE](http://java.oracle.com) 1.8+ or JDK 1.8+  ### Installation  #### Binary Releases  - **Web Application**: [0.1.0](https://github.com/hzxie/voj/releases/download/0.1.0/voj.war) - **Judger (Windows, 64 Bit)**: [0.1.0](https://github.com/hzxie/voj/releases/download/0.1.0/voj-judger-windows-x64.jar) - **Judger (Linux, 64 Bit)**: [0.1.0](https://github.com/hzxie/voj/releases/download/0.1.0/voj-judger-linux-x64.jar)  #### Source Releases  **NOTE:**   - [Maven](http://maven.apache.org) 3+ and [GCC](http://gcc.gnu.org/) 4.8+ with POSIX thread model is required. - Make sure add the Maven and GCC to the PATH.  After extracting the source, run these commands from a terminal:  For Web Application:  ``` cd web mvn package -DskipTests ```  The terminal will return message as following:  ``` [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 10.168 s [INFO] Finished at: 2015-11-26T13:20:09+08:00 [INFO] Final Memory: 24M/210M [INFO] ------------------------------------------------------------------------ ```  And you'll get a package named `voj.web.war` in the `target` folder.  For Judger:  **Windows**:  ``` cd %JAVA_HOME%\include\win32 copy jawt_md.h  .. copy jni_md.h  ..  cd judger mvn package -DskipTests ```  **Linux**:  ``` cd $JAVA_HOME/include/linux cp jawt_md.h jni_md.h ..  cd SOURCE_CODE_PATH/judger mvn package -DskipTests ```  The terminal will return message as following:  ``` [INFO] Executing tasks  jni:      [echo] Generating JNI headers      [exec] Cannot find type 'org.springframework.beans.factory.annotation.Value' ...      [exec] Cannot find type 'org.springframework.beans.factory.annotation.Value' ...      [exec] mkdir -p target/cpp      [exec] g++ -c -std=c++11 -Wall -fPIC -I ... -o target/cpp/Judger.Core.Runner.o [INFO] Executed tasks ... [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 12.432 s [INFO] Finished at: 2015-11-26T13:22:46+08:00 [INFO] Final Memory: 81M/513M [INFO] ------------------------------------------------------------------------ ```  And you'll get a package named `voj.judger.jar` in the `target` folder.  ### Configuration  #### Setup the ActiveMQ  To reduce the memory of ActiveMQ, you can edit `activemq.xml` in `ACTIVEMQ_HOME\conf`.  Please find following content in this file, and change it to proper values that suitable for your servers.  ``` <systemUsage>     <systemUsage>         <memoryUsage>             <!-- Change this value -->             <memoryUsage limit="128 mb" />         </memoryUsage>         <storeUsage>             <!-- Change this value -->             <storeUsage limit="4 gb"/>         </storeUsage>         <tempUsage>             <!-- Change this value -->             <tempUsage limit="4 gb"/>         </tempUsage>     </systemUsage> </systemUsage> ```  #### Setup the Web Application  Create a database in MySQL, import `voj.sql`.  Edit the values in `/WEB-INF/classes/voj.properties` of the file `voj.web.war`.  You can open it with archive manager software such as `WinRAR`.  After then, you can copy this file `voj.web.war` to `TOMCAT_HOME/webapps`.  **IMPORTANT:** For Windows users, please edit `server.xml` of your Tomcat configuration:  ``` <Connector connectionTimeout="20000" port="8080" protocol="HTTP/1.1"     redirectPort="8443" useBodyEncodingForURI="true"> </Connector> ```  #### Setup the Judger  Edit the values in `/voj.properties` of the file `voj.judger.jar`.  You can run the judger using following command :  **Windows**:  ``` javaw -jar voj.judger.jar ```  **Linux**:  ``` sudo java -jar voj.judger.jar ```  **Important:**  If you are using Linux, please run following commands using `root`:  ``` # Shutdown and Kill process is not allowed for non-root user chmod 700 /sbin/init chmod 700 /sbin/poweroff chmod 700 /usr/bin/pkill ```  ---  ### Contribution  We're glad that you want to improve this project.   - **We NEED TRANSLATORS** for multi-language support(English and Chinese have supported). - You can report bugs [here](https://github.com/hzxie/voj/issues). - You can also create a pull request if you can fix the bug. - If you want to add features to the project, please tell us in the [issues](https://github.com/hzxie/voj/issues) page before developing.  Thanks for your corporation.  ### License  This project is open sourced under GNU GPL v3.
KEN-LJQ/WMS	# WMS 基于SSM框架的仓库管理系统    ## 功能  * 系统操作权限管理。系统提供基本的登入登出功能，同时系统包含两个角色：系统超级管理员和普通管理员，超级管理员具有最高的操作权限，而普通管理员仅具有最基本的操作权限，而且仅能操作自己被指派的仓库。 * 请求URL鉴权。对于系统使用者登陆后进行操作发送请求的URL，后台会根据当前用户的角色判断是否拥有请求该URL的权限。 * 基础数据信息管理。对包括：货物信息、供应商信息、客户信息、仓库信息在内的基础数据信息进行管理，提供的操作有：添加、删除、修改、条件查询、导出为Excel和到从Excel导入。 * 仓库管理员管理。对仓库管理员信息CRUD操作，或者为指定的仓库管理员指派所管理的仓库。上述中的仓库管理员可以以普通管理员身份登陆到系统。 * 库存信息管理。对库存信息的CRUD操作，导入导出操作，同时查询的时候可以根据仓库以及商品ID等信息进行多条件查询。 * 基本仓库事务操作。执行货物的入库与出库操作。 * 系统登陆日志查询。超级管理员可以查询某一用户在特定时间段内的系统登陆日志。 * 系统操作日志查询。超级管理员可以查询某一用户在特定时间段内对系统进行操作的操作记录。、 * 密码修改。    ## 使用到的框架和库  * Apache POI * MyBatis * Spring Framework * Spring MVC * Apache Shiro * Ehcache * Apache Commons * Log4j * Slf4j * Jackson * C3P0 * Junit * MySQL-Connector * jQuery * Bootstrap    ## 部分截图  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/WMS-%E6%88%AA%E5%9B%BE1.PNG)  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/MWS-%E6%88%AA%E5%9B%BE2.PNG)  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/WMS-%E6%88%AA%E5%9B%BE3.PNG)  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/WMS-%E6%88%AA%E5%9B%BE4.PNG)  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/WMS-%E6%88%AA%E5%9B%BE5.PNG)  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/WMS-%E6%88%AA%E5%9B%BE7.PNG)  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/WMS-%E6%88%AA%E5%9B%BE8.PNG)  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/WMS-%E6%88%AA%E5%9B%BE8.PNG)  ![](https://raw.githubusercontent.com/KEN-LJQ/MarkdownPics/master/Resource/2017-3-9/WMS-%E6%88%AA%E5%9B%BE9.PNG)
zserge/anvil-examples	# Anvil samples  [Anvil][1] is a tiny reactive UI library for Android. Inspired by [React][2] and [Mithril][3], it brings declarative data binding, unidirectional data flow and componentization and other things that would make your code look cleaner and easier to maintain.  This repository contains small examples of how Anvil can be used.  ## Example projects  * [Hello][4] - simple static layout with a classical text message 	- how to start Anvil project 	- how to write layouts without XML * [Counter][5] - simple click counter 	- how to bind variables to views 	- how to bind event listeners to views 	- how easy is to to keep UI in sync with data (automatic rendering) * [Login form][6] - two input fields, push button and some logic behind them 	- how to use text watcher bindings 	- how to use Java 8 method references as event bindings * [Item picker][7] - animated item picker component with next/prev buttons 	- how to use animations 	- how to use states 	- how to use currentView() to get access to the real View object 	- how to use Java8 lambdas in Anvil * [Currency exchange app][8] - fetches latest currency rates from the backend, calculates converted values as you type. 	- how to separate model logic from the view logic 	- how to separate view styling from view hierarchy 	- how to bind adapters 	- how to get two-directional data binding for text input * [Countdone clone][9] (current Anvil example) - pomodoro-like app: define how long the task should take and see if you finish it in time 	- how to use backstack having just one activity 	- how to save component state 	- how to use custom fonts and icon fonts * [Todo app][11] - classical MVC example: add tasks, check tasks, remove checked tasks 	- how to use list adapters 	- how the same app would look with [Java 7][10], [Java 8][11] and [Kotlin][12]  [1]: https://github.com/zserge/anvil/ [2]: http://facebook.github.io/react/ [3]: http://mithril.js.org/  [4]: ./hello/ [5]: ./counter/ [6]: ./login/ [7]: ./anim-picker/ [8]: ./currency/ [9]: ./countdone/ [10]: ./todo/ [11]: ./todo-java8/ [12]: ./todo-kotlin/
Yaccc/Dreamvc	Dreamvc ===================================  A simple and support the restful structure of the Java MVC framework, I have little talent and less learning, we hope the exhibitions >`Dreamvc` combines the ideas of `Struts2` and `SpringMVC` framework，But `Dreamvc` has two entries(filter and servlet)，`Dreamvc` combines the template mechanism of `Python-flask` framework，Achieve their own template，Self expanding，At present, the `JSP` and `velocity` templates are implemented by Dreamvc.Dreamvc provides the developer's IOC interface can be combined with any IOC framework，Dreamvc uses the `Struts2` interceptor mechanism(Stack)，Annotation convenient way，Matching algorithm can be fuzzy matching / precision matching，The parameters of the method are injected into the `javassist` or `Spring framework`.  ####IOC factory interface - As long as the implementation of this interface, you can let Dramvc and any IOC container ```java package org.majorxie.dreamvc.ioc.factory; import java.util.List; import javax.servlet.ServletConfig; import javax.servlet.ServletContext; /**  *IOC Factory  * @author xiezhaodong(majorxie@139.com)  *2014-10-24  */ public interface IocFactory { 	/** 	 * init config 	 */ 	void init(ServletContext context); 	/** 	 * destory 	 */ 	void destroy(); 	/** 	 *get all controller 	 */ 	List<Object> getControllers()throws Exception; 	/** 	 * get all interceptors 	 */ 	List<Object> getInterceptors(); 	/** 	 * init others object from IOC 	 */ 	List<Object> getOthers(); } ``` - Then the implementation of full path incoming classes on the line in `web.xml`, I implemented a default `Springioc` (below) ```xml <init-param> 	<param-name>container</param-name> 	<param-value>org.majorxie.dreamvc.ioc.factory.SpringIocFactory</param-value> </init-param> ``` ####Template mode integration, default JSP template - Combined with the idea of flask framework. So that users can choose their own templates such as JSP/velocity/freemarker, etc., as long as the successor to the template factory (as follows) ```java package org.majorxie.dreamvc.template; import org.majorxie.dreamvc.tag.Contextconfig.StrategyConfig; /**  * Python flask framework, abstract a template method  * @author xiezhaodong  *2014-11-14  */ public abstract class TemplateFactory { 	private static TemplateFactory instance; 	public static void setInstance(TemplateFactory instance) { 		TemplateFactory.instance = instance; 	} 	public static TemplateFactory getInstance(){ 		return instance; 	} 	public abstract void init(StrategyConfig config); 	public abstract Template initTemplate(String path,ForwardType type) throws Exception; } ``` - Achieve this interface, complete the implementation of the template (see the implementation of the JSP template specifically) ```java package org.majorxie.dreamvc.template; import java.util.Map; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; public interface Template { 	void handleRender(HttpServletRequest req,HttpServletResponse resp,Map<String, Object> models)throws Exception;  } ``` ```xml  </init-param> 	    <init-param> 	    <param-name>template</param-name> 	    <param-value>org.majorxie.dreamvc.template.JspTemplateFactory</param-value>  </init-param> ``` >If you are using the default `jsp` template, you can give this parameter, dreamvc will automatically help you select the JSP template  ###How to use - Construction of the project with `pom.xml`, which is necessary for the three party packages and construction (example/example2.0 is a complete example). as follows ```xml 1. git clone git@github.com:Yaccc/Dreamvc.git 2. cd Dreamvc & mvn  install ``` - add dependencies and the necessary configuration to pom.xml ```xml <dependency>       <groupId>org.majorxie.dreamvc</groupId>       <artifactId>mvc-core</artifactId>       <version>1.0-SNAPSHOT</version> </dependency> ....Omit the spring configuration ``` - Define your controller to `applicationContext.xml` ```xml <?xml version="1.0" encoding="UTF-8"?> <beans  .... <bean id="myController" class="your_Controller"></bean><!--add controller --> <!--add your interceptor--> <!--<bean id="interceptor1" class="example.interceptor.Interceptor_01"></bean>--> <!--<bean id="interceptor_02" class="example.interceptor.Interceptor_02"></bean>--> 	 </beans> ``` - The simplest `web.xml` configuration (see `example`). ```xml  <!-- Configuration file location, the default is /WEB-INF/applicationContext.xml -->     <context-param>         <param-name>contextConfigLocation</param-name>         <param-value>/WEB-INF/applicationContext.xml</param-value>     </context-param>     <!-- The Spring context listener -->     <listener>         <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>     </listener>     <filter>         <filter-name>DispatcherFilter</filter-name>         <filter-class>org.majorxie.dreamvc.mvc.dispatcher.DispatcherFilter         </filter-class><!-- Select filter to enter, or select servlet to enter (as above) -->         <init-param>             <param-name>container</param-name>             <param-value>org.majorxie.dreamvc.ioc.factory.SpringIocFactory</param-value><!-- Select springioc as the IOC container -->         </init-param>         <init-param>             <param-name>CodeEnhancement</param-name>             <param-value>SpringAsm</param-value><!-- Choose SpringAsm or javassist -->          </init-param>         <init-param>             <param-name>template</param-name>             <param-value></param-value><!-- Select the template for the return of the template here and not automatically select the JSP template -->         </init-param>     </filter>  <filter-mapping>         <filter-name>DispatcherFilter</filter-name>         <url-pattern>*.do</url-pattern>     </filter-mapping> ``` - `Controller` how to write, how to use the template? ```java @Controller//Using controller annotations to indicate the class or implement the controller interface public class ConTest {     @RequestURI("/login.do")//It is suggested that the.Do is similar to /user/login/check.do, and the best parameters are passed, and no delivery will be reported to 404. 	public Renderer hehe(String name,int  s) throws IOException{//Bean is currently not supported, as long as the traditional parameters 			//Pass, function return value can make String, void, render.render said only template currently has /JsonTemplate/TextTemplate/ 			//JSP template TemplateRender, the default jump is forword jump, you can see the constructor using FORWARD.Rediect set the client jump 			//Server side jump can pass the map object, you can also like the following way 			TemplateRender render=new TemplateRender("WEB-INF/pages/test.jsp"); 			render.addVaule("posts", "qwoeqwe"); 			return render; 	}  	@RequestURI("/check.do") 	public void haha() { 		try { 			System.out.println("do something..."); 			} catch (Exception e) { 			    e.printStackTrace(); 			} 	} } ``` >If you want to get servletapi, you can get the request object in `ActionContext.getHttpServletRequest (), and the other is the same, see the `example` project  #### The use of the interceptor First, you must implement the `Interceptor` interface or the `AbstractInterceptor` class to implement the `doInterceptor () and `afterInterceptor () method, and must be used.`InterceptorURI` comment to specify the path to intercept, as follows ```java @InterceptorURI(url="/login.do") public class Interceptor_02 extends AbstractInterceptor {  			@Override 			public boolean doInterceptor() { 				System.out.println("strat——02"); 				return true; 			}  			@Override 			public void afterInterceptor() { 				System.out.println("end_02"); 			} } @InterceptorURI(url="/*") public class LoginInterceptor implements Interceptor { 			public void destory() { 			} 			public void init() { 			} 			public boolean doInterceptor() { 				System.out.println("login_start"); 				return true; 			} 			public void afterInterceptor() { 				System.out.println("login_end"); 			} } ``` `interceptor` `true` will return to the release, the execution of the next interceptor, `false` does not return a corresponding executing method And the highest degree of matching path will be a priority，While blocking the path of the relative length must be less than or equal to the path length. Equal to the uncertain path with `*` instead of For example, the path is `/user/login/check.do` So I can use `/*/*/check.do` to intercept it,You can also use the `/user/*/check.do` to match, if the short path to the end of the star, then the front of the path should be relatively the same.  - [English document](https://github.com/xiexiaodong/Dreamvc/blob/master/README.md) - [中文文档](https://github.com/xiexiaodong/Dreamvc/blob/master/README_ZH_CN.md)
florinpatrascu/micro	![](http://micro-docs.simplegames.ca/images/micro-logo.png)  **Micro**, for short: **(μ)** or **[Mu](http://en.wikipedia.org/wiki/Mu_\(letter\))**, is a modular Model View Controller framework ([MVC Pull](http://en.wikipedia.org/wiki/Web_application_framework#Push-based_vs._pull-based)) for rapid web development. Flexible and powerful at the same time, Micro lets you choose your tools from a decently sized pool of well known products:   - [Velocity](http://micro-docs.simplegames.ca/views/engines.md#Velocity), [Markdown](http://micro-docs.simplegames.ca/views/engines.md#Markdown), [Freemarker](http://micro-docs.simplegames.ca/views/engines.md#Freemarker), [Mustache (java)](http://micro-docs.simplegames.ca/views/engines.md#Mustache), [Stringtemplate](http://micro-docs.simplegames.ca/views/engines.md#StringTemplate); for designing the dynamic content of your web pages.  - [Beanshell](http://www.beanshell.org/), server side [Javascript(Rhino)](http://www.mozilla.org/rhino/), [JRuby](http://jruby.org/) and [more](http://commons.apache.org/bsf/); for prototyping controllers before converting them to Java classes, if need be.  #### Using Micro  The easiest way to start developing with Micro is to download the source code from [Github](https://github.com/florinpatrascu/micro)      git clone https://github.com/florinpatrascu/micro  Build it:      cd micro     mvn clean install test -Dtest=MicroGenericTest     # or:     mvn clean install -DskipTests=true  # to skip the tests  #### Creating a new Micro web application Create a new webapp using the [Micro quickstart Maven archetype](archetypes/README.md)      cd archetypes/quickstart     mvn install  #### Usage from command line      mvn archetype:generate \       -DarchetypeGroupId=ca.simplegames.micro\       -DarchetypeArtifactId=micro-quickstart \       -DarchetypeVersion=0.2.2 \       -DgroupId=com.mycompany \       -DartifactId=myproject  #### Usage from [IntelliJ IDEA](https://www.jetbrains.com/idea/) Open IntelliJ. Choose `File/Import/Existing Project` and point it to `myproject` directory. Or, you can add the `quickstart` archetype in IntelliJ by simply following the few next steps:  - `File/New Project` - select `Create from archetype` and click the `Add Archetype` button - choose `ca.simplegames.micro:micro-quickstart` from the list  And simply follow the dialog prompted by IntelliJ :)  #### Maven depedency  ```` xml   <dependencies>     <dependency>       <groupId>ca.simplegames.micro</groupId>       <artifactId>micro-core</artifactId>       <version>0.2.2</version>     </dependency>   </dependencies> ````  ### Start your web application from command line Launching the generated application using the embedded Jetty web server is very easy:      cd myproject     mvn compile install exec:java      #or: mvn exec:java You can also easily start your web application from IntelliJ.   When the Micro web app is started, you will almost immediately see something like this:           _ __ ___ ( ) ___ _ __ ___         | '_ ` _ \| |/ __| '__/ _ \         | | | | | | | (__| | | (_) |         |_| |_| |_|_|\___|_|  \___/         = a modular micro MVC Java framework  and you can visit your web application by pointing your browser to: [http://localhost:8080](http://localhost:8080)  We hope Micro will help you develop web applications while increasing the fun quotient of programming as well.  Have fun! µ  ### Documentation The documentation is a work in progress and can be found here: [micro-docs.simplegames.ca](http://micro-docs.simplegames.ca). It is hosted at Heroku, using Micro itself for publishing. You can fork the documentation site and send pull requests. This is the Github repo for the docs: [micro-docs](https://github.com/florinpatrascu/micro-docs)  Feedback and contributions to the project, no matter what kind, are always very welcome.  ### Submitting an Issue We use the [GitHub issue tracker](https://github.com/florinpatrascu/micro/issues) to track bugs and features. Before submitting a bug report or feature request, check to make sure it hasn't already been submitted. When submitting a bug report, please include a [Gist](https://gist.github.com/) that includes a stack trace and any other details that may be necessary to reproduce the bug, including your Java version and operating system. Ideally, a bug report should include a pull request with failing specs.  ### Special thanks   - to my [wife](http://twitter.com/simonuta), for understanding my endless passion for programming.   - [JPublish.org](http://jpublish.org/) - a rusty but trusty framework. There are core concepts in Micro designed as continuations of the ideas developed for JPublish; Templates and Repositories, for example.   - Many thanks to [Anthony Eden](https://github.com/aeden) for being an inspiring developer and a model for many of us.   - Many thanks to [Frank Carver](https://github.com/efficacy) for contributing ideas to the [JRack](https://github.com/florinpatrascu/jrack), many of these being ported back into JRack and used by Micro.<p></p>   - [Spring framework](http://www.springsource.org/) - the localization support in Micro was extracted from selected classes originally developed for the early Spring framework.   - [Apache Wink](http://en.wikipedia.org/wiki/Apache_Wink) - used as a future support for [JSR-311](http://www.jcp.org/en/jsr/detail?id=311).   - to all our **contributors** and **supporters**. Cheers!  <hr>  ### License      Copyright (c) 2012-2016 Florin T.Pătraşcu      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at           http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
h819/spring-boot	# spring-boot    ## 概述  java 语言经过了多年的发展，生态系统完善，完全满足企业级应用。如果技术选型得当，对常用功能组件有积累，可以快速搭建系统，把主要精力放在业务逻辑上。    本项目尝试利用已有的开源项目，经过合理的配置与整合，构建一个常用的框架，项目开发时可以拿来即用，而不必再为选某一种实现而比来比去大费脑筋。    Spring 现在基本上也一统江湖了，只要你想要的解决方案，Spring 基本上都有相关实现，可以多看看他的 [项目](http://spring.io/projects/)。    我的习惯是，一边学习一边敲代码，读书笔记就用注释写在代码示例里，我只记住我做过什么就可以了，当需要相关功能的时候，到示例里去找，扫一眼代码写法，读一下注释，基本上也就知道怎么编写了。    初学者多读读注释，也算是学习的一个过程。有经验的同学，也帮我指点一下理解的偏差。    随着对相关原理理解的加深，代码在不停的 Refactor，就算是自己的编程经验总结吧。    项目取名为 **spring-boot** ，意在显式的指出用的是 **Spring Boot**。    这些项目会随着自己的总结，不停的添加和优化，示例也会越来越多。过一阵在 aliyun 上把做好的应用搭建起来，也好有个直观感受。    ## 项目简介    :recycle:  ### spring-boot-web    > 基于 Spring Boot 的 J2EE 开发实践，不发明什么，只是探索一种快速开发体验，开箱即用。主要功能  - 基于角色的访问控制(RBAC) ，可以任意创建用户、角色并分配菜单权限和资源权限。  - 用户创建、邮件激活  - ...    #### Core  - spring boot    #### Data  - spring data JPA  - hibernate  - querydsl(不用, JPA 可代替)    #### Web  - spring mvc  - FreeMarker Template  - Bootstrap  - JQuery  - Ace Admin    #### Security  - spring security  - spring security oauth2    #### Web Server  - tomcat  - jetty    #### DB Server  - mysql  - oracle    #### Environment  - Intellij IDEA  - Maven  - git    #### Utils  - h819 commons  - Apache Commons  - Guava      #### Ask  - [Google](http://www.google.com/) (一定想办法上)  - [git hub ](https://github.com//)(你能想到的，基本都有实现)  - [Stack Overflow ](http://stackoverflow.com/)  - [spring boot examples](https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples)  - [oschina ](http://www.oschina.net/)    #### Test  Chrome,Firefox, Edge, IE    :recycle:    ### h819-commons  > h819-commons  这是一个基础工具包，能总结出来的都放在这里，可以生成 jar 文件引用到其他项目中，模拟 [apache commons](https://commons.apache.org/) 做法，做成一个符合自己需要的工具包。    可以多翻翻代码，里面有各种例子和总结。    值得提到的工具有：    #### java se commons    Components | Description | Source  ---|---|---  Ftp | 可以连接 ftp 和 sftp，支持断点续传，比较文件是否发生变换 | /commons  Exec  | java 执行系统命令 | /commons  Pdf  | Pdf 新建、加密解密、加水印、页数统计、删除指定页等 | /commons  QRCode  | 二维码 | /commons  Others  | 还有一些常用工具，就不列举了 ... | /commons      #### web 工具  Components | Description | Source | Demo  ---|---|---|---  DTOUtils | PO to DTO 工具。使用 ***hibernate*** 的同学，估计对这个比较挠头，每次转换都费时费力，还容易出错。DTOUtils 可以实现自动转换，截断递归关联，对于级联层次很深的对象，可以指定转换深度。比目前大多数人采用 bean copy 的方案好。这个有时间我写一篇博客，详细说一下。 | /web | [url](###)   Spring JPA  | spring jpa 动态查询工具，可以动态组装查询条件，自动分页，很好用 | /web | [url](###)   Jqgird  | [Jqgrid](http://www.trirand.com/blog/?page_id=6/) 工具类，可以方便的处理查询条件。 | /web | [url](###)   ZTree | [ZTree](http://www.ztree.me/v3/main.php#_zTreeInfo/) java utils ，功能强大，做后台管理用。 | /web| [demo](http://www.canhelp.cn/boot/example/tree/manage/ztree.html)  Fuelux Tree  | [FueluxTree](http://getfuelux.com/javascript.html#tree/) java utils ，ui 很好看，做展示用吧。 | /web | [demo](http://www.canhelp.cn/boot/example/tree/manage/fuelux.html)  flexpaper  | [flexpaper](http://flexpaper.devaldi.com/annotate-pdf-documents-online.jsp) 在线文档展示的一种解决方案 | /web | [url](###)   Others | 逐步添加 ... | /web    :recycle:  ### spring-security-oauth  > spring security oauth2 , annotation 。需要注意的大坑是，oauth2-server 和 oauth2-resource 不能配置在一起，否则不能出现登录界面。貌似 xml 方式没问题。  配置了很久，没有解决。    - spring-security-oauth2-server  - spring-security-oauth2-resource  - spring-security-oauth2-client    :recycle:  ### h819-ztree  > ztree java project.      ## Contact  :e-mail: h81900 at outlook . com
white-cat/jeeweb	JeeWeb敏捷开发平台  ===============  * 	QQ交流群： 570062301  * 	官方网站： [https://www.jeeweb.cn](https://www.jeeweb.cn)  * 	官方论坛： [http://bbs.jeeweb.cn](http://bbs.jeeweb.cn)  * 	项目演示： [https://demo.jeeweb.cn](https://demo.jeeweb.cn)  * 开源中国项目地址： [http://git.oschina.net/dataact/jeeweb](http://git.oschina.net/dataact/jeeweb)  * Mybatis版本： [https://github.com/white-cat/jeeweb-mybatis](https://github.com/white-cat/jeeweb-mybatis)  * Myeclipse非Maven版本快盘下载： [https://pan.baidu.com/s/1hrFKF2k](https://pan.baidu.com/s/1hrFKF2k)    简介  -----------------------------------  JeeWeb是一款基于SpringMVC+Spring+Hibernate的敏捷开发系统；它是一款具有代码生成功能的智能快速开发平台；是以Spring Framework为核心容器，Spring MVC为模型视图控制器，Hibernate为数据访问层， Apache Shiro为权限授权层，Ehcahe对常用数据进行缓存，Disruptor作为并发框架，Bootstrap作为前端框架的优秀 **开源** 系统。    JeeWeb是一款 **全开源开发平台** ，特别 **代码生成器模块也采用开源模式** ，各位开发者可以根据自己的需要改造出更加适合自己的代码生成器，不管是做项目、学习、接私活它都将是你的最佳拍档；    JeeWeb主要定位于企业快速开发平台建设，已内置很多优秀的基础功能和高效的 **代码生成** 工具，包括：系统权限组件、数据权限组件、数据字典组件、核心工具组件、视图操作组件、代码生成、 **UI模版标签** 库等。前端界面风格采用了结构简单、性能优良、页面美观大气的Twitter Bootstrap页面展示框架。采用分层设计、提交数据安全编码、密码加密、访问验证、数据权限验证。使用Maven做项目管理，提高项目的易开发性、扩展性。    目前功能模块代码生成器、权限框架、数据字典、数据缓存、并发框架、数据监控、计划任务、多数据源管理、附件管理、类似mybatis动态SQL、UI模板标签、短信发送、邮件发送、统计功能等功能。    JeeWeb的开发方式采用（ **代码生成器快速设计生成代码->手工完善逻辑->丰富模板标签快速前端开发** ），可以快速协助java开发人员解决60%的重复工作，让开发人员更多关注业务逻辑的实现，框架使用前端模板标签，解放JAVA开发人员的开发压力，提高开发效率，为企业节省项目研发成本，减少开发周期。    JeeWeb 技术特点  -----------------------------------  JeeWeb使用目前流程的WEB开发架构技术，如 **SpringMVC, Hibernate,Apache Shiro, Disruptor , ehcache, Jquery ,BootStrap** 等等，支持多种数据库MySQL, Oracle, sqlserver等。  **分层设计：使用分层设计，分为dao，service，Controller，view层，层次清楚，低耦合，高内聚。**      安全考虑：严格遵循了web安全的规范，前后台双重验证，参数编码传输，密码md5加密存储，shiro权限验证，从根本上避免了SQL注入，XSS攻击，CSRF攻击等常见的web攻击手段。    JeeWeb 功能特点  -----------------------------------  * 	采用SpringMVC+Spring+Hibernate+Shiro+ Ehcache+Disruptor+Jquery + Boostrap + Ztree等基础前后端架构架构  * 	采用面向声明的开发模式， 基于泛型编写极少代码即可实现复杂的数据展示、数据编辑、表单处理等功能，在不使用代码生成器的情况下，也只需要很少的代码就能实现基础的CURD操作，再配合在线开发与代码生成器的使用，更加加快了开发的进度，将J2EE的开发效率成本提高，可以将代码减少60%以上。  * 	在线开发(通过在线配置实现一个表模型的增删改查功能，无需一行代码，支持用户自定义表单布局)   * 	代码生成器，支持多种数据模型,根据表生成对应的Entity,Service,Dao,Controller,JSP等,增删改查功能生成直接使用  * 	UI标签开发库，针对前端UI进行标准封装表，页面统一采用UI标签实现功能：数据datagrid,treegrid,FileInput,Editor,GridSelect等，实现JSP页面零JS，开发维护简洁高效  * 	查询过滤器：只需前端配置，后台动态拼SQL追加查询条件；支持多种匹配方式（全匹配/模糊查询/包含查询/不匹配查询）  * 	移动平台支持，对Bootstrap(兼容Html5)进行标准封装  * 	国际化（支持多语言，国际化的封装为多语言做了便捷支持）  *   多数据源（在线配置数据源，数据源工作类封装）  * 	数据权限：整合Shiro权限  *   计划任务控制（在线配置计划任务、方便计划任务的时间调整规划）  *   邮件发送（配置邮件模版、邮件帐号的在线配置、邮件异步发送、邮件发送日志功能统计）  *   短信发送（配置短信模版、短信帐号的在线配置、短信异步发送、短信发送日志功能统计、支持短信发送平台动态切换）  *   多种首页风格切换,支持自定义首页风格。（Inspinia风格|ACE风格）  *   数据统计报表：丰富的报表统计功能  * 	支持多种浏览器: Google, 火狐, IE,360 等  * 	支持数据库: Mysql,Oracle10g,SqlServer等  * 	基础权限: 用户，角色，菜单权限  * 	Web容器测试通过的有Jetty和Tomcat,Weblogic  * 	要求JDK1.7+    技术选型  ===============    1、后端    * 核心框架：Spring Framework  * 安全框架：Apache Shiro  * 视图框架：Spring MVC  * 服务端验证：Hibernate Validator  * 布局框架：SiteMesh  * 任务调度：Quartz  * 持久层框架：Hibernate   * 数据库连接池：Alibaba Druid  * 缓存框架：Ehcache  * 并发框架：Disruptor  * 日志管理：SLF4J、Log4j  * 工具类：Apache Commons、Jackson、Xstream、     2、前端    * JS框架：jQuery。  * CSS框架：Twitter Bootstrap  * 客户端验证：Validform。  * 富文本在线编辑：markdown、simditor、Summernote、CodeMirror自由切换  * 文件上传工具:Bootstrap fileinput  * 数据表格：jqGrid  * 对话框：layer  * 树结构控件：jQuery zTree  * 日期控件： datepicker  * 代码高亮： syntaxhighlighter    简单使用说明  -----------------------------------  * 导入sql/jeeweb-mysql-v1.0.sql文件到mysql数据库  * 导入项目到Eclipse.  * 修改数据库配置文件dbconfig.properties中的账号密码.  * 启动项目,管理员账号admin/密码123456    技术文档  -----------------------------------  * [JeeWeb 开发环境搭建入门(完善中...)]  * [JeeWeb 开发手册(完善中...)]  * [JeeWeb  常见问题贴(完善中...)]  * [JeeWeb 视频教程(完善中...)]  * [JeeWeb 官方百度网盘](http://pan.baidu.com/s/1hrFKF2k)    平台目录结构说明  -----------------------------------  ```  ├─main  │  │    │  ├─java  │  │   │  │  │   └─cn.jeeweb----------------平台主代码  │  │             │  │  │             ├─core----------------平台核心模块存放目录  │  │             │    ├─common----------------项目公用的部分(例如基础controller、dao、service、以及动态SQL/HQL实现)  │  │             │    │  │  │             │    ├─disruptor-------------并发框架的实现(短信发送模块、邮件发送模块)  │  │             │    │  │  │             │    ├─filter、interceptor---安全过滤器、其他一些公用拦截器  │  │             │    │  │  │             │    ├─mapper----------------各种Object到Xml、Object到Json的映射转换类  │  │             │    │  │  │             │    ├─model-----------------前段暂时的各种JSON实体  │  │             │    │  │  │             │    ├─quartz----------------quartz定时任务实现  │  │             │    │  │  │             │    ├─query-----------------前端请求，后端自动组装、以及分页的查询模块  │  │             │    │  │  │             │    ├─repository------------持久层相关类  │  │             │    │  │  │             │    ├─security--------------安全相关类  │  │             │    │  │  │             │    ├─tag-------------------GRID标签、form标签、html组件加载标签等  │  │             │    │  │  │             │    └─utils-----------------一些工具类  │  │             │      │  │             └─modules----------------内置功能模块（按照业务模块分类）  │  │                  ├─charts----------------图表模块  │  │                  │  │  │                  └─sys-------------------权限模块  │  │                     │  │  │                     ├─controller---控制层  │  │                     │  │  │                     ├─entity-------实体层  │  │                     │  │  │                     ├─service------服务层  │  │                     │  │  │                     ├─security-----安全相关  │  │                     │  │  │                     ├─tags----------------标签  │  │                     │  │  │                     └─utils-----------------一些工具类  │  │     │  │  │  ├─resources----------------平台资源文件  │  │     │  │  │     ├─ehcache----------------缓存配置目录（ehcache.xml缓存配置文件）  │  │     │   │  │     ├─i18n-------------------国际化信息所在的文件名文件目录（messages.properties项目国际化、ValidationMessages.propertieshibernate validator验证）  │  │     │   │  │     ├─mapper----------------一些映射（特别tag/html/中的文件，为html组件加载包）  │  │     │   │  │     ├─statement-------------动态SQL/HQL(是更灵活的操作SQL/HQL)  │  │     │   │  │     ├─codegen.properties-----代码生成配置  │  │     │   │  │     ├─dbconfig.properties----数据库配置  │  │     │   │  │     ├─jeeweb.properties------项目的一些配置  │  │     │   │  │     ├─shiro.properties-------shiro配置  │  │     │   │  │     ├─shiro.properties-------代码生成器的模板文件，以及其他模板存放目录  │  │     │   │  │     ├─*.properties----------（其他配置，可以查看配置的前缀，对应具体业务的配置,如何:sms.properties短信、email.properties邮件配置）  │  │     │   │  │     └─spring-*.xml-----------Spring相关文件  │  │  │  └─webapp----------------web页面和静态资源存放的目录  │      │  │      └─WEB-INF  │            │  │            ├─static----------------静态资源目录  │            │    │  │            │    ├─ace----------------ace样式JS,CSS文件  │            │    ├─vendors----------------第三方的JS，css,按照第三方包名单独保存  │            │    ├─common-----------------系统的公用JS.CSS文件  │            │    └─modules----------------功能模块CSS,以及JS,按照模板名单独存放  │            └─webpage----------------视图文件目录  │                 ├─decorators-------------视图装饰目录  │                 ├─error------------------系统异常映射相关页面  │                 └─modules----------------内置核心功能模块视图相关文件（按照模板名单独存放）  ```  系统演示  -----------------------------------  ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233138_66acc47c_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233150_79627fa7_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233200_33d385db_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233211_5e7fc693_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233222_8ab40914_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233239_ebb4d8bb_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233252_11c11f7d_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233303_da10ce13_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233314_7bcc9728_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233324_44fbe21c_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233335_358d1208_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233347_79912823_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233356_3b2a0c61_1394985.png "在这里输入图片标题")  代码示例  -----------------------------------  ###  [1].GRID列表  ```  <grid:grid id="codegenGrid"  url="${adminPath}/codegen/table/ajaxList">      <grid:column label="sys.common.key" hidden="true"   name="id"/>      <grid:column label="codegen.table.tabletype"   width="60" name="tableType"   dict="tabletype"  query="true" queryMode="select"  />      <grid:column label="codegen.table.table.name"   width="120"  name="tableName"  query="true" />  	<grid:column label="codegen.table.remarks"  name="remarks" />  	<grid:column label="codegen.table.sync.database"  width="80" dict="sf" formatterClass="0:label label-danger;1:label label-success" name="syncDatabase" />  	  	<grid:column label="sys.common.opt"  name="opt" formatter="button" width="300"/>  	<grid:button title="sys.common.remove"  groupname="opt" function="rowConfirm" tipMsg="确认要移除该条记录吗？" outclass="btn-warning" innerclass="fa-remove" url="${adminPath}/codegen/table/{id}/remove" />  	<grid:button groupname="opt" function="delete" tipMsg="确认要删除该条记录,删除会删除对应的表结构，请谨慎操作！" />  	<grid:button title="codegen.table.sync.database"  groupname="opt" function="rowConfirm"  tipMsg="确认要强制同步数据库吗？同步数据库将删除所有数据重新建表！" outclass="btn-info" innerclass="fa-database"  url="${adminPath}/codegen/table/{id}/syncDatabase" />  	<grid:toolbar  function="create"  winwidth = "1000px"/>  	<grid:toolbar  function="update"  winwidth = "1000px"/>  	<grid:toolbar title="codegen.table.import" icon="fa-database" function="createDialog" url="${adminPath}/codegen/table/importDatabase"  />  	<grid:toolbar title="codegen.table.gen" icon="fa-file-code-o" function="updateDialog" url="${adminPath}/codegen/table/{id}/generateCode"  />  	<grid:toolbar title="codegen.table.createmenu" icon="fa-anchor" function="updateDialog" url="${adminPath}/codegen/table/{id}/createMenu"  />  	  	<grid:toolbar  function="search"/>  	<grid:toolbar  function="reset"/>  </grid:grid>  ```  ![JSP列表图片](https://git.oschina.net/uploads/images/2017/0630/205011_87420e1e_1394985.png "JSP列表图片")    ###  [2].TREEGRID列表  ```  <grid:grid id="menuGridId" async="true" treeGrid="true"  expandColumn="name"  sortname="sort" url="${adminPath}/sys/menu/ajaxTreeList">  	<grid:column label="sys.common.key" hidden="true"   name="id" />  	<grid:column label="sys.menu.name"  name="name"  query="true" condition="like"/>  	<grid:column label="sys.menu.url"  name="url"  />      <grid:column label="sys.menu.permission"  name="permission"  />      <grid:column label="sys.menu.isshow"  name="isshow" dict="sf"/>            <grid:column label="sys.common.opt"  name="opt" formatter="button" width="100"/>  	<grid:button   groupname="opt" function="delete" />        	<grid:toolbar  function="create"/>  	<grid:toolbar  function="update"/>  	<grid:toolbar  function="delete"/>  	<grid:toolbar  function="search"/>  	<grid:toolbar  function="reset"/>  </grid:grid>  ```  ![TREEGRID](https://git.oschina.net/uploads/images/2017/0630/205353_af457e21_1394985.png "TREEGRID")  ###  [3].表单代码  ```  <form:form id="userForm" modelAttribute="data" method="post" class="form-horizontal">  		<form:hidden path="id"/>  		<table  class="table table-bordered  table-condensed dataTables-example dataTable no-footer">  		   <tbody>  		       <tr>  		         <td  class="width-15 active text-right">	<label><font color="red">*</font>用户名:</label></td>  		         <td  class="width-35" >  		             <form:input path="username" class="form-control" ajaxurl="${adminPath}/sys/user/validate"  validErrorMsg="用户名重复"  htmlEscape="false"  datatype="*"  nullmsg="请输入用户名！"/>  		             <label class="Validform_checktip"></label>  		         </td>  		          <td  class="width-15 active text-right">	  		              <label><font color="red">*</font>姓名:</label>  		         </td>  		         <td class="width-35" >  		             <form:input path="realname" class="form-control " datatype="*" nullmsg="请输入姓名！" validErrorMsg="用户名重复" htmlEscape="false" />  		             <label class="Validform_checktip"></label>  		         </td>  		      </tr>  		      <tr>  		         <td  class="width-15 active text-right">	  		              <label><font color="red">*</font>邮箱:</label>  		         </td>  		         <td class="width-35" >  		             <form:input path="email" class="form-control" ajaxurl="${adminPath}/sys/user/validate"   datatype="e" nullmsg="请输入邮箱！"  htmlEscape="false" />  		             <label class="Validform_checktip"></label>  		         </td>  		         <td  class="width-15 active text-right">	  		           	 <label><font color="red">*</font>联系电话:</label>  		         </td>  		         <td  class="width-35" >  		             <form:input path="phone" class="form-control" ajaxurl="${adminPath}/sys/user/validate"  htmlEscape="false"  datatype="m"  nullmsg="请输入用户名！"/>  		             <label class="Validform_checktip"></label>  		         </td>  		      </tr>  		      <tr>  		         <td  class="width-15 active text-right">	  		              <label><font color="red">*</font>密码:</label>  		         </td>  		         <td class="width-35" >  		             <input type="password" value="" name="password"  class="form-control" datatype="*6-16" nullmsg="请设置密码！" errormsg="密码范围在6~16位之间！" />  		             <label class="Validform_checktip"></label>  		         </td>  		         <td  class="width-15 active text-right">	<label><font color="red">*</font>确认密码:</label></td>  		         <td  class="width-35" >  		             <input type="password" value="" name="userpassword2" class="form-control" datatype="*" recheck="password" nullmsg="请再输入一次密码！" errormsg="您两次输入的账号密码不一致！" />  		             <label class="Validform_checktip"></label>  		         </td>  		      </tr>  		      <tr>  		         <td class="active"><label class="pull-right"><font color="red">*</font>用户角色:</label></td>  		         <td>  		         	<form:checkboxes path="roleIdList" nested="false" items="${allRoles}" itemLabel="name" itemValue="id" htmlEscape="false" cssClass="i-checks required"/>  		            		         </td>  		      </tr>  		      <tr>  				<td class="width-15 active"><label class="pull-right">组织机构:</label></td>  				<td colspan="3">  				   <form:treeselect title="请选择组织机构" path="organizationIds"  nested="false"  dataUrl="${adminPath}/sys/organization/treeData" labelName="parentname" labelValue="${organizationNames}" multiselect="true" />	     				</td>  		      </tr>  		       		   </tbody>  		   </table>     	</form:form>  ```  ![表单](https://git.oschina.net/uploads/images/2017/0630/205612_2d09fd89_1394985.png "表单")
white-cat/jeeweb-mybatis	JeeWeb敏捷开发平台(Mybatis)  ===============  * 	QQ交流群： 570062301  * 	官方网站： [https://www.jeeweb.cn](https://www.jeeweb.cn)  * 	官方论坛： [http://bbs.jeeweb.cn](http://bbs.jeeweb.cn)  * 	项目演示： [https://mybatis.jeeweb.cn](https://mybatis.jeeweb.cn)  *  Hibernate版本： [https://github.com/white-cat/jeeweb/](https://github.com/white-cat/jeeweb/)    简介  -----------------------------------  JeeWeb Mybatis版本是一款基于SpringMVC+Spring+Mybatis+Mybatis Plus的敏捷开发系统；它是一款具有代码生成功能的智能快速开发平台；是以Spring Framework为核心容器，Spring MVC为模型视图控制器，Mybatis为数据访问层， Apache Shiro为权限授权层，Ehcahe对常用数据进行缓存，Disruptor作为并发框架，Bootstrap作为前端框架的优秀 **开源** 系统。    JeeWeb是一款 **全开源开发平台** ，特别 **代码生成器模块也采用开源模式** ，各位开发者可以根据自己的需要改造出更加适合自己的代码生成器，不管是做项目、学习、接私活它都将是你的最佳拍档；    JeeWeb主要定位于企业快速开发平台建设，已内置很多优秀的基础功能和高效的 **代码生成** 工具，包括：系统权限组件、数据权限组件、数据字典组件、核心工具组件、视图操作组件、代码生成、 **UI模版标签** 库等。前端界面风格采用了结构简单、性能优良、页面美观大气的Twitter Bootstrap页面展示框架。采用分层设计、提交数据安全编码、密码加密、访问验证、数据权限验证。使用Maven做项目管理，提高项目的易开发性、扩展性。    目前功能模块代码生成器、权限框架、数据字典、数据缓存、并发框架、数据监控、计划任务、多数据源管理、附件管理、类似mybatis动态SQL、UI模板标签、短信发送、邮件发送、统计功能等功能。    JeeWeb的开发方式采用（ **代码生成器快速设计生成代码->手工完善逻辑->丰富模板标签快速前端开发** ），可以快速协助java开发人员解决60%的重复工作，让开发人员更多关注业务逻辑的实现，框架使用前端模板标签，解放JAVA开发人员的开发压力，提高开发效率，为企业节省项目研发成本，减少开发周期。    JeeWeb 技术特点  -----------------------------------  JeeWeb使用目前流程的WEB开发架构技术，如 **SpringMVC, Mybatis,Apache Shiro, Disruptor , ehcache, Jquery ,BootStrap** 等等，支持多种数据库MySQL, Oracle, sqlserver等。  **分层设计：使用分层设计，分为dao，service，Controller，view层，层次清楚，低耦合，高内聚。**      安全考虑：严格遵循了web安全的规范，前后台双重验证，参数编码传输，密码md5加密存储，shiro权限验证，从根本上避免了SQL注入，XSS攻击，CSRF攻击等常见的web攻击手段。    JeeWeb 功能特点  -----------------------------------  * 	采用SpringMVC+Spring+Mybatis+Mybatis Plus+Shiro+ Ehcache+Disruptor+Jquery + Boostrap + Ztree等基础前后端架构架构  * 	采用面向声明的开发模式， 基于泛型编写极少代码即可实现复杂的数据展示、数据编辑、表单处理等功能，在不使用代码生成器的情况下，也只需要很少的代码就能实现基础的CURD操作，再配合在线开发与代码生成器的使用，更加加快了开发的进度，将J2EE的开发效率成本提高，可以将代码减少60%以上。  * 	在线开发(通过在线配置实现一个表模型的增删改查功能，无需一行代码，支持用户自定义表单布局)   * 	代码生成器，支持多种数据模型,根据表生成对应的Entity,Service,Dao,Controller,JSP等,增删改查功能生成直接使用  * 	UI标签开发库，针对前端UI进行标准封装表，页面统一采用UI标签实现功能：数据datagrid,treegrid,FileInput,Editor,GridSelect等，实现JSP页面零JS，开发维护简洁高效  * 	查询过滤器：只需前端配置，后台动态拼SQL追加查询条件；支持多种匹配方式（全匹配/模糊查询/包含查询/不匹配查询）  * 	移动平台支持，对Bootstrap(兼容Html5)进行标准封装  * 	国际化（支持多语言，国际化的封装为多语言做了便捷支持）  *   多数据源（在线配置数据源，数据源工作类封装）  * 	数据权限：整合Shiro权限  *   计划任务控制（在线配置计划任务、方便计划任务的时间调整规划）  *   邮件发送（配置邮件模版、邮件帐号的在线配置、邮件异步发送、邮件发送日志功能统计）  *   短信发送（配置短信模版、短信帐号的在线配置、短信异步发送、短信发送日志功能统计、支持短信发送平台动态切换）  *   多种首页风格切换,支持自定义首页风格。（Inspinia风格|ACE风格）  *   数据统计报表：丰富的报表统计功能  * 	支持多种浏览器: Google, 火狐, IE,360 等  * 	支持数据库: Mysql,Oracle10g,SqlServer等  * 	基础权限: 用户，角色，菜单权限  * 	Web容器测试通过的有Jetty和Tomcat,Weblogic  * 	要求JDK1.7+    技术选型  ===============    1、后端    * 核心框架：Spring Framework  * 安全框架：Apache Shiro  * 视图框架：Spring MVC  * 服务端验证：Hibernate Validator  * 布局框架：SiteMesh  * 任务调度：Quartz  * 持久层框架：Mybatis  * 数据库连接池：Alibaba Druid  * 缓存框架：Ehcache  * 并发框架：Disruptor  * 日志管理：SLF4J、Log4j  * 工具类：Apache Commons、Jackson、Xstream、     2、前端    * JS框架：jQuery。  * CSS框架：Twitter Bootstrap  * 客户端验证：Validform。  * 富文本在线编辑：markdown、simditor、Summernote、CodeMirror自由切换  * 文件上传工具:Bootstrap fileinput  * 数据表格：jqGrid  * 对话框：layer  * 树结构控件：jQuery zTree  * 日期控件： datepicker  * 代码高亮： syntaxhighlighter    简单使用说明  -----------------------------------  * 导入sql/jeeweb-mysql-v1.0.sql文件到mysql数据库  * 导入项目到Eclipse.  * 修改数据库配置文件dbconfig.properties中的账号密码.  * 启动项目,管理员账号admin/密码123456      平台目录结构说明  -----------------------------------  ```  ├─main  │  │    │  ├─java  │  │   │  │  │   └─cn.jeeweb----------------平台主代码  │  │             │  │  │             ├─core----------------平台核心模块存放目录  │  │             │    ├─common----------------项目公用的部分(例如基础controller、dao、service、以及动态SQL/HQL实现)  │  │             │    │  │  │             │    ├─disruptor-------------并发框架的实现(短信发送模块、邮件发送模块)  │  │             │    │  │  │             │    ├─filter、interceptor---安全过滤器、其他一些公用拦截器  │  │             │    │  │  │             │    ├─mapper----------------各种Object到Xml、Object到Json的映射转换类  │  │             │    │  │  │             │    ├─model-----------------前段暂时的各种JSON实体  │  │             │    │  │  │             │    ├─quartz----------------quartz定时任务实现  │  │             │    │  │  │             │    ├─query-----------------前端请求，后端自动组装、以及分页的查询模块  │  │             │    │  │  │             │    ├─repository------------持久层相关类  │  │             │    │  │  │             │    ├─security--------------安全相关类  │  │             │    │  │  │             │    ├─tag-------------------GRID标签、form标签、html组件加载标签等  │  │             │    │  │  │             │    └─utils-----------------一些工具类  │  │             │      │  │             └─modules----------------内置功能模块（按照业务模块分类）  │  │                  ├─charts----------------图表模块  │  │                  │  │  │                  └─sys-------------------权限模块  │  │                     │  │  │                     ├─controller---控制层  │  │                     │  │  │                     ├─entity-------实体层  │  │                     │  │  │                     ├─service------服务层  │  │                     │  │  │                     ├─security-----安全相关  │  │                     │  │  │                     ├─tags----------------标签  │  │                     │  │  │                     └─utils-----------------一些工具类  │  │     │  │  │  ├─resources----------------平台资源文件  │  │     │  │  │     ├─ehcache----------------缓存配置目录（ehcache.xml缓存配置文件）  │  │     │   │  │     ├─i18n-------------------国际化信息所在的文件名文件目录（messages.properties项目国际化、ValidationMessages.propertieshibernate validator验证）  │  │     │   │  │     ├─mapper----------------一些映射（特别tag/html/中的文件，为html组件加载包）  │  │     │   │  │     ├─statement-------------动态SQL/HQL(是更灵活的操作SQL/HQL)  │  │     │   │  │     ├─codegen.properties-----代码生成配置  │  │     │   │  │     ├─dbconfig.properties----数据库配置  │  │     │   │  │     ├─jeeweb.properties------项目的一些配置  │  │     │   │  │     ├─shiro.properties-------shiro配置  │  │     │   │  │     ├─shiro.properties-------代码生成器的模板文件，以及其他模板存放目录  │  │     │   │  │     ├─*.properties----------（其他配置，可以查看配置的前缀，对应具体业务的配置,如何:sms.properties短信、email.properties邮件配置）  │  │     │   │  │     └─spring-*.xml-----------Spring相关文件  │  │  │  └─webapp----------------web页面和静态资源存放的目录  │      │  │      └─WEB-INF  │            │  │            ├─static----------------静态资源目录  │            │    │  │            │    ├─ace----------------ace样式JS,CSS文件  │            │    ├─vendors----------------第三方的JS，css,按照第三方包名单独保存  │            │    ├─common-----------------系统的公用JS.CSS文件  │            │    └─modules----------------功能模块CSS,以及JS,按照模板名单独存放  │            └─webpage----------------视图文件目录  │                 ├─decorators-------------视图装饰目录  │                 ├─error------------------系统异常映射相关页面  │                 └─modules----------------内置核心功能模块视图相关文件（按照模板名单独存放）  ```  系统演示  -----------------------------------  ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233138_66acc47c_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233150_79627fa7_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233200_33d385db_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233211_5e7fc693_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233222_8ab40914_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233239_ebb4d8bb_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233252_11c11f7d_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233303_da10ce13_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233314_7bcc9728_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233324_44fbe21c_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233335_358d1208_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233347_79912823_1394985.png "在这里输入图片标题")    ![输入图片说明](https://git.oschina.net/uploads/images/2017/0711/233356_3b2a0c61_1394985.png "在这里输入图片标题")      ## 感谢  @baomidou  mybatis 增强工具包，简化 CRUD 操作  https://git.oschina.net/baomidou/mybatis-plus
kejunxia/AndroidMvc	# AndroidMvc Framework  [![Build Status](https://travis-ci.org/kejunxia/AndroidMvc.svg?branch=ci-travis)](https://travis-ci.org/kejunxia/AndroidMvc) [![Coverage Status](https://coveralls.io/repos/kejunxia/AndroidMvc/badge.svg)](https://coveralls.io/r/kejunxia/AndroidMvc) [![jCenter](https://api.bintray.com/packages/kejunxia/maven/android-mvc/images/download.svg)](https://bintray.com/kejunxia/maven/android-mvc/_latestVersion) [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-AndroidMvc-green.svg?style=true)](https://android-arsenal.com/details/1/4098)  ## Features    - Easy to implement MVC/MVP/MVVM pattern for Android development   - Enhanced Android life cycles - e.g. a view needs to refresh when being brought back to foreground but not on rotation, onResume() is not specific enough to differentiate the two scenarios. Android mvc framework provides more granular life cycles   - All fragment life cycles are mapped into controllers thus logic in life cycles are testable on JVM   - Easy navigation between pages. Navigation is done in controllers instead of views so navigation can be unit tested on JVM   - Easy unit test on JVM since controllers don't depend on any Android APIs   - Built in event bus. Event bus also automatically guarantees post event view events on the UI thread   - Automatically save and restore instance state. You don't have to touch onSaveInstance and onCreate(savedInstanceState) with countless key-value pairs, it's all managed by the framework.   - [Dependency injection with Poke to make mock easy](https://github.com/kejunxia/AndroidMvc/tree/master/library/poke)   - Well tested - non-Android components are tested as the test coverage shown above (over 90%). For Android dependent module "android-mvc", it's tested by real emulator with [this UI test module](https://github.com/kejunxia/AndroidMvc/tree/master/library/android-mvc-test), even with  "Don't Keep Activities" turned on in dev options to guarantee your app doesn't crash due to loss of instance state after it's killed by OS in the background!  ## More details on   [Website](http://kejunxia.github.io/AndroidMvc)  ## Code quick glance  Let's take a quick glance how to use the framework to **navigate** between screens first, more details will be discussed later.  The sample code can be found in [Here](https://github.com/kejunxia/AndroidMvc/tree/master/samples/simple-mvc).  It is a simple counter app that has a master page and detail page. This 2 pages are represented by two fragments  - CounterMasterScreen paired with CounterMasterController - CounterDetailScreen paired with CounterDetailController  #### Controller In CounterMasterController, to navigate simply call ```java public void goToDetailView(Object sender) {     //Navigate to CounterDetailController which is paired by CounterDetailScreen     navigationManager.navigate(sender).to(CounterDetailController.class); } ```  #### View In CounterMasterScreen call the navigation method wrapped by the controller ```java buttonGoToDetailScreen.setOnClickListener(     new View.OnClickListener() {         @Override         public void onClick(View v) {             //Use counterController to manage navigation to make navigation testable             controller.goToDetailView(v);         }     });      ```  If you use Butterknife, the code can be shorten as below. Also you can use Android Data Binding library to shorten the code similarly ```java @OnClick(R.id.fragment_master_buttonShowDetailScreen) void goToDetailPage(View v) {     controller.goToDetailScreen(v); } ```  In CounterDetailScreen ```java @Override public void update() {     /**      * Controller will call update() whenever the controller thinks the state of the screen      * changes. So just bind the state of the controller to this screen then the screen is always      * reflecting the latest state/model of the controller. This is a simple solution but works for most cases.      * This solution can be thought as refreshing the whole web page in a browser. If you want more granular       * control like ajax to update partial page, define more callbacks in View for MVP pattern and events for MVVM       * pattern and call them in the controller when needed.      */     display.setText(controller.getCount()); } ```  #### Unit test  ```java //Act: navigate to MasterScreen navigationManager.navigate(this).to(CounterMasterController.class);  //Verify: location should be changed to MasterScreen Assert.assertEquals(CounterMasterController.class.getName(),         navigationManager.getModel().getCurrentLocation().getLocationId());  //Act: navigate to DetailScreen controller.goToDetailScreen(this);  //Verify: Current location should be at the view paired with CounterDetailController Assert.assertEquals(CounterDetailController.class.getName(),         navigationManager.getModel().getCurrentLocation().getLocationId()); ```   ## Division between Views and Controllers  - **View**: Mainly fragments that bind user interactions to controllers such as tap, long press and etc.  Views reflect the model managed by their controllers. - **Controller**: Controllers expose methods to its view peer to capture user inputs. Once the state  of the controller changes, the controller needs to notify its view the update. Usually by calling  view.update() or post an event to its view. - **Model**: Represents the state of view and managed by the controller. It can be accessed by  controller.getModel(). But only read it to bind the model to views but don't modify the model from  views. Modification of model should only be done by controller. - **Manager**: What about controllers have shared logic? Break shared code out into managers.  If managers need to access data. Inject services into managers. Managers can be thought as partial  controllers serve multiple views through the controllers depending on them. - **Service**: Services are below controller used to access data such as SharedPreferences,  database, cloud API, files and etc. It provides abstraction for controllers or managers that can be  easily mocked in unit tests for controllers. They the data access layer can be replaced quickly.  For example, when some resources are removed from local data to remote data, just simply replace  the services implementation to access web api instead of database or sharedPreferences.  See the illustration below  ![AndroidMvc Layers](http://i.imgur.com/dfW8TLM.png)  ## Download Here is the the latest version number in jCenter  [![Download](https://api.bintray.com/packages/kejunxia/maven/android-mvc/images/download.svg)](https://bintray.com/kejunxia/maven/android-mvc/_latestVersion)  **Maven:** - lib **android-mvc**      ```xml     <dependency>         <groupId>com.shipdream</groupId>         <artifactId>android-mvc</artifactId>         <version>[LatestVersion]</version>     </dependency>     ``` - lib **android-mvc-core**      ```xml     <dependency>         <groupId>com.shipdream</groupId>         <artifactId>android-mvc-core</artifactId>         <version>[LatestVersion]</version>     </dependency>     ```  **Gradle:** - lib **android-mvc**      ```groovy     compile "com.shipdream:android-mvc:[LatestVersion]"     ``` - lib **android-mvc-core**      ```groovy     compile "com.shipdream:android-mvc-core:[LatestVersion]"     ```  ## More details on   [Website](http://kejunxia.github.io/AndroidMvc)
b3log/latke	Latke [![Build Status](https://travis-ci.org/b3log/latke.png?branch=master)](https://travis-ci.org/b3log/latke) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.b3log/latke/badge.svg)](http://repo1.maven.org/maven2/org/b3log/latke) ----  ## 概览  Yet another simple web framework based on Java servlet technology.  * [为什么又要造一个叫 Latke 的轮子](https://hacpai.com/article/1403847528022) * [Latke 快速上手指南](https://hacpai.com/article/1466870492857) * [Latke 配置剖析](https://hacpai.com/article/1474087427032) * [Latke 源码解析（一）Servlet 部分](https://hacpai.com/article/1493267456529) * [Latke 源码解析（二）IOC 部分](https://hacpai.com/article/1493620909167)  ```xml <dependency>     <groupId>org.b3log</groupId>     <artifactId>latke</artifactId>     <version>2.3.17</version> </dependency> ```  社区支持：  * 论坛：https://hacpai.com/tag/Latke * Q 群：242561391  ## 案例  * [Demos](https://github.com/b3log/latke-demo): Hello World demo for newbies * [Solo](https://github.com/b3log/solo): A blogging system written in Java, feel free to create your or your team own blog.  * [Symphony](https://github.com/b3log/symphony): A real-time community forum written in Java
caelum/vraptor4	![image](https://cloud.githubusercontent.com/assets/1529021/7015058/0844e14c-dca4-11e4-8d7b-e0546b6ec74d.png)  [![][travis img]][travis] [![][maven img]][maven] [![][release img]][release] [![][license img]][license]  [travis]:https://travis-ci.org/caelum/vraptor4 [travis img]:https://travis-ci.org/caelum/vraptor4.svg?branch=master  [maven]:http://search.maven.org/#search|gav|1|g:"br.com.caelum"%20AND%20a:"vraptor" [maven img]:https://maven-badges.herokuapp.com/maven-central/br.com.caelum/vraptor/badge.svg  [release]:https://github.com/caelum/vraptor4/releases [release img]:https://img.shields.io/github/release/caelum/vraptor4.svg  [license]:LICENSE [license img]:https://img.shields.io/badge/License-Apache%202-blue.svg  A web MVC action-based framework, on top of CDI, for fast and maintainable Java development.   ## Downloading directly or using it through Maven  For a quick start, you can use this snippet in your maven POM:  ```xml <dependency>     <groupId>br.com.caelum</groupId>     <artifactId>vraptor</artifactId>     <version>4.2.0-RC3</version> <!--or the latest version--> </dependency> ```  Or you can download it directly [at our artifacts repository](https://bintray.com/caelum/VRaptor4/br.com.caelum.vraptor).  More detailed prerequisites and dependencies can be found [here](http://www.vraptor.org/en/docs/dependencies-and-prerequisites/).  ## Documentation [More detailed documentation](http://www.vraptor.org/en/docs/one-minute-guide/) and [Javadoc](http://www.vraptor.org/javadoc/) are also available at [VRaptor's website](http://www.vraptor.org/en/).  Looking for more? Take a look at our [articles and presentations' page](http://www.vraptor.org/en/docs/articles-and-presentations).  ## Building in your machine  If you want to build VRaptor, execute:  	mvn package  VRaptor uses Maven as build tool. So you can easily import it into your favorite IDE. In Eclipse you can import as "Maven project".  ## Contribute to VRaptor  Do you want to contribute with code, documentation or bug report?  You can find guidelines to contribute to VRaptor [here](http://www.vraptor.org/en/docs/how-to-contribute/ "Contribute").  ## Compatibility checks  You can check compatibility with previous versions of `vraptor-core` by running:  ``` mvn clirr:clirr ```  A full report will be generated at `target/site/clirr-report.html` file.
devinhu/androidone	一个牛逼哄哄的Android框架One    ##One框架能帮您带来什么？    * One框架分为两个项目，OneCore为核心工程，androidOne为演示项目，依赖oneCore    * One整个框架为MVC模式搭建，基于android framework为核心，集成Android世界中的主流技术选型    * 以Pragmatic风格的Android应用参考示例，是android项目最佳实践的总结与演示    * 以“复杂的世界里，一个就够了”为理念，励志帮助Android开发人员快速搭建一个简单高效的android开发框架!      ##异步模块     * 封装EventBus类，将异步框架单独抽出来，任何耗时操作（不仅仅是网络请求）都可以放到异步模块里    * 与网络模块分离实现，可以直接写单元测试类测试接口，让接口调试更方便    * 支持多并发、取消操作    * 多个请求，一个回调接口处理，让页面代码更简洁    * 建议一般在BaseActivity、BaseFragment中实现    	*  实现参考类 [AsyncTaskManager.java](https://github.com/devinhu/androidone/blob/master/studioOne/oneCore/src/main/java/com/sd/core/network/async/AsyncTaskManager.java)    	*  使用参考类 [BaseActivity.java](https://github.com/devinhu/androidone/blob/master/studioOne/androidOne/src/main/java/com/sd/one/activity/BaseActivity.java)    	*  使用参考类 [BaseFragment.java](https://github.com/devinhu/androidone/blob/master/studioOne/androidOne/src/main/java/com/sd/one/activity/BaseFragment.java)      ##HTTP请求模块     * 采用第三方AsyncHttpClient方案，支持http、https方式，支持get、post、put、delete方法，支持GZIP、File格式，支持Retry、Cacel策略，堪称完美！     * 改造实现SyncHttpClient，支持同步，并支持RESTFUL风格，调接口时可直接单元测试    	*  实现参考类 [SyncHttpClient.java](https://github.com/devinhu/androidone/blob/master/studioOne/oneCore/src/main/java/com/sd/core/network/http/SyncHttpClient.java)      ##DownloadManager资源下载模块    * 改造实现BreakpointHttpResponseHandler支持多并发、多文件上传、断点续传、暂停、继续、删除下载任务    ```javascript  	/**  	 * [下载器管理类，支持并发、暂停、继续、删除任务操作以及断点续传]  	 *   	DownloadManager downloadMgr = DownloadManager.getInstance();  	downloadMgr.setDownLoadCallback(new DownLoadCallback(){  		  		@Override  		public void onLoading(String url, int bytesWritten, int totalSize) {  			super.onLoading(url, bytesWritten, totalSize);  		}  		  		@Override  		public void onSuccess(String url) {  			super.onSuccess(url);  		}  		  		@Override  		public void onFailure(String url, String strMsg) {  			super.onFailure(url, strMsg);  		}  	});  	  	//添加下载任务  	downloadMgr.addHandler(url);  	*   	**/  ```    ##BluetoothManager蓝牙处理模块    ```javascript  	/**  	 * [蓝牙管理类]  	 *   	 */  	BluetoothManager bluetoothManager = BluetoothManager.getInstance(new BluetoothCallBack(){  		@Override  		public void onStateChange(int bluetoothState, String message) {  			switch(bluetoothState){  				//蓝牙不可用  				case BluetoothService.STATE_UNAVAILABLE:  					NToast.shortToast(mContext, "蓝牙不可用");  					break;  					  				//蓝牙未连接  				case BluetoothService.STATE_NONE:  					NToast.shortToast(mContext, "蓝牙未连接");  					break;  					  				//蓝牙空闲  				case BluetoothService.STATE_LISTEN:  					break;  					  				//蓝牙正连接  				case BluetoothService.STATE_CONNECTING:  					NToast.shortToast(mContext, "蓝牙正连接");  					break;  					  				//蓝牙已连接, 当如果连接上了，message就是蓝牙的名称  				case BluetoothService.STATE_CONNECTED:  					NToast.shortToast(mContext, "蓝牙已连接");  					mBluetoothState = true;  					break;  			}  		}    		@Override  		public void onResult(int requsetCode, String data) {  			//回调结果在页面显示  			  		}  	});    	//发送蓝牙请求  	bluetoothManager.request(SEND_INL_CODE, charStr);    	//断开  	bluetoothManager.stop();    ```    ##Common模块    * 页面堆栈管理ActivityPageManager    * 各种自定义dialog    * 支持hybrid开发     * 各种工具类    * 各种动画效果      ##SharedPreferences管理    * 支持直接put、get对象。      ##LruCache管理    * 封装LruCache，只缓存CACHE_SIZE大小的数量，超过CACHE_SIZE自动释放前面的对象，建议页面间传参使用。      ##Exception系统异常处理    * Bugtags是新一代的、专为移动测试而生的缺陷发现及管理工具。移动App集成Bugtags SDK后，测试人员就可以直接在App里所见即所得的提交 bug，SDK会自动截屏、收集App运行时数据，如：设备信息，控制台数据，用户的操作步骤等，团队人员在Bugtags云端高效的跟踪及管理bug。      ##Parse解析管理    * 支持XML、JSON、JSOAP解析    * 一行代码轻松转JAVA对象    	*  采用fastjson实现java、json互转    	*  采用xstream实现Java、xml互转，支持注解解析    	*  自主封装，支持soapObject转Java对象      ##CacheManager缓存管理    * 磁盘缓存，缓存对象需实现序列化接口，提供读取、失效，清除方法。一般用于对接口数据的缓存。    ```javascript     /**       * 缓存使用示例       *        * @return       * @throws HttpException       */      public AdResponse getAdList() throws HttpException {      	AdResponse response = null;      	          RequestParams params = getRequestParams();          params.put("method", "fmms.getAdvertisementList");          params.put("data", "{}");                    //根据请求得到唯一的缓存Key          String key = getCacheKey(AdResponse.class.getSimpleName());                    //读取缓存          if(CacheManager.isInvalidCache(key, INVALID_TIME_1DAY)){          	response = CacheManager.readObject(key);          	if(response != null && response.isSuccess()){          		 return response;          	}          }                    String result = httpManager.post(mContext, Constants.DOMAIN, getSignParams(params), ContentType);          if(!TextUtils.isEmpty(result)){          	//一句话解析成对象              response = jsonToBean(result, AdResponse.class);              if(response != null && response.isSuccess()){              	 //获取数据成功，写入缓存              	CacheManager.writeObject(response, key);              }          }                    //最后都没有数据，还是从缓存中取          if(response == null){          	response = CacheManager.readObject(key);          	if(response != null && response.isSuccess()){          		return response;          	}          }                    return response;      }  ```    ##BroadcastManager广播管理    * 为了发送广播更加方便，自主封装了BroadcastManager，方便好用。    ```javascript  	/**  	 * [BroadcastManager使用示例]  	 *   	//在任何地方发送广播  	BroadcastManager.getInstance(mContext).sendBroadcast(FindOrderActivity.ACTION_RECEIVE_MESSAGE);  	   	//页面在oncreate中初始化广播  	BroadcastManager.getInstance(mContext).addAction(ACTION_RECEIVE_MESSAGE, new BroadcastReceiver(){  		@Override  		public void onReceive(Context arg0, Intent intent) {  			String command = intent.getAction();  			if(!TextUtils.isEmpty(command)){  				if((ACTION_RECEIVE_MESSAGE).equals(command)){  					//获取json结果  					String json = intent.getStringExtra("result");  					//做你该做的事情  				}  			}  		}  	});        	//页面在ondestory销毁广播  	BroadcastManager.getInstance(mContext).destroy(ACTION_RECEIVE_MESSAGE);  	*   	**/  ```    ##DB模块    * 采用[GreenDao详见](http://greendao-orm.com/)ORM方案，直接实现Java Object的CURD方法就可以操作数据库，非常好用，极力推荐。     * 实现DBManager，连获取Dao的代码都不用写了，不管通过DaoGenerator生成的对象如何变化，通过DBManager可以让你拿到任何Dao对象，从而实现数据库操作。    * [DaoGenerator](https://github.com/devinhu/androidone/tree/master/DaoGenerator)工程自动生成model、dao、session对象等代码，拷过来直接使用即可。    ```javascript  	/**  	 * 数据库示例  	 */  	public void testDB(){  		  		NoteDao dao = DBManager.getInstance(getContext()).getDaoSession().getNoteDao();  		  		//add  		for(int i=0; i<=4; i++){  			Note bean = new Note();  			bean.setComment("comment");  			bean.setDate(new Date());  			bean.setText("text");  			dao.insert(bean);  		}  		  		//udpate  		List<Note> list = dao.loadAll();  		if(!list.isEmpty()){  			Note bean = list.get(2);  			bean.setComment("comment_comment");  			dao.update(bean);  		}  		  		//query  		if(!list.isEmpty()){  			for(Note note : list){  				NLog.e("testDemo", note.getComment());  			}  		}  			  		//delete  		if(!list.isEmpty()){  			dao.delete(list.get(0));  		}  	}    ```    ##注解模块    * 集成[butterknife详见](http://jakewharton.github.io/butterknife/)注解框架，一个No Magic的Android注入框架，用过的人都说好，极力推荐。      ##支付模块    * 集成支付宝支付和微信支付    	* 支付宝示例：客户端封装参数，调用支付宝  	```javascript  		String PARTNER = "2088XXXXXXXXXXXX";  		String SELLER = "demo@yahoo.cn";  		String RSA_PRIVATE = "私钥";  		PayUtils payutils = new PayUtils(activity, PARTNER, SELLER, RSA_PRIVATE, "服务器回调订单地址[异步处理]");  		payutils.setPayListener(new PayListener() {  			@Override  			public void onPayResult(int state, String message) {  				  			}  		});  		  		try {  			payutils.pay("测试商品", "测试商品详情", "0.01", "唯一订单号，服务器生成");  		} catch (Exception e) {  			e.printStackTrace();  		}  	```    	* 支付宝示例：服务器封装所有的参数，返回url直接调用支付宝  	```javascript  		PayUtils1 payutils= new PayUtils1(BespeakActivity.this, new PayListener() {  			@Override  			public void onPayResult(int state, String message) {  				switch (state) {  					case PayUtils1.PAY_SUC_CODE:  						MessageDialog dialog = new MessageDialog(mContext, getString(R.string.bespeak_order_suc));  						dialog.setConfirmListener(new OnClickListener() {  							@Override  							public void onClick(View v) {  								BespeakActivity.this.finish();  							}  						});  						dialog.show();  						break;  						  					case PayUtils1.PAY_DOING_CODE:  					case PayUtils1.PAY_GOODS_CODE:  					case PayUtils1.PAY_INIT_CODE:  					case PayUtils1.PAY_FAIL_CODE:  						NToast.shortToast(mContext, message);  						break;  				}  			}  		});  		payutils.pay(res.getData().getUrl());  	```    	* 微信支付示例  	```javascript  		PayReq payReq = res.getData().getPayReq();  		payReq.packageValue = "Sign=WXPay";  		IWXAPI msgApi = WXAPIFactory.createWXAPI(this, null);  		msgApi.registerApp(Constants.WEIXIN_APP_ID);  		msgApi.sendReq(payReq);  	```    ##图片下载模块    * 采用Fresco解决方案，有关使用情况请参见[Fresco详见](https://github.com/facebook/fresco)    * 内存管理、图片的渐进式呈现、Gif图和WebP格式强大的特性让你不能自拔！[特性详见](http://www.fresco-cn.org/)      ##日志    * NLog日志输出类  * config.properties文件(assets文件夹下)配置日志开关  ```javascript  #is debug mode, if debug is true that log is open, if debug is false that log is close.  debug=true  ```      ##教程  * [优酷视频教程地址](http://v.youku.com/v_show/id_XMTQwNTU3NjI4NA==.html?qq-pf-to=pcqq.c2c)，非常感谢[融云阿明](https://github.com/devinhu/SeaStar)的辛苦录制！  * [PPT手把手教程地址](https://github.com/devinhu/androidone/tree/master/androidOne%E5%BF%AB%E9%80%9F%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6ppt)    ##常用网址推荐  [Android开发技术周报](http://www.androidweekly.cn/)    [http://www.androiddevtools.cn/](http://www.androiddevtools.cn/)    [android-studio中文站](http://www.android-studio.org/index.php)    [android-开源项目](http://www.23code.com/page/3/)    ##结语    * 看到这里，估计您和您的小伙伴们都惊呆了吧！赶快动手试试吧！    * 具体使用请参考androidOne演示工程。    * 我改变不了这个世界！这个世界也休想将我改变！    * 如果任何问题或者建议，欢迎沟通。    * QQ群：195104825  * AndroidOne GitHub地址：https://github.com/devinhu/androidone
caelum/vraptor	VRaptor 3 ========= ![Build status](https://secure.travis-ci.org/caelum/vraptor.png)  A web MVC framework for fast and maintainable development. Documentation can be found at http://vraptor3.vraptor.org  Latest version ============== The latest version is available at https://github.com/caelum/vraptor4 and it should be used for new projects.   It's documentation can be found at http://vraptor.org  Downloading directly or using it through Maven ============================================== For a quick start, you can use this snippet in your maven POM:  ```xml <dependency>     <groupId>br.com.caelum</groupId>     <artifactId>vraptor</artifactId>     <version>3.5.4</version> <!--or the latest version--> </dependency> ```  Building in your machine ========================  If you want to use the example applications contained within vraptor, execute:  	mvn package  And then configure the projects in your eclipse using maven:  	mvn eclipse:eclipse  Contribute to VRaptor ===================== Want to contribute with code, documentation or bug report?  You can find guidelines to contribute to VRaptor [here](http://vraptor3.vraptor.org/en/docs/how-to-contribute/ "Contribute").
allwefantasy/ServiceFramework	# ServiceFramework Wiki  [README-EN](https://github.com/allwefantasy/ServiceFramework/blob/master/README-EN.md)  ServcieFramework 定位在 **移动互联网后端** 领域,强调开发的高效性，其开发效率可以比肩Rails.  ServcieFramework 目前更新频率较高,我现在一直疏于更新中央仓库的版本。所以不再更新maven中央仓库。   现在编译步骤：      git clone git@github.com:allwefantasy/ServiceFramework.git     mvn install -Pscala-2.11 -Pjetty-9 -Pweb-include-jetty-9  如果你想切换scala版本，则使用：      ./dev/change-version-to-2.10.sh  经过以上步骤即可使用  ### 项目示例  [https://github.com/allwefantasy/godear](https://github.com/allwefantasy/godear) 该项目是一个RSS订阅系统。 里面展示了ServiceFramework各种典型用法，包括如何构造非JSON Rest API的具有页面的接口。  ### 在Maven中使用该项目  接着确保 项目根目录下有config/application.yml,config/logging.yml 两个文件即可。示例可参看该项目中config文件夹。  QuickStart：[搭建自己的第一个项目](https://github.com/allwefantasy/ServiceFramework/blob/master/doc/ServiceFrameworkWiki-example.md)  ServiceFramework 特点：  1. ActiveRecord化的Model层，支持 MongoDB 和 MySQL.       		    List<Tag> tags = Tag.where(map("name","java")).fetch;     2. 完全重新设计的Controller层,大量便利的函数。创新的过滤器设计，比如下面的代码表示validate 方法会拦截 push方法             static {              beforeFilter("validate", WowCollections.map(only, WowCollections.list("push")));            }  3. 大部分对象使用IOC自动管理,使用简单。    		   @inject 		   Service service;     4. 不依赖容器，单元测试简单，从action到service,都可做到测试代码最少    	     @Test 	     public void search() throws Exception { 	         RestResponse response = get("/doc/blog/search", map( 	                 "tagNames", "_10,_9" 	         )); 	         Assert.assertTrue(response.status() == 200); 	         Page page = (Page) response.originContent(); 	         Assert.assertTrue(page.getResult().size() > 0); 	     }  5. 接口调用监控  	* 接口 QPS 监控 	* 接口平均响应耗时监控 	* 接口调用量(如果是http的话，则是各种状态码统计) 	* 内置http接口，提供json数据展示以上的系统状态  6. 1.2 以上版本集成了Dubbo,具有Dubbo的所有有点。同时还添加RestProtocol协议，可以像RPC一样调用现有的HTTP服务。 所有工作只需要定义一套 Interface接口即可。          在ServiceFramework中，调用一个同样也是由ServiceFramework开发的HTTP接口可以变得非常简单。          @At(path = "/say/hello", types = {RestRequest.Method.GET})             public void sayHello() {                 render(200, "hello" + param("kitty"));         }          这里很简单通过调用 http://127.0.0.1/say/hello?kitty=wow ，服务会返回hellowow 这样的字符串。         使用方可以通过HttpClient直接调用这个接口。为了方便调用方，服务提供方可以添加一个接口：          public interface TagController {             @At(path = "/say/hello", types = {RestRequest.Method.GET, RestRequest.Method.POST})             public HttpTransportService.SResponse sayHello(RestRequest.Method method, Map<String, String> params);              @At(path = "/say/hello", types = {RestRequest.Method.GET})             public HttpTransportService.SResponse sayHello3(@Param("kitty") String kitty);          接着，调用方引入这个接口，就可以像这样调用了：          tagController.sayHello(RestRequest.Method.GET, WowCollections.map("kitty", "你好，太脑残")).getContent()          或者          tagController.sayHello3("哇塞，天才呀").getContent()          服务提供者可以针对一个http接口定义出任意个方法，每个方法都之定义一部分参数，这样可以有效方便调用者使用。  7. 如果你不使用Dubbo，你也可以非常容易的调用第三方的标准HTTP接口，达到类似RPC调用的效果。     * 将第三方HTTP API 做个申明，例如有个搜索接口(Scala代码示例)     			   trait SearcherClient { 			  @At(path = Array("/v2/~/~/_search"), types = Array(GET, POST)) 			  @BasicInfo( 			    desc = "索引服务", 			    state = State.alpha, 			    testParams = "", 			    testResult = "", 			    author = "WilliamZhu", 			    email = "allwefantasy@gmail.com" 			  ) 			  def search(params: Map[String, String], content: String, method: net.csdn.modules.http.RestRequest.Method): java.util.List[HttpTransportService.SResponse] 			 			}      * 接着在需要使用该接口的地方调用如下代码构建SearcherClient对象。记住，这个对象只能构建一次(Scala代码示例)      				   val _searchClient = AggregateRestClient.buildClient[SearcherClient](hostAndPorts, new SearchEngineStrategy(), httpRequest) 				   //其中，hostAndPorts 为域名和端口。 				   //可以是多个。SearchEngineStrategy 是自定义实现如何调用后端服务， 				   //是轮训的负载均衡还是有特别的逻辑            * 现在可以使用了(Scala代码示例)      		    val res = _searchClient.search( 		        url._2.toMap ++ Map("index" -> index, "type" -> ctype), 		        query, 		        RestRequest.Method.POST).searchResult          使用该种方式调用第三方API会产生Trace日志。      7. 服务降级限流 ServiceFramework主要面向后端服务，如果没有自我保护机制，系统很容易过载而不可用。经过一定的容量规划，或者通过对接口调用平均响应耗时的监控， 我们可以动态调整 ServiceFramework 的QPS限制，从而达到保护系统的目的。这些都可以通过配置以及内置的http接口完成。 监控将会是ServiceFramework后续的重点。早期ServiceFramework也通过日志让用户对自己系统有更多的感性认识，日志会打印：  	 * http请求url 	 * 整个请求耗时 	 * 数据库耗时(如果有) 	 * 响应状态码 	  你可以很方便的通过shell脚本做各项统计    8. Thrift 和 RESTFul 只需简单配置即可同时提供 Thrift 和 RESTFul 接口      			  		###############http config################## 		http: 		    port: 7700 		    disable: false  		thrift: 		    disable: false 		    services: 		        net_csdn_controller_thrift_impl_CLoadServiceImpl: 		           port: 7701 		           min_threads: 100 		           max_threads: 1000		          		    servers: 		        load: ["127.0.0.1:7701"]  	   9. 支持 Velocity, 页面可直接访问所有实例变量以及helper类的方法。支持Velocity 进行模板配置  	  			    @At(path = "/hello", types = GET) 			    public void hello() { 			        render(200, map( 			                "name", "ServiceFramework" 			        ), ViewType.html); 			    }     ## QuickStart  Step 1 >   克隆项目      	git clone https://github.com/allwefantasy/ServiceFramework     Step 2 >   导入到IDE.   Step 3 >   根据你自己的数据库信息 编辑修改 config/application.yaml .注意如果你使用mysql,需要disable 调 mongodb.反之亦然   				     datasources:         mysql:            host: 127.0.0.1            port: 3306            database: wow            username: root            password: root            disable: false         mongodb:            host: 127.0.0.1            port: 27017            database: wow            disable: false         redis:             host: 127.0.0.1             port: 6379             disable: true 		             Step4 >   在Mysql中导入 sql/wow.sql.   Step5 >   新建 com.example.model.Tag 类.  			public class Tag extends Model  			{ 			 			}  Step6 >   新建 com.example.controller.http.TagController            public class TagController extends ApplicationController  			{ 			   @At(path = "/hello", types = RestRequest.Method.GET) 			    public void hello() { 			        Tag tag = Tag.create(map("name", "java")); 			        tag.save(); 			        render(200, map( 			                "tag", tag 			        ), ViewType.html); 			    } 			} 			 Step7 >	新建 template/tag/hello.vm   			Hello $tag.name!  Hello  world!		  Step8 >   创建启动类      public class ExampleApplication {      public static void main(String[] args) {         ServiceFramwork.scanService.setLoader(ExampleApplication.class);         Application.main(args);     }     }      Step9 >   运行  ExampleApplication  Step10 >  浏览器中输入  http://127.0.0.1:9002/hello .同时查看数据库，你会发现tag表已经有数据了。     Step11 >  写个Action单元测试  编辑 runner.DynamicSuite  在 initEnv方法第一行处添加        ServiceFramwork.scanService.setLoader(ExampleApplication.class);  Step12 > 创建测试类 test.com.example.TagControllerTest      public class TagControllerTest extends BaseControllerTest { 	    @Test 	    public void testHello() throws Exception { 	        Tag.deleteAll(); 	        RestResponse response = get("/hello", map()); 	        Assert.assertTrue(response.status() == 200); 	        String result = response.content(); 	        Assert.assertEquals("Hello java!  Hello  world!", result); 	    }     }  Step13 >  运行 DynamicSuiteRunner 跑起测试  Step14 >  补充：你也可以不使用DynamicSuiteRunner去跑。直接使用IDE跑单元测试类。需要做的是在你的单元测试类中加几句代码：      static {         initEnv(ExampleApplication.class);     }  加这句主要是保证启动容器，并且采用了合适的类加载器。  QuickStart 的一些常见错误:  1. application 文件 数据连接配置错误。单元测试一定需要单独配置test的配置。因为单元测试一般可能会会有数据清理等，系统强制使用    test的配置。  2. ServiceFramework 是使用配置文件来找类并且加载的，所以你需要正确配置contorller等所在位置。在上述测试中，包名和类名必须保证和示例一致。如果你需要使用不同的package,那么你需要修改application.yml中的application 配置。如下:    		  application: 		    controller: com.example.controller.http 		    model:      com.example.model 		    document:   com.example.document 		    service:    com.example.service 		    util:       com.example.util 		    test:       test.com.example       Model层基于如下开源项目:   * [ActiveORM](https://github.com/allwefantasy/active_orm) * [MongoMongo](https://github.com/allwefantasy/mongomongo)   ServiceFramework 不适合遗留项目。我们倾向于在一个全新的项目中使用它。      ## Doc Links  * [Summary](https://github.com/allwefantasy/ServiceFramework/tree/master/doc/ServiceFrameworkWiki-start.md) * [Model](https://github.com/allwefantasy/ServiceFramework/tree/master/doc/ServiceFrameworkWiki-model.md) * [Controller](https://github.com/allwefantasy/ServiceFramework/tree/master/doc/ServiceFrameworkWiki-controller.md) * [Test](https://github.com/allwefantasy/ServiceFramework/tree/master/doc/ServiceFrameworkWiki-test.md) * [Deploy](https://github.com/allwefantasy/ServiceFramework/tree/master/doc/ServiceFrameworkWiki-deploy.md)  ## Step by Step tutorial Step-by-Step-tutorial-for-ServiceFramework(continue...)  * [Step-by-Step-tutorial-for-ServiceFramework(1)](https://github.com/allwefantasy/service_framework_example/blob/master/README.md) * [Step-by-Step-tutorial-for-ServiceFramework(2)](https://github.com/allwefantasy/service_framework_example/blob/master/doc/Step-by-Step-tutorial-for-ServiceFramework\(2\).md) * [Step-by-Step-tutorial-for-ServiceFramework(3)](https://github.com/allwefantasy/service_framework_example/blob/master/doc/Step-by-Step-tutorial-for-ServiceFramework\(3\).md) * [Step-by-Step-tutorial-for-ServiceFramework(4)](https://github.com/allwefantasy/service_framework_example/blob/master/doc/Step-by-Step-tutorial-for-ServiceFramework\(4\).md)    ##  Some projects based on ServiceFramework  * [QuickSand](https://github.com/allwefantasy/QuickSand)
nutzam/nutz	<p align="center"><a href="https://nutz.cn" target="_blank"><img width="100" src="https://github.com/nutzam/nutz/raw/master/doc/ci/logo.png"></a></p>  [![Build Status](https://travis-ci.org/nutzam/nutz.png?branch=master)](https://travis-ci.org/nutzam/nutz) [![Circle CI](https://circleci.com/gh/nutzam/nutz/tree/master.svg?style=svg)](https://circleci.com/gh/nutzam/nutz/tree/master) [![Coverity Scan Build Status](https://scan.coverity.com/projects/4917/badge.svg)](https://scan.coverity.com/projects/4917/) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.nutz/nutz/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.nutz/nutz/) [![codecov.io](http://codecov.io/github/nutzam/nutz/coverage.svg?branch=master)](http://codecov.io/github/nutzam/nutz?branch=master) [![GitHub release](https://img.shields.io/github/release/nutzam/nutz.svg)](https://github.com/nutzam/nutz/releases) [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html) [![Skywalking Tracing](https://img.shields.io/badge/Skywalking%20Tracing-enable-brightgreen.svg)](https://github.com/wu-sheng/sky-walking)  ## 项目目标  在力所能及的情况下，最大限度的提高 Java 开发人员的生产力。  *Talk is cheap. Show me the code!!*  Nutz遵循Apache协议,完全开源,文档齐全,永远免费(商用也是)  ## 项目各种资源地址  *   [项目官网](https://nutzam.com) *   [Nutz社区](https://nutz.cn/) 秒回, 就是这么爽 *   在线文档     *   [官网](https://nutzam.com/core/nutz_preface.html) Nutz手册,涵盖方方面面     *   [w3cschool上的文档](http://www.w3cschool.cn/nutz/) [由vincent109维护](https://github.com/vincent109) *   [各种插件](http://github.com/nutzam/nutzmore) 您能想到的都有哦(基本上`^_^`) *   [好玩的Nutzbook](http://nutzbook.wendal.net) 几分钟搭建一个demo有何不可? 入门从这里开始 *	[在线javadoc](https://nutzam.com/javadoc/) 注释就是这么全  ## Nutz生态系统  ![nutz系统架构](nutz-graph.png)  ### Maven 资源  ```xml 		<dependency> 			<groupId>org.nutz</groupId> 			<artifactId>nutz</artifactId> 			<version>1.r.62</version> 		</dependency> ```   详情: https://nutzam.com/core/basic/maven.html  ## Gradle 依赖  ```gradle compile(group: 'org.nutz', name: 'nutz', version:'1.r.62') ```   ## Sponsorship  YourKit supports open source projects with its full-featured Java Profiler. YourKit, LLC is the creator of [YourKit Java Profiler](http://www.yourkit.com/java/profiler/index.jsp)  and [YourKit .NET Profiler](http://www.yourkit.com/.net/profiler/index.jsp), innovative and intelligent tools for profiling Java and .NET applications.  ![YourKit Logo](https://cloud.githubusercontent.com/assets/1317309/4507430/7119527c-4b0c-11e4-9245-d72e751e26ee.png)  JetBrains IntelliJ IDEA  http://www.jetbrains.com  ## 关于我们  广州市文尔软件科技有限公司
biezhi/blade	<p align="center">      <a href="https://lets-blade.com"><img src="http://7xls9k.dl1.z0.glb.clouddn.com/blade-logo.png" width="650"/></a>  </p>  <p align="center">Based on <code>Java8</code> + <code>Netty4</code> to create lightweight, high-performance, simple and elegant Web framework 😋</p>  <p align="center">Spend <b>1 hour</b> to learn it to do something interesting, a Spring in addition to the framework of the best choice.</p>  <p align="center">      🐾 <a href="" target="_blank">Quick Start</a> |       📘 <a href="https://biezhi.gitbooks.io/blade-in-action" target="_blank">Blade In Action</a> |       🎬 <a href="https://www.youtube.com/playlist?list=PLK2w-tGRdrj5TV2lxHFj8hcg4mbmRmnWX" target="_blank">Video Tutorial</a> |       🌚 <a href="" target="_blank">Contribution</a> |       💰 <a href="https://lets-blade.com/donate" target="_blank">Donate</a> |      🇨🇳 <a href="README_CN.md">简体中文</a>  </p>  <p align="center">      <a href="https://travis-ci.org/biezhi/blade"><img src="https://img.shields.io/travis/biezhi/blade.svg?style=flat-square"></a>      <a href="http://codecov.io/github/biezhi/blade?branch=dev"><img src="https://img.shields.io/codecov/c/github/biezhi/blade/dev.svg?style=flat-square"></a>      <a href="http://search.maven.org/#search%7Cga%7C1%7Cblade-mvc"><img src="https://img.shields.io/maven-central/v/com.bladejava/blade-mvc.svg?style=flat-square"></a>      <a href="LICENSE"><img src="https://img.shields.io/badge/license-Apache%202-4EB1BA.svg?style=flat-square"></a>      <a href="https://gitter.im/biezhi/blade"><img src="https://badges.gitter.im/biezhi/blade.svg?style=flat-square"></a>  </p>    ***    ## What Is Blade?    `Blade` is a pursuit of simple, efficient Web framework, so that `JavaWeb` development even more powerful, both in performance and flexibility.  If you like to try something interesting, I believe you will love it.  If you think this item is good can [star](https://github.com/biezhi/blade/stargazers) support or [donate](https://lets-blade.com/donate) it :blush:    ## Features    * [x] A new generation of MVC frameworks that do not depend on more libraries  * [x] Get rid of SSH's bloated, modular design  * [x] source less than `500kb`, learning is also simple  * [x] Restful style routing design  * [x] template engine support, view development more flexible  * [x] high performance, 100 concurrent tps 6w/s  * [x] Run the `JAR` package to open the web service  * [x] Streaming API style  * [x] supports plug-in extensions  * [x] support webjars resources  * [x] built-in a variety of commonly used middleware  * [x] Built-in JSON output  * [x] JDK8 +    ## Overview    » Simplicity: The design is simple, easy to understand and doesn't introduce many layers between you and the standard library. The goal of this project is that the users should be able to understand the whole framework in a single day.<br/>  » Elegance: `blade` supports the RESTful style routing interface, has no invasive interceptors and provides the writing of DSL grammar.<br/>  » Easy deploy: support `maven` package `jar` file running.<br/>    ## Get Start    Grab via `Maven`：    ```xml  <dependency>  	<groupId>com.bladejava</groupId>  	<artifactId>blade-mvc</artifactId>  	<version>2.0.2</version>  </dependency>  ```    or `Gradle`:    ```sh  compile 'com.bladejava:blade-mvc:2.0.2'  ```    Write `main` method, lets `Hello World`：    ```java  public static void main(String[] args) {      Blade.me().get("/", (req, res) -> {          res.text("Hello Blade");      }).start();  }  ```    Using browser open http://localhost:9000 so you can see the first `Blade` application!    ## API Example    ```java  public static void main(String[] args) {      // Create Blade，using GET、POST、PUT、DELETE      Blade.me()          .get("/user/21", getting)          .post("/save", posting)          .delete("/remove", deleting)          .put("/putValue", putting)          .start();  }  ```    ## REST URL Parameters    ```java  public static void main(String[] args) {      Blade blade = Blade.me();      // Create a route: /user/:uid      blade.get("/user/:uid", (request, response) -> {  		Integer uid = request.pathInt("uid");  		response.text("uid : " + uid);  	});  	      // Create two parameters route      blade.get("/users/:uid/post/:pid", (request, response) -> {  		Integer uid = request.pathInt("uid");  		Integer pid = request.pathInt("pid");  		String msg = "uid = " + uid + ", pid = " + pid;  		response.text(msg);  	});  	      // Start blade      blade.start();  }  ```    ## Form Parameters    ```java  public static void main(String[] args) {      Blade.me().get("/user", ((request, response) -> {           Optional<Integer> ageOptional = request.queryInt("age");           ageOptional.ifPresent(age -> System.out.println("age is:" + age));       })).start();  }  ```    ## Upload File    ```java  public void upload(@MultipartParam FileItem fileItem){      byte[] data = fileItem.getData();      // Save the temporary file to the specified path      Files.write(Paths.get(filePath), data);  }  ```    Or    ```java  public void upload(Request request){      request.fileItem("img").ifPresent(fileItem -> {          byte[] data = fileItem.getData();          // Save the temporary file to the specified path          Files.write(Paths.get(filePath), data);                    });  }  ```    ## Before hook    ```java  public static void main(String[] args) {      // All requests are exported before execution before      Blade.me().before("/*", (request, response) -> {          System.out.println("before...");      }).start();  }  ```    How easy it all looks, but the features above are the tip of the iceberg, and there are more surprises to see in the documentation and sample projects:    + [FirstBladeApp](https://github.com/bladejava/first-blade-app)  + [Doc Service](https://github.com/biezhi/grice)  + [More Examples](https://github.com/bladejava/blade-demos)    ## Use the Blade site    + Blog System：https://github.com/otale/tale  + Community Application：https://github.com/junicorn/roo  + Pictures social：https://github.com/biezhi/nice  + SS Panel：https://github.com/biezhi/ss-panel    ## Update    [update log](https://github.com/biezhi/blade/blob/master/UPDATE_LOG.md)    ## Contact    - Blog:[http://biezhi.me](http://biezhi.me)  - Mail: biezhi.me@gmail.com    ## Contributor    Thank you very much for the developers to help in the project, if you are willing to contribute, welcome!    - [mfarid](https://github.com/mfarid)  - [daimajia](https://github.com/daimajia)  - [shenjie1993](https://github.com/shenjie1993)  - [sumory](https://github.com/sumory)  - [udaykadaboina](https://github.com/udaykadaboina)  - [SyedWasiHaider](https://github.com/SyedWasiHaider)  - [Awakens](https://github.com/Awakens)  - [shellac](https://github.com/shellac)  - [SudarAbisheck](https://github.com/SudarAbisheck)    ## Licenses    Please see [Apache License](LICENSE)
spring-projects/spring-mvc-showcase	Spring MVC Showcase ------------------- Demonstrates the capabilities of the Spring MVC web framework through small, simple examples. After reviewing this showcase, you should have a good understanding of what Spring MVC can do and get a feel for how easy it is to use. Includes project code along with a supporting slideshow and screen cast.  In this showcase you'll see the following in action:  * The simplest possible @Controller * Mapping Requests * Obtaining Request Data * Generating Responses * Message Converters * Rendering Views * Type Conversion * Validation * Forms * File Upload * Exception Handling  To get the code: ------------------- Clone the repository:      $ git clone git://github.com/SpringSource/spring-mvc-showcase.git  If this is your first time using Github, review http://help.github.com to learn the basics.  To run the application: -------------------	 From the command line with Maven:      $ cd spring-mvc-showcase     $ mvn tomcat7:run [-Dmaven.tomcat.port=<port no.>] (In case 8080 is busy]   or  In your preferred IDE such as SpringSource Tool Suite (STS) or IDEA:  * Import spring-mvc-showcase as a Maven Project * Drag-n-drop the project onto the "SpringSource tc Server Developer Edition" or another Servlet 2.5 or > Server to run, such as Tomcat.  Access the deployed web application at: http://localhost:8080/spring-mvc-showcase/  Note: -------------------  This showcase originated from a [blog post](http://blog.springsource.com/2010/07/22/spring-mvc-3-showcase/) and was adapted into a SpringOne presentation called [Mastering MVC 3](http://www.infoq.com/presentations/Mastering-Spring-MVC-3).  A screen cast showing the showcase in action is [available in QuickTime format](http://s3.springsource.org/MVC/mvc-showcase-screencast.mov).
jdmp/java-data-mining-package	# Java Data Mining Package > #### A Java library for machine learning and data analytics  ## Project Website:   https://jdmp.org  ## About  The Java Data Mining Package (JDMP) is an open source Java library for data analysis and machine learning.  It facilitates the access to data sources and machine learning algorithms (e.g. clustering, regression,  classification, graphical models, optimization) and provides visualization modules.  JDMP provides a number of algorithms and tools, but also interfaces to other machine learning and data  mining packages (Weka, LibLinear, Elasticsearch, LibSVM, Mallet, Lucene, Octave).  ##In a Nutshell:  - Includes many machine learning algorithms - Multi-threaded and lighting fast - Handle terabyte-sized data - Visualize and edit as heatmap, graph, plot - Treat every type of data as a matrix - TXT, CSV, PNG, JPG, HTML, XLS, XLSX, PDF, LaTeX, Matlab, MDB - Free and open source (LGPL)  ## Quick Start  ``` // load example data set ListDataSet dataSet = DataSet.Factory.IRIS();  // create a classifier NaiveBayesClassifier classifier = new NaiveBayesClassifier();  // train the classifier using all data classifier.trainAll(dataSet);  // use the classifier to make predictions classifier.predictAll(dataSet);  // get the results double accuracy = dataSet.getAccuracy();  System.out.println("accuracy: " + accurary); ```  ## References  - Holger Arndt: [The Java Data Mining Package – A Data Processing Library for Java](https://holger-arndt.de/library/COMPSAC2009-jdmp-draft.pdf), 33rd Annual IEEE International Computer Software and Applications Conference (COMPSAC), 2009  ## License  The Java Data Mining Package is licensed under the [GNU Lesser General Public License v3.0](http://www.gnu.org/licenses/lgpl-3.0.en.html).
qcri/Arabesque	# Arabesque: Distributed graph mining made simple  [http://arabesque.io](http://arabesque.io)  *Current Version:* 1.0.1-BETA  Arabesque is a distributed graph mining system that enables quick and easy development of graph mining algorithms, while providing a scalable and efficient execution engine running on top of Hadoop.  Benefits of Arabesque: * Simple and intuitive API, specially tailored for Graph Mining algorithms. * Transparently handling of all complexities associated with these algorithms. * Scalable to hundreds of workers. * Efficient implementation: negligible overhead compared to equivalent centralized solutions.  Arabesque is open-source with the Apache 2.0 license.  ## Requirements for running  * Linux/Mac with 64-bit JVM * [A functioning installation of Hadoop2 with MapReduce (local or in a cluster)](http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide/)  ## Preparing your input Arabesque currently takes as input graphs with the following formats:  * **Graphs label on vertex(default)** ``` # <num vertices> <num edges> <vertex id> <vertex label> [<neighbour id1> <neighbour id2> ... <neighbour id n>] <vertex id> <vertex label> [<neighbour id1> <neighbour id2> ... <neighbour id n>] ... ```  * **Graphs label on edges** To enable processing label on edges, in the yaml file, add the following lines ``` arabesque.graph.edge_labelled: true arabesque.graph.multigraph: true   # Set this to true if multiple edges                                      # exist between two vertices. ``` Input format ``` # <num vertices> <num edges> <vertex id> <vertex label> [<neighbour id1> <edge label> <neighbour id2> <edge label>... ] <vertex id> <vertex label> [<neighbour id1> <edge label> <neighbour id2> <edge label>... ] ... ```  Vertex ids are expected to be sequential integers between 0 and (total number of vertices - 1).  ## Test/Execute the included algorithms  You can find an execution-helper script and several configuration files for the different algorithms under the [scripts folder in the repository](https://github.com/Qatar-Computing-Research-Institute/Arabesque/tree/master/scripts):  * `run_arabesque.sh` - Launcher for arabesque executions. Takes as parameters one or more yaml files describing the configuration of the execution to be run. Configurations are applied in sequence with configurations in subsequent yaml files overriding entries of previous ones. * `cluster.yaml` - File with configurations related to the cluster and, so, common to all algorithms: number of workers, number of threads per worker, number of partitions, etc. * `<algorithm>.yaml` - Files with configurations related to particular algorithm executions using as input the [provided citeseer graph](https://github.com/Qatar-Computing-Research-Institute/Arabesque/tree/master/data):   * `fsm.yaml` - Run frequent subgraph mining over the citeseer graph.   * `cliques.yaml` - Run clique finding over the citeseer graph.   * `motifs.yaml` - Run motif counting over the citeseer graph.   * `triangles.yaml` - Run triangle counting over the citeseer graph.  **Steps:**  1. Compile Arabesque using    ```   mvn package   ```   You will find the jar file under `target/`    2. Copy the newly generated jar file, the `run_arabesque.sh` script and the desired yaml files onto a folder on a computer with access to an Hadoop cluster.   3. Upload the input graph to HDFS. Sample graphs are under the `data` directory. Make sure you have initialized HDFS first.    ```   hdfs dfs -put <input graph file> <destination graph file in HDFS>   ```  4. Configure the `cluster.yaml` file with the desired number of containers, threads per container and other cluster-wide configurations.  5. Configure the algorithm-specific yamls to reflect the HDFS location of your input graph as well as the parameters you want to use (max size for motifs and cliques or support for FSM).  6. Run your desired algorithm by executing:    ```   ./run_arabesque.sh cluster.yaml <algorithm>.yaml   ```  7. Follow execution progress by checking the logs of the Hadoop containers.  8. Check any output (generated with calls to the `output` function) in the HDFS path indicated by the `output_path` configuration entry.   ## Implementing your own algorithms The easiest way to get to code your own implementations on top of Arabesque is by forking our [Arabesque Skeleton Project](https://github.com/Qatar-Computing-Research-Institute/Arabesque-Skeleton). You can do this via [Github](https://help.github.com/articles/fork-a-repo/) or manually by executing the following:  ``` git clone https://github.com/Qatar-Computing-Research-Institute/Arabesque-Skeleton.git $PROJECT_PATH cd $PROJECT_PATH git remote rename origin upstream git remote add origin $YOUR_REPO_URL ```
dnmilne/wikipediaminer	wikipediaminer ==============  An open source toolkit for mining Wikipedia
jt6211/hadoop-dns-mining	# hadoop-dns-mining  This is a small framework for performing large amounts of DNS lookups using Hadoop. This is a work in progress, pull requests are welcome.  ## Here are the steps for getting it working:  ### Download, compile and install the Maxmind JAR into maven           wget http://geolite.maxmind.com/download/geoip/api/java/GeoIPJava-1.2.5.zip     unzip GeoIPJava-1.2.5.zip     cd GeoIPJava-1.2.5/source/com/maxmind/geoip/     javac *.java     cd ../../../     zip -r maxmind.jar com/     mvn install:install-file -Dfile=maxmind.jar -DgroupId=com.maxmind -DartifactId=geo-ip -Dversion=1.2.5 -Dpackaging=jar       ### Obtain the Maxmind IP Geo Database           wget http://geolite.maxmind.com/download/geoip/database/GeoLiteCity.dat.gz     gzip -d GeoLiteCity.dat.gz       ### Obtain the Maxmind ASN Database           wget http://www.maxmind.com/download/geoip/database/asnum/GeoIPASNum.dat.gz     gzip -d GeoIPASNum.dat.gz       ### Create/obtain large lists of domain names (e.g. domains.txt) and copy them into HDFS          # you may want to split these domain files before placing in HDFS in order to use more mappers     split -a 5 -d -l 100000  domains.txt domains_     hadoop fs -put domains_* /data/domains/       ### Download and build this project           git clone https://jt6211@github.com/jt6211/hadoop-dns-mining.git     cd hadoop-dns-mining     mvn package assembly:assembly       ### Run the various MapReduce jobs           # These are the records that will be requested     REC_TYPES=A,MX,NS,TXT          JAR=target/hadoop-dns-mining-1.0-SNAPSHOT-job.jar      	# performs A record, MX record, and NS record lookups on each domain provided using 50      # resolving threads per Mapper using the nameserver of 8.8.8.8 and store the results in      # HDFS in /data/dns-mining/01_raw 	# Note: choose the nameserver wisely, otherwise you may overload it.  In testing I mainly      #  used a bind server deployed on each hadoop node so my nameserver was 127.0.0.1     time hadoop jar $JAR io.covert.dns.collection.CollectionJob \         -D dns.collection.num.resolvers=50 \         -D dns.collection.nameservers=8.8.8.8 \         IN \         "$REC_TYPES" \         /data/domains/ \         /data/dns-mining/01_raw          # parse the raw responses into JSON (one record per RR in the DNS responses)     time hadoop jar $JAR io.covert.dns.parse.ParseJob \         /data/dns-mining/01_raw \         /data/dns-mining/02_parsed          # lookup any IP addresses in the results in the maxmind DBs and enrich the records     time hadoop jar $JAR io.covert.dns.geo.GeoJob \         -files /usr/local/lib/maxmind/GeoLiteCity.dat,/usr/local/lib/maxmind/GeoIPASNum.dat \         GeoLiteCity.dat \         GeoIPASNum.dat \         /data/dns-mining/02_parsed  \         /data/dns-mining/03_enriched          # run a filter job for the rec types requested as well as for rec types that commonly occur in      # the results as part of normal queries.  This will separate the various DNS records into their     # own directories in HDFS     for X REC `echo "$REC_TYPES,SOA,NS,CNAME" | sed 's/,/\n/g'| sort -u`;      do         time hadoop jar $JAR io.covert.dns.filtering.FilterJob \             "type == '$REC'" \             /data/dns-mining/03_enriched \             /data/dns-mining/04_filtered-type=$REC;      done          # This is a JEXL expression that filters out target fields that are IP addresses      # and returns the target field lowercased     TARGET_EXPR='if(target !~ "^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\.$")return target.toLowerCase()'          # extract the 'target' field from the MX records     time hadoop jar $JAR io.covert.dns.extract.ExtractorJob "$TARGET_EXPR" \         /data/dns-mining/04_filtered-type=MX /data/dns-mining/05_extracted-mailservers          # extract the 'target' field from the NS records     time hadoop jar $JAR io.covert.dns.extract.ExtractorJob "$TARGET_EXPR" \         /data/dns-mining/04_filtered-type=NS /data/dns-mining/05_extracted-nameservers          HOST_EXPR='if(host !~ "^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\.$")return host.toLowerCase()'          # extract the 'host' field from the SOA records     time hadoop jar $JAR io.covert.dns.extract.ExtractorJob "$HOST_EXPR" \         /data/dns-mining/04_filtered-type=SOA /data/dns-mining/05_extracted-nameservers-SOA
uma-pi1/mgfsm	MG-FSM ======  Introduction ------------  Sequential pattern mining aims to discover hidden patterns and relationships in collections of sequential data; it has been successfully applied in a number of data mining tasks including text mining (e.g., finding frequent phrases), market basket analysis (e.g., finding frequent sequences of product sales), and web usage mining (e.g., finding frequent sequences of page visits). In particular, in the context of text mining, frequent phrases (commonly referred to as n-grams) are widely used in applications such as machine translation, speech recognition and information extraction.  [MG-FSM][1] is a scalable, general-purpose frequent sequence mining algorithm built for MapReduce. It takes as input a collection of sequences (e.g., a text collection or Web usage logs) and mines frequent sequences subject to a number of constraints such as minimum frequency, maximum length, or proximity constraints (position-based or temporal). A detailed description of MG-FSM can be found [here][2].  ###Contributors [Iris Miliaraki] [4], [Klaus Berberich] [5], [Rainer Gemulla] [6], [Kaustubh Beedkar] [7] and [Dhruv Gupta] [8].   MG-FSM overview --------------- Given a collection of input sequences (a sequence database), MG-FSM finds frequent subsequences that  +  Occur in at least σ ≥ 1  sequences (support threshold). +  Have length at most λ ≥ 2 (length threshold). +  Have gap at most γ ≥ 0 between consecutive items (gap threshold).  In additition to these constraints, MG-FSM also supports other types of constraints. Please refer to the available command line options for details.   Building MG-FSM ---------------  > 1. Prerequisites for building MG-FSM     - Java JDK 1.6 or 1.7     - [Maven][3] 3.2 or higher > 2. Check out the MG-FSM source code to some directory, we call it here as MGFSM_HOME > 3. Compiling: > >           $ cd MGFSM_HOME >           $ mvn clean install  > 4. Set the environment variables: > >           $ export JAVA_HOME=/location of Java >           $ export HADOOP_HOME=/location of Hadoop   After following the above methods, you can run the MG-FSM algorithm with appropriate arguments. Also, note that you can run the algorithm over HDFS data (in distributed mode) or local file system data (in sequential mode).   Running MG-FSM --------------  The input sequence file(s) should be of the following format: > > >       s1  Central Park is the best place to be on a sunny day in New York >       s2  Monday was a sunny day in New York >       s3  It was a sunny and beautiful New York City afternoon That is, each line represents a sequence, and has the format: > > >       SequenceId item1 item2 item3 ... itemN In this case, the first token of each line (whitespace delimited) becomes the *sequence id*, and all additional text of the line is interpreted as a sequence of items. For example, the above sequence database contains 3 sequences (s1, s2 and s3).   ### Running in sequential mode The sequential version of the algorithm runs locally on a single machine. Following example illustrates how to execute in such a mode: > > >       $ MGFSM_HOME/bin/mgfsm -i /path/to/input/dir/ -o /path/to/output/dir/ -s σ -g γ -l λ -m s When executing the “-(s)equential” mode the input data file(s) should be in “.txt” format (i.e. “.txt” extension should be present in the name) for proper execution.  ### Running in distributed mode To execute the algorithm on a Hadoop cluster, issue the “distributed” mode to the algorithm. Following example illustrates: > > >       $ MGFSM_HOME/bin/mgfsm -i /path/to/input/dir/ -o /path/to/output/dir/ -s σ -g γ -l λ -m d Note: The current version of MG-FSM is tested with *Hadoop 2.5.0* and *Hadoop 2.6.0*.  ### Output format Frequent sequences are written to a file in the output directory, where each line in the file has the following format: > > >       sequence <tab> #frequency  Example ------- Assume the following input sequences are in a file(s) (.txt) in a directory called SAMPLE_INPUT/ > > >       s1 Central Park is the best place to be on a sunny day in New York >       s2 Monday was a sunny day in New York >       s3 It was a sunny and beautiful New York City afternoon  Run the following command to find frequent all subsequences with support (σ = 3), maximum length (λ = 3) and maximum gap (γ = 2). > > >       $ MGFSM_HOME/bin/mgfsm -i SAMPLE_INPUT/ -o SAMPLE_OUTPUT/ -s 3 -g 2 -l 3 -m s  Sample output: > > >       sunny New York      3 >       a sunny New         3 >       New York            3 >       sunny New           3 >       a sunny             3  ## Command line options  | Option            | Short Hand 	| Optional 	| Default Value 	| Description                                                                                                                                                                                                  	| |---------------	|------------	|----------	|---------------	|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	| | support       	| s          	| Yes      	| 1             	| The minimum number of times the sequence to be mined must be present in the Database                                                                                                                         	| | gamma         	| g          	| Yes      	| 2             	| The maximum amount of gap that can be taken for a sequence to be mined by MG-FSM                                                                                                                             	| | lambda        	| l          	| Yes      	| 5             	| The maximum length of the sequence to be mined is determined by the this parameter.                                                                                                                          	| | execMode      	| m          	| Yes      	| s             	| Method of execution viz. (s)equential or (d)istributed                                                                                                                                                       	| | type          	| t          	| Yes      	| a             	| Specify the output type.  Expected values for type: 1. (a)ll 2. (m)aximal 3. (c)losed                                                                                                                        	| | keepFiles     	| k          	| Yes      	| -             	| Store the intermediary files for later use or runs. Further, if -k is not specified  the intermediary output will be saved to a temporary location.  The files stored are: 1. Dictionary 2.Encoded Sequences 	| | resume        	| r          	| Yes      	| -             	| Resume running further runs of the MG-FSM algorithm on already  encoded transaction file located in the folder specified in input.                                                                           	| | input         	| i          	| Yes      	| -             	| Path where the input transactions / database text file is located.                                                                                                                                           	| | output        	| o          	| No       	| -             	| Path where the output files are to written.                                                                                                                                                                  	| | tempDir       	| tempDir    	| Yes      	| -             	| Specify the temporary directory to be used for the map– reduce jobs                                                                                                                                          	| | numReducers   	| N          	| Yes      	| 90            	| Specify the number of reduce task.                                                                                                                                                                           	| | partitionSize 	| p          	| Yes      	| 10000         	| Explicitly specify the partition size.                                                                                                                                                                       	| | indexing      	| id         	| Yes      	| full          	| Specify the indexing mode. Options are : 1. none 2. minmax 3. full                                                                                                                                           	| | split         	| sp         	| Yes      	| false         	| Explicitly specify whether or not to allow split by setting this flag.                                                                                                                                       	|  **Additional Notes**  +   -(k)eepFiles and -(r)esume options are mutually exclusive. +   -(i)nput is optional as -(r)esume can also be used for pointing to the input source. +   -(i)nput and -(r)esume are mutually exclusive. +   -(o)utput is the only mandatory option. It points to the location where the translated frequent sequences will be written.       [1]: http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/software/mg-fsm/      "Mgfsm" [2]: http://resources.mpi-inf.mpg.de/d5/mg-fsm/mg-fsm-sigmod13.pdf "SIGMOD paper" [3]: http://maven.apache.org/ "Maven"  [4]: http://irismili.wordpress.com/ [5]: http://people.mpi-inf.mpg.de/~kberberi/ [6]: http://dws.informatik.uni-mannheim.de/en/people/professors/prof-dr-rainer-gemulla/ [7]: http://people.mpi-inf.mpg.de/~kbeedkar/ [8]: mailto:dhgupta@mpi-inf.mpg.de
mayconbordin/streaminer	Streaminer ========== [![Download](https://api.bintray.com/packages/mayconbordin/maven/streaminer/images/download.svg)](https://bintray.com/mayconbordin/maven/streaminer/_latestVersion)  A collection of algorithms for mining data streams, including frequent itemsets, quantiles, sampling, moving averages, set membership and cardinality.  ## Releases  Maven:  ```xml <dependency>     <groupId>com.github.mayconbordin</groupId>     <artifactId>streaminer</artifactId>     <version>1.1.1</version> </dependency> ```  ## [API Documentation](http://mayconbordin.github.io/streaminer/api/)   ## Frequent Itemsets  ### Algorithms   - CountSketch [\[1\]](#ref1)   - CountMinSketch [[2]](#ref2)   - LossyCounting [[3]](#ref3)   - Majority [[4]](#ref4)   - MisraGries [[5]](#ref5)   - SpaceSaving [[6]](#ref6)   - StickySampling [[3]](#ref3)   - RealCounting   - SimpleTopKCounting   - TimeDecayCountMinSketch   - TimeDecayRealCounting   - AMSSketch   - CCFCSketch   - CGT  ### Usage  Except for the `CountMinSketchAlt` class, all algorithms implement the `IRichFrequency` interface. Here's an example using the `SpaceSaving` algorithm:  ```java Random r = new Random(); int counters = 20; double support = 0.01; double maxError = 0.1;  IRichFrequency<Integer> counter = new SpaceSaving<Integer>(counters, support, maxError); for (int i=0 i<1000; i++) {     counter.add(r.nextInt(100), 1); }  // get the top 10 items List<CountEntry<Integer>> topk = counter.peek(10);  // print the items for (CountEntry<Integer> item : topk) {     System.out.println(item); }  // get the frequency of a single item int item = 25; long freq = counter.estimateCount(item); System.out.println(item + ": " + freq); ```  ### Time Decaying Algorithms  `TimeDecayRealCounting` and `TimeDecayCountMinSketch` are algorithms that use a  decay function to update the current values of their counts in order to give more importance to newer values, while older values will slowly fade away.  The decay function implements the `DecayFormula` interface. Currently there are  three implementations: the exponential (`ExpDecayFormula`), the linear (`LinDecayFormula`),  and the logarithmic (`LogDecayFormula`).  Those counting algorithms implement a different interface called `ITimeDecayFrequency`, as both methods for adding and estimating the frequency need an additional argument,  the timestamp.   ## Top-K  ### Algorithms    - StreamSummary [[6]](#ref6)   - ConcurrentStreamSummary   - Frequent   - StochasticTopper  ### Usage  The basic usage of a Top-K algorithm is basically the same as the frequent itemset, except that these algorithms do not support the `estimateCount` method.  ```java ITopK<String> counter = new StreamSummary<String>(3);  String[] stream = {"X", "X", "Y", "Z", "A", "B", "C", "X", "X", "A", "C", "A", "A"}; for (String i : stream) {     counter.add(i); }  List<CountEntry<String>> topk = counter.peek(3); for (CountEntry<String> item : topk) {     System.out.println(item); } ```   ## Quantiles  ### Algorithms    - CKMSQuantiles [[7]](#ref7)   - Frugal2U [[8]](#ref8)   - GKQuantiles [[9]](#ref9)   - MPQuantiles [[10]](#ref10)   - QDigest [[11]](#ref11)   - WindowSketchQuantiles [[12]](#ref12)   - RSSQuantiles [[13]](#ref13)   - EnsembleQuantiles   - ExactQuantiles   - ExactQuantilesAll   - SimpleQuantiles   - SumQuantiles   - TDigest  ### Usage  ```java double[] quantiles = new double[]{0.05, 0.25, 0.5, 0.75, 0.95}; IQuantiles<Integer> instance = new Frugal2U(quantiles, 0);  RandomEngine r = new MersenneTwister64(0); Normal dist = new Normal(100, 50, r); int numSamples = 1000;          for(int i = 0; i < numSamples; ++i) {     int num = (int) Math.max(0, dist.nextDouble());     instance.offer(num); }  for (double q : quantiles) {     System.out.println(q + ": " + instance.getQuantile(q)); } ```   ## Cardinality  ### Algorithms    - AdaptiveCounting [[14]](#ref14)   - LogLog [[15]](#ref15)   - HyperLogLog [[16]](#ref16)   - HyperLogLogPlus [[17]](#ref17)   - LinearCounting [[18]](#ref18)   - CountThenEstimate   - BJKST [[26]](#ref26)   - FlajoletMartin [[27]](#ref27)   - KMinCount  ### Usage  ```java ICardinality card = new LogLog(8);  for (int i=0; i<100; i++) {     card.offer(Math.random()*100.0); }  System.out.println("Cardinality: " + card.cardinality()); ```   ## Average  ### Algorithms    - MovingAverage   - ExponentialMovingAverage   - SimpleEWMA   - VariableEWMA   - TEWMA [[25]](#ref25)  ### Usage  ```java // create a EWMA with 15 seconds of age for the metrics in the period IAverage avg = new VariableEWMA(15.0);  for (int i=0; i<100; i++) {     avg.add(Math.random()*100.0);     if (i%10 == 0)         System.out.println("Average: " + avg.getAverage()); } ```   ## Membership  ### Algorithms    - BloomFilter [[22]](#ref22)   - BloomFilterAlt (alternative implementation)   - CountingBloomFilter [[19]](#ref19)   - VarCountingBloomFilter (with variable `bucketsPerWord`)   - DynamicBloomFilter [[20]](#ref20)   - RetouchedBloomFilter [[21]](#ref21)   - StableBloomFilter [[23]](#ref23)   - TimingBloomFilter [[24]](#ref24)   - ODTDBloomFilter [[28]](#ref28)  ### Usage  ```java IFilter bloom = new BloomFilter(1000, 32, Hash.MURMUR_HASH);  for (int i = 0; i < 100; i++) {     String val = UUID.randomUUID().toString();     Key k = new Key(val.getBytes());     bloom.add(k);     System.out.println(val + " exists? " + bloom.membershipTest(k)); } ```   ## Sampling  ### Algorithms    - BernoulliSampler   - ChainSampler [[29]](#ref29)   - ReservoirSampler   - SystematicSampler   - WRSampler (With Replacement)   - WeightedRandomSampler   - L0Sampler [[30]](#ref30)   - SpaceSavingSampler   - FrequentSampler  ### Usage  ```java // Create a sampler with 30% probability ISampler sampler = new BernoulliSampler(0.3);  Random rand = new Random();  // Create a dummy stream of ints List<Integer> stream = new ArrayList<Integer>(1000); for (int i=0; i<1000; i++)     stream.add(rand.nextInt(100));  for (Integer tuple : stream) {     if (sampler.next()) {         // tuple was sampled, do something     } else {         // tuple was ignored, move on     } } ```   ## Classifiers  ### Algorithms    - Perceptron   - NaiveBayes   - NaiveBayesWOP   - BoundedBayes   - LossyBayes   - MultiBayes   - MultiLossyBayes   - MultiTopKBayes   - SticySamplingBayes   - TopKBayes   - MajorityClass   - RandomClassifier   - MultiRandomClassifier   - AROWClassifier (Adaptive Regularization of Weight Vectors) [[32]](#ref32)   - BWinnowClassifier (Balanced Winnow Classifier) [[33]](#ref33)   - PAClassifier, MultiClassPAClassifier [[34]](#ref34)   - WinnowClassifier  ### Usage  ```java NaiveBayes nb = new NaiveBayes(); nb.setLabelAttribute("play");              ICsvListReader listReader = new CsvListReader(         new FileReader("src/test/resources/golf.csv"),          CsvPreference.EXCEL_NORTH_EUROPE_PREFERENCE);  listReader.getHeader(true);  List<String> list; while( (list = listReader.read()) != null ) {     Data data = new DataImpl();     data.put("outlook", list.get(0));     data.put("temperature", Integer.parseInt(list.get(1)));     data.put("humidity", Integer.parseInt(list.get(2)));     data.put("wind", Boolean.parseBoolean(list.get(3)));     data.put("play", list.get(4));      nb.learn(data); }  Data test = new DataImpl(); test.put("outlook", "sunny"); test.put("temperature", "cool"); test.put("humidity", "high"); test.put("windy", "TRUE");  String prediction = nb.predict(test); System.out.println("Item is: " + test); System.out.println("Prediction is: " + prediction); ```  ## Clustering  ### Algorithms      - K-Means   - BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) [[31]](#ref31)   ## References  `[1]` <a name="ref1"></a>Charikar, Moses, Kevin Chen, and Martin Farach-Colton. "Finding frequent items in data streams." Automata, Languages and Programming. Springer Berlin Heidelberg, 2002. 693-703.  `[2]` <a name="ref2"></a>Cormode, Graham, and S. Muthukrishnan. "An improved data stream summary: the count-min sketch and its applications." Journal of Algorithms 55.1 (2005): 58-75.  `[3]` <a name="ref3"></a>Manku, Gurmeet Singh, and Rajeev Motwani. "Approximate frequency counts over data streams." Proceedings of the 28th international conference on Very Large Data Bases. VLDB Endowment, 2002.  `[4]` <a name="ref4"></a>M. J. Fischer and S. L. Salzberg. "Finding a Majority Among N Votes: Solution to Problem 81-5(Journal of Algorithms, June 1981)", Journal of Algorithms, 3:4, December 1982, pp. 362-380.  `[5]` <a name="ref5"></a>Misra, Jayadev, and David Gries. "Finding repeated elements." Science of computer programming 2.2 (1982): 143-152.  `[6]` <a name="ref6"></a>Metwally, Ahmed, Divyakant Agrawal, and Amr El Abbadi. "Efficient computation of frequent and top-k elements in data streams." Database Theory-ICDT 2005. Springer Berlin Heidelberg, 2005. 398-412.  `[7]` <a name="ref7"></a>Cormode, Graham, et al. "Effective computation of biased quantiles over data streams." Data Engineering, 2005. ICDE 2005. Proceedings. 21st International Conference on. IEEE, 2005.  `[8]` <a name="ref8"></a>Ma, Qiang, S. Muthukrishnan, and Mark Sandler. "Frugal Streaming for Estimating Quantiles." Space-Efficient Data Structures, Streams, and Algorithms. Springer Berlin Heidelberg, 2013. 77-96.  `[9]` <a name="ref9"></a>Greenwald, Michael, and Sanjeev Khanna. "Space-efficient online computation of quantile summaries." ACM SIGMOD Record. Vol. 30. No. 2. ACM, 2001.  `[10]` <a name="ref10"></a>Munro, J. Ian, and Mike S. Paterson. "Selection and sorting with limited storage." Theoretical computer science 12.3 (1980): 315-323.  `[11]` <a name="ref11"></a>Shrivastava, Nisheeth, et al. "Medians and beyond: new aggregation techniques for sensor networks." Proceedings of the 2nd international conference on Embedded networked sensor systems. ACM, 2004.  `[12]` <a name="ref12"></a>Arasu, Arvind, and Gurmeet Singh Manku. "Approximate counts and quantiles over sliding windows." Proceedings of the twenty-third ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. ACM, 2004.  `[13]` <a name="ref13"></a>Gilbert, Anna C., et al. "How to summarize the universe: Dynamic maintenance of quantiles." Proceedings of the 28th international conference on Very Large Data Bases. VLDB Endowment, 2002.  `[14]` <a name="ref14"></a>Cai, Min, et al. "Fast and accurate traffic matrix measurement using adaptive cardinality counting." Proceedings of the 2005 ACM SIGCOMM workshop on Mining network data. ACM, 2005.  `[15]` <a name="ref15"></a>Durand, Marianne, and Philippe Flajolet. "Loglog counting of large cardinalities." Algorithms-ESA 2003. Springer Berlin Heidelberg, 2003. 605-617.  `[16]` <a name="ref16"></a>Flajolet, Philippe, et al. "HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm." DMTCS Proceedings 1 (2008).  `[17]` <a name="ref17"></a>Heule, Stefan, Marc Nunkesser, and Alexander Hall. "HyperLogLog in practice: algorithmic engineering of a state of the art cardinality estimation algorithm." Proceedings of the 16th International Conference on Extending Database Technology. ACM, 2013.  `[18]` <a name="ref18"></a>Whang, Kyu-Young, Brad T. Vander-Zanden, and Howard M. Taylor. "A linear-time probabilistic counting algorithm for database applications." ACM Transactions on Database Systems (TODS) 15.2 (1990): 208-229.  `[19]` <a name="ref19"></a>Fan, L., Cao, P., Almeida, J., & Broder, A. Z. (2000). Summary cache: a scalable wide-area web cache sharing protocol. IEEE/ACM Transactions on Networking (TON), 8(3), 281-293.  `[20]` <a name="ref20"></a>Guo, Deke, Jie Wu, Honghui Chen, and Xueshan Luo. "Theory and Network Applications of Dynamic Bloom Filters." In INFOCOM, pp. 1-12. 2006.  `[21]` <a name="ref21"></a>Donnet, Benoit, Bruno Baynat, and Timur Friedman. "Retouched bloom filters: allowing networked applications to trade off selected false positives against false negatives." In Proceedings of the 2006 ACM CoNEXT conference, p. 13. ACM, 2006.  `[22]` <a name="ref22"></a>Bloom, Burton H. "Space/time trade-offs in hash coding with allowable errors." Communications of the ACM 13, no. 7 (1970): 422-426.  `[23]` <a name="ref23"></a>Deng, Fan, and Davood Rafiei. "Approximately detecting duplicates for streaming data using stable bloom filters." Proceedings of the 2006 ACM SIGMOD international conference on Management of data. ACM, 2006.  `[24]` <a name="ref24"></a>Dautrich Jr, Jonathan L., and Chinya V. Ravishankar. "Inferential time-decaying Bloom filters." Proceedings of the 16th International Conference on Extending Database Technology. ACM, 2013.  `[25]` <a name="ref25"></a>Martin, Ruediger, and Michael Menth. "Improving the Timeliness of Rate Measurements." In MMB, pp. 145-154. 2004.  `[26]` <a name="ref26"></a>Bar-Yossef, Ziv, et al. "Counting distinct elements in a data stream." Randomization and Approximation Techniques in Computer Science. Springer Berlin Heidelberg, 2002. 1-10.  `[27]` <a name="ref27"></a>Flajolet, Philippe, and G. Nigel Martin. "Probabilistic counting algorithms for data base applications." Journal of computer and system sciences 31.2 (1985): 182-209.  `[28]` <a name="ref28"></a>Bianchi, Giuseppe, Nico d'Heureuse, and Saverio Niccolini. "On-demand time-decaying bloom filters for telemarketer detection." ACM SIGCOMM Computer Communication Review 41.5 (2011): 5-12.  `[29]` <a name="ref29"></a>Babcock, Brian, Mayur Datar, and Rajeev Motwani. "Sampling from a moving window over streaming data." Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2002.  `[30]` <a name="ref30"></a>Cormode, Graham, Donatella Firmani, Graham Cormode, and Donatella Firmani. "On Unifying the Space of ℓ0-Sampling Algorithms." In ALENEX, pp. 163-172. 2013.  `[31]` <a name="ref31"></a>Zhang, Tian, Raghu Ramakrishnan, and Miron Livny. "BIRCH: an efficient data clustering method for very large databases." ACM SIGMOD Record. Vol. 25. No. 2. ACM, 1996.  `[32]` <a name="ref32"></a>Crammer, Koby, Alex Kulesza, and Mark Dredze. "Adaptive regularization of weight vectors." Advances in Neural Information Processing Systems. 2009.  `[33]` <a name="ref33"></a>Carvalho, Vitor R., and William W. Cohen. "Single-pass online learning: Performance, voting schemes and online feature selection." Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006.  `[34]` <a name="ref34"></a>Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. "Online passive-aggressive algorithms." The Journal of Machine Learning Research 7 (2006): 551-585.  ## Similar Libraries    - **Java**     - [stream-lib](https://github.com/addthis/stream-lib)     - [MOA (Massive Online Analysis)](http://code.google.com/p/moa/)     - [stream-mining](https://bitbucket.org/cbockermann/stream-mining)     - [jerboa](https://github.com/vandurme/jerboa)     - [hoidla](https://github.com/pranab/hoidla)   - **Scala**     - [algebird](https://github.com/twitter/algebird)     - [fleet](https://github.com/noelwelsh/fleet)   - **C/C++**     - [sketch library](http://hadjieleftheriou.com/sketches/index.html)     - [StreamSketch](https://github.com/absolute8511/StreamSketch)     - [StreamingAlgorithms](https://github.com/bmoscon/StreamingAlgorithms)     - [Streaming-Data-Algorithms](https://github.com/markfuge/Streaming-Data-Algorithms)     - [scrunch](https://github.com/jkff/scrunch)     - [MassDAL](http://www.cs.rutgers.edu/~muthu/massdal-code-index.html)     - [sketches](http://www.cise.ufl.edu/~frusu/code.html)   - **JavaScript**     - [node-streamcount](https://github.com/jhurliman/node-streamcount)     - [node-datastream](https://github.com/mayconbordin/node-datastream)
mauricioaniche/repodriller	# RepoDriller  [![Build Status](https://travis-ci.org/mauricioaniche/repodriller.svg?branch=master)](https://travis-ci.org/mauricioaniche/repodriller)  RepoDriller is a Java framework that helps developers on mining software repositories. With it, you can easily extract information from any Git repository, such as commits, developers, modifications, diffs, and source codes, and quickly export CSV files.  Take a look at our documentation and [our many examples](https://github.com/mauricioaniche/repodriller-tutorial). Or [talk to us](https://groups.google.com/forum/#!forum/repodriller).  # Documentation  ## Getting Started  You simply have to start a Java Project in Eclipse. RepoDriller is on Maven, so you can download all its dependencies by only adding this to your pom.xml. Or, if you want, you can see [an example](github.com/mauricioaniche/change-metrics):  ``` <dependency> 	<groupId>org.repodriller</groupId> 	<artifactId>repodriller</artifactId> 	<version>1.3.1</version> </dependency>  ```  Always use the latest version in Maven. You can see them here: [http://www.mvnrepository.com/artifact/org.repodriller/repodriller](http://www.mvnrepository.com/artifact/org.repodriller/repodriller) . You can also see a [fully function pom.xml example](https://gist.github.com/mauricioaniche/3eba747930aea97e4adb).  RepoDriller needs a _Study_. The interface is quite simple: a single _execute()_ method:  ```java import org.repodriller.RepoDriller; import org.repodriller.Study;  public class MyStudy implements Study {  	public static void main(String[] args) { 		new RepoDriller().start(new MyStudy()); 	}  	@Override 	public void execute() { 		// do the magic here! ;) 	} } ```  All the magic goes inside this method. In there, you will have to configure your study, projects to analyze, metrics to be executed, and output files. Take a look in the example. That's what we have to configure:  ```java public void execute() { 	new RepositoryMining() 		.in(<LIST OF PROJECTS>) 		.through(<COMMITS>) 		.process(<PROCESSOR>, <OUTPUT>) 		.mine(); } ```  Let's start with something simple: we will print the name of the developers for each commit. For now, you should not care about all possible configurations. We will analyze all commits in the project at "/Users/mauricioaniche/workspace/repodriller", outputing _DevelopersVisitor_ to "/Users/mauricioaniche/Desktop/devs.csv".  *   in(): We use to configure the project (or projects) that will be analyzed. *   through(): The list of commits to analyze. We want all of them. See `Commits` class for a list of available options. *   filters(): Possible filters to commits, e.g., only commits in a certain branch  *   reverseOrder(): Commits will be analysed in reverse order. Default starts from the first commit to the latest one. *   process(): Visitors that will pass in each commit. *   mine(): The magic starts!  ```java public void execute() { 	new RepositoryMining() 		.in(GitRepository.singleProject("/Users/mauricioaniche/workspace/repodriller")) 		.through(Commits.all()) 		.process(new DevelopersVisitor(), new CSVFile("/Users/mauricioaniche/Desktop/devs.csv")) 		.mine(); } ```   In practice, RepoDriller will open the Git repository and will extract all information that is inside. Then, the framework will pass each commit to all processors. Let's write our first _DevelopersProcessor_. It is fairly simple. All we will do is to implement _CommitVisitor_. And, inside of _process()_, we print the commit hash and the name of the developer. RepoDriller gives us nice objects to play with all the data:  ```java import org.repodriller.domain.Commit; import org.repodriller.persistence.PersistenceMechanism; import org.repodriller.scm.CommitVisitor; import org.repodriller.scm.SCMRepository;  public class DevelopersVisitor implements CommitVisitor {  	@Override 	public void process(SCMRepository repo, Commit commit, PersistenceMechanism writer) { 		 		writer.write( 			commit.getHash(), 			commit.getCommitter().getName() 		);  	} } ```  That's it, we are ready to go! If we execute it, we will have the CSV printed into "/Users/mauricioaniche/Desktop/devs.csv". [Take a look](https://gist.github.com/mauricioaniche/e0d2f8d4c09ef15d17fad25ec582c706).  **I bet you never found a framework simple as this one!**  ## Configuring the project  The first thing you configure in RepoDriller is the project you want to analyze. RepoDriller currently suports Subversion and Git repositories. The _SubversionRepository_ and _GitRepository_ classes contains two factory methods to that:  *   _singleProject(path)_: When you want to analyze a single repository. *   _allProjectsIn(path)_: When you want to analyze many repositories. In this case, you should pass a path to which all projects are sub-directories of it. Each directory will be considered as a project to RepoDriller.  You can also initialize git repositories with their remote HTTP URLs. In this case, RepoDriller will clone the remote repository in order to manipulate the repository history. The _GitRemoteRepository_ class contains the same factory methods of _GitRepository_, but you can also configure it, like this:   ``` 	GitRemoteRepository 		.hostedOn(gitUrl)							// URL like: https://github.com/mauricioaniche/repodriller.git 		.inTempDir(tempDir)							// <Optional> 		.asBareRepos()								// <Optional> (1) 		.buildAsSCMRepository()) ```  (1) You can clone as a bare repository, if your study will work only with repository metadata (commit history info, modifications, etc.) and won't need to checkout/reset files.  Git offers the `first-parent` filter, which can also be used in RepoDriller. To that end, just pass a flag to the factory:  ``` GitRepository.single("/your/project", true); ```   ## Logging  RepoDriller uses log4j to print useful information about its execution.  **Note that this includes Exceptions and their stack traces, which will not appear in standard output.** We recommend you have a log4.xml:  ```xml <?xml version="1.0" encoding="UTF-8" ?> <!DOCTYPE log4j:configuration SYSTEM "log4j.dtd">  <log4j:configuration xmlns:log4j="http://jakarta.apache.org/log4j/">      <appender name="main" class="org.apache.log4j.ConsoleAppender">         <layout class="org.apache.log4j.PatternLayout">             <param name="ConversionPattern" value="%d{HH:mm:ss} %5p %m%n"/>         </layout>     </appender>      <category name="org.repodriller">         <priority value="INFO"/>         <appender-ref ref="main"/>     </category>      <category name="/">         <priority value="INFO"/>         <appender-ref ref="main"/>     </category>  </log4j:configuration> ```  Put this file in `{project-root}/src/main/resources/` and you should see logs when RepoDriller runs.  ## Selecting the Commit Range  RepoDriller allows you to select the range of commits to be processed. The class _Commits_ contains different methods to that:  *   _all()_: All commits. From the first to the last. *   _onlyInHead()_: It only analyzes the most recent commit. *   _single(hash)_: It only analyzes a single commit with the provided hash. *   _monthly(months)_: It selects one commit per month, from the beginning to the end of the repo. *   _list(commits...)_: The list of commits to be processed. *   _range(start,end)_: The range of commits, starting at "start" hash, ending at "end" hash. *   _betweenDates(from,to)_: The range of commits, starting at "from" timestamp, ending at "to" timestamp. *   _since(date)_: All commits that appear after a certain timestamp.  One interesting thing about RepoDriller is that it avoids huge commits. When a commit contains too many files (> 50), it will be ignored.  ## Filtering commits  RepoDriller comes with a set of common filters that you can apply. As an example, the `OnlyInBranches` filter makes sure that your Study will only visit commits which exist in specific branches.  * _OnlyInBranches_: Only visits commits that belong to certain branches.  * _OnlyInMainBranch_: Only visits commits that belong to the main branch of the repository. * _OnlyNoMerge_: Only visits commits that are not merge commits. * _OnlyModificationsWithFileTypes_: Only visits commits in which at least one modification was done in that file type, e.g.,  if you pass ".java", then, the study will visit only commits in which at least one Java file was modified; clearly, it will skip other commits.  You can choose more than one filter as they can be decorated. A working example is:  ``` .filters( 	new OnlyModificationsWithFileTypes(Arrays.asList(".java", ".xml")), 	new OnlyInBranches(Arrays.asList("master")),  	new OnlyNoMerge(),  	new OnlyInMainBranch() ); ```  Also, you can create your own filter. All you have to do is to extend `CommitFilter` and implement the `shouldAccept()` method. This method should only return **true** if the commit _should_ be visited, else it should return **false**.  ## Getting Modifications  You can get the list of modified files, as well as their diffs and current source code. To that, all you have to do is to get the list of _Modification_s that exists inside _Commit_. A _Commit_ contains a hash, a committer (name and email), an author (name, and email) a message, the date, its parent hash, and the list of modification.  ```java @Override public void process(SCMRepository repo, Commit commit, PersistenceMechanism writer) { 	 	for(Modification m : commit.getModifications()) { 		writer.write( 				commit.getHash(), 				commit.getAuthor().getName(), 				commit.getCommitter().getName(), 				m.getFileName(), 				m.getType() 		); 		 	} } ```  A _Modification_ contains a type (ADD, COPY, RENAME, DELETE, MODIFY), a diff (with the exact format Git delivers) and the current source code. Remember that it is up to you to handle deleted or renamed files in your study.  ## Branches  RepoDriller puts all commits from all branches in a single sentence. It means that different commits from different branches will appear. It is your responsibility to filter the branches in your _CommitVisitor_.  The _Commit_ class contains the _getBranches()_ method. It returns the list of branches in which a commit belongs to. If you want to use only commits in the master branch, you can simply check whether 'master' in inside this set.  Note about the implementation: This is not supported by JGit, so it makes use of Git directly.  ## Diffs  Modifications contains _diffs_ from the current version and the last one. This diff is extracted directly from Git output. A common example of the output can be seen below. Diffs have their own format (the @@ indicates how you should read it).  ``` diff --git a/GitRepository.java b/GitRepository.java index f38a97d..2b96b0e 100644 --- a/GitRepository.java +++ b/GitRepository.java @@ -72,7 +72,7 @@ public class GitRepository implements SCM {           private static Logger log = Logger.getLogger(GitRepository.class);   -       public GitRepository(String path) { +       public GitRepository2(String path) {                 this.path = path;                 this.maxNumberFilesInACommit = checkMaxNumberOfFiles();                 this.maxSizeOfDiff = checkMaxSizeOfDiff(); ```  To facilitate the parsing, RepoDriller offers `DiffParser` class. This utility class parses the diff and returns two separate lists: lines (number and content) from the previous version and lines  from the new version. As one diff may contain different blocks of diffs (happens when the file was modified in two parts that are far from each other), the parser returns 1 or more diff blocks.  ``` // parse the diff DiffParser parsedDiff = new DiffParser(diff);  // return all the lines in the old file List<DiffLine> oldLines = parsedDiff.getBlocks().get(0).getLinesInOldFile(); // return all the lines in the new file List<DiffLine> oldLines = parsedDiff.getBlocks().get(0).getLinesInNewFile(); ```  You may configure the context of the algorithm. Check section _Configuring Git options_ in this documentation.  ## Blame  The _SCM_ class contains a blame() method, which allows you to blame a file in a specific commit:  `List<BlamedLine> blame(String file, String commitToBeBlamed, boolean priorCommit)`  You should pass the file name (relative path), the commit which the file should be blamed, and a boolean informing whether you want the file to be blamed _before_ (priorCommit=true) or _after_ (priorCommit=false) the changes of that particular commit.  ## Managing State in the Visitor  If you need to, you can store state in your visitors. As an example, if you do not want to process a huge CSV, you can pre-process something before. As an example, if you want to count the total number of modified files per developer, you can either output all developers and the quantity of modifications, and then sum it later using your favorite database, or do the math in the visitor. If you decide to do it, it will be your responsibility to save the results afterwards.  ```java import java.util.HashMap; import java.util.Map;  import org.repodriller.domain.Commit; import org.repodriller.persistence.PersistenceMechanism; import org.repodriller.scm.CommitVisitor; import org.repodriller.scm.SCMRepository;  public class ModificationsVisitor implements CommitVisitor {  	private Map<String, Integer> devs; 	 	public ModificationsVisitor() { 		this.devs = new HashMap<String, Integer>(); 	} 	 	@Override 	public void process(SCMRepository repo, Commit commit, PersistenceMechanism writer) { 		 		String dev = commit.getCommitter().getName(); 		if(!devs.containsKey(dev)) devs.put(dev, 0); 		 		int currentFiles = devs.get(dev); 		devs.put(dev, currentFiles + commit.getModifications().size()); 		 	}  } ```  ## Getting the Current Revision  If you need more than just the metadata from the commit, you may also check out to the revision to access all files. This may be useful when you need to parse all files inside that revision.  To that, we will _checkout()_ the revision, and get all _files()_. It returns the list of all files in the project at that moment. Then, it is up to you to do whatever you want. In here, we will use our _NumberOfMethodsVisitor_ to count the number of files in all Java files. Please, remember to _reset()_ as soon as you finish playing with the files.  ```java import java.io.ByteArrayInputStream; import java.io.File; import java.io.FileInputStream; import java.util.List;  import org.apache.commons.io.IOUtils;  import com.mystudy.example4.NumberOfMethodsVisitor;  import org.repodriller.domain.Commit; import org.repodriller.parser.jdt.JDTRunner; import org.repodriller.persistence.PersistenceMechanism; import org.repodriller.scm.CommitVisitor; import org.repodriller.scm.RepositoryFile; import org.repodriller.scm.SCMRepository;  public class JavaParserVisitor implements CommitVisitor {  	@Override 	public void process(SCMRepository repo, Commit commit, PersistenceMechanism writer) {  		try { 			repo.getScm().checkout(commit.getHash()); 		 			List<RepositoryFile> files = repo.getScm().files(); 			 			for(RepositoryFile file : files) { 				if(!file.fileNameEndsWith("java")) continue; 				 				File soFile = file.getFile(); 				 				NumberOfMethodsVisitor visitor = new NumberOfMethodsVisitor(); 				new JDTRunner().visit(visitor, new ByteArrayInputStream(readFile(soFile).getBytes())); 				 				int methods = visitor.getQty(); 				 				writer.write( 						commit.getHash(), 						file.getFullName(), 						methods 				); 				 			} 			 		} finally { 			repo.getScm().reset(); 		} 	} 	  	private String readFile(File f) { 		try { 			FileInputStream input = new FileInputStream(f); 			String text = IOUtils.toString(input); 			input.close(); 			return text; 		} catch (Exception e) { 			throw new RuntimeException("error reading file " + f.getAbsolutePath(), e); 		} 	} } ```  ## Dealing with Threads  If your machine has multiple cores, RepoDriller can execute the commit visitor over many threads. This is just another configuration you set in _RepositoryMining_. The _withThreads()_ lets you configure the number of threads the framework will use to process everything.  We suggest you use threads unless your workflow must _checkout_ revisions. We only keep one copy of the repository on disk, so two threads would conflict if they both ran _checkout_.  ```java @Override public void execute() { 	new RepositoryMining() 		.in(GitRepository.singleProject("/Users/mauricioaniche/workspace/repodriller")) 		.through(Commits.all()) 		.withThreads(3) 		.process(new JavaParserVisitor(), new CSVFile("/Users/mauricioaniche/Desktop/devs.csv")) 		.mine(); } ```  ## Configuring Git options  RepoDriller takes a few decisions on the Git configuration. You can change them by passing a system property. Example: `java -Dgit.maxfiles=2000 -jar ...`.  Existing variables:  - *git.maxfiles*: The max quantity of files in a single commit. Commits with more files than this constant are ignored. Default is 200.    - *git.maxdiff*: The max number of lines in a diff. Diffs higher than that are ignored. Default is 100000.  - *git.diffcontext*: The size of the content that is used by the diff algorithm. Default is git default.  ## Creating your own CommitRange  (not written yet)  ## Creating your own PersistenceMechanism  (not written yet)  ## Using ASTs to parse source code  Repodriller does not come with JDT or any other code parser (as it used to be in old versions).  However, it naturally fits with such tools.   The [repodriller JDT plugin](https://github.com/mauricioaniche/repodriller-plugin-jdt) adds all JDT libraries to your study.  If you want to learn more about JDT, check the documentation for [ASTVisitor class](http://help.eclipse.org/juno/index.jsp?topic=%2Forg.eclipse.jdt.doc.isv%2Freference%2Fapi%2Forg%2Feclipse%2Fjdt%2Fcore%2Fdom%2FASTVisitor.html). In addition, if you are looking for traditional code metrics in Java, you may use our [CK project](https://github.com/mauricioaniche/ck).  We currently do not have any out-of-the-box plugin for other languages. However, we do not expect any integration problems with them.  # Advice to researchers  ## Git  You should read this paper: - Bird, Christian, et al. "The promises and perils of mining git." Mining Software Repositories, 2009. MSR'09. 6th IEEE International Working Conference on. IEEE, 2009. [Link](http://cs.queensu.ca/~ahmed/home/teaching/CISC880/F10/papers/MiningGit_MSR2009.pdf).  # How do I cite RepoDriller?  For now, cite the repository.   # How can I discuss about it?  You can subscribe to our mailing list: https://groups.google.com/forum/#!forum/repodriller.  # How to Contribute  Required: Git, Maven.  ``` git clone https://github.com/mauricioaniche/repodriller.git cd repodriller/test-repos unzip \*.zip ```  Then, you can:  * compile : `mvn clean compile` * test    : `mvn test` * eclipse : `mvn eclipse:eclipse` * build   : `mvn clean compile assembly:single`  # License  This software is licensed under the Apache 2.0 License.
ShifuML/shifu	[<img src="images/logo/shifu.png" alt="Shifu" align="left">](http://shifu.ml)<div align="right"><div>[![Build Status](https://travis-ci.org/ShifuML/shifu.svg?branch=develop)](https://travis-ci.org/ShifuML/shifu)</div><div>[![Maven Central](https://maven-badges.herokuapp.com/maven-central/ml.shifu/shifu/badge.svg)](https://maven-badges.herokuapp.com/maven-central/ml.shifu/shifu)</div></div>  #  ## Download Please [download](https://github.com/ShifuML/shifu/wiki/shifu-0.10.5-hdp-yarn.tar.gz) latest shifu [here](https://github.com/ShifuML/shifu/wiki/shifu-0.10.5-hdp-yarn.tar.gz).  ## Getting Started After shifu downloading, build your first model with Shifu [tutorial](https://github.com/ShifuML/shifu/wiki/Tutorial---Build-Your-First-ML-Model). More details about shifu can be found in our [wiki pages](https://github.com/ShifuML/shifu/wiki).  ## What is Shifu? Shifu is an open-source, end-to-end machine learning and data mining framework built on top of Hadoop. Shifu is designed for data scientists, simplifying the life-cycle of building machine learning models. While originally built for fraud modeling, Shifu is generalized for many other modeling domains.  One of Shifu's pros is an end-to-end modeling pipeline in machine learning. With only configurations settings, a whole machine pipeline can be built and model can be much more easy to develop and push to production. The pipeline defined in Shifu is in below:  ![Shifu Pipeline](https://raw.githubusercontent.com/wiki/ShifuML/shifu/images/new-shifu-pipeline.png)  Shifu provides a simple command-line interface for each step of the model building process, including  * Statistic calculation & variable selection to determine the most predictive variables in your data * [Variable normalization](https://github.com/ShifuML/shifu/wiki/Variable%20Transform%20in%20Shifu) * [Distributed variable selection based on sensitivity analysis](https://github.com/ShifuML/shifu/wiki/Variable%20Selection%20in%20Shifu) * [Distributed neural network model training](https://github.com/ShifuML/shifu/wiki/Distributed%20Neural%20Network%20Training%20in%20Shifu) * [Distributed tree ensemble model training](https://github.com/ShifuML/shifu/wiki/Distributed%20Tree%20Ensemble%20Model%20Training%20in%20Shifu) * Post training analysis & model evaluation  Shifu’s fast Hadoop-based, distributed neural network / logistic regression / gradient boosted trees training can reduce model training time from days to hours on TB data sets. Shifu integrates with Pig workflows on Hadoop, and Shifu-trained models can be integrated into production code with a simple Java API. Shifu leverages Pig, Akka, Encog and other open source projects.  [Guagua](https://github.com/ShifuML/guagua), an in-memory iterative computing framework on Hadoop YARN is developed as sub-project of Shifu to accelerate training progress.  More details about shifu can be found in our [wiki pages](https://github.com/ShifuML/shifu/wiki)  ## Conference  * [QCON Shanghai 2015](http://2015.qconshanghai.com/presentation/2827) [Slides](http://www.slideshare.net/pengshanzhang/large-scale-machine-learning-at-pay-pal-risk)  * [BDTC Beijing 2016](http://bdtc2016.hadooper.cn/dct/page/70107)  * [Strata Beijing 2017](https://strata.oreilly.com.cn/strata-cn/public/schedule/detail/59593?locale=en)  ## Contributors   - Zhanghao Hu (zhanhu@paypal.com)  - Grahame Jastrebski (gjastrebski@paypal.com)  - Lavar Li (lulli@paypal.com)  - Mark Liu (yliu15@paypal.com)  - David Zhang (pengzhang@paypal.com)  - Xin Zhong (xinzhong@paypal.com)  - Simon Zhang (jzhang13@paypal.com)  - Sharma Nitin (nsharma1@paypal.com)  ## Google Group  Please join [Shifu group](https://groups.google.com/forum/#!forum/shifuml) if questions, bugs or anything else.  ## Copyright and License  Copyright 2012-2017, PayPal Software Foundation under the [Apache License](LICENSE.txt).
CeON/CoAnSys	CoAnSys =======  COntent ANalysis SYStem is a framework for mining scientific publications using Apache Hadoop. It is primarily developed by employees of the Centre for Open Science (CeON) at Interdisciplinary Centre for Mathematical and Computational Modelling (ICM), University of Warsaw (UW).
sharispe/slib	Slib ====  `Slib` is a JAVA library dedicated to semantic data mining based on texts and/or ontology processing. The library is composed of various modules dedicated to specific treatments - they can be used in the context of information retrieval, data analysis, recommendation system design...   * *slib-sml*, the module dedicated to [The Semantic Measures Library](http://www.semantic-measures-library.org) (SML), a library dedicated to semantic measures (similarity/relatedness) computation, evaluation and analysis. See dedicated web site: http://www.semantic-measures-library.org for more information on both semantic measures and this module.  * *slib-graph* a simple in-memory graph engine used to manipulate graphs of URIs - based on on the sesame library (RDF/OWL data loading...). This module provides an easy way to process semantic graphs (e.g. RDF graph) as graphs in which traversal can easily be performed. Numerous algorithms commonly used to process semantic graphs are implemented.                                                                                   * *slib-tools* various command-line tools performing processes on semantic graph/data     * [SML-Toolkit](https://github.com/sharispe/slib/tree/master/slib-tools/slib-tools-sml-toolkit), a command-line tool dedicated to semantic similarity/relatedness computation.     * Ontofocus, a command-line tool which can be used to perform efficient transitive reductions on potentially large taxonomies.      * *slib-utils* SLIB Utility classes * *slib-indexer* Module which provides easy-to-use utility classes for indexing specific datasets. * *slib-example* Source code examples   ## Licence  [Cecill license](http://www.cecill.info/licences.en.html): a free software license adapted to both international and French legal matters, in the spirit of and retaining compatibility with the GNU General Public License (src: Wikipedia).
YahooArchive/samoa	<!--   Copyright (c) 2013 Yahoo! Inc. All Rights Reserved.    Licensed under the Apache License, Version 2.0 (the "License");   you may not use this file except in compliance with the License.   You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software   distributed under the License is distributed on an "AS IS" BASIS,   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   See the License for the specific language governing permissions and   limitations under the License. See accompanying LICENSE file. -->  [![Build Status](https://travis-ci.org/yahoo/samoa.svg?branch=master)](https://travis-ci.org/yahoo/samoa)  SAMOA: Scalable Advanced<br/>Massive Online Analysis. =================  ### This repository is discontinued. The development of SAMOA has moved over to the [Apache Software Foundation](http://samoa.incubator.apache.org). Please subscribe to the [dev mailing list](http://incubator.apache.org/projects/samoa.html) to participate in the development, and use [Jira](https://issues.apache.org/jira/browse/SAMOA) to report bugs and propose new features. The new repository is mirrored on [GitHub](https://github.com/apache/incubator-samoa).  SAMOA is a platform for mining on big data streams. It is a distributed streaming machine learning (ML) framework that contains a  programing abstraction for distributed streaming ML algorithms.  SAMOA enables development of new ML algorithms without dealing with  the complexity of underlying streaming processing engines (SPE, such  as Apache Storm and Apache S4). SAMOA also provides extensibility in integrating new SPEs into the framework. These features allow SAMOA users to develop  distributed streaming ML algorithms once and to execute the algorithms  in multiple SPEs, i.e., code the algorithms once and execute them in multiple SPEs.  ## Build  ###Storm mode  Simply clone the repository and install SAMOA. ```bash git clone git@github.com:yahoo/samoa.git cd samoa mvn -Pstorm package ```  The deployable jar for SAMOA will be in `target/SAMOA-Storm-0.0.1-SNAPSHOT.jar`.  ###S4 mode  If you want to compile SAMOA for S4, you will need to install the S4 dependencies manually as explained in [Executing SAMOA with Apache S4](../../wiki/Executing-SAMOA-with-Apache-S4).  Once the dependencies if needed are installed, you can simply clone the repository and install SAMOA.  ```bash git clone git@github.com:yahoo/samoa.git cd samoa mvn -Ps4 package ```  ###Local mode  If you want to test SAMOA in a local environment, simply clone the repository and install SAMOA.  ```bash git clone git@github.com:yahoo/samoa.git cd samoa mvn package ```  The deployable jar for SAMOA will be in `target/SAMOA-Local-0.0.1-SNAPSHOT.jar`.  ## Slides  [SAMOA Slides](https://speakerdeck.com/gdfm/samoa-a-platform-for-mining-big-data-streams)  G. De Francisci Morales [SAMOA: A Platform for Mining Big Data Streams](http://melmeric.files.wordpress.com/2013/04/samoa-a-platform-for-mining-big-data-streams.pdf) Keynote Talk at [RAMSS ’13](http://www.ramss.ws/2013/program/): 2nd International Workshop on Real-Time Analysis and Mining of Social Streams WWW, Rio De Janeiro, 2013.  ## License  The use and distribution terms for this software are covered by the Apache License, Version 2.0 (http://www.apache.org/licenses/LICENSE-2.0.html).
SpongeBobSun/Prodigal	# Prodigal  Music Player APP looks and feels like a classic device.  Bring back the good old player to life.  <a href="https://play.google.com/store/apps/details?id=bob.sun.prodigal"><img src="artworks/google-play-badge.png" height="60"></a>  ### Screen Record  <img style="float:left" src="artworks/demov1.1.gif" width="300"/>   ### Screenshots  | ![Home](artworks/home_cover.png)         | ![Cover](artworks/coverflow.png)   | ![Home2](artworks/home_np.png)      | ![Now](artworks/now_playing.png)       | | ---------------------------------------- | ---------------------------------- | ----------------------------------- | -------------------------------------- | | Home page                                | Browse Cover                       | Now playing widget                  | Now playing page                       | | ![home_cover2](artworks/home_cover2.png) | ![Cover](artworks/cover_flow2.png) | ![Home2](artworks/now_playing2.png) | ![Now](artworks/theme_default.png)     | | Home page                                | Browse Cover                       | Now playing page                    | Theme 'default'                        | | ![Now](artworks/theme_gray.png)          | ![Now](artworks/theme_ruby.png)    | ![Now](artworks/theme_sunset.png)   | ![Now](artworks/theme_transparent.png) | | Theme 'grey'                             | Theme 'ruby'                       | Theme 'sunset'                      | Theme 'transparent'                    |    ### Customizing Themes  See this [link](https://github.com/SpongeBobSun/Prodigal/blob/master/Themes.md) for customizing themes.    ### Building  To build Prodigal for Android you need import the project in AndroidStudio.  Before you hit the run or debug button, you may need to run the `zipThemes` gradle task, which will zip the theme folder and copy it to `assets` folder.  After themes are zipped as assets, you can switch to the `app` configuration and hit the run button.   ### Credits  Icons in this app are brought you by [Icons8.com](https://icons8.com)  [Picasso](https://github.com/square/picasso)  [SwipeStack](https://github.com/flschweiger/SwipeStack)  [Grant](https://github.com/anthonycr/Grant)  [NumberProgressBar](https://github.com/daimajia/NumberProgressBar)  [Android-Coverflow](https://github.com/crosswall/Android-Coverflow)  [Material-Intro](https://github.com/HeinrichReimer/material-intro)
TheAndroidMaster/Pasta-Music	# Pasta-Music A [material design](https://material.google.com/) music player for Android using google's [ExoPlayer](https://github.com/google/ExoPlayer) library, based off [Pasta for Spotify](https://github.com/TheAndroidMaster/Pasta-for-Spotify).  ## About  Pasta Music is a material design music player for android that attempts to create a better user experience than the standard music players which can have too many features or be generally confusing to users. It was created to show an improvement in design and to allow older and slower devices to have quicker access to local music files. Some examples of this are as follows: ### Design: - Touch areas are increased for small devices to be able to open things like menus and playlists more easily than the layout in the official app - A lot of access relies on swipe navigation within the app to speed up general user experience - The shuffle and repeat buttons have been removed from the now playing screen and moved into an "order tracks" dialog that is accessible from the menu of playlists and albums, and shows up in the settings menu. This provides greater consistency: in most music players you can order tracks by name, date, etc when viewing a list, but this all becomes obsolete once shuffle is enabled. Moving all order-related options to the same place makes more sense from the perspective of both new and existing users. - The backgrounds of different elements on the screen change color according to the album arts, allowing the user to quickly identify items based on color as well as text and image, and blend the image with the rest of the app so that it doesn't seem out of place. - Most parts of the app can be customized from the settings menu to allow the user to change their experience, including the main color scheme of the app and whether to display items as cards, tiles, or lists.  ### Performance: - The app uses Aidan Follestad's [Async](https://github.com/afollestad/async) library to load content separately from the UI thread, which allows the user to navigate the app while content is loading, for example: navigating back to the previous task while content is loading will cancel the download. - [Butterknife](http://jakewharton.github.io/butterknife/), by Jake Wharton, is used to bind views instead of the standard view binding method. Truthfully I have no idea what this means but it saves time so just go with it. ;) - [Glide](https://github.com/bumptech/glide) is used to load image urls provided by the spotify api. This saves a lot of loading time by asynchronously loading an image while scrolling as well as compressing it to speed up the download as much as possible.  ## Features - Shows recently added music and featured playlists - A favorites section for playlists, albums, songs, and artists - Search through all of Spotify's database - View different categories of music - Dynamic backgrounds that adapt to the album art - A light/dark theme in the settings menu - Options to change the global color scheme of the app - Change the ordering of songs in playlists and albums  ### Screenshots  Home Screen | Now Playing ----------- | ----------- ![](http://theandroidmaster.github.io/images/screenshots/image4155.png) | ![](http://theandroidmaster.github.io/images/screenshots/image4646.png)  ## Contributing ### Issues Okay, there aren't really any guidelines over issue formatting provided that you don't create duplicate issues and test the app throughly before creating an issue (ex: try clearing the app data).  ### Pull Requests I usually don't have any organization over how I handle issues and what I commit at any given time. If I'm interrupted in the middle of a session, I might commit a half-finished class that causes an error before the project even compiles. To prevent good work going to waste or having to be copied and pasted a lot to prevent merge conflicts, please contact me before you start working on any changes. This way we can decide who will work on the project when, and exactly what changes they will be making.  ## Links  - [Google Plus Community](https://plus.google.com/communities/101536497390778012419) - [Website](http://theandroidmaster.github.io/apps/pasta/)  #### Contributors: - [James Fenn](http://theandroidmaster.github.io/) - [Jan-Lk Else](https://plus.google.com/+JanLkElse) - [Patrick J](http://pddstudio.com/) - [Vukašin Anđelković](https://plus.google.com/+Vuka%C5%A1inAn%C4%91elkovi%C4%87zavukodlak) - [Kevin Aguilar](https://plus.google.com/+KevinAguilarC)  ## License  ``` Copyright 2016 James Fenn  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```
OpenSilk/Orpheus	## Orpheus: offspring of Apollo, music player from hell  There's kinda a lot going on here. The main app is in app3, with libraries in core-*, common-*, library-*, plugin-*, there are some other 'dead' modules here as well that will be removed someday.  The primary focus for Orpheus is to build a nice core app that can be extended through plugins. External libraries are supported. Pluggable renderers are supported but must be bundled with the app, external renderers are still in the works.  ### Building  OpenSilk projects are managed by repo      mkdir OpenSilk     repo init -u https://github.com/OpenSilk/repo-manifest     repo sync     cd Orpheus     ./gradlew app3:assembleDebug  ### Contributing  OpenSilk projects are managed with gerrit at `review.opensilk.org`      cd Orpheus     repo start my-branch .     #make changes     git commit -a -m 'stuffs'     repo upload .  ### License  GPLv3 `http://www.gnu.org/licenses/gpl.txt`  Portions under Apache2 where specified
Mavamaarten/vk_music_android	#vk Music An android music player for vk.com, inspired by the Google Play Music app and Material Design.  Vk Music has the following features: - Your tracks - Search - Playlists - Recommendations (radio) - Popular tracks - Notification with playback controls - Fullscreen lockscreen background - Downloading  ![screenshots](screenshots.png)  ##Used libraries  - [Dagger2](https://github.com/google/dagger) for dependency injection - [Icepick](https://github.com/frankiesardo/icepick) for saving and restoring instance state - [RxJava](https://github.com/ReactiveX/RxJava) & [RxAndroid](https://github.com/ReactiveX/RxAndroid) - [Paper](https://github.com/pilgr/Paper) - [SlidingUpPanel](https://github.com/umano/AndroidSlidingUpPanel) - [Glide](https://github.com/bumptech/glide) - [MaterialDrawer](https://github.com/mikepenz/MaterialDrawer) - [Dexter](https://github.com/Karumi/Dexter) - [Crashlytics](https://www.crashlytics.com)  It also uses - Android Databinding - (some) Vector Drawables  ##License GNU General Public License v3.0 ``` Copyright (c) 2016 Maarten Van Giel / iCapps  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details. ```
PangHaHa12138/MusicPlayerdemo	# MusicPlayerdemo a MusicPlayer for Android # 低仿网易云音乐，可扫描本地音乐播放 # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/p1.png) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/p2.png) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/p3.png) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/p4.png) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/p5.png) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/p6.png) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/1.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/2.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/3.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/4.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/5.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/6.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/7.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/8.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/9.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/10.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/11.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/12.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/tantan.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/luo.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/ofo.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/mobai.gif) # ![image](https://github.com/PangHaHa12138/MusicPlayerdemo/blob/master/Screenshot/yu.gif)
Freelander/ting	## Ting  Ting is a material design music social media player app, have music playback, download function.  Ting Official website: http://tinger.herokuapp.com/  At present, In the app only access to Ting official website users to share the first 20 songs  info and songs comment info.  #### [中文文档](https://github.com/Freelander/ting/blob/master/README_ZH.md)  ## ScreenShots ![image](https://github.com/Freelander/ting/blob/master/screenshots/1.png) ![image](https://github.com/Freelander/ting/blob/master/screenshots/2.png) ![image](https://github.com/Freelander/ting/blob/master/screenshots/3.png) ![image](https://github.com/Freelander/ting/blob/master/screenshots/4.png) ![image](https://github.com/Freelander/ting/blob/master/screenshots/5.png) ![image](https://github.com/Freelander/ting/blob/master/screenshots/6.png) ![image](https://github.com/Freelander/ting/blob/master/screenshots/7.png) ![image](https://github.com/Freelander/ting/blob/master/screenshots/8.png)  ##### [DownLoad](http://fir.im/cd7b)  ## Thanks  **Open source library**   Project | Introduction ---- | ---- [Picasso](https://github.com/square/picasso) | A powerful image downloading and caching library for Android [AndroidSlidingUpPanel](https://github.com/umano/AndroidSlidingUpPanel) | This library provides a simple way to add a draggable sliding up panel to your Android application [material-ripple](https://github.com/balysv/material-ripple) | Android L Ripple effect wrapper for Views [numberprogressbar](https://github.com/daimajia/NumberProgressBar) | A beautiful, slim Android ProgressBar [Volley](https://github.com/mcxiaoke/android-volley) | Google network frameworks [Gson](https://github.com/google/gson) | A Java library that can be used to convert Java Objects into their JSON representation [ButterKnife](https://github.com/JakeWharton/butterknife) | Bind Android views and callbacks to fields and methods [SwipeBackLayout](https://github.com/ikew0ng/SwipeBackLayout) | An Android library that help you to build app with swipe back gesture  ## License  ``` Copyright 2015~2016 Freelander of copyright owner  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```
frankred/jSona	![jSona screenshot](https://dl.dropboxusercontent.com/u/3669658/github/jSona/logo.png "jSona logo")  jSona is a configuration file(JSON), [vlcj](https://github.com/caprica/vlcj) and [JavaFx](http://www.oracle.com/technetwork/java/javafx/overview/index.html) based music and media player. The aim of jSona is to always keep your playlists in synch with your music folders. For fast fulltext search jSona uses [Apache Lucene](http://lucene.apache.org/core/). The follwing features are fully supported:    - Supports all common media formats that VLC [supports](https://wiki.videolan.org/VLC_Features_Formats/)   - Load artist information and images via [last.fm](http://www.lastfm.de/api) and [MusicBrainz](http://musicbrainz.org/)   - Include your music folders   - Create multiple playlists   - Fulltext search  ##Screenhot ![jSona screenshot](https://dl.dropboxusercontent.com/u/3669658/github/jSona/jsona_ui_1.0.5_java7u8.png "Hey dude...")  ##New features You want **new features**? On the following page you can vote for and submit new feature requests. [http://jsona.idea.informer.com](http://jsona.idea.informer.com)  ##Download jSona has a dependency to VLC3, which only can be found here: [http://nightlies.videolan.org/build/](http://nightlies.videolan.org/). * [Current version](https://dl.dropboxusercontent.com/u/3669658/github/jSona/binary/jSona-1.0.5.jar) * [Version 1.0.5](https://dl.dropboxusercontent.com/u/3669658/github/jSona/binary/jSona-1.0.5.jar) * [Version 1.0.4](https://dl.dropboxusercontent.com/u/3669658/github/jSona/binary/jSona-1.0.4.jar) * [Version 1.0.3](https://dl.dropboxusercontent.com/u/3669658/github/jSona/binary/jSona-1.0.3.zip) * [Version 1.0.2](https://dl.dropboxusercontent.com/u/3669658/github/jSona/binary/jSona-1.0.2.zip) * [Version 1.0.1](https://dl.dropboxusercontent.com/u/3669658/github/jsona/binary/jSona-1.0.1.zip) * [Version 1.0.0](https://dl.dropboxusercontent.com/u/3669658/github/jSona/binary/jSona-1.0.0.zip)  ###Install VLC3 on Ubuntu ```sudo add-apt-repository ppa:videolan/master-daily```  ```sudo apt-get update```  ```sudo apt-get install vlc```  ##Developpement Information If you want to develop under a linux system you need to [recompile the OpenJDK](http://stackoverflow.com/questions/18547362/javafx-and-openjdk) beacause JavaFX is not included, or you have to install Oracles JDK where JavaFX is included. To enjoy a good JavaFX support please use [Java SDK 8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) within your IDE.  ##Configuration file / config.json  Here is an example of the default configuration file. You have to setup your **VLC path** correctly. If you use Java 32-bit/64-bit you also have to use VLC-32-bit/64-bit. ```json {   "ALLOW_JSONA_TO_OVERWRITE_ME": true,   "MAX_SEARCH_RESULT_AMOUNT": 512,   "VOLUME": 100,   "FOLDERS": [     "D:/media/music",     "C:/share",     "C:/downloads/music",     "\\nas\share\music"   ],   "INCLUDE_EXTENSIONS": [     ".mp3",     ".wav",     ".wma",     ".flc",     ".aac",   ],   "PLAYBACK_MODE": "NORMAL",   "RECENTLY_ADDED_UNITL_TIME_IN_DAYS": -7,   "THEME": "grey",   "KEY_SKIP_TIME": 10,   "WINDOW_OS_DECORATION": true,   "TITLE": "jSona - open source project by Frank Roth",   "MIN_HEIGHT": 600,   "MIN_WIDTH": 720,   "COLORIZE_ITEMS": true,   "SCANNER_AND_TAGGER_LOGGING_GRANULARITY": 128 } ```  And here an explanation of all possible attributes:  #####ALLOW_JSONA_TO_OVERWRITE_ME If you set this value to true then jsona will overwrite your config.json with the current jSona config. For example if you change the volume in the player then the **VOLUME** value gets overwritten. There only a few configuration attributes that can be change during runtime (e.g.: PLAYBACK_MODE).    #####MAX_SEARCH_RESULT_AMOUNT Maximum search results of the lucene engine (smaller is faster)  #####VOLUME Default startup volume. Will be overwritten by jSona (always save the recently changed volume).  #####FOLDERS Your music folders. Care of JSON-Syntax and correct backslashes (/). jSona also supports the usage of the Uniform Naming Convention and folders in a network (e.g.: "\\\\servername\folder\path"). See example above...   #####INCLUDE_EXTENSIONS This array says what kind of files you want to add to jSona. If it is empty or this field is deleted then every file with every extension will be included to your playlists.  #####PLAYBACK_MODE Playbackmode of jSona. Choose one of them: {NORMAL, SHUFFLE}. Will be overwritten by jSona (always save the recently changed playback mode).  #####RECENTLY_ADDED_UNITL_TIME_IN_DAYS How long do you want to show new songs in the "New" tab. If you choose -7 then new songs will be displayed for one week in the "New" tab. This number should always be negative.  #####THEME Currently there is only one theme available: {"grey"}.  #####KEY_SKIP_TIME If you change the duration slider with the help of the arrow keys(left or right) or hotkeys the slider rewind or skips 10 seconds.  #####WINDOW_OS_DECORATION If you set the value on **false** the normal OS window decoration will be used. If you set the value on **true** it has no os window decoration. It will look a little bit more beautiful!  ![jSona screenshot - undecorated: false](https://dl.dropboxusercontent.com/u/3669658/github/jSona/jsona_undecorated_2.png "undecorated: false")  #####TITLE Window title of jSona. Will only be displayed if the **WINDOW_UNDECORATED** property is set to **false**.  #####HEIGHT Height of the window in pixels.  #####WIDTH Width of the window in pixels.  #####MIN_HEIGHT Minimum window height.  #####MIN_WIDTH Minimum window width.  #####COLORIZE_ITEMS If the value is set to **true** the same music items will be displayed with a different smooth background color. If the value is set to **false** then the default JavaFX list background will be used (see screenshot above).  ![jSona screenshot](https://dl.dropboxusercontent.com/u/3669658/github/jSona/jsona_colorized_items.png "You Got Rick Rolled!")  #####FILEPATH_BASED_MUSIC_INFORMATIONS There is the possibility to define rules to detect music information with the help of the file path. Currently there are two kind of rules {"ROOT_SUBFOLDER_LEVEL_RULE", "FILENAME_RULE"}. The "ROOT_SUBFOLDER_LEVEL_RULE" is a rule based on the subfolder level according to the root directory. With the help of the "FILENAME_RULE" you can match everything according to the filename (not file path) of the file. It is possible to ignore file endings and to replace underscores with a space.  The follwing examples should help you with to use these rules.  If you have a folder structure like this and this is your music file name: ``` C:\media\music\Rock\ACDC\Highway to Hell\03 - Walk All Over You.mp3 ```  Your root folder is ``` C:\media\music ```  With the follwing rules you can match the genre (%GENRE%), the artist (%ARTIST%), the title (%TITLE%), the album (%ALBUM%) and the track number (%TRACK_NO%): ```json {  ...  "FILEPATH_BASED_MUSIC_INFORMATIONS": [     {       "rule": "ROOT_SUBFOLDER_LEVEL_RULE",       "params": {         "PATTERN": "%GENRE%",         "REPLACE_UNDERSCORES_WITH_SPACES": false,         "FOLDER_LEVEL": 1       }     },     {       "rule": "ROOT_SUBFOLDER_LEVEL_RULE",       "params": {         "PATTERN": "%ARTIST%",         "REPLACE_UNDERSCORES_WITH_SPACES": false,         "FOLDER_LEVEL": 2       }     },      {       "rule": "ROOT_SUBFOLDER_LEVEL_RULE",       "params": {         "PATTERN": "%ALBUM%",         "REPLACE_UNDERSCORES_WITH_SPACES": false,         "FOLDER_LEVEL": 3       }     },         {       "rule": "FILENAME_RULE",       "params": {         "PATTERN": "%TRACK_NO% - %TITLE%",         "IGNORE_FILE_ENDING": true,         "REPLACE_UNDERSCORES_WITH_SPACES": true       }     }   ] } ``` Every matching **%VARIABLE%** will be trimmed at the ending, so it does not mather if you choose **%TRACK_NO% - %TITLE%** or **%TRACK_NO%-%TITLE%** as a pattern. It is also possible to ignore areas in the path by producing non declared Variables like: %TMP%, %I_DONT_NEED_THAT%, %IGNORE%... you can create anything...  #####SCANNER_AND_TAGGER_LOGGING_GRANULARITY Logging every file in the scanner and tagging process can be very time expensive. Because of that you can define the granularity of the scanner and tagging logging. If the value is set to 1 every file is logged (time expensive). If the value is set to 128 only every 128th and the last file will be logged. This value can be every number > 0.  #####HOTKEYS Here is a list of all modifiers and keys that can be used: https://github.com/frankred/jSona/wiki/Key-Codes. Currently only global hotkeys work. Supported application events are: {PLAYER_VOLUME_UP, PLAYER_VOLUME_DOWN, PLAYER_PLAY_PAUSE, VIEW_HIDE_SHOW, PLAYER_NEXT, PLAYER_PREVIOUS, PLAYER_TIME_UP, PLAYER_TIME_DOWN}  ```json {  ...  "HOTKEYS": [     {       "key": 107,       "event": "PLAYER_VOLUME_UP",       "global": true     },     {       "key": 109,       "event": "PLAYER_VOLUME_DOWN",       "global": true     },     {       "key": 19,       "event": "PLAYER_PLAY_PAUSE",       "global": true     },     {       "key": 49,       "modifiers": [         128       ],       "event": "VIEW_HIDE_SHOW",       "global": true     }   ], } ```  ##Changelog  ###1.0.5 * Bugfixing {ListView, Layout} * Repeat mode added (THX [Naios](https://github.com/Naios)) * File extension whitelist mechanism added (THX [Naios](https://github.com/Naios))  ###1.0.4 * Bugfixing {ListView, Title Information changed, ListView layout, fileCreated, fileModified} * Equalizer implemented (for testing only, unstable, unsavable)  ###1.0.3 * Loading animation for each folder added in frontend.  ###1.0.2 * HOTKEYS can now be defined in the config.json. * VOLUME_UP_DOWN_AMOUNT amount for hotkeys can be defined in the config.json.  ###1.0.1 * Music information like artist or title can now be detected from the filepath with the help of detector rules in the config.json. * Logging granularity of file scanner and tagger can now be defined in the config.json.  ##Installation and Start Download the current zip file and extract it. Then put in your correct VLC path into the config.json file and start jSona with the following command. ``` java -jar jSona-1.0.4.jar ``` jSona uses JavaFX so a current Java virtual machine with JavaFX support should be installed.  If you want to run jSona without getting showed the console (only works on Windows) use the following command: ``` start javaw -jar jSona-1.0.4.jar ```  ##Help / FAQ Here is a screenshot of jSona that explains the easy to use user interface. ![jSona explaining the UI](https://dl.dropboxusercontent.com/u/3669658/github/jSona/jsona_explaining_the_ui.png)  ##Thank you very much! This project is based on a set of amazing projects. Thank you to all programmers! * [VLCJ](https://github.com/caprica/vlcj) * [Undecorator](https://github.com/in-sideFX/Undecorator) * [java-string-similarity](https://github.com/rrice/java-string-similarity) * [last.fm-java](https://code.google.com/p/lastfm-java/) * [Apache Lucene](http://lucene.apache.org/core/) * [JavaFX](http://www.oracle.com/technetwork/java/javafx/overview/index.html) * [Yootheme](http://www.yootheme.com/icons)  ##License MIT - **Free Software, Hell Yeah!**
rohanoid5/Muzesto	# Muzesto Muzesto is an open source android music player. Muzesto emphasizes on the visualization factor to make the whole audio listening experience immersive and unique. The Floating Play/Pause widget works as a companion to the app.   <a href="https://play.google.com/store/apps/details?id=rohanoid5.twar"><img alt="Get it on Google Play" src="https://play.google.com/intl/en_us/badges/images/generic/en-play-badge.png" height=50px/></a>  # Screenshots ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_1487691478.png) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-22-18-16-29-968.jpg) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-22-18-17-02-709.jpg) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-22-18-17-52-647.jpg) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-22-18-18-23-832.jpg) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-22-18-19-07-785.jpg) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-22-18-20-41-357.jpg) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-22-18-20-48-627.jpg) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-22-18-20-54-947.jpg) ![](https://raw.githubusercontent.com/rohanoid5/Muzesto/master/Screenshot/Screenshot_2016-06-23-00-36-41-263.jpg)  # Contact Me rohanoid5@gmail.com  ## Credits * [TimelyTextView](https://github.com/adnan-SM/TimelyTextView) * [MultiViewPager](https://github.com/Pixplicity/MultiViewPager) * [PlayPauseButton](https://github.com/recruit-lifestyle/PlayPauseButton) * [CircularSeekBar](https://github.com/devadvance/circularseekbar) * [Nammu](https://github.com/tajchert/Nammu)  ## License  >(c) 2016 Rohan Dey   >This is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.   >This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.   >You should have received a copy of the GNU General Public License along with this app. If not, see <https://www.gnu.org/licenses/>.
spheras/messic	[![Build Status](https://travis-ci.org/spheras/messic.svg?branch=master)](https://travis-ci.org/spheras/messic)      Messic       Copyright (C) 2013      This program is free software: you can redistribute it and/or modify     it under the terms of the GNU General Public License as published by     the Free Software Foundation, either version 3 of the License, or     (at your option) any later version.      This program is distributed in the hope that it will be useful,     but WITHOUT ANY WARRANTY; without even the implied warranty of     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     GNU General Public License for more details.      You should have received a copy of the GNU General Public License     along with this program.  If not, see <http://www.gnu.org/licenses/>.  MESSIC ======  Messic comes from the union of two words:   - **Mess**: _A dirty or untidy state of things or of a place_   - **Music**: _Vocal or instrumental sounds (or both) combined in such a way as to produce beauty of form, harmony, and expression of emotion_  The aim of this project is to convert this mess of music into an ordered catalog of music and play it in a confortable way. The main characteristics will be:  * It sorts your library of music as automatically as possible. * It gives you absolute control of your music. You can always go to the file system and do whatever you want with your music. * It plays your music in a very very confortable and beautiful interface. * It is executed via web, so you could run this service at home for every device. * It is multiuser. * It is focused on managing your home catalog of music. We want to manage and listen to our own catalog of music.  Please, visit us at our web page to get more information: http://spheras.github.io/messic  or you can visit our wiki also: https://github.com/spheras/messic/wiki   See you!
marverenic/Jockey	# Jockey Jockey is a music player for Android based on Google's Material Design standards. The point of Jockey is to be a simple, lightweight media player with features designed for music enthusiasts and casual listeners alike. Currently Jockey is notably lacking a few enthusiast features like crossfade and replay gain which may be added in the future.  [![Get it on Google Play](https://github.com/marverenic/Jockey/raw/master/screenshots/play-badge.png)](https://play.google.com/store/apps/details?id=com.marverenic.music)     <img align="left" height="480px" src="https://github.com/marverenic/Jockey/raw/master/screenshots/Library5_framed.png"> <img align="left" height="480px" src="https://github.com/marverenic/Jockey/raw/master/screenshots/NowPlaying5_framed.png"> <img align="left" height="480px" src="https://github.com/marverenic/Jockey/raw/master/screenshots/Artist5_framed.png"> <img height="320px" src="https://github.com/marverenic/Jockey/raw/master/screenshots/Albums9_framed.png"> <img height="320px" src="https://github.com/marverenic/Jockey/raw/master/screenshots/NowPlaying9_framed.png">  ### Downloads You can get Jockey on [Google Play](https://play.google.com/store/apps/details?id=com.marverenic.music), and [opt-in to Beta testing](https://play.google.com/apps/testing/com.marverenic.music) if you want to try new features before they're released.  ### Permissions #### Android 6.0 and higher **Storage**   Jockey needs permission to Storage so that it can scan for music and play songs. Without this permission, Jockey can't work and will just kind of stare at you passive-aggressively until you grant this permission #### Android 5.1 and lower **Read and write to external storage**   Used to save local data; primarily used for Last.fm cache, library storage, and other small miscellaneous files.   **Internet**   Used to retrieve information and thumbnails for artists from Last.fm and upload anonymous usage and crash data with Crashlytics   **Network State**   Used to prevent Jockey from using mobile data (if this preference is enabled)   **Keep awake**   Used to play music while the device's screen is off  ### Building Jockey from Source To build a release APK of Jockey, you'll need to either setup Crashlytics using your own API key, or remove the dependency and all logging calls. You can specify your own API key by making a new file in `app/fabric.properties` and add the following lines:   ``` apiSecret="yourApiSecret" apiKey="yourApiKey" ```  If you want to remove the crashlytics dependency instead, simply delete the `com.marverenic.music.utils.CrashlyticsTree` class, and remove references to `CrashlyticsTree` from `com.marverenic.music.JockeyApplication#setupTimber()`.  ### Bugs & contributing Feel free to post suggestions, bugs, comments on code in the project, or just anything that isn't as smooth as it should be as an issue on Github. You don't need to submit crashes unless you're running a version of Jockey you've built yourself – crashes are automatically reported through Crashlytics. If you're feeling adventerous, you're more than welcome to fork Jockey and submit a pull request either to implement a new feature, fix a bug, or clean up the code.  #### Submitting Feature Requests Feature requests should be submitted through the Github issue tracker. Before submitting a feature request, make sure that it hasn't been requested before. If it's already been requested, but is in a closed issue that hasn't been marked as "wontfix", feel free to resubmit it in case it's gotten lost.  When submitting a feature request, please make a single issue for each feature request (i.e. don't submit an issue that contains a list of features). Such issues are hard to keep track of and often get lost.  #### Pivotal Tracker Jockey has a [Pivotal Tracker page](https://www.pivotaltracker.com/n/projects/1594253) which contains the most up-to-date information about planned work. This dashboard is public, but is not open for modification. The best way to add new stories to the dashboard is through Github issues.  Jockey's Pivotal Tracker page includes stories for upcomming features, chores for developer-oriented tasks, bugs, and release markers. Stories are assigned point values using the fibonacci scale to determine roughly how long it will take to implement (1 point is a couple of hours, 2 points is several hours, 3 points is roughly a full day or part of a weekend, 5 points is most of a week, and 8 points is more than a week). These point values are only estimates and shouldn't be treated as guarantees of how long it will take to implement a feature. The time associated with a story's point value can also vary greatly depending on how much time I can devote to the project during the week.  Similarly, release markers, release dates, and prioritizations are very flexible and can change at any time.  ### License Jockey is licensed under an Apache 2.0 license
SusionSuc/Boring	# 随 心 >刚开始写这个项目的时候是为了自己的毕业设计。 >后来慢慢的就想借这个项目来巩固自己的Android编程技能，也可以更好的站在全局来考虑项目的开发，而不只是需求的迭代开发。  ## APP细节  ### API - APP的API     - 音乐部分，是在网络上收集的网易云音乐的API。用了挺散的， 比如 ： https://github.com/javaSwing/MusicAPI     - 知乎日报， https://github.com/izzyleung/ZhihuDailyPurify/wiki/%E7%9F%A5%E4%B9%8E%E6%97%A5%E6%8A%A5-API-%E5%88%86%E6%9E%90     - 段子、图片， API市场 - 对于第三方开源库的使用     - 基础库： RxJava, Retrofit, fresco, eventbus     - 侧滑退出 - 项目编码     - 对于复杂的逻辑， 采用的MVP编写。     - 由于本人是处女座，代码阅读起来应该还是比较容易的。 - github     - https://github.com/SusionSuc/Boring     - 当然还是希望可以给一个star的... 谢谢  ### 模块设计架构 - 音乐播放模块的大体架构     - MusicServie负责维护音乐播放         - 管理 MediaPlayer         - 管理 播放队列     - Client 与 MusicService的通讯         - MusicService 会开启一个广播接收者，根据相应的广播Action，处理相应的事件         - 抽取音乐播放Action类， 即发送特定的Action来控制音乐播放         - Client 通过广播接收者，来更新音乐播放相关UI : 进度、播放状态等         - Client 通过Action类，向Service的广播接收者发送特定的Action，来实现音乐的控制。 - 阅读模块的大体架构     - 知乎阅读         - StickHeader的实现，         - 对于知乎文章的展示， 利用RxJava请求文章内容，文章的CSS样式， 然后拼接 Html内容，进行展示     - 段子         - 普通的RecyclerView列表     - 图片         - 通过分类window来切换图片请求的URI         - 图片的查看，简单的实现缩放退出 - 整个APP的收藏模块     - 利用第三方关系型数据库， 泛型， 实现了简单的对象存储     - 最简单的收藏就是： 收藏：把对象存入数据库， 删除收藏： 把对象从数据库中删除  - APP 中RecylerView的使用     - 通过对Adapter和ViewHolder的抽取     - 整个具体UI的展现， 可以说是面向 ItemHandler 的编程。  - 接下来事情不是很多，对整个APP还会慢慢优化。   ## APP截图  <img src="./screenshot/mainpage1.jpg" width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/mainpage2.jpg " width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/musicdetail.jpg" width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/playlist.jpg" width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/readingpage.jpg" width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/essaydetail.jpg" width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/drawerpage.jpg" width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/joke.jpg" width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/imagepage.jpg" width = "210" height = "375" alt="图片名称" align=center /> <img src="./screenshot/changepage.jpg" width = "210" height = "375" alt="图片名称" align=center />
fastbootmobile/encore	# Encore Music  ![Logo](http://www.encoremusic.io/wp-content/uploads/2015/07/ic_web_cropped_logo.png)  [Forum](http://forum.xda-developers.com/apps/encore-music/encore-music-to-experience-music-t3166515)  [Google Play](https://play.google.com/store/apps/details?id=com.fastbootmobile.encore.app)  # Overview  Encore is an open-source music app for Android allowing you to listen to music from multiple plug-in sources, make cross-platform playlists, create automated mixes, recognize music, and much more. It also supports casting to Chromecast, and contains an Android TV UI.  ## Making plugins  If you are interested in making plug-ins for Encore, you should check out the provider libary SDK at https://github.com/fastbootmobile/encore-providerlib.  ## License  Encore Music is licensed under GPL. Check LICENSE.md file for more informations about the main app and the plug-ins licenses.
C-Aniruddh/ACEMusicPlayer	ACEMusic Player =================== ACEMusic is a free, powerful and elegant music player for Android. ACEMusic is based off <a href="https://github.com/psaravan/JamsMusicPlayer">JamsMusicPlayer</a>.   Get the Music player ===================== <center><a href="https://play.google.com/store/apps/details?id=com.aniruddhc.acemusic.player&hl=en"><img src="http://www.sololearn.com/files/605/Get-it-on-Google-Play.png"></a></center> <center>OR</center> <center>Get it from the <a href="http://forum.xda-developers.com/android/apps-games/complete-material-design-acemusic-t2904994">xda-thread(free)</a></center> Screenshots ============ <img src="http://i.imgur.com/rL2BepI.jpg" width=761 height=480> <img src="http://i.imgur.com/tdLiCVY.jpg" width=761 height=480>  Features ========= * Unofficial Google Play Music support. [Work under process] * ID3v3 tag editing. * Album artist sorting/tag support. * Blacklist ability for artists, album artists, albums, songs, genres and playlists. * 9 band equalizer with bass boost, and reverb. * Individual EQ settings for each artist, album artist, album, song, genre, or playlist. * File/folder browsing. * 2 different base themes and 9 different color schemes.  Forked Projects ================ Forking of projects is allowed. Please add the required notice in the application, when forking the project.  Thanks to Saravan Pantham. I will add the projects here too.  Developed By ============ Saravan Pantham - saravan.pantham@gmail.com Aniruddh Chandratre - c.aniruddh98@gmail.com  Libraries Used =========================== * CircularImageView - https://github.com/lopspower/CircularImageView * DragSortListView - https://github.com/bauerca/drag-sort-listview * LicensesDialogLibrary - https://github.com/Wicowyn/LicensesDialogLibrary * Picasso - https://github.com/square/picasso * QuickScroll - https://github.com/andraskindler/quickscroll * VelocityViewPager - https://github.com/Benjamin-Dobell/VelocityViewPager * ViewPagerIndicatorLibrary - https://github.com/JakeWharton/Android-ViewPagerIndicator * Android Asynchronous HTTP Client - http://loopj.com/android-async-http/ * Android BitmapCache - https://github.com/chrisbanes/Android-BitmapCache * ListViewAnimations - https://github.com/nhaarman/ListViewAnimations * Apache Commons IO - http://commons.apache.org/proper/commons-io/ * Apache Commons Lang - http://commons.apache.org/proper/commons-lang/ * DashClock API - https://code.google.com/p/dashclock/ * Google HTTP Client - https://code.google.com/p/google-http-java-client/ * Google HTTP Client (Android) - https://code.google.com/p/google-http-java-client/wiki/Android * JAudioTagger - http://www.jthink.net/jaudiotagger/ * Google Analytics - https://developers.google.com/analytics/devguides/collection/android/resources * NineOldAndroids - https://github.com/JakeWharton/NineOldAndroids/ * Android Support Library - http://developer.android.com/tools/support-library/index.html  License ======== All source code is licensed under Apache License 2.0. If you create your own app (free or commercial) that uses some or all of the code from this codebase, you MUST attribute it to the original author, no exceptions. You can do so by displaying an "About" or "Licenses" screen in your app that links back to this GitHub page AND includes the following notice:       Copyright 2014 - Saravan Pantham      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.   And, as well as this(if basing over ACEMusic, lots of changes have been done) :       Copyright 2014 - Aniruddh Chandratre      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
SubstanceMobile/GEM	#GEM Player (Beta) App Manager: Adrian Vovk (@AnimbusDev)  [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-GEM%20Player-brightgreen.svg?style=flat)](http://android-arsenal.com/details/3/2679) [![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](https://github.com/Substance-Project/GEM/blob/indev/LICENSE.md) [![Stable Version](https://img.shields.io/badge/stable-0.2.3-orange.svg)](https://github.com/Substance-Project/GEM/tree/stable) [![WIP Version](https://img.shields.io/badge/indev-0.3.0-yellow.svg)](https://github.com/Substance-Project/GEM/releases)  GEM Player is a part of the Substance Mobile suite of apps & promises to be nimble, functional, and customizable. It is largely inspired by the music player demoed at Google I/O 2014.  ####Contributing If you want to contribute, go ahead! Just keep in mind that: * Translations are accepted, but soon we will be switching to a different service so this will not apply. * Keep it organised: we would be very grateful of you kept up with the proper syntax (naming variables properly, keeping up with the dev's bracket style, etc.) * Your PR might be rejected, but the change may be applied in a different way. If so, the manager will have to point you to the commit(s) of the change. * Please test your modifications (if you cannot test it for some reason, please just say so in the PR). Also, please mark the last commit you tested your PR with. We will take care of the rest. * If your PR isn't compatible with other PRs, don't worry. The dev will fix it. * These may be updated over time, so please keep watch.   ####Developing In order to develop for GEM Player, you will have to run some commands and do some things. First off, you will have to do this: 1. `cd` into your cloned GEM directory 2. Find your os:   * For linux: `source envsetup.sh`   * For Windows: `envsetup` 3. Follow prompts. This will ask you for a keystore file, a key alias, a key password, and a store password. 4. The script will configure your environment. It will create files where necessary and will edit your PATH (in order for some nice tools to work). **Only accept the request to edit the PATH if this is the first Substance app you are developing for** 5. Complete  ####Screenshots: ![GEM Preview](http://i.imgur.com/d25pxdS.png) ---  ####Links: [Explanation of Substance](https://github.com/Substance-Project/GEM/wiki/Substance-Open-Source), [Substance Site](https://substanceproject.net)
markzhai/LyricHere	Lyric Here [![GitHub release](https://img.shields.io/badge/sample%20apk-2.0.0beta-brightgreen.svg?style=flat)](https://github.com/markzhai/LyricHere/releases/download/v2.0-beta/lyric-here.apk)  ==========  Material design music and lyric player. Using Android's new android.media.MediaMetadata series api to implement. ([For Chinese 中文戳这里](https://github.com/markzhai/LyricHere/blob/master/README_CN.md))    Pre-requisites  --------------  - Android SDK v14    Features  -----------  - Local music browser and player.  - Music player widget, notification widget.  - Lyric directly refresh on notification, see it whenever you want.  - Powerful LyricView which supports scrolling up and down to change offset.  - Receive broadcast from popular music players and pop up lyric open notification.    TODO(Pull request is welcomed)  ------------------------------  - Upgrade LyricView to more powerful.  - Use Google Design Support UI (AppBar, CoordinatorLayout, etc.)  - Better implementation for Music player, show lyric directly.  - Download lyric from server and upload to server.  - Add support lyric file encoding auto-recognize.    Tested  ------  - Nexus 6 (5.1.1)  - OnePlus (4.3)    Screenshots  -----------  ![Browse local music file](art/Screenshot_2015-09-12-23-14-37.jpg "Browse local music file")  ![Fullscreen music player](art/Screenshot_2015-09-12-21-13-22.jpg "Fullscreen music player")  ![Lyric explorer](art/Screenshot_2015-09-12-21-13-40.jpg "Lyric explorer")  ![Lyric player](art/Screenshot_2015-03-20-17-11-09.jpg "Lyric player")  ![Lyric encoding picker](art/Screenshot_2015-03-20-17-11-28.jpg "Lyric encoding picker")  ![Notification](art/Screenshot_2015-09-09-23-12-51.jpg "Notification")  ![Lock Screen Background](art/Screenshot_2015-09-12-22-43-59.jpg "Lock Screen Background")    LIBRARY  -------  - Android Support Library (cardview, appcompat, design, mediarouter)  - [Butter Knife](https://github.com/JakeWharton/butterknife)  - [DBFlow](https://github.com/Raizlabs/DBFlow)  - [Icepick](https://github.com/frankiesardo/icepick)  - [mosby](https://github.com/sockeqwe/mosby)  - [android-ColorPickerPreference](https://github.com/attenzione/android-ColorPickerPreference)  - [LyricView](https://github.com/markzhai/LyricView)
aliumujib/Orin	# Orin Orin is a music player based on Karim Abou Zeid\'s open source Phonograph music player, I always wanted to implement  Aleksandar Tešić\'s designs of a music player that can be found here! https://material.uplabs.com/posts/a-music-player-in-material-design-a-concept, but haven\'t had the time to start one from scratch, so well I used Phonograph as a base , I dont have permissions to use these designs yet so I can\'t release the app on the Playstore  <img src="SCREENDATA/GIF1.gif" width="250" /> <img src="SCREENDATA/SCREENDATA1.png" width="850" /> <img src="SCREENDATA/SCREENDATA2.png" width="850" /> <img src="SCREENDATA/SCREENDATA3.png" width="850" />  #### Demo  See demo by downloading [Demo Apk](https://github.com/aliumujib/Orin/blob/master/apk/app-debug.apk)  #### How to run ```bash # Get the project git clone https://github.com/aliumujib/Orin.git  # Change directory cd Orin/app  # Change fabric key cd replace YOUR_FABRIC_CRASHLYTICS_KEY_HERE with your own crashlytics key  # Open with android studio import with Android Studio and do what you will ```  # License GNU General Public License v3.0  Permissions of this strong copyleft license are conditioned on making available complete source code of licensed works and modifications, which include larger works using a licensed work, under the same license. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights.
zhengken/LyricViewDemo	[![](https://jitpack.io/v/zhengken/LyricViewDemo.svg)](https://jitpack.io/#zhengken/LyricViewDemo) # LyricView LyricView is a powerful and flexible custom view to display lyrics within music player under Android  ## Screenshot ![](/screenshot/lyricview.png) [LyricViewDemo.apk][1]      [YouTube][2] ## Usage ### Gradle dependency     step 1 Add the JitPack repository to your build file     allprojects { 		repositories { 			... 			maven { url 'https://jitpack.io' } 		} 	} 	 	step 2 Add the dependency     dependencies {             compile 'com.github.zhengken:LyricViewDemo:v1.2'     }  ### XML code     //step 1     <me.zhengken.lyricview.LyricView             android:id="@+id/custom_lyric_view"             android:layout_width="match_parent"             android:layout_height="match_parent" /> ### Java code     //step 2     LyricView mLyricView = (LyricView)findViewById(R.id.custom_lyric_view);          //step 3     mLyricView.setLyricFile(lyricFile);          //step 4, update LyricView every interval     mLyricView.setCurrentTimeMillis(progress);          //step 5, implement the interface when user drag lyrics and click the play icon     mLyricView.setOnPlayerClickListener(new LyricView.OnPlayerClickListener() {             @Override             public void onPlayerClicked(long progress, String content) {                              }         }); ### XML attributes |Attributes|Format|Default|Description| |:--|:--|:--|:--| |fadeInFadeOut|boolean|false|Enable lyrics fadeInFadeOut or not| |hint|string|No Lyrics|Display when not exist lyric file| |hintColor|color|`#FFFFFF`|The color of hint text| |textSize|dimension|16sp|The text size of lyrics| |textColor|color|`#8D8D8D`|The color of lyrics| |highlightColor|color|`#FFFFFF`|The color of current lyric that playing| |textAlign|enum|CENTER|The alignment of lyrics| |maxLength|dimension|300dp|Line feed when lyric'width beyond maxLength| |lineSpace|dimension|25dp|Line space| ### Java API |Methods|Description| |:--|:--| |`setOnPlayerClickListener(OnPlayerClickListener listener)`|Callback when click the play icon| |`setAlignment(@Alignment int alignment)`|Set the alignment of the lyrics| |`setCurrentTimeMillis(long current)`|Scroll lyrics to the specify TimeMillis| |`setLyricFile(File file)`|Set the lyric file, and auto set the charset by `juniversalchardet-1.0.3`| |`setLyricFile(File file, String charset)`|Set the lyric file with the specified charset| |`setTypeface(Typeface typeface)`|Set the typeface of lyrics| |`reset()`|Reset the LyricView| ## Thanks [@码农小阿飞][3] ## License     Copyright 2016 zhengken      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.     [1]: https://github.com/zhengken/LyricViewDemo/tree/master/sample   [2]: https://youtu.be/Mjp9I6-0KHs   [3]: http://blog.csdn.net/mario_0824
ps3mediaserver/ps3mediaserver	# PS3 Media Server  by shagrath  WARNING : This project may still work, but is not maintained since 2016.  [![Build Status](http://pms.smoeller.de/buildStatus/icon?job=pms+%28trunk%29)](http://pms.smoeller.de/job/pms%20%28trunk%29/)  - [Links](#links) - [Thanks](#thanks) - [Installation](#installation) - [Building](#building) - [Development](#development) - [License](#license)  PS3 Media Server is a cross-platform DLNA-compliant UPnP Media Server. Originally written to support the PlayStation 3, PS3 Media Server has been expanded to support a range of other media renderers, including smartphones, TVs, music players and more.  ## Links  * [Website](http://www.ps3mediaserver.org/) * [Forum](http://www.ps3mediaserver.org/forum/) * [Downloads](http://sourceforge.net/projects/ps3mediaserver/files/) * [Source code](https://github.com/ps3mediaserver/ps3mediaserver) * [Issue tracker](https://code.google.com/p/ps3mediaserver/issues/list)  ## Thanks  Thanks to:  * Redlum * tcox * SubJunk * taconaut * tomeko * lightglitch * chocolateboy * ditlew * Raptor399 * renszarv * happy.neko  for major code contributions.  Thanks to:  * meskibob * otmanix  for documentation and contributions to the community.  * boblinds and snoots for the network test cases :) * sarraken, bleuecinephile, bd.azerty, fabounnet for the support and feedback * smo for the Jenkins server  See the [CHANGELOG](https://github.com/ps3mediaserver/ps3mediaserver/blob/master/CHANGELOG.txt) for more thanks.  ## Installation  The [download site](http://sourceforge.net/projects/ps3mediaserver/files/) has the latest releases of PS3 Media Server for Windows and Mac OS X as well as tarballs for Linux/Unix and debs for manual installation on Debian/Ubuntu.  For Debian and Ubuntu packages, see [here](http://www.ps3mediaserver.org/forum/viewtopic.php?f=3&t=13046).  For instructions on installing and running PMS from a tarball, see [INSTALL.txt](https://github.com/ps3mediaserver/ps3mediaserver/blob/master/INSTALL.txt).  ## Building  PMS can be built using the following commands:      git clone git://github.com/ps3mediaserver/ps3mediaserver.git     cd ps3mediaserver     mvn com.savage7.maven.plugins:maven-external-dependency-plugin:resolve-external     mvn com.savage7.maven.plugins:maven-external-dependency-plugin:install-external     mvn package  See [BUILD.md](https://github.com/ps3mediaserver/ps3mediaserver/blob/master/BUILD.md) for detailed information on setting up a PMS build environment.  ## Development  If you plan to commit source code, be sure to configure git to deal properly with cross platform line endings.  On Mac OS X and Linux:      git config --global core.autocrlf input  On Windows:      git config --global core.autocrlf true  For more information, see http://help.github.com/line-endings/  See [DEVELOP.md](https://github.com/ps3mediaserver/ps3mediaserver/blob/master/DEVELOP.md) for detailed information on setting up a PMS development environment.  ## License  Copyright 2009-2013 shagrath.  PS3 Media Server is free software: you can redistribute it and/or modify it under the terms of the [GNU General Public License](https://github.com/ps3mediaserver/ps3mediaserver/blob/master/LICENSE.txt) as published by the Free Software Foundation, either version 2 of the License, or (at your option) any later version.
tomahawk-player/tomahawk-android	## tomahawk-android  *Music is everywhere, now you don’t have to be!*  Tomahawk, the critically acclaimed multi-source music player, is now available on Android. Given the name of an artist, album or song Tomahawk will find the best available source and play it - whether that be from Spotify, Deezer, GMusic, Soundcloud, Tidal, Official.fm, Jamendo, Beets, Ampache, Subsonic or your phone’s local storage. Tomahawk for Android also syncs your history, your loved tracks, artists, albums and your playlists to/from the desktop version of Tomahawk via our new music community, Hatchet. On Hatchet you can hear your friends' favorite tracks and see what they're currently listening to.  ![Tomahawk Screenshot1](/screenshots/screenshot1.png) | ![Tomahawk Screenshot2](/screenshots/screenshot2.png) | ![Tomahawk Screenshot3](/screenshots/screenshot3.png) ------ | -----  | -----  ## Beta and Nightly  Get the Beta version on Google Play: https://play.google.com/store/apps/details?id=org.tomahawk.tomahawk_android  Nightly builds are available here: http://download.tomahawk-player.org/nightly/android/?C=M;O=D  ## Development Setup  First of all you have to properly setup your Android SDK/NDK:  - Download and install the Android SDK http://developer.android.com/sdk/index.html     - Make sure you have updated and installed the following in your Android SDK Manager:         - "/Tools"         - the latest Android SDK Platform folder (e.g. "/Android 6.0 (API 23)")         - "/Extras/Android Support Repository" and "/Extras/Android Support Library"             - "/Extras/Google Play Services" and "/Extras/Google Repository"  Build it on the commandline with gradle:  - Simply run "./gradlew assembleDebug" for the debug build or "./gradlew assembleRelease" for   the release build in your tomahawk-android checkout directory. The built apk will be put into   "tomahawk-android/build/outputs/apk"  Setup using Android Studio and gradle (highly recommended):  - Open Android Studio and go to "File"->"Import Project" - Browse to your tomahawk-android checkout and click "OK". - Make sure that the radio-button "Use default gradle wrapper (recommended)" is selected. - Click "next" and that's it :) tomahawk-android should compile right away  Setup using other IDEs without gradle:  - Import tomahawk-android into the IDE of your choice - tomahawk-android depends on several 3rd party libraries. You can look up a list of all dependencies in ./app/build.gradle under dependencies{...} - Make sure you setup the support libraries correctly (http://developer.android.com/tools/support-library/setup.html) - Add all dependencies to your tomahawk-android project - tomahawk-android should now compile successfully.  If you have any further problems, feel free to join the #tomahawk.mobile irc channel on irc.freenode.org  ## Ready to contribute?  Drop us an e-mail at welisten@tomahawk-player.org or join our IRC Channel #tomahawk.mobile on irc.freenode.org  ## Code Style Guidelines for Contributors  In order to keep everything clean and cozy, please use the official Android code style format preset: - https://github.com/android/platform_development/tree/master/ide   (use the IntelliJ preset, if you're using Android Studio)  For a larger overview you should read the official Android "Code Style Guidelines for Contributors": - http://source.android.com/source/code-style.html  ## Plugin Apps Source Code  [Spotify Plugin App](https://github.com/tomahawk-player/tomahawk-android-spotify)      [Deezer Plugin App](https://github.com/tomahawk-player/tomahawk-android-deezer)
architjn/Auro	# Auro The world's first fastest, open source Music player for Android with latest design...  # Features - Fastest Music Player - Latest Designed - Open Source Music player - Customizable Options  # Team Members  - <a href="http://google.co.in/+architjn">Archit Jain</a>   ###### Thanks to - <a href="https://plus.google.com/+aleksandartesic">Aleksandar Tešić</a> for concept design  # Donate Paypal Donation id - architjn93@gmail.com  # Screenshots <img src="https://raw.githubusercontent.com/architjn/Auro/master/SCREENSHOTS/songs.png" height="500"/> &nbsp;&nbsp;&nbsp;&nbsp; <img src="https://raw.githubusercontent.com/architjn/Auro/master/SCREENSHOTS/artists.png" height="500"/> &nbsp;&nbsp;&nbsp;&nbsp; <img src="https://raw.githubusercontent.com/architjn/Auro/master/SCREENSHOTS/albums.png" height="500"/> &nbsp;&nbsp;&nbsp;&nbsp; <img src="https://raw.githubusercontent.com/architjn/Auro/master/SCREENSHOTS/player.png" height="500"/> &nbsp;&nbsp;&nbsp;&nbsp; <img src="https://raw.githubusercontent.com/architjn/Auro/master/SCREENSHOTS/upnext.png" height="500"/> &nbsp;&nbsp;&nbsp;&nbsp; <img src="https://raw.githubusercontent.com/architjn/Auro/master/SCREENSHOTS/album.png" height="500"/>  # Download Step 1 - Join Community <a href="https://goo.gl/kUsuK6">HERE</a> [Required in order to optin for beta testing]<br> Step 2 - Opt in for Beta program <a href="https://play.google.com/apps/testing/com.architjn.acjmusicplayer">HERE</a><br> Step 3 - Download from playstore <a href="https://play.google.com/store/apps/details?id=com.architjn.acjmusicplayer">HERE</a><br>  # Libraries used - Google AppCompat  - Google CardView - Google RecyclerView - Google Design Library - Google Palette - <a href="https://github.com/square/picasso">Picasso</a> - jbundle http client - <a href="https://github.com/afollestad/material-dialogs">Material Dialogs</a>  # Points should be followed ### If you are creating your own music player  1. Proper credits should be given to Auro's owner  2. Icon of your application **should not** be similar to Auro's Icon  3. Name **should not** be similar to Auro   ##### (If any of the points above is not followed, we can take any legal action)   # License <a href="https://github.com/architjn/Auro/blob/master/LICENSE">Check here</a>
kabouzeid/Phonograph	# Phonograph [![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](https://github.com/kabouzeid/Phonograph/blob/master/LICENSE.txt)  **A material designed local music player for Android.**  ![Screenshots](./art/art.jpg?raw=true)  <a href="https://play.google.com/store/apps/details?id=com.kabouzeid.gramophone">   <img height="50" alt="Get it on Google Play"       src="https://play.google.com/intl/en_us/badges/images/apps/en-play-badge.png" /> </a>
hefuyicoder/ListenerMusicPlayer	# ListenerMusicPlayer  [![license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/hefuyicoder/ListenerMusicPlayer#license) [![platform](https://img.shields.io/badge/platform-Android-yellow.svg)](https://www.android.com) [![API](https://img.shields.io/badge/API-16%2B-brightgreen.svg?style=flat)](https://android-arsenal.com/api?level=16)  ### Introdution  一款优雅的遵循 Material Design 的开源音乐播放器，UI参考 腾讯轻听 音乐播放器,使用 Lastfm Api 与 酷狗歌词Api。项目架构采用 mvp-clean，基于 Retrofit2 + Dagger2 + Rxjava + RxBus + Glide。  A grace open source music player which following the google material design. Using lastfm api and kugou lyric api.App UI base on tencent qingting music player.   ### Screenshots  ![screenshots](materials/screenshot.png)  ### Gif Preview  ![gif](materials/2017-02-10%2018_14_47.gif)  ### Features  - 遵循 Material Design 规范，界面清新，交互优雅。 - 基于 MVP-CLEAN + Retrofit2 + Dagger2 + Rxjava + Glide - 功能模块： 我的歌曲、我的歌单、文件夹、我喜欢、最近播放、最近添加、播放排行、本地搜索等。 - 支持显示歌词及缓存 - 支持耳机线控播放，耳机拔出自动暂停 - 动态刷新媒体库，及时获知媒体文件变更 - 日夜间模式切换，支持动态换肤  ### Thanks  Thanks to these projects and libraries:  - Reference Project : [Timber](https://github.com/naman14/Timber) 、 [轻听](https://play.google.com/store/apps/details?id=com.tencent.qqmusiclocalplayer) - Pictures : [Material design icon](https://github.com/google/material-design-icons) 、 腾讯轻听App - Api : [LastFM](http://www.last.fm/zh/api) 、 [酷狗音乐](http://119.29.39.252/index.php/2016/10/20/1-2/) - Library : [RxJava](https://github.com/ReactiveX/RxJava) 、 [Retrofit](https://github.com/square/retrofit) 、 [Glide](https://design.google.com/icons/) 、 [AndroidSlidingUpPanel](https://github.com/umano/AndroidSlidingUpPanel)等等  ### Statement  感谢[轻听](https://play.google.com/store/apps/details?id=com.tencent.qqmusiclocalplayer)提供参考，轻听是一款十分良心的音乐播放器，本人也非常喜欢，欢迎大家前往下载以获得更好的使用体验。本项目部分数据来自于干LastFM和酷狗歌词Api，一切数据解释权都归LastFM和酷狗所有。  ### End  > 注意：此开源项目仅做学习交流使用，如果你觉得不错，对你有帮助，欢迎点个fork，star，follow，也可以帮忙分享给你更多的朋友，这是给我们最大的动力与支持。  ### Contact Me  - Github: github.com/hefuyicoder - Email: hefuyicoder@gmail.com  ### License  ``` MIT License  Copyright (c) 2017 Hefuyi  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  ```
harjot-oberai/MusicDNA	# MusicDNA - A Music Player like no other <img src = "https://github.com/harjot-oberai/MusicStreamer/blob/master/screenshots/splash.png" width = "800"><br><br> <a href='https://play.google.com/store/apps/details?id=com.sdsmdg.harjot.MusicDNA&pcampaignid=MKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1'><img alt='Get it on Google Play' src='https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png' width="200"/></a> [<img src="https://f-droid.org/badge/get-it-on.png"       alt="Get it on F-Droid"       width="200">](https://android.izzysoft.de/repo/apk/com.sdsmdg.harjot.MusicDNA)<br> A Music Player for android that makes use of the Visualzer Class for rendering a beautiful **DNA** (***Visualization***) of the currently playing music.  ## Background The Music Player draws inspiration from [paullewis's music-dna](https://github.com/paullewis/music-dna/). The Player uses the FFT Data supplied by the **Visualizer** class of Android , calculates the Amplitude at that particular moment and plots the **DNA**.  ## The Player MusicDNA combines the usefullness of a traditional Music Player app with the beautiful visualizations. The Player allows users to play both **local music** as well as Stream Music directly from **SoundCloud**. The Player also packs a **Powerful Equalizer with** ***BassBoost and Reverb*** effects  ## Build Instructions  After cloning the repo, create a file `Config.java` under `\app\src\main\java\com\sdsmdg\harjot\MusicDNA` Put the following code in it ``` package com.sdsmdg.harjot.MusicDNA;  public class Config {     public static final String CLIENT_ID = "YOUR_SOUNDCLOUD_CLIENT_ID";     public static final String API_URL = "https://api.soundcloud.com";     public static final String GENIUS = "YOUR_GENIUS_API_KEY"; } ``` Replace `YOUR_SOUNDCLOUD_CLIENT_ID` with a client id received from SoundCloud or leave it blank if you don't want to use SoundCloud streaming.Get SoundCloud Client ID from here : [https://developers.soundcloud.com/](https://developers.soundcloud.com/)<br> Replace `YOUR_GENIUS_API_KEY` with an API key received from Genius or leave it blank if you don't want to use lyrics from genius.com. Get Genius API key from here : [https://genius.com/api-clients/new](https://genius.com/api-clients/new)<br> After that just import the project into Android Studio.  ## The Player - *In Action*  Video Demo : [http://sendvid.com/b2hhc1pi](http://sendvid.com/b2hhc1pi)<br> The video's length was cut short due to ADB screenrecord's limit of 3:00 min. <br> Credits for combining audio and video [Piyush Mehrotra](https://github.com/hm98)  <img src = "https://github.com/harjot-oberai/MusicStreamer/blob/master/screenshots/dna1.png" width = "300"> <img src = "https://github.com/harjot-oberai/MusicStreamer/blob/master/screenshots/home.png" width = "300"> <img src = "https://github.com/harjot-oberai/MusicStreamer/blob/master/screenshots/equalizer.png" width = "300"> <img src = "https://github.com/harjot-oberai/MusicStreamer/blob/master/screenshots/savedDNA.png" width = "300"> <img src = "https://github.com/harjot-oberai/MusicStreamer/blob/master/screenshots/albums_artists.png" width = "600"> <img src = "https://github.com/harjot-oberai/MusicStreamer/blob/master/screenshots/fav_recents.png" width = "600">  ## License MusicDNA is under `CC BY-NC-SA` license.
psaravan/JamsMusicPlayer	Jams Music Player =================== Jams is a free, powerful and elegant music player for Android. Jams used to be a trial/paid app on the Play Store. Due to my lack of adequate free time and Google's <a href="http://phandroid.com/2014/09/18/google-play-now-requires-devs-to-make-their-home-address-public/">new requirement for paid developers to publicly display their home address</a>, I've decided that I can no longer provide the level of support and regular updates that paid users expect from a developer.   Rather than letting Jams completely die off, I've open sourced the app and hosted it here to encourage other developers to potentially improve it, use it in their own personal projects, and re-release it to users. I will still occasionally update this app based on my free time, but it will probably be beta quality code.   If you are a developer who's interested in using Jams' current codebase, I encourage you to fork this repository and/or directly contribute to it. If you decide to launch your own fork of Jams on the Play Store, feel free to drop me a line so I can feature it on this GitHub page.  Screenshots ============ <img src="http://i.imgur.com/2hdMFzP.jpg" width=761 height=480> <img src="http://i.imgur.com/tdLiCVY.jpg" width=761 height=480>  Features ========= * Unofficial Google Play Music support. * ID3v3 tag editing. * Custom libraries support. * Album artist sorting/tag support * Blacklist ability for artists, album artists, albums, songs, genres and playlists. * 9 band equalizer with bass boost, virtualizer, and reverb. * Individual EQ settings for each artist, album artist, album, song, genre, or playlist. * File/folder browsing. * Scrobbling. * Crossfade with customizable duration. * Auto-download album art from the internet. * 2 different base themes and 9 different color schemes.  Forked Projects ================  * <a href="http://forum.xda-developers.com/android/apps-games/complete-material-design-acemusic-t2904994">ACEMusic Player (Beta)</a>  If you're using Jams' codebase in your current project/app and would like to have it featured here, email me at saravan.pantham@gmail.com.  Legacy Changelogs (until 2/24/2014) ====================================== Check out the <a href="https://github.com/psaravan/JamsMusicPlayer/blob/master/CHANGELOGS.md">CHANGELOGS</a> file.   <b>Note: </b> Changelogs will no longer be updated in the <a href="http://github.com/psaravan/JamsMusicPlayer/blob/master/CHANGELOGS.md">CHANGELOGS</a> file. Check the individual commit comments for all further update descriptions.  Developed By ============ Saravan Pantham - saravan.pantham@gmail.com  Libraries Used =========================== * CircularImageView - https://github.com/lopspower/CircularImageView * DragSortListView - https://github.com/bauerca/drag-sort-listview * LicensesDialogLibrary - https://github.com/Wicowyn/LicensesDialogLibrary * Picasso - https://github.com/square/picasso * QuickScroll - https://github.com/andraskindler/quickscroll * VelocityViewPager - https://github.com/Benjamin-Dobell/VelocityViewPager * ViewPagerIndicatorLibrary - https://github.com/JakeWharton/Android-ViewPagerIndicator * Android Asynchronous HTTP Client - http://loopj.com/android-async-http/ * Android BitmapCache - https://github.com/chrisbanes/Android-BitmapCache * ListViewAnimations - https://github.com/nhaarman/ListViewAnimations * Apache Commons IO - http://commons.apache.org/proper/commons-io/ * Apache Commons Lang - http://commons.apache.org/proper/commons-lang/ * DashClock API - https://code.google.com/p/dashclock/ * Google HTTP Client - https://code.google.com/p/google-http-java-client/ * Google HTTP Client (Android) - https://code.google.com/p/google-http-java-client/wiki/Android * JAudioTagger - http://www.jthink.net/jaudiotagger/ * Google Analytics - https://developers.google.com/analytics/devguides/collection/android/resources * NineOldAndroids - https://github.com/JakeWharton/NineOldAndroids/ * Android Support Library - http://developer.android.com/tools/support-library/index.html  License ======== All source code is licensed under Apache License 2.0. If you create your own app (free or commercial) that uses some or all of the code from this codebase, you MUST attribute it to the original author, no exceptions. You can do so by displaying an "About" or "Licenses" screen in your app that links back to this GitHub page AND includes the following notice:       Copyright 2014 - Saravan Pantham      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
ryanhoo/StylishMusicPlayer	# A Stylish Music Player  [![license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/ryanhoo/StylishMusicPlayer#license) [![platform](https://img.shields.io/badge/platform-Android-yellow.svg)](https://www.android.com) [![API](https://img.shields.io/badge/API-16%2B-brightgreen.svg?style=flat)](https://android-arsenal.com/api?level=16) [![gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/stylist-music-player/bug-report) [![PRs Welcome](https://img.shields.io/badge/prs-welcome-brightgreen.svg)](http://makeapullrequest.com)  ![Artboard](materials/Artboard.png)  ## TODO  - Lyric Support - Settings  ## Acknowledgements  Thanks to these projects and libraries:  **Libraries**  - [RxJava](https://github.com/ReactiveX/RxJava) - [RxAndroid](https://github.com/ReactiveX/RxAndroid) - [Retrofit](https://github.com/square/retrofit) - [Butter Knife](https://github.com/JakeWharton/butterknife) - [Calligraphy](https://github.com/chrisjenx/Calligraphy) - [LiteOrm](https://github.com/litesuits/android-lite-orm)  **Design**  - [Material icons](https://design.google.com/icons/)   ## License  > The MIT License (MIT)  > Copyright (c) 2016 Ryan Hoo  > Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  > The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  > THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
naman14/Timber	# Timber [![Build Status](https://travis-ci.org/naman14/Timber.svg?branch=master)](https://travis-ci.org/naman14/Timber)  [WIP][BETA]-Material Design Music Player  <a href='https://play.google.com/store/apps/details?id=naman14.timber&pcampaignid=MKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1'><img alt='Get it on Google Play' src='https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png' height=90px/></a>  <a href='https://f-droid.org/repository/browse/?fdid=naman14.timber'><img alt='Get it on F-Droid' src='https://guardianproject.info/wp-content/uploads/2014/07/logo-fdroid.png' height=60px/></a>  ## Screenshots  <img src="https://raw.githubusercontent.com/naman14/Timber/master/graphics/screen2.png" width="360" height="640"> <img src="https://raw.githubusercontent.com/naman14/Timber/master/graphics/screen3.png" width="360" height="640"> <img src="https://raw.githubusercontent.com/naman14/Timber/master/graphics/screen7.png" width="360" height="640"> <img src="https://raw.githubusercontent.com/naman14/Timber/master/graphics/screen1.png" width="360" height="640"> <img src="https://raw.githubusercontent.com/naman14/Timber/master/graphics/screen5.png" width="360" height="640"> <img src="https://raw.githubusercontent.com/naman14/Timber/master/graphics/screen4.png" width="360" height="640"> <img src="https://raw.githubusercontent.com/naman14/Timber/master/graphics/screen6.png" width="360" height="640">  ## Features - Material design - Browse Songs, Albums, Artists - Create and edit playlists - 6 different now playing styles - Homescreen widgets - Browse device folders - Dark theme and UI customisability - Gestures for track switching - LastFM scrobble - Android Wear and Android Auto support - Playing queue in notification (Xposed)  ## Changelog  Changelog is available [here](https://github.com/naman14/Timber/blob/master/Changelog.md)  ## Credits  * CyanogenMod's [Eleven Music Player](https://github.com/CyanogenMod/android_packages_apps_Eleven) * [TimelyTextView](https://github.com/adnan-SM/TimelyTextView) * [MultiViewPager](https://github.com/Pixplicity/MultiViewPager) * [PlayPauseButton](https://github.com/recruit-lifestyle/PlayPauseButton) * [CircularSeekBar](https://github.com/devadvance/circularseekbar) * [Nammu](https://github.com/tajchert/Nammu)  #Donate Paypal donation email- namandwivedi14@gmail.com   ## License  >(c) 2015 Naman Dwivedi   >This is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.   >This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.   >You should have received a copy of the GNU General Public License along with this app. If not, see <https://www.gnu.org/licenses/>.
JTechMe/JumpGo	![header](./header.png) #### Finally! Out of the beta! [![Build Status](https://travis-ci.org/JTechMe/JumpGo.svg)](https://travis-ci.org/JTechMe/JumpGo) [![GitHub license](https://img.shields.io/badge/license-MPLv2-orange.svg?style=flat-square)](https://github.com/JTechMe/JumpGo/blob/master/Mozilla%20Public%20License%20v.%202.0)  #### Download  [<img src="https://f-droid.org/badge/get-it-on.png" alt="Get it on F-Droid" height="60">](https://f-droid.org/app/com.jtechme.jumpgo) <a href="https://play.google.com/store/apps/details?id=com.jtechme.jumpgo"><img src="https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png" height="60"></a> [<img src="getapkfromgithub.png" alt="Download APK from GitHub" height="60">](https://github.com/JTechMe/JumpGo/releases/latest) <a href="https://jtechme.store.aptoide.com/app/market/com.jtechme.jumpgo/109/20056689/JumpGo"><img height="60px" alt="Android app on Aptoide" src="aptoidebadge.png"></a> [<img src="availableatamazon.png" alt="Available at Amazon" height="60">](https://www.amazon.com/JTechMe-JumpGo-Browser/dp/B01KFAPKJA/ref=sr_1_1?s=mobile-apps&ie=UTF8&qid=1471302360&sr=1-1)  #### Master Branch * [![Build Status](https://travis-ci.org/JTechMe/JumpGo.svg?branch=master)](https://travis-ci.org/JTechMe/JumpGo)  #### Dev Branch * [![Build Status](https://travis-ci.org/JTechMe/JumpGo.svg?branch=dev)](https://travis-ci.org/JTechMe/JumpGo)  #### Features * Bookmarks  * History  * Multiple search engines (Google, Bing, Yahoo, StartPage, DuckDuckGo, etc.)  * Incognito mode  * Follows Google design guidelines  * Unique utilization of navigation drawer for tabs  * Google search suggestions  * Orbot Proxy support and I2P support  ## 2017 Development Checklist - [ ] Add support for desktop/mobile bookmark sync - [ ] Add JumpGo Login - [x] Updated UI  - [ ] Include material design cards for settings and about  - [ ] Add CycleMenu to increase screen real-estate - [ ] Add Theme Creator - [ ] Option to backup Bookmarks to Drive  - [ ] Add Google login API  #### Permissions  * ````INTERNET````: For accessing the web  * ````WRITE_EXTERNAL_STORAGE````: For downloading files from the browser  * ````READ_EXTERNAL_STORAGE````: For downloading files from the browser  * ````ACCESS_FINE_LOCATION````: For sites like Google Maps, it is disabled by default in settings and displays a pop-up asking if a site may use your location when it is enabled  * ````ACCESS_NETWORK_STATE````: Required for the WebView to function by some OEM versions of WebKit  #### The Code * Please contribute code back if you can. The code isn't perfect. * Please add translations/translation fixes as you see need  #### Contributing * Contributions are always welcome * If you want a feature and can code, feel free to fork and add the change yourself and make a pull request * PLEASE use the ````dev```` branch when contributing as the ````master```` branch is supposed to be for stable builds. I will not reject your pull request if you make it on master, but it will annoy me and make my life harder.   * Find the ````dev```` branch at [https://github.com/JTechMe/JumpGo/tree/dev](https://github.com/JTechMe/JumpGo/tree/dev) * Code Style     * Standard Java camel case     * Member variables are preceded with an 'm'     * Use 4 spaces instead of a tab (\t)  #### Setting Up the Project Due to the inclusion of the netcipher library for Orbot proxy support, importing the project will show you some errors. To fix this, first run the following git command in your project folder (NOTE: You need the git command installed to use this): ```` git submodule update --init --recursive ```` Once you run that command, the IDE should automatically import netcipher and a couple submodules in as separate projects. Than you need to set the netcipher library project as a libary of the browser project however your IDE makes you do that. Once those steps are done, the project should be all set up and ready to go. [Please read this tutorial for more information on git submodules](http://www.vogella.com/tutorials/Git/article.html#submodules)  #### Building JumpGo Starting Thursday 9/7/2017, JumpGo for Android sources now have two product flavors, one for the main JumpGo com.jtechme.jumpgo application, and one for the JumpGo Dev com.jtechme.jumpgodev application. ````gradle productFlavors {         jumpgomain {             buildConfigField "boolean", "FULL_VERSION", "Boolean.parseBoolean(\"true\")"             applicationId "com.jtechme.jumpgo"             versionCode project.versionCode_main             versionName project.versionNameMain         }          jumpgoDev {             buildConfigField "boolean", "FULL_VERSION", "Boolean.parseBoolean(\"true\")"             applicationId "com.jtechme.jumpgodev"             versionCode project.versionCode_dev             versionName project.versionNameDev         } } ```` ##### The ````jumpgodev```` flavor is now used for rapid, bleeding-edge, versions of JumpGo. No real testing is preformed on dev builds before they're uploaded. To build this flavor make sure the manifest includes: ````xml     android:icon="@mipmap/ic_launcher"     android:roundIcon="@mipmap/ic_launcher_round" ````  In Android Studio: * Select Build>Generate Signed APK * Select Flavors>jumpgodev * Click Finish  ##### The ````jumpgomain```` flavor is used for all other build variants such as alpha, beta, and production. To build this flavor make sure the manifest to includes one of the following: * Alpha ````xml     android:icon="@mipmap/ic_launcher_alpha"     android:roundIcon="@mipmap/ic_launcher_alpha_round" ````  * Beta ````xml     android:icon="@mipmap/ic_launcher_alpha"     android:roundIcon="@mipmap/ic_launcher_alpha_round" ````  * Production/Stable ````xml     android:icon="@mipmap/ic_launcher"     android:roundIcon="@mipmap/ic_launcher_round" ````  In Android Studio: * Select Build>Generate Signed APK * Select Flavors>jumpgomain * Click Finish  #### License JumpGo Project [![GitHub license](https://img.shields.io/badge/license-MPLv2-orange.svg?style=flat-square)](https://github.com/JTechMe/JumpGo/blob/master/Mozilla%20Public%20License%20v.%202.0) ```` Copyright 2015 Josiah Horton  JumpGo Project Family        This Source Code Form is subject to the terms of the       Mozilla Public License, v. 2.0. If a copy of the MPL       was not distributed with this file, You can obtain one at        http://mozilla.org/MPL/2.0/ ```` Lightning Browser [![GitHub license](https://img.shields.io/badge/license-MPLv2-orange.svg?style=flat-square)](https://github.com/anthonycr/Lightning-Browser/blob/dev/Mozilla%20Public%20License%20v.%202.0) ```` Copyright 2014 Anthony Restaino  Lightning Browser     This Source Code Form is subject to the terms of the    Mozilla Public License, v. 2.0. If a copy of the MPL    was not distributed with this file, You can obtain one at     http://mozilla.org/MPL/2.0/ ```` This application is derived from the Lightning Browser source code found at [https://github.com/anthonycr/Lightning-Browser](https://github.com/anthonycr/Lightning-Browser). Changes made to this repo are intended for the JumpGo Browser but can be coppied to the Lightning Browser dev branch at [https://github.com/anthonycr/Lightning-Browser/tree/dev](https://github.com/anthonycr/Lightning-Browser/tree/dev) if requested.  If you have any questions regarding the open-source license, please contact me at [jtechme.org@gmail.com](jtechme.org@gmail.com)
xdevs23/Cornowser	# <img src="https://raw.githubusercontent.com/xdevs23/Cornowser/master/img/icons/icon_m_downsized.png" width="48" /><sup>Cornowser</sup> [![Codacy Badge](https://api.codacy.com/project/badge/Grade/e26b6905697d4fdfb2a00d6db25decd3)](https://www.codacy.com/app/xdevs23/Cornowser?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=xdevs23/Cornowser&amp;utm_campaign=Badge_Grade) [![Travis CI Badge](https://travis-ci.org/xdevs23/Cornowser.svg)](https://travis-ci.org/)<br /> The modern premium web browser for Android. This project is still in development phase.   <hr />  **Important:** The Crosswalk project has been discontinued a while ago. Since it is practically dead and I don't have time for continuing development on this browser as of now, this project is (at least temporarily) discontinued.  Cornowser is intended to be **fast**, **modern** and **inituitive**. Powered by the [Crosswalk rendering engine](http://crosswalk-project.org), basing on Chromium, this browser is as powerful and modern, as you get it from latest Chromium releases.  Also visit our [XDA thread](http://forum.xda-developers.com/android/apps-games/app-cornowser-t3287890)!  I, [xdevs23](http://github.com/xdevs23), do the main work on this project, [Thunderbottom](http://github.com/Thunderbottom) helps and supports me. We also have some translators for our app. Please visit our [Wiki](https://github.com/xdevs23/Cornowser/wiki) to get more information.   <hr /> This browser achieves **486** out of **555** points in the [Beta HTML5Test](http://beta.html5test.com). **Note:** due to changes in the Beta HTML5 Test, all scores are now lower. Please don't consider the updated screenshots as a worse score than before. Google Chrome for Android achieves **488** points.  According to these results **this browser is ready for HTML5**.  ### Chrome score: <img src="https://github.com/xdevs23/Cornowser/raw/screenshots/chrome52html5test.png" width="240" />  ### Cornowser score: <img src="https://github.com/xdevs23/Cornowser/raw/screenshots/cornowser1094html5test.png" width="240" />  ## Screenshots  See [our XDA thread with screenshots](http://forum.xda-developers.com/devdb/project/?id=13347#screenshots).  <hr />  # License  ##### MIT License  Don't forget to mention me and don't change the license, everything else is described in the license. You can basically do whatever you want but there are some limitations. <img src="http://emojipedia-us.s3.amazonaws.com/cache/a3/22/a32265bb1e91b1f642f53fc4f4edb5cc.png" width="24" />  Any version of Cornowser and full source until including the **git tag** <code>MIT</code> is licensed under the MIT license, any newer version and all sources are licensed under the MIT-SGV license. Exceptions may apply (external libraries etc. which have another license).  ``` The MIT License by Simao Gomes Viana (MIT-SGV)  Copyright (c) 2015-2016 Simao Gomes Viana  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software or in a separate file called LICENSE.  Any distribution or modification of this software (that is accessible/available publicly) by anyone else than the original copyright holder must be fully available and up-to-date as open source. Exceptions apply when the original copyright holder explicitly permits the usage, modification, redistribution and/or any kind of distribution as a closed-source product.  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```
crazyhitty/minx	# Minx A minimalistic, text based web browser for android.  <a href="https://play.google.com/store/apps/details?id=com.chdev.ks.minx">   <img alt="Get it on Google Play"        src="https://developer.android.com/images/brand/en_generic_rgb_wo_45.png" /> </a>  [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Minx-brightgreen.svg?style=flat)](http://android-arsenal.com/details/3/2502)  #Who is Minx for ? It is intended for those users who don’t like adverts on webpages or enjoy reading content from websites but unfortunately these websites don’t scale well on smartphone screens. Minx grabs textual data from websites and represents it to the user in a readable format. This app also removes any possibility of websites tracking user data as no cookies or user details are shared with the website. It uses jsoup to parse html data into string values which can further be displayed to the user.  #Features * Ad free and no user information tracking. + Load any url into the app and it will load it as long as the url is valid. + Change font size according to your liking. + Save webpages offline. + Secure your archive with a PIN.  #Screenshots ![](https://lh3.googleusercontent.com/Di5uZK0e9eWvWCzLVQ46DgqLrrniiAM-WKxHz1YnWw9oVggwHPf5BM7BSwnUu7nSmQ=h900-rw) ![](https://lh3.googleusercontent.com/Lj921COnloZt_JWOsCdBQlapvqIDE3UhheVE39FZ2lcBG1XLDFlNmB9HrXDT_Io7BA=h900-rw) ![](https://lh3.googleusercontent.com/7VYVwBPdIIA0kYYx7TBv1nYqQL0-Rmh4TywFHuBT5uhziMzUIKMcFicCGc83zn0_3g=h900-rw) ![](https://lh3.googleusercontent.com/Xknt4ot54VZ7WIcv7ETU0en-H5Ld7dOl8cOD-TPSiV8_yGHP5h68AFaGPQP0s09GHhk=h900-rw) ![](https://lh3.googleusercontent.com/PEr6BIJivSGP5Ae6FBIPIxnstjm1iIgGrq6AxvtHMeRh1NE5aoHY4X_Y5Yrekiz6QNU=h900-rw) ![](https://lh3.googleusercontent.com/x5nGu-cYQ9hBFJAYP1jgyvMovsYEmuKS8dfhzDd_ZNZpd3BY17Z9kOVG3yIP5duM18A=h900-rw)
FINRAOS/JTAF-ExtWebDriver	JTAF-ExtWebDriver [![Build Status](https://secure.travis-ci.org/FINRAOS/JTAF-ExtWebDriver.png?branch=master)](http://travis-ci.org/FINRAOS/JTAF-ExtWebDriver) ================== [Extensions for WebDriver](http://finraos.github.io/JTAF-ExtWebDriver/) is an enhancement to the powerful WebDriver API, with robust features that keep your browser automation running smoothly. It includes a widget library, improved session management and extended functions over the existing WebDriver API.  Here is the link to [getting started](http://finraos.github.io/JTAF-ExtWebDriver/howitworks.html)  Releases ======== >[Release 1.0](https://github.com/FINRAOS/JTAF-ExtWebDriver/releases/tag/jtaf-extwebdriver-1.0) is available on Maven central repository! - 12/13/2013  >[Release 1.1](https://github.com/FINRAOS/JTAF-ExtWebDriver/releases/tag/jtaf-extwebdriver-1.1) has been released! - 02/03/2014  >[Release 1.2](https://github.com/FINRAOS/JTAF-ExtWebDriver/releases/tag/jtaf-extwebdriver-1.2) has been released! - 04/07/2014  >[Release 1.2.1](https://github.com/FINRAOS/JTAF-ExtWebDriver/releases/tag/jtaf-extwebdriver-1.2.1) patch release - 04/10/2014  >[Release 1.3](https://github.com/FINRAOS/JTAF-ExtWebDriver/releases/tag/jtaf-extwebdriver-1.3) has been released! - 05/02/2014  >[Release 1.4](https://github.com/FINRAOS/JTAF-ExtWebDriver/releases/tag/jtaf-extwebdriver-1.4) has been released! - 08/15/2014  >[Release 1.5](https://github.com/FINRAOS/JTAF-ExtWebDriver/releases/tag/jtaf-extwebdriver-1.5)  Contributing ============= We encourage contribution from the open source community to help make ExtWebDriver better. Please refer to the [development](http://finraos.github.io/JTAF-ExtWebDriver/contribute.html) page for more information on how to contribute to this project including sign off and the [DCO](https://github.com/FINRAOS/JTAF-ExtWebDriver/blob/master/DCO) agreement.  If you have any questions or discussion topics, please post them on [Google Groups](https://groups.google.com/forum/#!forum/jtaf-extwebdriver).  Building ========= ExtWebDriver uses Maven for build. Please install Maven by downloading it [here](http://maven.apache.org/download.cgi). ```sh # Clone ExtWebDriver git repo git clone git://github.com/FINRAOS/JTAF-ExtWebDriver.git cd JTAF-ExtWebDriver  # Run package to compile and create jar mvn package ```  Running Tests ============== ExtWebDriver uses Maven plugins to execute both the unit and integration tests. The integration tests run against a locally deployed app (using Jetty server) using HtmlUnitDriver. You can run all of the tests by executing: ```sh mvn verify ``` You can run individual tests by executing it from your IDE or through command line. If you want to run an integration test, you need to deploy the app locally first before execution. You can start the application server by executing: ```sh mvn jetty:start ```  Requirements ============== ExtWebDriver requires Java SE version 8 or above available [here](http://www.oracle.com/technetwork/java/javase/downloads/index.html)  License Type ============= JTAF projects including ExtWebDriver is licensed under [Apache License Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
alexvictoor/web-logback	[![Build Status](https://travis-ci.org/alexvictoor/web-logback.svg?branch=master)](https://travis-ci.org/alexvictoor/web-logback)  web-logback ===========  ![screenshot](src/site/screenshot.png)  Logback appender leveraging "HTML5 Server Sent Event" (SSE) to push logs on browser consoles.  It is based on worderful Netty framework to implement a lightweight HTTP SSE server.  Usage ------  Activation requires 3 steps: - configuration of your build to add a dependency to this project  - configuration of the appender in the logback.xml configuration file - inclusion of a javascript snippet in your HTML code to open a SSE connection  If you use maven, below the xml fragment you should add in the dependencies section of your pom file: ```xml <dependency>   <groupId>com.github.alexvictoor</groupId>   <artifactId>web-logback</artifactId>   <version>0.2</version> </dependency> ```  Below an XML fragment example that shows how to configure logback on the server side ```xml <configuration> ...   <appender name="WEBOUT" class="com.github.alexvictoor.weblogback.BrowserConsoleAppender">     <encoder>       <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern> <!-- Use whatever pattern you want -->     </encoder>     <port>8765</port> <!-- Optional, this is the port on which the HTTP SSE server will listen. Default port is 8765 -->     <active>true</active> <!-- Optional, if false the appender is disabled. Default value is true -->     <buffer>1</buffer> <!-- Optional, the size of a replay buffer for late joiners that will receive already emmited messages -->   </appender> ... </configuration> ```  In the browser side, the easiest way to get the logs is to include in your HTML document logback.js. This script is delivered by the embedded HTTP SSE server at URL path "/logback.js".   During developpement, if you are running the appender with default setting you can simply add the following declaration in your HTML code:      <script type="text/javascript" src="http://localhost:8765/logback.js"></script>    It gets even simpler when using a bookmarklet. To do so use your browser to display the "homepage" of the embedded HTTP SSE server (at URL http:// HOST : PORT where HOST & PORT are the parameters you have used in the log4net configuration). The main purpose of this "homepage" is to test your web-logback configuration but it also brings a ready to use bookmarklet (named "Get Logs!"). This bookmarklet looks like code fragment below:      (function () {          var jsCode = document.createElement('script');          jsCode.setAttribute('src', 'http://HOST:PORT/logback.js');          document.body.appendChild(jsCode);      }());  Why SSE? -------- [Server Sent Events](https://en.wikipedia.org/wiki/Server-sent_events) while being quite simple to implement, allows to stream data form a server to a browser leveraging on the HTTP protocol, just the HTTP protocol. Resources required on the server side are also very low. Simply put, to stream text log it makes no sens to use WebSocket which is much more complex.    All newest browsers implement SSE, except IE... (no troll intended). Chrome even implements a nice reconnection strategy. Hence when you are using a Chrome console as a log viewer, log streams will survive to a restart of a server side process, reconnection are handled automatically by the browser.  Multiple server-side process? ----------------------------- If you want to watch logs coming from several process, you just need one single browser console!   Several bookmarlets can be executed on same browser location. To distinguisg logs coming from one service to logs coming from another one, you can use different log4net pattern, use a log prefix, or use custom styles (see next section).    Custom colors and styles? ------------------------- Once you are connected to several log streams, you will might want to get different visual appearance for those streams.   By default all streams use default browser log styles. Styles can be customized by adding special attributed to logback.js script tag:      <script          src="http://HOST:PORT/logback.js"          style="font-size: 18px; background: cyan" >     </script>  Styles attrbutes can also be specific to a logging category:      <script          src="http://HOST:PORT/logback.js"          style="color: black;"          style-error="color: red; font-size: 18px;" >     </script>  With above example, all logs are written in black on white, size 12px, except error logs written in red, size 18px.   Below a bookmarklet code that gives similar results:      (function () {          var jsCode = document.createElement('script');          jsCode.setAttribute('src', 'http://HOST:PORT/logback.js');          jsCode.setAttribute('style' 'color: black;');         jsCode.setAttribute('style-error' 'color: red; font-size: 18px;');         document.body.appendChild(jsCode);      }());  Custom styles can be specify for debug (style-debug), info (style-info) and warn (style-warn) logs as well.    Disclaimer --------- 1. For obvious security concerns, **do not activate it in production!**   2. Do not try to use this appender to log Netty activity... this might generate an infinite loop 3. This is a first basic not opimized implementation, no batching of log messages, etc
zanata/zanata-platform	# Zanata  Zanata is a web-based system for translators to translate documentation and software online using a web-browser. It is written in Java and uses modern web technologies like JBoss EAP, CDI, GWT, Hibernate, and a REST API. It currently supports translation of DocBook/Publican documentation through PO files, and a number of other formats. Projects can be uploaded to and downloaded from a Zanata server using a Maven plugin or a command line client.  For *developers and writers*: By using Zanata for your document translations, you can open up your project for translations without opening your entire project in version control.  For *translators*: No need to deal with PO files, gettext or a version control system - just log in to the website, join a language team and start translating, with translation memory (history of similar translations) and the ability to see updates from other translators in seconds.  Find out about Zanata here: http://zanata.org/   Zanata is Free software, licensed under the [LGPL][].  [LGPL]: http://www.gnu.org/licenses/lgpl-2.1.html   ## Developers: Building Zanata from source  ### Prerequisites  You will need: - Java SDK 8 (OpenJDK recommended) - zsh (for the build script) - npm (optional) - MySQL or MariaDB (optional) - JBoss EAP 7 or WildFly 10 (optional) - Linux or Mac OSX. Windows works too, sometimes.  A full build needs to download and install node, npm, mysql and WildFly/EAP, some of which are platform-dependent.  ### Building  The build script you need to know about is [./build](`./build`). It covers all your Zanata-building needs. Disclaimer: may not cover all your Zanata-building needs.  The `-h` argument prints the build script's help.  #### Building on Mac OS X and macOS Sierra  Currently, there is an extra step needed to build Zanata on a Mac. For `./build` to work, you will need to point to the correct Java directory using the following command (using the correct JDK version on your Mac):  `export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.X_XX.jdk/Contents/Home"`   This JAVA_HOME workaround will not be needed in the next release of the Maven wrapper: io.takari:maven-wrapper:0.1.7.  See [https://github.com/takari/maven-wrapper/pull/14](https://github.com/takari/maven-wrapper/pull/14) for details.  #### Build everything with unit tests  `./build --all` - Builds the entire project (client and server ) fairly quickly, skipping integration tests and static analysis (checkstyle, etc), but including unit tests.  NB: If you need to run functional tests later without rebuilding, you should add `-i` to install the war file to your Maven repo after packaging.  #### Quickly build a .war file for later tests or docker deployment  `./build --server -iQ` - Builds and installs the project as quickly as possible, skipping all checks and verifications (i.e. tests, checkstyle, etc).  The binaries will be installed to your Maven repo for usage in later (partial) builds and tests.  #### Quickly build and run a server for testing  `./build -w` - Builds zanata-war and starts a JBoss/WildFly server using the cargo plugin. This is intended for starting a Zanata instance with the aim of running functional tests from an IDE.  #### Development using docker (experimental)  For a quick Zanata development environment with Docker, please visit the [docker README](docker/README.md).  ### Source code note Please note that any references to pull request numbers in commit messages (eg merge nodes) prior to 20 October 2016 are referring to the old repositories (before they were merged into the zanata-platform repository):  * https://github.com/zanata/zanata-api/pulls/ * https://github.com/zanata/zanata-client/pulls/ * https://github.com/zanata/zanata-common/pulls/ * https://github.com/zanata/zanata-parent/pulls/ * https://github.com/zanata/zanata-server/pulls/  GitHub tries to auto-link numbers to pull requests, but such links will generally be incorrect for old commit messages.
satyagraha/logviewer	# logviewer    ## Overview    This project provides a capability to tail log files in a web browser. It uses the emerging   [websockets](http://en.wikipedia.org/wiki/WebSocket) technology to stream new log lines to the browser for   display in a scrollable text area. The log files tailed may be either on a filesystem locally mounted on   the web application server, or on a remote server accessible via [ssh](http://en.wikipedia.org/wiki/Secure_Shell).    ### Browser Compatibility    The client-side software is written in Javascript, and browser compatibility as at autumn 2012 is as follows:    - Firefox - excellent, tested  - Chrome - excellent, tested  - MS IE - limited  - Safari - believed to work, but not tested    A comprehensive table of browser websocket support is [here](http://caniuse.com/websockets).    ### Server-side Support    The server-side implementation of websockets does not as yet have a standard Java API. Therefore  different web container providers require the use of container-specific classes to service websocket  actions.    The implementation provided here comes with:    - a common core log viewer service which is independent of the container  - a common [J2EE servlet](http://en.wikipedia.org/wiki/Java_Servlet) 2.5 abstraction module  - an [Apache Tomcat](http://tomcat.apache.org/) 7.0.30 servlet adapter using the two common modules  - a [codehaus Jetty](http://jetty.codehaus.org/jetty/) 7.6.7 servlet adapter using the two common modules  - a [Glassfish Grizzly](http://grizzly.java.net/) implementation using the common core service only    A separate [Play framework](http://www.playframework.com/) project using the common core is available [here]().    There is no reason implementations could not be provided for other containers without too much difficulty.      ## Getting Started     - Ensure you have [Git](http://git-scm.com/) and [Maven](http://maven.apache.org/) installed on your system  - Copy the URL in the _Git Read-Only_ entry field at the top of this web page to the clipboard  - Change working directory to an appropriate location for the checkout, then execute: `git clone url`  - Change working directory to the newly created _logviewer_ subdirectory  - Edit the file `logviewer-common/src/main/resources/LogConfigDefault.properties` to adjust log  directory if required  - Ensure you have environment variable `JAVA_HOME` set to reference a Java 6 JDK (not JRE), e.g. on Windows:   - `set JAVA_HOME=C:\Program Files\Java\jdk1.6.0_35`  - Build the complete system thus:   - `mvn.bat clean install`  - For Jetty, execute:    - `cd logviewer-webapp`    - `mvn.bat -P jetty clean jetty:run`  - For Tomcat, execute:   - `cd logviewer-webapp`   - `mvn.bat -P tomcat clean tomcat7:run`  - For Grizzly, execute:   - `cd logviewer-grizzly`   - `mvn.bat exec:java`   - Open web URL [http://localhost:8080/logviewer/display.html](http://localhost:8080/logviewer/display.html)  - The resulting web page should be visible in the usual way    ### Tailing Server-local Files    Click the _pick log File_ pulldown to select a file to be tailed.     ### Tailing Server-remote Files    - Enter a URI in the entry field, typically in the form: `ssh://userid@hostname/path/to/file`  - Note that URI's need to be URL-encoded for embedded spaces or other special characters  - If using password authentication, add a password in the appropriate entry field  - If using passphrase authentication, add a passphrase in the appropriate entry field  - Click the _Tail_ button  - Defaults for various ssh parameters may be set via the properties file mentioned above    ### IDE Users    Note that, at the time of writing, the standard Eclipse Maven plugin _m2e_ has an issue whereby  source code cannot be found when debugging with the above Jetty/Tomcat7 run configuratations: this  is due to dynamic code loading, see the [bug report](https://bugs.eclipse.org/bugs/show_bug.cgi?id=384065).  As an interim fix, an additional _m2e_ extension plugin may be installed from  [here](https://github.com/ifedorenko/com.ifedorenko.m2e.sourcelookup) which provides the currently  missing functionality.      ### Javadoc    Execution of the command:    - `mvn.bat javadoc:javadoc`    at base directory level will result in Javadoc being created under `javadoc/site/apidocs`.      ## Principles of Operation     Communication between client- and server-side is very simple, using one JSON-encoded object, declared as a `LogMessage`  in the Java codebase. On loading the web page the user is presented with a list of available log files, and  when one is selected then updates are sent to refresh the web page text area. The updating is continuous, unless  paused via the provided checkbox.      The following additional Javascript libraries are used:    - [JQuery](http://jquery.com/) - comprehensive DOM manipulation capabilities  - [jquery-json](http://code.google.com/p/jquery-json/) - JSON encoding and decoding    - [Underscore](http://documentcloud.github.com/underscore/) - functional programming    The server-side component implements the appropriate container servlet and responds to the messages as necessary. We use the  Apache [CommonsIO Tailer](http://commons.apache.org/io/api-release/index.html?org/apache/commons/io/input/Tailer.html)  to perform the tracking of the log file.    A simple implementation of the standard Java [Executor](http://docs.oracle.com/javase/6/docs/api/index.html?java/util/concurrent/Executor.html)  interface is provided to manage the necessary thread pool.    ## Log Generator    A utility program named `LogGenerator` is provided: it writes to a log file on a regular basis to simulate what  would happen with a real log file, and is useful during testing. The log file location is configured in  `logviewer-common/src/main/resources/generator.log4j.properties`.    To run the generator, from the main working directory do the following:     - `set JAVA_HOME=C:\Program Files\Java\jdk1.6.0_35`   - `cd logviewer-common`   - `mvn.bat -P generator exec:java -Dexec.args=100`    which generates 100 lines of output then exits. Omit the line limit number to make the program run indefinitely.    ## Notes    - Developer contributions welcome  - Credit is due to JCraft Inc, developers of the [Jsch Java SSH library](http://www.jcraft.com/jsch/)    ## License    [Apache V2.0](http://www.apache.org/licenses/)    ## Revision History    - 0.0.5 - Support for Jetty 9.2.x  - 0.0.4 - Refactored and added Grizzly support  - 0.0.3 - Added ssh support  - 0.0.2 - Split into separate maven modules  - 0.0.1 - Initial version
korpling/ANNIS	[![Build Status](https://travis-ci.org/korpling/ANNIS.svg?branch=develop)](https://travis-ci.org/korpling/ANNIS)  ANNIS is an open source, versatile web browser-based search and visualization architecture for complex multilevel linguistic corpora with diverse types of annotation.  ANNIS, which stands for ANNotation of Information Structure, has been designed to provide access to the data of the SFB 632 - "Information Structure: The Linguistic Means for Structuring Utterances, Sentences and Texts".  Since information structure interacts with linguistic phenomena on many levels, ANNIS addresses the SFB's need to concurrently annotate, query and visualize data from such varied areas as syntax, semantics, morphology, prosody, referentiality, lexis and more. For project working with spoken language, support for audio / video annotations is also required.  * Homepage (including user and administration documentation): http://annis-tools.org/ * Developer Documentation: http://korpling.github.io/ANNIS/doc/ * Code: https://github.com/korpling/ANNIS * Bug and issue tracking: https://github.com/korpling/ANNIS/issues
zkoss/zkspreadsheet	No readme file	
testcontainers/testcontainers-java	# TestContainers  > TestContainers is a Java 8 library that supports JUnit tests, providing lightweight, throwaway instances of common databases, Selenium web browsers, or anything else that can run in a Docker container.  [![Build Status](https://travis-ci.org/testcontainers/testcontainers-java.svg?branch=master)](https://travis-ci.org/testcontainers/testcontainers-java)  ![Testcontainers logo](docs/logo.png)  # [Read the documentation here](http://www.testcontainers.org)  ## License  See [LICENSE](LICENSE).  ## Copyright  Copyright (c) 2015, 2016 Richard North and other authors.  See [AUTHORS](AUTHORS) for contributors.
teiid/teiid	teiid =====  Teiid is a data virtualization system that allows applications to use data from multiple, heterogeneous data stores.  ## Useful Links - Website - http://teiid.org - Documentation - https://teiid.gitbooks.io/documents/content/ - Documentation Project - https://teiid.gitbooks.io - JIRA Issues -  https://issues.jboss.org/browse/TEIID - User Forum - https://community.jboss.org/en/teiid?view=discussions - Wiki - https://community.jboss.org/wiki/TheTeiidProject  ## To build Teiid - install JDK 1.8 or higher - install maven 3.2+ - http://maven.apache.org/download.html - Create a github account and fork Teiid - Use the -Ddocker=true argument to enable image building, which requires a running Docker daemon.  Enter the following:  	$ git clone https://github.com/<yourname>/teiid.git 	$ cd teiid 	$ mvn clean install -P dev -s settings.xml 	 you can find the deployment artifacts in the "teiid/build/target" directory once the build is completed.  Licenses -------  The default license for all submodules is the [Apache Software License (ASL) v2.0][1]  Where applicable individual submodules will provide additional copyright and license information.  [1]: view-source:https://www.apache.org/licenses/LICENSE-2.0
qubole/quark	<!-- {% comment %}   Copyright (c) 2015. Qubole Inc   Licensed under the Apache License, Version 2.0 (the "License");   you may not use this file except in compliance with the License.   You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software   distributed under the License is distributed on an "AS IS" BASIS,   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   See the License for the specific language governing permissions and   limitations under the License. See accompanying LICENSE file. {% endcomment %} -->  [![Build Status](https://travis-ci.org/qubole/quark.svg)](https://travis-ci.org/qubole/quark) [![Join the chat at https://gitter.im/qubole/quark](https://badges.gitter.im/qubole/quark.svg)](https://gitter.im/qubole/quark?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  Introduction ============  Quark (/ˈkwɔrk/ or /ˈkwɑrk/) simplifies and optimizes access to data for data analysts by  managing relationships between tables across all databases in an organization. Quark defines  materialized views and olap cubes, using them to route queries between tables stored in different  databases. Materialized views simplify the management of cold and hot data between a data lake  and a data warehouse. OLAP cubes optimize reports by rerouting queries to cubes stored in a fast  database. Quark is distributed as a JDBC jar and will work with most tools that integrate through  JDBC.   Mailing List: quark-dev@googlegroups.com   Subscribe: quark-dev+subscribe@googlegroups.com   Unsubscribe: quark-dev+unsubscribe@googlegroups.com   Gitter: https://gitter.im/qubole/quark    Documentation ============= For a quick start guide and detailed documentation refer to [doc](http://qubole-quark.readthedocs.org/en/latest/)  Developer Setup =============== Quark now uses Qubole's fork of [incubator-calcite](https://github.com/qubole/incubator-calcite).  Checkout calcite and install jars. The following code will install `1.5.0-qds-r6-SNAPSHOT`      git clone git@github.com:qubole/incubator-calcite.git     cd incubator-calcite     git checkout qds-1.5     mvn install -DskipTests      Checkout and compile Quark  Make changes in incubator-calcite ---------------------------------  You have to create a dev branch, change versions, develop and merge the changes once you are done.       git checkout -b dev_branch qds-1.5     change version to 1.5.0-qds-dev_branch in all pom.xml files. (We need to improve this)     change version in quark-calcite/pom.xml      Once you merge your changes back to `qds-1.5`, bump up the versions in all pom.xml files.  Release a new version of Quark ==============================  Releases are always created from `master`. During development, `master`  has a version like `X.Y.Z-SNAPSHOT`.    To create a release, the version has to be changed, compile, deploy and  bump the development version.       # Change version as per http://semver.org/     mvn versions:set -DnewVersion=X.Y.Z -DgenerateBackupPoms=false     git commit -m "Prepare release X.Y.Z" -a     git tag -a X.Y.Z -a "A useful comment here"     git push     git push --tags     # SSH to build machine if required     #Deploy to Maven Central     mvn deploy -P release     #Set new development version.     mvn versions:set -DnewVersion=X.Y.(Z+1)-SNAPSHOT -DgenerateBackupPoms=false     git commit -m "Set Development Version to X.Y.(Z+1)" -a
epam/Wilma	Wilma =========== Wilma is a **Service Virtualization** tool, that combines the capabilities of a **Service Stub** and a **HTTP/HTTPS Transparent Proxy**.  Its main purpose is to support development and testing applications/services/components those functionality relies/depends on another application/services/components that can be owned by you or can be owned by 3rd party as well.  The selected architecture approach makes it capable to use it in unit, integration, functional, end-to-end, performance test environments and environments those contains microservices.  It can be used for manual tests too. It is expandable easily via plugins and configurable on-the-fly. It is written in Java, and the solution consists of two standalone applications:  * **Wilma** application is the highly configurable **Service Virtualization** tool * **Wilma Message Search** application (optional component) provides high performance searching capability of the request-response pairs that were logged by Wilma application.  # Quick intro for end users ## Wilma application #### Requirements * JRE 7 or 8 * The latest [release](https://github.com/epam/Wilma/releases) of Wilma application downloaded and extracted into a folder.   #### Configuring Components/Services to use Wilma The most simple way to do this is by configuring the Component/Service to use Wilma **as HTTP(S) proxy**.  In case of Java components/services, this can be done by adding a few VM arguments to the run configuration:  ``` JAVA_PROXY_FLAGS=-Dhttp.proxyHost=[wilma-url] -Dhttp.proxyPort=[wilma-proxy-port] -Dhttps.proxyHost=[wilma-url] -Dhttps.proxyPort=[wilma-proxy-port] java ${JAVA_PROXY_FLAGS} ... ```  #### Configure and run Wilma  To run Wilma with simplest configuration, just download the release, extract it and run: `java -jar wilma-x.y.z.jar wilma.conf.properties`  See [this page](http://epam.github.io/Wilma/endusers/index.html) for more detailed information on how to configure Wilma, and Component/Service that uses Wilma.  #### Docker Image of Wilma * **Docker image** of Wilma is available on DockerHub, see details [here](https://github.com/epam/Wilma/wiki/Docker-image-of-Wilma)  ## Contact, questions and answers In order to get the latest news, follow this project on GitHub. The latest documentation can be found at [http://epam.github.io/Wilma/](http://epam.github.io/Wilma/). Feel free to seek for assistance/advise, or discuss usage scenarios by submitting new [Issue](https://github.com/epam/Wilma/issues) on GitHub or by joining to [wilma-users](https://groups.google.com/forum/#!forum/wilma-users) mailing list.  ## Wilma Message Search application * Running Wilma Message Search application is optional, Wilma itself does not require it. * This optional component just makes your life easier in case you would like to run quick searches on the messages logged by Wilma. * To run Wilma Message Search application, java JDK must be used. With JRE, it will not work properly. * **Docker image** of combined Wilma and Wilma Message Search application is available on DockerHub, see details [here](https://github.com/epam/Wilma/wiki/Docker-image-of-Wilma)  #### Requirements * JDK 7 or 8 * The latest [release](https://github.com/epam/Wilma/releases) of Wilma Message Search application downloaded and extracted into a folder.  #### Running `java -jar wilma-message-search-x.y.z.jar message.search.conf.properties`  # Quick intro for developers/contributors  There are several ways you can help us: * **Raise an [issue](https://github.com/epam/Wilma/issues).** Have you found something that does not work as expected? Let us know about it. * **Suggest a feature by submitting an [issue](https://github.com/epam/Wilma/issues) or by sending an [e-mail](https://groups.google.com/forum/#!forum/wilma-users) to us.** It's even better if you come up with a new feature and write us about it. * **Write some code.** We would love to see pull requests to this tool. Feel free to contribute (send pull request) on GitHub.  #### Advised working environment * Java JDK 8 * IntelliJ / Eclipse * Gradle, Checkstyle, Git Integration for the IDE  #### Building with Gradle The project can be built by following the instructions described [here](https://github.com/epam/Wilma/wiki/DEV,-Build-from-Scratch). This way of build is recommended for contributors only, End-Users, please use the pre-built downloadable releases from [here](https://github.com/epam/Wilma/releases), or use the docker image. Actual build status: [![Build Status](https://travis-ci.org/epam/Wilma.svg?branch=master)](https://travis-ci.org/epam/Wilma)  ## Detailed information * Check the [Wiki](https://github.com/epam/Wilma/wiki) and [Issues](https://github.com/epam/Wilma/issues) link on GitHub * Check further documentation at [http://epam.github.io/Wilma/](http://epam.github.io/Wilma/)  # License - GPLv3.0 Copyright 2013-2017 EPAM Systems  Wilma is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  Wilma is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with Wilma.  If not, see <http://www.gnu.org/licenses/>.
jbrisbin/vcloud	# Virtual Private Cloud Utilities  ## JMX MBean Invoker  The mbean-invoker module provides a listener inside the Tomcat/tcServer instance that waits for messages telling it to invoke JMX operations or return the values of JMX attributes. Designed to be used in conjunction with monitor and management software to unobtrusively, and with minimal system overhead, manage a vcloud of Tomcat/tcServer nodes.  #### Dependencies ####  * RabbitMQ AMQP Java client libraries (ver 1.7). * Jackson JSON Parser/Generator (ver 1.5).  ## Cloud-based Session Clustering  This module provides a session Manager and session Store that work in concert with RabbitMQ to provide session failover and cluster-wide load-balancing without relying on sticky sessions.  NOTE: I've switched my servers to using SLF4J/Log4J. I blogged about this switch:  http://jbrisbin.wordpress.com/2010/04/20/change-logging-package-to-slf4jlog4j-in-tcservertomcat/  I've created a branch of this that uses the default Tomcat JULI logging. (Note: I deleted that branch and will re-create it when I get the latest changes I've checked in JULI-fied).  ## AMQP Log4J Appender ##  This module provides an appender which publishes Log4J events to your RabbitMQ servers for the purpose of aggregating log files from multiple sources into a coherent single sink. To come: a command-line utility to sniff the logging queue  ## AMQP Distributed ClassLoader ##  This module adds remote classloading support to your application. Simply configure a listener to  the configured queue that has the appropriate classes in the classpath. When using the client  ClassLoader in your application, it will load the class file via RabbitMQ.  ## Distributed Asynchronous Cache ##  This modules turns RabbitMQ into an asynchronous NoSQL cache.  ## Sponsors ##  YourKit is kindly supporting open source projects with its full-featured Java Profiler. YourKit, LLC is the creator of innovative and intelligent tools for profiling Java and .NET applications. Take a look at YourKit's leading software products: <a href="http://www.yourkit.com/java/profiler/index.jsp">YourKit Java Profiler</a> and <a href="http://www.yourkit.com/.net/profiler/index.jsp">YourKit .NET Profiler</a>.
AxonFramework/AxonBank	# Axon Bank This is a sample application with the purpose of showing the capabilities of Axon Framework.  ## Domain As you may have guessed from the name of the application, Axon Bank concerns itself with the banking domain. The application consists of 2 aggregates: bank account and bank transfer. We have tried to find an appropriate balance between complexity and simplicity. The application is meant to be complex enough to showcase interesting building blocks provided by Axon Framework. But we did not want people to get lost in the business logic of the application.  ## Technical Details We have tried to keep the application self-contained. It is built with Spring Boot and therefore does not require you to have Tomcat installed.  There are two ways to run the application: on a single node and on multiple nodes using Docker.  Running the application on a single node does not require you to install any dependencies. The storage of data is all done in memory and does not require you to run an external data store.  The distributed version requires Docker and Docker Compose to be installed. There are 4 containers involved in the distributed version: 1 container running MySQL, 1 container running RabbitMQ and 2 containers running instances of Axon Bank. The MySQL server is used for the storage of events, sagas and query side data. RabbitMQ acts as a dedicated STOMP broker. STOMP is used in combination with WebSockets. The dedicated STOMP broker is needed to keep the interfaces of both Axon Bank instances in sync. The commands are distributed between the containers running Axon Bank with the DistributedCommandBus.  ## Usage In order to run the single node version you may execute the following commands:  * `mvn clean install` * `java -jar web/target/axon-bank-web-0.0.1-SNAPSHOT.jar`.  The distributed version can be run using the following commands:  * `mvn clean install` * `mvn -pl web docker:build` * `docker-compose up db` (this will create and initialize the db container, you can stop the container after it has been initialized) * `docker-compose up`  Once all containers are running you can access each instance of Axon Bank by visiting [http://localhost:8080/](http://localhost:8080/) and [http://localhost:8081/](http://localhost:8081/).
abashev/vfs-s3	Amazon S3 driver for VFS (Apache Commons Virtual File System) =============================================================   Branch | Description | Build Status ------------ | ------------- | ------------ branch-2.4.x | **Current** See below | [![Build Status](https://secure.travis-ci.org/abashev/vfs-s3.png?branch=branch-2.4.x)](http://travis-ci.org/abashev/vfs-s3) branch-2.3.x | **Out-dated** | [![Build Status](https://secure.travis-ci.org/abashev/vfs-s3.png?branch=branch-2.3.x)](http://travis-ci.org/abashev/vfs-s3) branch-2.2.x | **Out-dated** | [![Build Status](https://secure.travis-ci.org/abashev/vfs-s3.png?branch=branch-2.2.x)](http://travis-ci.org/abashev/vfs-s3)  Using with Maven ----------------  Add this section to your repository configuration      <repositories>         <repository>             <id>vfs-s3.repository</id>             <name>vfs-s3 project repository</name>             <url>http://dl.bintray.com/content/abashev/vfs-s3</url>             <snapshots>                 <enabled>false</enabled>             </snapshots>         </repository>     </repositories>  And use it as dependency      <dependency>         <groupId>com.github</groupId>         <artifactId>vfs-s3</artifactId>         <version>2.4.2</version>     </dependency>   Direct downloads from Bintray ----------------  We need this patched version of _commons-vfs_ because some concurrency issues could be solved only internally  Branch | Build Status  ------------ |  ------------  commons-vfs2 | [![Download](https://api.bintray.com/packages/abashev/vfs-s3/commons-vfs2/images/download.svg) ](https://bintray.com/abashev/vfs-s3/commons-vfs2/_latestVersion)  vfs-s3 | [![Download](https://api.bintray.com/packages/abashev/vfs-s3/vfs-s3/images/download.svg) ](https://bintray.com/abashev/vfs-s3/vfs-s3/_latestVersion)   TODO for branch-2.4.x development --- 1. Merge changes back to `commons-vfs` project    Sample Java Code ----------------  	// Create bucket 	FileSystemManager fsManager = VFS.getManager(); 	FileObject dir = fsManager.resolveFile("s3://simple-bucket/test-folder/"); 	dir.createFolder();  	// Upload file to S3 	FileObject dest = fsManager.resolveFile("s3://test-bucket/backup.zip"); 	FileObject src = fsManager.resolveFile(new File("/path/to/local/file.zip").getAbsolutePath()); 	dest.copyFrom(src, Selectors.SELECT_SELF);   Running the tests ----------------- For running tests you need active credentials for AWS. You can specify them as  1.  Shell environment properties          export AWS_ACCESS_KEY=AAAAAAA         export AWS_SECRET_KEY=SSSSSSS  2. Or any standard ways how to do it in AWS SDK (iam role and so on)   **Make sure that you never commit your credentials!**
vmware/weathervane	![Weathervane](doc/images/VMW-Weathervane-Logo-SML.png)  ## Overview  Weathervane is an application-level performance benchmark designed to allow the investigation of performance tradeoffs in modern virtualized and cloud infrastructures. It consists of an application, a workload driver that can drive a realistic and repeatable load against the application, and a run-harness that automates the process of executing runs and collecting results and relevant performance data. It can be used to investigate the performance characteristics of cloud and virtual infrastructures by deploying the application on the environment of interest, driving a load against the application, and examining the resulting performance metrics. A common use-case would involve varying some component or characteristic of the infrastructure in order to compare the effect of the alternatives on application-level performance.  ![A Weathervane Deployment](doc/images/wvDeployment1.png)  The figure above shows the logical layout of a Weathervane deployment. The Weathervane application is a web-application for hosting real-time auctions. This Auction application uses a scalable architecture that allows deployments to be easily sized for a large range of user loads. A deployment of the application involves a wide variety of support services, such as caching, messaging, NoSQL data-store, and relational database tiers. These services can be deployed in VMs or in Docker containers.  The number of instances of some of the services can be scaled elastically at run time in response to a preset schedule or to monitored performance metrics. In addition, Weathervane supports deploying and driving loads against multiple instances of the Auction application in a single run, thus enabling performance investigations of resource constraints or other aspects of multi-tenancy.  The Weathervane run-harness manages the complexity of configuring, deploying, and tuning the application, automating most of the tasks involved in configuring the application services and running the benchmark.  Additional background information about Weathervane can be found at [https://blogs.vmware.com/performance/2015/03/introducing-weathervane-benchmark.html](https://blogs.vmware.com/performance/2015/03/introducing-weathervane-benchmark.html) and [http://blogs.vmware.com/performance/2017/04/weathervane-performance-benchmarking-now-open-source.html](http://blogs.vmware.com/performance/2017/04/weathervane-performance-benchmarking-now-open-source.html).  ## Warning  You should exercise caution and good judgement about where you deploy Weathervane.  Weathervane can place a significant load on system resources that can impact the performance of unrelated workloads.  As a result Weathervane is not intended to be used in production environments.    In addition, the setup procedure for Weathervane may open ports or present other security issues that are not acceptable in many secure environments.  A Weathervane deployment is intended only for a performance test environment and may not be hardened sufficiently for all security requirements.  As a result, Weathervane should never be installed on virtual machines or OS instances that have been or may ever be deployed in a production environment, or that contain sensitive or personal information.    ## Getting Started  Weathervane runs on one or more hosts or virtual machines (VMs) running the Centos 7 operating system.  It includes a script to automate the initial configuration of the VM, including the installation of all necessary services.  Simple runs of the benchmark can be performed on a single VM, but the application and workload driver can be scaled out to perform benchmark runs involving dozens of VMs.  The process of setting up and running Weathervane is covered in detail in the [Weathervane User's Guide](weathervane_users_guide.pdf).    ## Documentation  The [Weathervane User's Guide](weathervane_users_guide.pdf) covers all aspects of installing, deploying, and running the Weathervane benchmark.  ## Communicating about Weathervane  The Weathervane project team can be contacted in the following ways:  - [Slack](https://vmwarecode.slack.com/messages/weathervane): This is the primary community channel. If you don't have an @vmware.com or @emc.com email, please sign up at https://code.vmware.com/web/code/join to get a Slack invite.  - [Gitter](https://gitter.im/vmware/weathervane): Gitter is monitored but go to slack if you need a response quickly.  Feel free to contact us with questions or comments.  Please use these method to contact us for questions specific to your use of Weathervane.  We also encourage the use of Issues in the GitHub repository for any questions or suggestions that may be of general interest.  We may move discussion of questions or issues from Slack to a GitHub Issue if we feel that the question or answer may be of more general interest.  Before creating an Issue, please read [ISSUES.md](ISSUES.md).  ## Issues  The Weathervane project encourages liberal use of Issues for bugs, feature requests, documentation shortcoming, or general questions.  Please read the issue guideline in [ISSUES.md](ISSUES.md) before filing an Issue.  Be sure to follow the guidelines regarding issue labels to ensure a prompt response.  ## Releases & Major Branches  The branching and release strategy for Weathervane is discussed in [BRANCHING.md](BRANCHING.md).  ## Contributing  The Weathervane project team welcomes contributions from the community. Before you start working with Weathervane, please read our [Developer Certificate of Origin](https://cla.vmware.com/dco). All contributions to this repository must be signed as described on that page. Your signature certifies that you wrote the patch or have the right to pass it on as an open-source patch. For more detailed information, refer to [CONTRIBUTING.md](CONTRIBUTING.md).  ## License  Weathervane is available under the the [BSD 2-clause "Simplified" License](LICENSE.txt).  Weathervane uses additional components which are downloaded when the project is built and which have additional licensing terms. The licenses for these components can be found in the [license file](LICENSE.txt).
mjanicek/rembulan	[![Build Status](https://travis-ci.org/mjanicek/rembulan.svg?branch=master)](https://travis-ci.org/mjanicek/rembulan)  # Rembulan  (*Rembulan* is Javanese/Indonesian for *Moon*.)   ## About  Rembulan is an implementation of Lua 5.3 for the Java Virtual Machine (JVM), written in pure Java with minimal dependencies. The goal of the Rembulan project is to develop a correct, complete and scalable Lua implementation for running sandboxed Lua programs on the JVM.  Rembulan implements Lua 5.3 as specified by the [Lua Reference Manual](http://www.lua.org/manual/5.3/manual.html), explicitly attempting to mimic the behaviour of PUC-Lua whenever possible. This includes language-level features (such as metamethods and coroutines) and the standard library.   ## Status  The majority of language-level features is implemented, and may be expected to work. If you find behaviour that does not conform to Lua 5.3 as defined by the Lua Reference Manual, please [open a new issue](https://github.com/mjanicek/rembulan/issues).  See also the [completeness table](doc/CompletenessTable.md) that maps out the current completeness status of Rembulan with regard to PUC-Lua, in particular the standard library.  ## Frequently asked questions (FAQ)  #### What is Rembulan good for?  Lua is a small, beautifully-designed and simple-yet-powerful programming language. Lua has been traditionally used as an embedded scripting language. Rembulan aims to serve a similar purpose on the JVM, with an explicit focus on sandboxing the client Lua programs.  There are two main use-cases for Rembulan: running untrusted Lua scripts on the JVM, and enhancing Java applications by adding the ability to script them with Lua.  #### Does Rembulan implement the Lua C API?  No, at this point Rembulan requires libraries to be written against its Java interface.   #### Does Rembulan work with Lua bytecode?  No. The Lua bytecode (i.e., the bytecode generated by PUC-Lua's `luac`) is considered an implementation detail by both Rembulan and the Lua Reference Manual. Rembulan implements its own compiler and compiles to Java bytecode directly. It uses its own intermediate representation (IR) annotated with statically-inferred type information, but does not expose it to the user, and the IR has no serialisable form.   For more information about the Rembulan compiler, see the [compiler overview](doc/CompilerOverview.md).    #### How are coroutines implemented?   See [How are coroutines implemented?](doc/CoroutinesOverview.md)    ## Using Rembulan  Rembulan requires a Java Runtime Environment (JRE) version 7 or higher.  ### Documentation  Generated JavaDocs are available online:   * [Runtime module](https://mjanicek.github.io/rembulan/apidocs/rembulan-runtime/index.html)  * [Compiler](https://mjanicek.github.io/rembulan/apidocs/rembulan-compiler/index.html)  * [Standard Library](https://mjanicek.github.io/rembulan/apidocs/rembulan-stdlib/index.html)  There are also a few short texts in the `doc` folder:   * [How are coroutines implemented?](doc/CoroutinesOverview.md)  * [Overview of the Rembulan compiler](doc/CompilerOverview.md)  * [Rembulan completeness table](doc/CompletenessTable.md)  ### Building from source  To build Rembulan, you will need the following:   * Java Development Kit (JDK) version 7 or higher  * Maven version 3 or higher  Maven will pull in the remaining dependencies as part of the build process.  To fetch the latest code on the `master` branch and build it, run  ```sh git clone https://github.com/mjanicek/rembulan.git cd rembulan     mvn install ```  This will build all modules, run tests and finally install all artifacts into your local Maven repository.      #### Standalone REPL  Much like PUC-Lua, Rembulan contains a standalone REPL. This is provided in the module `rembulan-standalone`. To build the REPL, run  ```sh mvn package -DskipTests -Dmaven.javadoc.skip=true -DstandaloneFinalName=rembulan ```  The standalone REPL is packaged as a self-contained, executable [Capsule](http://www.capsule.io) and is placed in the directory `rembulan-standalone/target`.   To run the REPL:  ```sh cd rembulan-standalone/target ./rembulan-capsule.x ```  The standalone REPL mimics the behaviour or the standalone PUC-Lua interpreter and may be used as its drop-in replacement.  ``` $ ./rembulan-capsule.x Rembulan 0.1-SNAPSHOT (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60) > print("hello world!") hello world! ```  ### Using Rembulan from Maven  There are no releases yet, but snapshot artifacts are published to the Sonatype OSSRH Snapshot Repository. To use the snapshot artifacts, add the following configuration to your `pom.xml`:  ```xml <repositories>   <repository>     <id>sonatype-ossrh-snapshots</id>     <name>Sonatype OSSRH (Snapshots)</name>     <url>https://oss.sonatype.org/content/repositories/snapshots/</url>     <snapshots />   </repository> </repositories> ```  To include the **runtime** as a dependency:  ```xml <dependency>   <groupId>net.sandius.rembulan</groupId>   <artifactId>rembulan-runtime</artifactId>   <version>0.1-SNAPSHOT</version> </dependency> ```  To include the **compiler** as a dependency:  ```xml <dependency>   <groupId>net.sandius.rembulan</groupId>   <artifactId>rembulan-compiler</artifactId>   <version>0.1-SNAPSHOT</version> </dependency> ```  To include the **standard library** as a dependency:  ```xml <dependency>   <groupId>net.sandius.rembulan</groupId>   <artifactId>rembulan-stdlib</artifactId>   <version>0.1-SNAPSHOT</version> </dependency> ```  Note that `rembulan-compiler` and `rembulan-stdlib` both pull in `rembulan-runtime` as a dependency, but are otherwise independent. (I.e., to use the compiler and the standard library, you need to declare both `-compiler` and `-stdlib` as dependencies, but do not need to include `-runtime`).  ## Getting started  Rembulan compiles Lua functions into Java classes and loads them into the JVM; the compiler performs a type analysis of the Lua programs in order to generate a more tightly-typed code whenever feasible.  Since the JVM does not directly support coroutines, Rembulan treats Lua functions as state machines and controls their execution (i.e., yields, resumes and pauses) using exceptions. Since the Rembulan runtime retains control of the control state, this technique is also used to implement CPU accounting and scheduling of asynchronous operations.  #### Example: Hello world    The following snippet loads the Lua program `print('hello world!')`, compiles it, loads it into a (non-sandboxed) state, and runs it:  (From [`rembulan-examples/.../HelloWorld.java`](rembulan-examples/src/main/java/net/sandius/rembulan/examples/HelloWorld.java))  ```java String program = "print('hello world!')";  // initialise state StateContext state = StateContexts.newDefaultInstance(); Table env = StandardLibrary.in(RuntimeEnvironments.system()).installInto(state);  // compile ChunkLoader loader = CompilerChunkLoader.of("hello_world"); LuaFunction main = loader.loadTextChunk(new Variable(env), "hello", program);  // execute DirectCallExecutor.newExecutor().call(state, main); ```  The output (printed to `System.out`) is:  ``` hello world! ```  #### Example: CPU accounting  Lua functions can be called in a mode that automatically pauses their execution once the given number of operations has been performed:  (From [`rembulan-examples/.../InfiniteLoop.java`](rembulan-examples/src/main/java/net/sandius/rembulan/examples/InfiniteLoop.java))  ```java String program = "n = 0; while true do n = n + 1 end";  // initialise state StateContext state = StateContexts.newDefaultInstance(); Table env = StandardLibrary.in(RuntimeEnvironments.system()).installInto(state);  // compile ChunkLoader loader = CompilerChunkLoader.of("infinite_loop"); LuaFunction main = loader.loadTextChunk(new Variable(env), "loop", program);  // execute at most one million ops DirectCallExecutor executor = DirectCallExecutor.newExecutorWithTickLimit(1000000);  try {     executor.call(state, main);     throw new AssertionError();  // never reaches this point! } catch (CallPausedException ex) {     System.out.println("n = " + env.rawget("n")); } ```  Prints:  ``` n = 199999 ```  The [`CallPausedException`](https://mjanicek.github.io/rembulan/apidocs/rembulan-runtime/net/sandius/rembulan/exec/CallPausedException.html) contains a *continuation* of the call. The call can be resumed: the pause is transparent to the Lua code, and the loop does not end with an error (it is merely paused).  #### Further examples  For further examples, see the classes in [`rembulan-examples/src/main/java/net/sandius/rembulan/examples`](rembulan-examples/src/main/java/net/sandius/rembulan/examples).  ### Project structure  Rembulan is a multi-module Maven build, consisting of the following modules that are deployed to Sonatype OSSRH:   * `rembulan-runtime` ... the core classes and runtime;  * `rembulan-compiler` ... a compiler of Lua sources to Java bytecode;  * `rembulan-stdlib` ... the Lua standard library;  * `rembulan-standalone` ... standalone REPL, a (mostly) drop-in replacement                              for the `lua` command from PUC-Lua.  There are also auxiliary modules that are not deployed:   * `rembulan-tests` ... project test suite, including benchmarks from                         the Benchmarks Game;  * `rembulan-examples` ... examples of the Rembulan API.                          ## Contributing  Contributions of all kinds are welcome!   ## License  Rembulan is licensed under the Apache License Version 2.0. See the file [LICENSE.txt](LICENSE.txt) for details.
commercetools/commercetools-jvm-sdk	# commercetools JVM SDK  ![SPHERE.IO icon](https://admin.sphere.io/assets/images/sphere_logo_rgb_long.png)  [![][travis img]][travis] [![][maven img]][maven] [![][license img]][license]  [](the link definitions are at the bottom)  The JVM SDK enables developers to use Java 8 methods and objects to communicate with the [commercetools platform](http://www.commercetools.com/) (formerly SPHERE.IO) rather than using plain HTTP calls. Users gain type-safety, encapsulation, IDE auto completion and an internal domain specific language to discover and formulate valid requests.  ## Using the SDK  * install [Java 8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) * (since May 19, 2016 new link) [<strong>Javadoc</strong>](http://commercetools.github.io/commercetools-jvm-sdk/apidocs/index.html), there you find also code snippets and insights     * [Getting Started](http://commercetools.github.io/commercetools-jvm-sdk/apidocs/io/sphere/sdk/meta/GettingStarted.html)     * [Release Notes](http://commercetools.github.io/commercetools-jvm-sdk/apidocs/io/sphere/sdk/meta/ReleaseNotes.html)     * [Contributing](http://commercetools.github.io/commercetools-jvm-sdk/apidocs/io/sphere/sdk/meta/ContributorDocumentation.html)   ## Installation  ### Java SDK with Maven  ````xml <dependency>   <groupId>com.commercetools.sdk.jvm.core</groupId>   <artifactId>commercetools-models</artifactId>   <version>1.24.0</version> </dependency> <dependency>   <groupId>com.commercetools.sdk.jvm.core</groupId>   <artifactId>commercetools-java-client</artifactId>   <version>1.24.0</version> </dependency>  <!-- experimental --> <dependency>   <groupId>com.commercetools.sdk.jvm.core</groupId>   <artifactId>commercetools-convenience</artifactId>   <version>1.24.0</version> </dependency> ```` * http://search.maven.org/#artifactdetails%7Ccom.commercetools.sdk.jvm.core%7Ccommercetools-models%7C1.24.0%7Cjar * http://search.maven.org/#artifactdetails%7Ccom.commercetools.sdk.jvm.core%7Ccommercetools-java-client%7C1.24.0%7Cjar * http://search.maven.org/#artifactdetails%7Ccom.commercetools.sdk.jvm.core%7Ccommercetools-convenience%7C1.24.0%7Cjar  ### Modules * `commercetools-java-client`: alias for commercetools-java-client-ahc-2_0 * `commercetools-java-client-apache-async`: uses Apache HTTP client * `commercetools-java-client-ahc-1_8`: uses async HTTP client 1.8 * `commercetools-java-client-ahc-1_9`: uses async HTTP client 1.9 (AHC 1.9 is incompatible to AHC 1.8) * `commercetools-java-client-ahc-2_0`: uses async HTTP client 2.0 (do not mix it with the other AHC modules) * `commercetools-models`: models which do not depend to a client implementation  ### Java SDK with gradle  ```` repositories {     mavenCentral() }  dependencies {     def jvmSdkVersion = "1.24.0"     compile "com.commercetools.sdk.jvm.core:commercetools-models:$jvmSdkVersion"     compile "com.commercetools.sdk.jvm.core:commercetools-java-client:$jvmSdkVersion"         compile "com.commercetools.sdk.jvm.core:commercetools-convenience:$jvmSdkVersion" } ````  ### Play/Scala SDK with SBT  see https://github.com/commercetools/commercetools-jvm-sdk-scala-add-ons  ### reactive streams * https://github.com/commercetools/commercetools-jvm-sdk-reactive-streams-add-ons  ### JVM SDK Contrib  Useful code from external developers  * https://github.com/commercetools/commercetools-jvm-sdk-contrib * contains also ProductImageUploadCommand from http://dev.commercetools.com/http-api-projects-products.html#upload-a-product-image   ## Short-term roadmap * https://github.com/commercetools/commercetools-jvm-sdk/milestones * https://waffle.io/commercetools/commercetools-jvm-sdk  ## Open Source Examples * [Sunrise Java](https://github.com/commercetools/commercetools-sunrise-java) - a shop using Play Framework 2.x with Handlebars.java as template engine, Google Guice for DI * [Donut](https://github.com/commercetools/commercetools-donut) - single product subscription shop example with Play Framework 2.x and Twirl (Plays default) as template engine * [commercetools Spring MVC archetype](https://github.com/commercetools/commercetools-spring-mvc-archetype) - template integrating the SDK with Spring DI and Spring MVC and showing just some products, thymeleaf template engine * [Reproducer Example](https://github.com/commercetools/commercetools-jvm-sdk-reproducer-example) - a demo which shows how to reproduce errors  ## Stability  1. Experimental features in the API are also experimental features of the SDK.     * this includes for example         * payments         * nested product attributes 1. The dependencies will only be updated in the next major version to improve stability. Of course, if bugs in libraries *occur*, we may need to update. 1. JVM SDK test dependencies and build tools can be updated because they don't affect the production code. 1. The JVM SDK has an abstract HTTP client layer so old or new http client versions can be used. 1. order import is experimental 1. the search methods with lambda parameters are beta `ProductProjectionSearch.ofCurrent().withQueryFilters(m -> m.categories().id().containsAll(categoryIds1))` 1. getters of draft objects might change since new HTTP API features can introduce polymorphism 1. final classes without public constructors can be transformed into an interface  ## Executing integration tests  1. create a NEW API client in the Admin Center (`https://admin.sphere.io/YOUR_PROJECT_KEY/developers/clients`) with all available permissions (at the time of writing it is manage_payments manage_my_profile manage_orders view_products view_customers view_payments view_types manage_my_orders manage_types manage_customers manage_products view_orders manage_project), the by default created client has just `manage_project` 1. in the Admin Center in the "DANGER ZONE" activate the checkbox "ENABLE MESSAGES" within the "Messages Settings" 1. set "de", "de-AT", "en" as languages in the Admin Center 1. set at least one country in the Admin Center 1. create a file "integrationtest.properties" inside the project root 1. fill it with the credentials of a new empty commercetools project which is for testing;  ``` projectKey=YOUR project key without quotes clientId=YOUR client id without quotes clientSecret=YOUR client secret without quotes apiUrl=https://api.sphere.io authUrl=https://auth.sphere.io ```  1. use `./mvnw verify` to execute all integration tests 1. use `./mvnw -Dit.test=*Product* -DfailIfNoTests=false verify` to execute all integration test classes which have "Product" in the class name 1. for running the unit tests use `./mvnw test` 1. alternatively use your IDE to execute the tests 1. for some IDEs like IntelliJ IDEA you need to add the Javac flag "-parameters", then also rebuild the whole project to apply the change   [](definitions for the top badges)  [travis]:https://travis-ci.org/commercetools/commercetools-jvm-sdk [travis img]:https://travis-ci.org/commercetools/commercetools-jvm-sdk.svg?branch=master  [maven]:http://search.maven.org/#search|gav|1|g:"com.commercetools.sdk.jvm.core"%20AND%20a:"commercetools-models" [maven img]:https://maven-badges.herokuapp.com/maven-central/com.commercetools.sdk.jvm.core/commercetools-models/badge.svg  [license]:LICENSE.md [license img]:https://img.shields.io/badge/License-Apache%202-blue.svg
NASAWorldWind/WorldWindAndroid	<img src="https://worldwind.arc.nasa.gov/css/images/nasa-logo.svg" height="100"/>  # WorldWind Android  [![Build Status](https://travis-ci.org/NASAWorldWind/WorldWindAndroid.svg?branch=develop)](https://travis-ci.org/NASAWorldWind/WorldWindAndroid) [![Download](https://api.bintray.com/packages/nasaworldwind/maven/WorldWindAndroid/images/download.svg)](https://bintray.com/nasaworldwind/maven/WorldWindAndroid/_latestVersion)   3D virtual globe API for Android, developed by NASA. Provides a geographic context with high-resolution terrain, for  visualizing geographic or geo-located information in 3D and 2D. Developers can customize the globe's terrain and  imagery. Provides a collection of shapes for displaying and interacting with geographic data and representing a range of  geometric objects.  - [worldwind.arc.nasa.gov](https://worldwind.arc.nasa.gov) has setup instructions, developers guides, API documentation and more - [WorldWind Forum](https://forum.worldwindcentral.com) provides help from the WorldWind community - [Android Studio](https://developer.android.com/sdk/) is used by the NASA WorldWind development team  ## Download  Download the [latest release](https://bintray.com/nasaworldwind/maven/WorldWindAndroid/_latestVersion) or grab via Gradle: ```groovy compile 'gov.nasa.worldwind.android:worldwind:0.7.0' ```  ## Snapshots  Get development build snapshots with the latest features and bug fixes from [oss.jfrog.org](https://oss.jfrog.org/): ```groovy repositories {     maven {         url 'https://oss.jfrog.org/artifactory/oss-snapshot-local'     } }  ...  compile 'gov.nasa.worldwind.android:worldwind:0.8.0-SNAPSHOT' ```  ## Releases and Roadmap  Official WorldWind Android releases have the latest stable features, enhancements and bug fixes ready for production use.  - [GitHub Releases](https://github.com/NASAWorldWind/WorldWindAndroid/releases/) documents official releases - [GitHub Milestones](https://github.com/NASAWorldWind/WorldWindAndroid/milestones) documents upcoming releases and the development roadmap - [Bintray](https://bintray.com/nasaworldwind/maven/WorldWindAndroid) contains official release binaries and Maven artifacts - [Travis CI](https://travis-ci.org/NASAWorldWind/WorldWindAndroid) provides continuous integration and release automation  ## License      NASA WORLDWIND      Copyright (C) 2001 United States Government     as represented by the Administrator of the     National Aeronautics and Space Administration.     All Rights Reserved.      NASA OPEN SOURCE AGREEMENT VERSION 1.3      This open source agreement ("agreement") defines the rights of use, reproduction,     distribution, modification and redistribution of certain computer software originally     released by the United States Government as represented by the Government Agency     listed below ("Government Agency"). The United States Government, as represented by     Government Agency, is an intended third-party beneficiary of all subsequent     distributions or redistributions of the subject software. Anyone who uses, reproduces,     distributes, modifies or redistributes the subject software, as defined herein, or any     part thereof, is, by that action, accepting in full the responsibilities and obligations      contained in this agreement.      Government Agency: National Aeronautics and Space Administration (NASA)     Government Agency Original Software Designation: ARC-15166-1     Government Agency Original Software Title: NASA WorldWind     User Registration Requested. Please send email with your contact information to Patrick.Hogan@nasa.gov     Government Agency Point of Contact for Original Software: Patrick.Hogan@nasa.gov      You may obtain a full copy of the license at:          https://worldwind.arc.nasa.gov/LICENSE.html
dCache/dcache	dCache ======  <img src="modules/dcache-webadmin/src/main/resources/org/dcache/webadmin/view/panels/header/dCache.png" height="165" width="200">  __dCache__ is a system for storing and retrieving huge amounts of data, distributed among a large number of heterogeneous server nodes, under a single virtual filesystem tree with a variety of standard access methods. Depending on the Persistency Model, dCache provides methods for exchanging data with backend (tertiary) Storage Systems as well as space management, pool attraction, dataset replication, hot spot determination and recovery from disk or node failures. Connected to a tertiary storage system, the cache simulates unlimited direct access storage space. Data exchanges to and from the underlying HSM are performed automatically and invisibly to the user. Beside HEP specific protocols, data in dCache can be accessed via __NFSv4.1 (pNFS)__, __FTP__ as well as through __WebDav__.  Contributors ============ dCache is a joined effort between [Deutsches Elektronen-Synchrotron DESY] (http://www.desy.de), [Fermi National Accelerator Laboratory] (http://www.fnal.gov) and [Nordic DataGrid Facility] (http://www.ndgf.org).  License =======  The project is licensed under __AGPL v3__. Some parts licensed under __BSD__ and __LGPL__. See the source code for details.  ####For more info, check official [dCache.ORG] (http://www.dcache.org) web page.####  Getting Started ===============  The file [BUILDING.md](BUILDING.md) describes how to compile dCache code and build various packages.  The file also describes how to create the __system-test__ deployment, which provides a quick and easy way to get a working dCache.  Running system-test requires no special privileges and all the generated files reside within the code-base.  How to contribute =================  **dCache** uses the linux kernel model where git is not only source repository, but also the way to track contributions and copyrights.  Each submitted patch must have a "Signed-off-by" line.  Patches without this line will not be accepted.  The sign-off is a simple line at the end of the explanation for the patch, which certifies that you wrote it or otherwise have the right to pass it on as an open-source patch.  The rules are pretty simple: if you can certify the below: ```      Developer's Certificate of Origin 1.1      By making a contribution to this project, I certify that:      (a) The contribution was created in whole or in part by me and I          have the right to submit it under the open source license          indicated in the file; or      (b) The contribution is based upon previous work that, to the best         of my knowledge, is covered under an appropriate open source         license and I have the right under that license to submit that         work with modifications, whether created in whole or in part         by me, under the same open source license (unless I am         permitted to submit under a different license), as indicated         in the file; or      (c) The contribution was provided directly to me by some other         person who certified (a), (b) or (c) and I have not modified         it.      (d) I understand and agree that this project and the contribution         are public and that a record of the contribution (including all         personal information I submit with it, including my sign-off) is         maintained indefinitely and may be redistributed consistent with         this project or the open source license(s) involved.  ``` then you just add a line saying ( git commit -s )      Signed-off-by: Random J Developer <random@developer.example.org>  using your real name (sorry, no pseudonyms or anonymous contributions.)
UrbanCode/terraform	**************************************************   _____ ___ ___ ___    _   ___ ___  ___ __  __   |_   _| __| _ \ _ \  /_\ | __/ _ \| _ \  \/  |    | | | _||   /   / / _ \| _| (_) |   / |\/| |    |_| |___|_|_\_|_\/_/ \_\_| \___/|_|_\_|  |_|  **************************************************                                                 Installation Instructions **************************************************   Requirements:    Java 1.6    vmrun (only for VMware environments)    Maven (for building/dependencies)      To use the VMware portion of Terraform, you will need to have access to   a VCenter server.    To use the Amazon Web Services portion of Terraform, you will need to have    an AWS Account with the EC2 services. This service is free, but see Amazon   for limitations.       (to get an AWS account: https://console.aws.amazon.com/console/home )    Windows Azure with Terraform wraps around the Azure Node.js-based CLI.    That tool depends on Node.js and npm, Node's package manager.   Follow the installation instructions here to get it:          https://www.windowsazure.com/en-us/manage/linux/other-resources/command-line-tools/        Since that command line tool is only available on Unix systems,    Terraform with Azure cannot currently be run on Windows. Support for a    wrapper around the Window's based Powershell commandlets may come in the future.      To use Rackspace Next Gen Cloud Servers as your provider with Terraform,   you must have a Rackspace account and an API key. Your account must   be set up to use whatever services you are requesting.   Some services (like Cloud Networks as of 11/5/12) are in a closed beta.   Building Terraform From Source ======================================   Set your current working directory to the extracted terraform directory.   To build terraform, run:      $ mvn package    Basic Installation ======================================   You will find a shell script in terraform/bin. This is used   to run Terraform.     To run terraform, use the provided terraform script for your operating system:      Unix     ./terraform [command] [input-xml-file] [input-credentials-file] [prop1=val1 prop2=val2 ...]     Windows     terraform [command] [input-xml-file] [input-credentials-file] [prop1=val1 prop2=val2 ...]          Allowed commands for all providers: create, destroy, suspend, resume     Additional allowed commands for VMware: snapshot    Amazon Web Services environments do not require any command line properties.    For more information on the AWS portion of Terraform, see the AWS section below.      Windows Azure environments do not require any command line properties.    For more information on the Azure portion of Terraform, see the Microsoft section below.    Starting a VMware environment requires specific command line properties.    See the VMware section below for more details.   Environment Templates ======================================   Environment Template examples can be found in example-config/xml-templates    A template is the structure of an environment, the blueprints. They are    formatted in xml and have a required structure to them. The root element of    every template must be <context> and must have an xmlns attribute. Underneath   the <context> must be an <environment> element.   What sits under the <environment> depends on the type of environment you want    to create. See each individual provider section below.    Properties can be used in the XML template with ${property.name}. The    properties are passed in as arguments on the command line (anything after the   3rd argument will be interpreted as a property).      e.g. if you set uDeploy.host=12.34.56.78 and uDeploy.port=7918, you can      then use these with        <param value="${uDeploy.host}"/> <param value="${uDeploy.host}"/>    Some properties are defined by the provider. See each provider's section    below (VMware / AWS / Microsoft).   Conf File Information ======================================   [  Global  ]    * log4j.properties   Located in the $TERRAFORM_HOME/conf folder.   This file contains the properties used to configure logging.       [   AWS    ]      [  VMware  ]     * ippool.conf   Located in the $TERRAFORM_HOME/conf folder.   Contains the start and end points for the IP pool used by vCenter.   The start is the begining IP address to start allocating for VMs.   It should be in the following format:    start=192.168.2.1    end=192.168.2.250       The files below are created in an environment-specific $TERRAFORM_HOME/temp    folder and deleted when the environment is deleted.    * .temp files - these are the beginning forms of the conf files listed below.     Content is added to them and placed in the appropriate conf file.    The following files are packaged with Terraform for use with VMWare.    * dhcpd.conf  This is the standard DHCPD configuration file for dhcpd, the Internet Systems   Consortium DHCP Server.    See http://linux.die.net/man/5/dhcpd.conf    * interfaces  This is a debian/ubuntu networking configuration file for the interfaces on   the router machine.    See http://support.arpnetworks.com/kb/vps/example-etcnetworkinterfaces-for-debian-and-ubuntu    * iptables.conf    This is the standard iptables configuration file for configuring a firewall   and allowable network traffic.    See http://linux.die.net/man/8/iptables    * isc-dhcp-server    This file indicates which network interfaces will be serving up DHCP   addresses. It is typically located in /etc/default and referenced by   /etc/init.d/isc-dhcp-server .  The only required content of this file is one line, as below  INTERFACES="eth1 eth2"  Quotation marks included. All network interfaces to be served DHCP addresses   should be listed, separated by one space.    ======================================
btrplace/scheduler	# Btrplace scheduler #  This repository contains the main sources of the flexible VM scheduler BtrPlace (see http://www.btrplace.org)  [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/btrplace/chat?utm_source=share-link&utm_medium=link&utm_campaign=share-link) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.btrplace/scheduler/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.btrplace/scheduler)   [![Build Status](https://api.travis-ci.org/btrplace/scheduler.svg?branch=master)](https://travis-ci.org/btrplace/scheduler) [![codecov.io](https://codecov.io/github/btrplace/scheduler/coverage.svg?branch=master)](https://codecov.io/github/btrplace/scheduler?branch=master) [![Coverity](https://scan.coverity.com/projects/8380/badge.svg)](https://scan.coverity.com/projects/btrplace-scheduler) [![Codacy Badge](https://api.codacy.com/project/badge/grade/ccaa68ef1c474d4e9f079de2b10d2672)](https://www.codacy.com/app/fabien-hermenier/scheduler) [![Dependency Status](https://www.versioneye.com/user/projects/56fa5da135630e003e0a8aed/badge.svg?style=flat-square)](https://www.versioneye.com/user/projects/56fa5da135630e003e0a8aed)  Contact: fabien.hermenier@unice.fr  ## Usage ##  Releases are available via Maven Central (see http://search.maven.org/#search%7Cga%7C1%7Corg.btrplace).  Snapshot versions are only available through a dedicated repository. Add the following entry in your `pom.xml` to get them:  ```xml <repositories>         <repository>             <id>sonatype-snapshots</id>             <url>https://oss.sonatype.org/content/repositories/snapshots</url>             <layout>default</layout>         </repository> </repositories> ```  Next, just declare the useful dependencies:  * `org.btrplace:scheduler-api`: the API defining a VM scheduler and the element it manipulates * `org.btrplace:scheduler-choco`: the default implementation of the VM scheduler using the Constraint Programming solver Choco * `org.btrplace:scheduler-json`: to serialize models using JSON * `org.btrplace:scheduler-split`: to split the instances to solve * `org.btrplace:btrpsl`: a scripting language to express constraints * `org.btrplace:bench`: a simple CLI to perform benchmarks * `org.btrplace:scheduler-examples`: the examples illustrated in the [tutorials](https://github.com/btrplace/scheduler/wiki/Tutorials) section  ## Getting Started ##  See the [tutorials](https://github.com/btrplace/scheduler/wiki/Tutorials)  ## Documentation ##  ### API documentation ###  The javadoc for every version is available as a jar in the repository. the HTML javadoc is available at:  * http://www.btrplace.org/apidocs for the last release * http://www.btrplace.org/apidocs-next for the next version to release (the code in the `master` branch)  ### General documentation ###  See the [wiki](https://github.com/btrplace/scheduler/wiki)  ## Contributing ##  Anyone can contribute to the project, from the source code to the documentation. In order to ease the process, see the [contribution guide](CONTRIBUTING.md).  ## Building from sources ##  Requirements: * JDK 8+ * maven 3+  The source of the released versions are directly available in the `Tag` section. You can also download them using github features. Once downloaded, move to the source directory then execute the following command to make the jar:      $ mvn clean install  If the build succeeded, the resulting jars will be automatically installed in your local maven repository.   ## Copyright ## Copyright (c) 2015 University of Nice-Sophia Antipolis. See `LICENSE.txt` for details
MissionCriticalCloud/cosmic	[![Build Status](https://beta-jenkins.mcc.schubergphilis.com/buildStatus/icon?job=cosmic/0001-cosmic-master-build)](https://beta-jenkins.mcc.schubergphilis.com/job/cosmic/job/0001-cosmic-master-build/)  # Cosmic  Cosmic is open source software designed to deploy and manage large networks of virtual machines, as a highly available, highly scalable Infrastructure as a Service (IaaS) cloud computing platform. Cosmic provides an on-premises (private) cloud offering, or as part of a hybrid cloud solution.  Cosmic is a turnkey solution that includes the entire "stack" of features most organizations want with an cloud: compute orchestration, Network-as-a-Service, user and account management, a full and open native API, resource accounting, and a first-class User Interface (UI).  Cosmic currently supports the following hypervisors: KVM and XenServer. Support for other hypervisors can be added if contributors can provide the infrastructure to test against.  Users can manage their cloud via Web interface, command line tools, and/or a full-featured query based API.  ## Getting Source Repository  Cosmic officials Git repository is located at:      https://github.com/MissionCriticalCloud/cosmic  ## Building from Source  Cosmic requires: - Java 8 - Maven settings configured to use [Cosmic's Nexus repository](https://beta-nexus.mcc.schubergphilis.com) (see [Maven settings](#maven-settings) below)  In order to build Cosmic, you have to follow these steps:      git clone https://github.com/MissionCriticalCloud/cosmic.git     cd cosmic     mvn clean install -P developer,systemvm  The steps above will build the essentials to get Cosmic management server working. Besides that, you will also need a hypervisor. See our [build stream configuration](https://beta-jenkins.mcc.schubergphilis.com) for more details.  This will run the UI and API:      cd cosmic-client     mvn -pl :cloud-client-ui jetty:run  Go to your brouwser and type: [http://localhost:8080/client] (http://localhost:8080/client)  ### Maven settings  Configure maven to look for artefacts in [Cosmic's Nexus repository](https://beta-nexus.mcc.schubergphilis.com):  ```vim ~/.m2/settings.xml ```  ```xml <settings>   <mirrors>      <mirror>       <!--This sends everything else to /public -->       <id>beta-nexus</id>       <mirrorOf>*</mirrorOf>       <url>https://beta-nexus.mcc.schubergphilis.com/content/groups/public</url>     </mirror>   </mirrors>   <profiles>       <profile>         <id>beta-nexus</id>         <!--Enable snapshots for the built in central repo to direct -->         <!--all requests to nexus via the mirror -->         <repositories>           <repository>             <id>central</id>             <url>http://central</url>             <releases><enabled>true</enabled></releases>             <snapshots><enabled>true</enabled></snapshots>           </repository>         </repositories>        <pluginRepositories>           <pluginRepository>             <id>central</id>             <url>http://central</url>             <releases><enabled>true</enabled></releases>             <snapshots><enabled>true</enabled></snapshots>           </pluginRepository>         </pluginRepositories>       </profile>     </profiles> </settings> ``` Either enable the `beta-nexus` profile on the command line, or make it enabled by default in your settings file. ```xml ...   <activeProfiles>     <!--make the profile active all the time -->     <activeProfile>beta-nexus</activeProfile>   </activeProfiles> ... ```  ## Links  Cosmic is a fork of Apache CloudStack and its API is mostly backwards compatible with CloudStack's API. So, all the documentation can be accessed from http://docs.cloudstack.apache.org.  [API documentation](http://apidoc.cosmiccloud.io/) for Cosmic is on a separate page.  ## Getting Involved  Please, join our Slack channel for more details:  * [Mission Critical Cloud](https://missioncriticalcloud.slack.com)  If you want an invite, please e-mail `int-cloud@schubergphilis.com` and we'll welcome you on Slack soon.  ## Development environment "The Bubble"  For ease of development, testing and evaluating we created a project called [The Bubble](https://github.com/MissionCriticalCloud/bubble-blueprint). The Bubble is a single host or VM, that hosts all the VMs to build a Cosmic Cloud. A special [Bubble Toolkit](https://github.com/MissionCriticalCloud/bubble-toolkit) project exists to automate common tasks. This is also where our CI scripts live.  ## Submitting code  Feel free to open a Pull Request. Our [CI system](https://beta-jenkins.mcc.schubergphilis.com/job/cosmic/) will automatically kick-in and build a real cloud based on your branch. The test results will be reported on the Gighub Pull Request. Our policy is to only merge when Pull Requests builds are green. After merge, another build is started to verify it once in master.  ## Reporting Security Vulnerabilities  If you've found an issue that you believe is a security vulnerability in a released version of Cosmic, please report it to `int-cloud@schubergphilis.com` with details about the vulnerability, how it might be exploited, and any additional information that might be useful.  ## Sponsors  The Cosmic team would like to recognize and thank the contributions of the following entities to the success of this project and resulting system: * [Schuberg Philis](https://www.schubergphilis.com): who is responsible for the birth of the Cosmic project (and all other related projects, see [vagrant-cloudstack](https://github.com/MissionCriticalCloud/vagrant-cloudstack) or [bubble-blueprint](https://github.com/MissionCriticalCloud/bubble-blueprint) for some examples) and resulting systems, and is relentlessly supporting all the development costs of Cosmic. * [JetBrains](https://www.jetbrains.com): who's motto is "Create anything" and by providing our team with free access to their amazing development environment suite (IntelliJ IDEA - Ultimate edition) allows us to create Cosmic.  ## License  Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.  Please see the [LICENSE](LICENSE) file included in the root directory of the source tree for extended license details.
subutai-io/base	# Subutai Server repository  This repository contains source code of Subutai Server Project. This is a multi-module Maven Java project.  ## Building the project  ###Prerequisites  To build the project, you need to have the following tools:  - Oracle JDK 7 or later    [Download Page (JDK 8)](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)      ###### Installation (manual way)    - Download the 32-bit or 64-bit Linux "compressed binary file" - it has a ".tar.gz" file extension.     - Uncompress it      `tar -xvf jdk-8-linux-i586.tar.gz`   (32-bit)      `tar -xvf jdk-8-linux-x64.tar.gz`   (64-bit)     The JDK 8 package is extracted into `./jdk1.8.0` directory. N.B.: Check carefully this folder name since Oracle seem to    change this occasionally with each update.     - Now move the JDK 8 directory to `/usr/lib`      ```bash     sudo mkdir -p /usr/lib/jvm     sudo mv ./jdk1.8.0 /usr/lib/jvm/     ```     - Now run      ```bash     sudo update-alternatives --install "/usr/bin/java" "java" "/usr/lib/jvm/jdk1.8.0/bin/java" 1     sudo update-alternatives --install "/usr/bin/javac" "javac" "/usr/lib/jvm/jdk1.8.0/bin/javac" 1     sudo update-alternatives --install "/usr/bin/javaws" "javaws" "/usr/lib/jvm/jdk1.8.0/bin/javaws" 1     ```     This will assign Oracle JDK a priority of 1, which means that installing other JDKs will [replace it as the default](http://askubuntu.com/q/344059/23678). Be sure to use a higher priority if you want Oracle JDK to remain the default.     - Correct the file ownership and the permissions of the executables:     ```bash    sudo chmod a+x /usr/bin/java    sudo chmod a+x /usr/bin/javac    sudo chmod a+x /usr/bin/javaws    sudo chown -R root:root /usr/lib/jvm/jdk1.8.0    ```     N.B.: Remember - Java JDK has many more executables that you can similarly install as above. `java`, `javac`, `javaws` are probably the most frequently required. This [answer lists](http://askubuntu.com/a/68227/14356) the other executables available.     - Run     ```bash    sudo update-alternatives --config java    ```     You will see output similar to the one below - choose the number of jdk1.8.0 - for example `3` in this list (unless you have have never installed Java installed in your computer in which case a sentence saying "There is nothing to configure" will appear):        ```bash    $ sudo update-alternatives --config java    There are 3 choices for the alternative java (providing /usr/bin/java).        Selection    Path                                            Priority   Status       ------------------------------------------------------------       0            /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java   1071      auto mode       1            /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java   1071      manual mode     * 2            /usr/lib/jvm/jdk1.7.0/bin/java                   1         manual mode       3            /usr/lib/jvm/jdk1.8.0/bin/java                   1         manual mode     Press enter to keep the current choice[*], or type selection number: 3    update-alternatives: using /usr/lib/jvm/jdk1.8.0/bin/java to provide /usr/bin/java (java) in manual mode    ```        Repeat the above for:        ```bash    sudo update-alternatives --config javac    sudo update-alternatives --config javaws    ```    __Note for NetBeans users!__     You need to [set the new JDK as default](http://stackoverflow.com/questions/2809366/changing-java-platform-on-which-netbeans-runs/2809447#2809447) editing the configuration file.      ###### Ubuntu: Installing Java 8   ```bash   sudo add-apt-repository ppa:webupd8team/java   sudo apt-get update   sudo apt-get install oracle-java8-installer   sudo apt-get install oracle-java8-unlimited-jce-policy   ```    ###### Setting JAVA_HOME      To check if JAVA_HOME is set or not, execute      ```bash   echo $JAVA_HOME   ```      If the result is empty or points to the version of Java that is not suitable, you need to set it.       ```bash   update-java-alternatives -l   sudo nano /etc/profile      Add following lines at the end:   export JAVA_HOME="path that you found in update-java-alternatives for your JDK without quotes"   export PATH=$JAVA_HOME/bin:$PATH      Save file      reset   ```      Check again.    - Unlimited strength files (specific for Java version)    [Download Page (JDK 8)](http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)      ```bash   unzip jce_policy-8.zip   cd UnlimitedJCEPolicyJDK8   sudo cp local_policy.jar US_export_policy.jar $JAVA_HOME/jre/lib/security   ```    OR    ```bash   sudo apt-get install oracle-java8-unlimited-jce-policy   ```    - Maven version 3.2.2 or later    [Download Page](https://maven.apache.org/download.cgi)      Before installing, remove older versions:      ```bash   sudo apt-get purge maven maven2 maven3   ```      ###### Installation (manual way - works only on Ubuntu 15.04)      - Unzip the binary with tar      ```bash   tar -zxf apache-maven-3.3.3-bin.tar.gz   ```      - Move the application directory to `/usr/local`      ```bash   sudo cp -R apache-maven-3.3.3 /usr/local   ```    - Make a soft link in `/usr/bin` for universal access of `mvn`      ```bash   sudo ln -s /usr/local/apache-maven-3.3.3/bin/mvn /usr/bin/mvn   ```      ###### Installation (apt repo - older versions)      ```bash   sudo apt-add-repository ppa:andrei-pozolotin/maven3   sudo apt-get update   sudo apt-get install maven3   ```      ###### Verify mvn installation    ```bash   mvn --version      Apache Maven 3.3.3 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T17:29:23+00:00)   Maven home: /usr/local/apache-maven-3.2.5   Java version: 1.7.0_80, vendor: Oracle Corporation   Java home: /usr/lib/jvm/java-7-oracle/jre   Default locale: en_US, platform encoding: UTF-8   OS name: "linux", version: "3.13.0-48-generic", arch: "amd64", family: "unix" ```  ###Build steps  - Clone the project by using:      `git clone https://github.com/subutai-io/base.git`  - Start maven build:      ```bash     cd base/management     mvn clean install     ```   If you want to create a Debian package, add additional flag        ```bash   mvn clean install -P deb     ```  After this you will have `management/server/server-karaf/target` directory with **subutai-{version}.tar.gz** archive which contains custom Karaf distribution of SS application.  Untar it to some directory and execute `{distr}/bin/karaf`.  After that go to `https://your_host_ip:8443` in your browser.   ###Development  For development purposes, access to management container can be opened by executing the following commands on RH-with-MH:  ``` sudo iptables -t nat -A PREROUTING -i eth0 -p tcp -m tcp --dport 2222 -j DNAT --to-destination 10.10.10.1:22 sudo iptables -t nat -A PREROUTING -i eth0 -p tcp -m tcp --dport 5005 -j DNAT --to-destination 10.10.10.1:5005 ```  This would open ports **2222** for ssh access and **5005** for debugger.  **CAUTION**: this must be used for development only. Highly dangerous to do this in production!
eBay/Winder	<img src="https://github.com/eBay/Winder/blob/master/docs/Winder-small.png"/>  # Winder Winder is a simple state machine based on Quartz Scheduler.  It helps to write multiple steps tasks on Quartz Scheduler.  Winder derived from a state machine which is widly used in eBay Cloud. eBay Platform As A Service(PaaS) uses it to deploy software to hundreds of thousands virtual machines.  The state machine is also using for maintaining hundreds of thousands VMs and keeping them healthy.  # Why do you need Winder?  ####Case 1: Your job has multiple tasks that can be finished in couple seconds, so that you have to let something take care of it. Winder can do it in background.  ####Case 2: You have bunch of items, each item needs to be processed in a couple of steps. Some of the steps can be guaranteed finishing in couple seconds. Winder can help you define the steps and let them work automatically.  ####Case 3: You have some batch tasks, they should be executed every day.  Winder can get this done easily.  # Production use cases  Production Use Cases: widely used in many distributed systems which eBay Cloud relies on.  **Software Deployment/PaaS**: As the key part of eBay Cloud, Platform As A Service(PaaS) is widely used in eBay. Developer uses it to roll out code. Winder takes the key role in PaaS.  **Virtual machine maintenance**: eBay has hundreds of thousands VMs. Keeping them healthy and available is very important. Winder makes this possible.  **Provisioning container or VM**: Provisioning requires lots of communications with other systems. It is also built on Winder.   # Releases  <a href="https://github.com/eBay/Winder/blob/master/releases/release-notes.txt">Release Notes</a>  &lt;dependency&gt;  &nbsp;&nbsp;&lt;groupId>org.ebayopensource.winder&lt;/groupId&gt;  &nbsp;&nbsp;&lt;artifactId>winder-core&lt;/artifactId&gt;  &nbsp;&nbsp;&lt;version>0.2.0&lt;/version&gt;  &lt;/dependency&gt;   # Examples  <a href="https://github.com/eBay/Winder/blob/master/winder-examples/src/main/java/org/ebayopensource/winder/examples/deployment1/DeploymentJob.java">**Software Deployment**</a>   <img src="https://github.com/eBay/Winder/blob/master/docs/SoftwareDeployment.png"/>  # Contributing <div style="text-align:right">   <img src="https://github.com/eBay/Winder/blob/master/docs/ebaysf-open-x.png" width="150px"/> </div> Refer to [CONTRIBUTING.md](/CONTRIBUTING.md) for more details on how to contribute code, documentation etc  **Winder** is freely usable, licensed under the [MIT license](LICENSE.md).
comunes/kune	Kune ======  [![Build Status](http://ci.comunes.org/buildStatus/icon?job=kune)](http://ci.comunes.org/job/kune/)  Kune is a free and open source federated collaborative social network, focused on collaboration rather than just on communication. That is, it provides online real-time collaborative editing (based on Apache Wave), decentralized social networking and web publishing, while focusing on workgroups rather than individuals. It aims to allow the creation of online spaces for collaborative work, where organizations and individuals can build projects online, coordinate common agendas, set up virtual meetings, publish on the web, and join organizations with similar interests. Kune is 100% free/libre software (AGPLv3), built with free/libre tools.  ## Try it  See http://kune.cc if you want to try Kune or directly use it for collaboration.  https://github.com/comunes/kune is the main Kune code repository.  ## Installation  See the [INSTALL](https://github.com/comunes/kune/blob/master/INSTALL.md) file for instructions on how to install and run Kune in your own server. Please follow closely.  ## Development  See the [DEV-GUIDE](https://github.com/comunes/kune/blob/master/DEV-GUIDE.md) file for developer information and guidelines.  ## Troubleshooting  See the [TROUBLESHOOT](https://github.com/comunes/kune/blob/master/TROUBLESHOOT.md) file if you have problems with the installation or the development.  ## Copyright and Credits  See the [COPYRIGHT](https://github.com/comunes/kune/blob/master/COPYRIGHT) and [CREDITS](https://github.com/comunes/kune/blob/master/CREDITS) files for legal info and author/contributors.  ## Bugs and feedback  See the [BUGS](https://github.com/comunes/kune/blob/master/BUGS) file for information about current problems. You can also use kune.cc for reporting feedback.
denimgroup/threadfix	**NOTE**: If you wish to download the latest build of ThreadFix please visit the [ThreadFix download page](http://www.threadfix.org/download/). Please **DO NOT** use the "Download ZIP" function from GitHub. If you DO use the "Download ZIP" function from GitHub you will just get a dump of the source code, but no ready-to-run Tomcat webserver and other facilities that make it really easy to get up and running with ThreadFix quickly. The normal [ThreadFix download](http://www.threadfix.org/download/) build comes pre-packaged and ready-to-run and is the preferred way to start using ThreadFix. You can set up your own [development environment](https://github.com/denimgroup/threadfix/wiki/Environment-Setup) but it is advised that first time users start with the [pre-packaged build](http://www.threadfix.org/download/).  ThreadFix is a software vulnerability aggregation and management system that reduces the time it takes to fix software vulnerabilities. ThreadFix imports the results from dynamic, static and manual testing to provide a centralized view of software security defects across development teams and applications. The system allows companies to correlate testing results and streamline software remediation efforts by simplifying feeds to software issue trackers. By auto generating application firewall rules, this tool allows organizations to continue remediation work uninterrupted. ThreadFix empowers managers with vulnerability trending reports that show progress over time, giving them justification for their efforts.  ThreadFix is licensed under the Mozilla Public License (MPL) version 2.0.  The main GitHub site for ThreadFix can be found here:  https://github.com/denimgroup/threadfix/  The Google Group for ThreadFix can be found here:  https://groups.google.com/forum/#!forum/threadfix  Instructions on setting up a development environment can be found here:  https://github.com/denimgroup/threadfix/wiki/Development-Environment-Setup  Further documentation can be found online here:  https://github.com/denimgroup/threadfix/wiki  Submit bugs to the GitHub issue tracker:  https://github.com/denimgroup/threadfix/issues  ThreadFix is a platform with a number of components. Each subdirectory should have its own pom.xml files to support Maven builds. The major components in the repository include:  * **threadfix-cli-endpoints** - Command-line utility to calculate the attack surface of an application and print it to standard output. This relies on the Hybrid Analysis Mapping (HAM) capabilities in the threadfix-ham/ component. * **theadfix-cli** - Command-line client for ThreadFix. This allows for scripting and automation of the ThreadFix platform. * **threadfix-extras** - Experimental tools and ThreadFix proof-of-concept projects. * **threadfix-ham** - Hybrid Analysis Mapping (HAM) technology used in ThreadFix that performs lightweight static analysis of application source code to calculate attack surfaces and map application attack surface endpoints to source code locations. * **threadfix-ide-plugin** - IDE plugins for Eclipse and IntelliJ that pulls vulnerability data from ThreadFix and highlights these vulnerabilities in application source code. * **threadfix-main** - Main ThreadFix server application. This is a Java-based Spring/Hibernate web application with associated web services. Other components of the ThreadFix platform call into the ThreadFix server. * **threadfix-scanner-plugin** - Scanner plugins that can connect to a ThreadFix server and import an application's attack surface to improve the thoroughness of dynamic scanning. Also allows for exporting scan results directly into ThreadFix (rather than saving files and uploading them.) * **threadfix-update** - Update scripts to upgrade the ThreadFix server database between versions.
orbit/orbit	<img src="http://www.orbit.cloud/img/orbit-logo-black.png" alt="Orbit Logo" width="200px"/>  [![Release](https://img.shields.io/github/release/orbit/orbit.svg)](https://github.com/orbit/orbit/releases) [![Maven Central](https://img.shields.io/maven-central/v/cloud.orbit/orbit-runtime.svg)](https://repo1.maven.org/maven2/cloud/orbit/) [![Javadocs](http://www.javadoc.io/badge/cloud.orbit/orbit-runtime.svg)](https://github.com/orbit/orbit/wiki/Javadocs) [![Build Status](https://img.shields.io/travis/orbit/orbit.svg)](https://travis-ci.org/orbit/orbit) [![Gitter](https://img.shields.io/badge/style-Join_Chat-ff69b4.svg?style=flat&label=gitter)](https://gitter.im/orbit/orbit) [![Twitter Follow](https://img.shields.io/twitter/follow/OrbitFramework.svg?style=flat&maxAge=86400)](https://twitter.com/orbitframework)  Orbit is a framework to write distributed systems using virtual actors on the JVM. It allows developers to write highly distributed and scalable applications while greatly simplifying clustering, discovery, networking, state management, actor lifetime and more.  <a href="https://github.com/orbit/orbit/wiki/Duke's-Choice-Award-2016"><img src="http://www.orbit.cloud/img/dca/dca_logo.png" alt="Duke's Choice Award Logo" width="200px" /></a><br /> Orbit received the 2016 Duke's Choice Award for Open Source, read [here](https://github.com/orbit/orbit/wiki/Duke's-Choice-Award-2016) for more information.   Full Documentation ======= See the [Wiki](https://github.com/orbit/orbit/wiki) for full documentation, examples and other information.  Developer & License ====== This project was developed by [Electronic Arts](http://www.ea.com) and is licensed under the [BSD 3-Clause License](LICENSE).  Simple Example ======= #### Java ```java public interface Hello extends Actor {     Task<String> sayHello(String greeting); }  public class HelloActor extends AbstractActor implements Hello {     public Task<String> sayHello(String greeting)     {         getLogger().info("Here: " + greeting);         return Task.fromValue("Hello There");     } }  Actor.getReference(Hello.class, "0").sayHello("Meep Meep"); ```  #### Scala ```scala trait Hello extends Actor  {   def sayHello(greeting: String): Task[String] }  class HelloActor extends AbstractActor[AnyRef] with Hello  {   def sayHello(greeting: String): Task[String] =    {     getLogger.info("Here: " + greeting)     Task.fromValue("Hello There")   } }  Actor.getReference(classOf[Hello], "0").sayHello("Meep Meep") ```
raphw/byte-buddy	Byte Buddy ==========  <a href="http://bytebuddy.net"> <img src="https://raw.githubusercontent.com/raphw/byte-buddy/gh-pages/images/logo-bg.png" alt="Byte Buddy logo" height="180px" align="right" /> </a>  runtime code generation for the Java virtual machine  [![Build Status (Travis CI)](https://circleci.com/gh/raphw/byte-buddy.svg?style=shield)](https://circleci.com/gh/raphw/byte-buddy) [![Build Status (AppVeyor CI)](https://ci.appveyor.com/api/projects/status/github/raphw/byte-buddy?branch=master&svg=true)](https://ci.appveyor.com/project/raphw/byte-buddy) [![Coverage Status](http://img.shields.io/coveralls/raphw/byte-buddy/master.svg)](https://coveralls.io/r/raphw/byte-buddy?branch=master) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/net.bytebuddy/byte-buddy-parent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/net.bytebuddy/byte-buddy-parent) [![Download from Bintray](https://api.bintray.com/packages/raphw/maven/ByteBuddy/images/download.svg) ](https://bintray.com/raphw/maven/ByteBuddy/_latestVersion)  Byte Buddy is a code generation and manipulation library for creating and modifying Java classes during the  runtime of a Java application and without the help of a compiler. Other than the code generation utilities  that [ship with the Java Class Library](http://docs.oracle.com/javase/8/docs/api/java/lang/reflect/Proxy.html),  Byte Buddy allows the creation of arbitrary classes and is not limited to implementing interfaces for the  creation of runtime proxies. Furthermore, Byte Buddy offers a convenient API for changing classes either  manually, using a Java agent or during a build.  In order to use Byte Buddy, one does not require an understanding of Java byte code or the [class file format](http://docs.oracle.com/javase/specs/jvms/se8/html/jvms-4.html). In contrast, Byte Buddy’s API aims for code  that is concise and easy to understand for everybody. Nevertheless, Byte Buddy remains fully customizable down  to the possibility of defining custom byte code. Furthermore, the API was designed to be as non-intrusive as  possible and as a result, Byte Buddy does not leave any trace in the classes that were created by it. For this  reason, the generated classes can exist without requiring Byte Buddy on the class path. Because of this feature,  Byte Buddy’s mascot was chosen to be a ghost.  Byte Buddy is written in Java 6 but supports the generation of classes for any Java version. Byte Buddy is a light-weight library and only depends on the visitor API of the Java byte code parser library  [ASM](http://asm.ow2.org/) which does itself  [not require any further dependencies](https://repo1.maven.org/maven2/org/ow2/asm/asm/5.0.4/asm-5.0.4.pom).  At first sight, runtime code generation can appear to be some sort of black magic that should be avoided and only few developers write applications that explicitly generate code during their runtime. However, this picture changes when creating libraries that need to interact with arbitrary code and types that are unknown at compile time. In this context, a library implementer must often choose between either requiring a user to implement library-proprietary interfaces or to generate code at runtime when the user’s types becomes first known to the library. Many known libraries such as for example *Spring* or *Hibernate* choose the latter approach which is popular among their users under the term of using [*Plain Old Java Objects*](http://en.wikipedia.org/wiki/Plain_Old_Java_Object). As a result, code generation has become an ubiquitous concept in the Java space. Byte Buddy is an attempt to innovate the runtime creation of Java types in order to provide a better tool set to those relying on code generation.  ___  <a href="http://bytebuddy.net"> <img src="https://raw.githubusercontent.com/raphw/byte-buddy/gh-pages/images/dukeschoice.jpg" alt="Duke's Choice award" height="110px" align="left" /> </a>  In October 2015, Byte Buddy was distinguished with a [*Duke's Choice award*](https://www.oracle.com/corporate/pressrelease/dukes-award-102815.html)  by Oracle. The award appreciates Byte Buddy for its "*tremendous amount of innovation in Java Technology*".  We feel very honored for having received this award and want to thank all users and everybody else who helped  making Byte Buddy the success it has become. We really appreciate it!  ___  Byte Buddy offers excellent performance at production quality. It is stable and in use by distiguished frameworks and tools such as [Mockito](http://mockito.org), [Hibernate](http://hibernate.org), [Google's Bazel build system](http://bazel.io) and [many others](https://github.com/raphw/byte-buddy/wiki/Projects-using-Byte-Buddy). Byte Buddy is also used by a large number of commercial products to great result. It is currently downloaded over a million times a year.  Hello World -----------  Saying *Hello World* with Byte Buddy is as easy as it can get. Any creation of a Java class starts with an instance of the `ByteBuddy` class which represents a configuration for creating new types:  ```java Class<?> dynamicType = new ByteBuddy()   .subclass(Object.class)   .method(ElementMatchers.named("toString"))   .intercept(FixedValue.value("Hello World!"))   .make()   .load(getClass().getClassLoader())   .getLoaded(); assertThat(dynamicType.newInstance().toString(), is("Hello World!")); ```  The default `ByteBuddy` configuration which is used in the above example creates a Java class in the newest version of the class file format that is understood by the processing Java virtual machine. As hopefully obvious from the example code, the created type will extend the `Object` class and overrides its `toString` method which should return a fixed value of `Hello World!`. The method to be overridden is identified by a so-called `ElementMatcher`. In the above example, a predefined element matcher `named(String)` is used which identifies methods by their exact names. Byte Buddy comes with numerous predefined and well-tested matchers which are collected in the `ElementMatchers` class and which can be easily composed. The creation of custom matchers is however as simple as implementing the ([functional](http://docs.oracle.com/javase/8/docs/api/java/lang/FunctionalInterface.html)) `ElementMatcher` interface.  For implementing the `toString` method, the `FixedValue` class defines a constant return value for the overridden method. Defining a constant value is only one example of many method interceptors that ship with Byte Buddy. By implementing the `Implementation` interface, a method could however even be defined by custom byte code.  Finally, the described Java class is created and then loaded into the Java virtual machine. For this purpose, a target class loader is required. Eventually, we can convince ourselves of the result by calling the `toString` method on an  instance of the created class and finding the return value to represent the constant value we expected.  A more complex example ----------------------  Of course, a *Hello World example* is a too simple use case for evaluating the quality of a code generation library. In reality, a user of such a library wants to perform more complex manipulations, for example by introducing hooks into the execution path of a Java program. Using Byte Buddy, doing so is however equally simple. The following example  gives a taste of how method calls can be intercepted.  Byte Buddy expresses dynamically defined method implementations by instances of the `Implementation` interface. In the previous example, `FixedValue` that implements this interface was already demonstrated. By implementing this interface,  a user of Byte Buddy can go to the length of defining custom byte code for a method. Normally, it is however easier to  use Byte Buddy's predefined implementations such as `MethodDelegation` which allows for implementing any method in  plain Java. Using this implementation is straight forward as it operates by delegating the control flow to any POJO. As  an example of such a POJO, Byte Buddy can for example redirect a call to the only method of the following class:  ```java public class GreetingInterceptor {   public Object greet(Object argument) {     return "Hello from " + argument;   } } ```  Note that the above `GreetingInterceptor` does not depend on any Byte Buddy type. This is good news because none of the classes that by Byte Buddy generates require Byte Buddy on the class path! Given the above `GreetingInterceptor`, we can use Byte Buddy  to implement the Java 8 `java.util.function.Function` interface and its abstract `apply` method:  ```java Class<? extends java.util.function.Function> dynamicType = new ByteBuddy()   .subclass(java.util.function.Function.class)   .method(ElementMatchers.named("apply"))   .intercept(MethodDelegation.to(new GreetingInterceptor()))   .make()   .load(getClass().getClassLoader())   .getLoaded(); assertThat((String) dynamicType.newInstance().apply("Byte Buddy"), is("Hello from Byte Buddy")); ```  Executing the above code, Byte Buddy implements Java's `Function` interface and implements the `apply` method as a delegation to an instance of the `GreetingInterceptor` POJO that we defined before. Now, every time that the `Function::apply` method is called, the control flow is dispatched to `GreetingInterceptor::greet` and the latter method's return value is returned from the interface's method.  Interceptors can be defined to take with more generic inputs and outputs by annotating the interceptor's parameters.  When Byte Buddy discovers an annotation, the library injects the dependency that the interceptor parameter requires.  An example for a more general interceptor is the following class:  ```java public class GeneralInterceptor {   @RuntimeType   public Object intercept(@AllArguments Object[] allArguments,                           @Origin Method method) {     // intercept any method of any signature   } } ```  With the above interceptor, any intercepted method could be matched and processed. For example, when matching `Function::apply`, the method's arguments would be passed as the single element of an array. Also, a `Method`  reference to `Fuction::apply` would be passed as the interceptor's second argument due to the `@Origin`  annotation. By declaring the `@RuntimeType` annotation on the method, Byte Buddy finally casts the returned  value to the return value of the intercepted method if this is necessary. In doing so, Byte Buddy also applies automatic boxing and unboxing.  Besides the annotations that were already mentioned there exist plenty of other predefined annotations. For  example, when using the `@SuperCall` annotation on a `Runnable` or `Callable` type, Byte Buddy injects proxy  instances that allow for an invocation of a non-abstract super method if such a method exists. And even if Byte Buddy does not cover au use case, Byte Buddy offers an extension mechanism for defining custom annotations.  You might expect that using these annotations ties your code to Byte Buddy. However, Java ignores annotations in case that they are not visible to a class loader. This way, generated code can still exist without Byte Buddy! You can find more information on the `MethodDelegation` and on all of its predefined annotations in its *javadoc* and in Byte Buddy's tutorial.  Changing existing classes ----------------------  Byte Buddy is not limited to creating subclasses but is also capable of redefining existing code. To do so, Byte Buddy offers a convenient API for defining so-called [Java agents](https://docs.oracle.com/javase/8/docs/api/java/lang/instrument/package-summary.html). Java agents are plain old Java programs that can be used to alter the code of an existing Java application during its runtime. As an example, we can use Byte Buddy to change methods to print their execution time. For this, we first define an interceptor similar to the interceptors in the previous examples:  ```java public class TimingInterceptor {   @RuntimeType   public static Object intercept(@Origin Method method,                                   @SuperCall Callable<?> callable) {     long start = System.currentTimeMillis();     try {       return callable.call();     } finally {       System.out.println(method + " took " + (System.currentTimeMillis() - start));     }   } } ```  Using a Java agent, we can now apply this interceptor to all types that match an `ElementMatcher` for a `TypeDescription`.  For the example, we choose to add the above interceptor to all types with a name that ends in `Timed`. This is done for the sake of similicity whereas an annotation would probably be a more appropriate alternative to mark such classes for a production agent. Using Byte Buddy's `AgentBuilder` API, creating a Java agent is as easy as defining the following agent class:  ```java public class TimerAgent {   public static void premain(String arguments,                               Instrumentation instrumentation) {     new AgentBuilder.Default()       .type(ElementMatchers.nameEndsWith("Timed"))       .transform((builder, type, classLoader, module) ->            builder.method(ElementMatchers.any())                  .intercept(MethodDelegation.to(TimingInterceptor.class))       ).installOn(instrumentation);     }   } } ```  Similar to Java's `main` method, the `premain` method is the entry point to any Java agent from which we apply the redefinition. As one argument, a Java agent receives an instace of the `Instrumentation` interface which allows Byte Buddy to hook into the JVM's standard API for runtime class redefinition.  This program is packaged together with a manifest file with the [`Premain-Class` attribute](https://docs.oracle.com/javase/8/docs/api/java/lang/instrument/package-summary.html) pointing to the `TimerAgent`. The resulting *jar* file can now be added to any Java application by setting `-javaagent:timingagent.jar` similar to adding a jar to the class path. With the agent active, all classes ending in `Timed` do now print their execution time to the console.   Byte Buddy is also capable of applying so-called runtime attachments by disabling class file format changes and using the `Advice` instrumentation. Please refer to the *javadoc* of the `Advice` and the `AgentBuilder` class for further information. Byte Buddy also offers the explicit change of Java classes via a `ByteBuddy` instance or by using the Byte Buddy *Maven* and *Gradle* plugins.  Where to go from here? ----------------------  Byte Buddy is a comprehensive library and we only scratched the surface of Byte Buddy's capabilities. However, Byte Buddy aims for being easy to use by providing a domain-specific language for creating classes. Most runtime code generation can be done by writing readable code and without any knowledge of Java's class file format. If you want to learn more about Byte Buddy, you can find such a [tutorial on Byte Buddy's web page](http://bytebuddy.net/#/tutorial) (There is also [a Chinese translation available](http://notes.diguage.com/byte-buddy-tutorial)).  Furthermore, Byte Buddy comes with a [detailed in-code documentation](http://bytebuddy.net/#/javadoc) and extensive test case coverage which can also serve as example code. Finally, you can find an up-to-date list of articles and presentations on Byte Buddy [in the wiki](https://github.com/raphw/byte-buddy/wiki/Web-Resources). When using Byte Buddy, make also sure to read the following information on maintaining a project dependency.  Getting support ----------------------------  #### Commercial ####  The use of Byte Buddy is free and does not require the purchase of a license. To get the most out of the library or to secure an easy start, it is however possible to purchase training, development hours or support plans. Rates are dependent on the scope and duration of an engagement. Please get in touch with <rafael.wth@gmail.com> for further information.  #### Free ####  General questions can be asked on [Stack Overflow](http://stackoverflow.com/questions/tagged/byte-buddy) or on the [Byte Buddy mailing list](https://groups.google.com/forum/#!forum/byte-buddy) which also serve as an archive for questions. Of course, bug reports will be considered also outside of a commercial plan. For open source projects, it is sometimes possible to receive extended help for taking Byte Buddy into use.  Dependency and API evolution ----------------------------  Byte Buddy is written on top of [ASM](http://asm.ow2.org/), a mature and well-tested library for reading and writing compiled Java classes. In order to allow for advanced type manipulations, Byte Buddy is intentionally exposing the ASM API to its users. Of course, the direct use of ASM remains fully optional and most users will most likely never require it. This choice was made such that a user of Byte Buddy is not restrained to its higher-level functionality but can implement custom implementations without a fuzz when it is necessary.  ASM has previously changed its public API but added a mechanism for API compatibility starting with version 4 of the library. In order to avoid version conflicts with such older versions, Byte Buddy repackages the ASM dependency into its own namespace. If you want to use ASM directly, use the `byte-buddy-dep` artifact offers a version of Byte Buddy with an explicit dependency to ASM. When doing so, you should then repackage *both* Byte Buddy and ASM into your namespace to avoid version conflicts.  License and development -----------------------  Byte Buddy is licensed under the liberal and business-friendly [*Apache Licence, Version 2.0*](http://www.apache.org/licenses/LICENSE-2.0.html) and is freely available on GitHub. Byte Buddy is further released to the repositories of Maven Central and on JCenter. The project is built using <a href="http://maven.apache.org/">Maven</a>. From your shell, cloning and building the project would go something like this:  ```shell git clone https://github.com/raphw/byte-buddy.git cd byte-buddy mvn package ```  On these commands, Byte Buddy is cloned from GitHub and built on your machine. Byte Buddy is currently tested for the [*OpenJDK*](http://openjdk.java.net/) versions 6 and 7 and the *Oracle JDK* versions 7 and 8 using Travis CI.  Please use GitHub's [issue tracker](https://github.com/raphw/byte-buddy/issues) for reporting bugs. When committing code, please provide test cases that prove the functionality of your features or that demonstrate a bug fix. Furthermore, make sure you are not breaking any existing test cases. If possible, please take the time to write some documentation. For feature requests or general feedback, you can also use the [issue tracker](https://github.com/raphw/byte-buddy/issues) or contact us on [our mailing list](https://groups.google.com/forum/#!forum/byte-buddy).
abstratt/textuml	---  ---    TextUML Toolkit  ===============      > Note: This documentation is meant to be viewed in HTML ([via this URL](http://abstratt.github.io/textuml/readme.html)). Relative links below will work via the HTML version, but not via the Markdown viewer.      [![Build Status on Cloudbees](https://textuml.ci.cloudbees.com/buildStatus/icon?job=textuml-toolkit)](https://textuml.ci.cloudbees.com/job/textuml-toolkit/)  [![Build Status on Travis CI](https://travis-ci.org/abstratt/textuml.svg?branch=master)](https://travis-ci.org/abstratt/textuml)        TextUML Toolkit is an **open-source IDE for UML** that lets you create  models at the same speed you write code. By adopting **a textual  notation**, the TextUML Toolkit provides benefits you will not get  elsewhere:    -   **increased modeling productivity**  -   **compatible** with any tools that support [Eclipse      UML2](http://wiki.eclipse.org/MDT-UML2-Tool-Compatibility "http://wiki.eclipse.org/MDT-UML2-Tool-Compatibility")      models  -   the features you like in your favorite IDE: **instant validation,      syntax highlighting, outline view, textual comparison**  -   **live graphical visualization** of your model as class diagrams    The TextUML Toolkit can be used both as a set of plug-ins for the Eclipse IDE, and as a part of a multi-tenant server-side application - as seen in [Cloudfier](http://github.com/abstratt/cloudfier/).     Using the Toolkit  ------------------------    -   **[Instructions](docs/install.html)** for installing the TextUML Toolkit. Note you can also      use TextUML in      [Cloudfier](http://cloudfier.com/ "http://cloudfier.com"), an online      IDE and deployment platform for TextUML-based applications.    -   **[TextUML      Tutorial](docs/tutorial.html "TextUML Tutorial")**      - a step-by-step tutorial showing how to create a model with      inheritance, attributes, operations and associations    -   **[TextUML Structural      Notation](docs/structure.html "TextUML Guide")**      - a reference guide to the TextUML notation for **structural**      modeling.    -   **[TextUML Behavioral      Notation](docs/behavior.html "TextUML Action Language")**      - a reference guide to the TextUML notation for **behavioral**      modeling (a.k.a. action language).    -   **[TextUML Toolkit      features](docs/features.html "TextUML Toolkit Features")**      - a summary of the features in the TextUML Toolkit, grouped by      release.    -   **[Repository      Properties](docs/repository_properties.html "Repository Properties")**      - customization properties that change the way the compiler works      (to enable built-in types, aliases etc).    -   **[FAQ](docs/faq.html "FAQ")**      - frequently asked questions about TextUML and the Toolkit    -   **[UML 101 with      TextUML](docs/uml_101.html "UML 101")**      - a series of articles explaining UML concepts using the TextUML      notation: multiplicity, profiles and stereotypes and templates.        -   **[Contributing](docs/contributing)**      - everything you need to know to contribute as a developer or tester.    [![](https://www.cloudbees.com/sites/default/files/styles/large/public/Button-Built-on-CB-1.png)](https://textuml.ci.cloudbees.com/job/textuml-toolkit/)    {% include github.html %}
ModelDriven/fUML-Reference-Implementation	Foundational UML Reference Implementation -----------------------------------------  This open source software is a reference implementation, consisting of software and related files, for the OMG specification called the Semantics of a Foundational Subset for Executable UML Models (fUML). The reference implementation is intended to implement the execution semantics of UML activity models, accepting an XMI file from a conformant UML model as its input and providing an execution trace of the selected activity model(s) as its output.  The reference implementation was developed as part of a Lockheed Martin Corporation funded project with Model Driven Solutions (a division of Data Access Technologies) in 2008. The objectives for making this reference implementation open source are to:  1. encourage tool vendors to implement this standard in their tools  2. provide a reference that can assist in evaluating vendor implementations conformance with the specification  3. encourage evolution of the reference implementation to support further enhancements to its functionality such as a model debugger, or animation of the execution trace  4. support evaluation and evolution of the the specification to support UML execution semantics and the execution semantics of its profiles suchas SysML.  ### Licensing  For licensing information, please see the file [Licensing-Information.txt](https://github.com/ModelDriven/fUML-Reference-Implementation/blob/master/org.modeldriven.fuml/Licensing-Information.txt) and the associated files [Common-Public-License-1.0.txt](https://github.com/ModelDriven/fUML-Reference-Implementation/blob/master/org.modeldriven.fuml/Common-Public-License-1.0.txt) and [Apache-License-2.0.txt](https://github.com/ModelDriven/fUML-Reference-Implementation/blob/master/org.modeldriven.fuml/Apache-License-2.0.txt).  ### Building  The implementation build requires the following to be installed:  * Oracle Java Version 8 or above - see http://java.oracle.com/ * Apache Maven Version 2.2 or above - see http://maven.apache.org/  To build from the command line:  1. In a Windows/DOS command window, navigate to the 'root' reference implementation directory. This directory is where the Apache Maven 'pom.xml' file can be found.  2. Use the following command:             mvn -DskipTests install  Several targets will be executed, and the message 'BUILD SUCCESSFUL' should be displayed. Generated and compiled code can be found under the 'target' directory.  To build using Eclipse:     The implementation build requires Eclipse Mars or above with the M2E plugin and dependencies:  1. Start Eclipse  2. Import the Maven project into Eclipse  * File->Import->Existing Maven Projects  * Select Next and for the "root directory" select the parent directory for the pom.xml file.  * The project should be imported successfully.  3. Run the tests  * Right click the project root (fuml)  * Run As->Maven test  4. Install  * Right click the project root (fuml)  * Run As->Maven install  ### Testing  1. In a Windows/DOS command window, navigate to the 'root' reference implementation directory. This directory is where the Apache Maven 'pom.xml' file can be found.  2. Use the following command:          mvn test  The test output can be found in the 'target/surefire-reports' directory.    Testing individual JUnit tests in Eclipse:   1. Right-click on any file under the 'src/test/java' folder with a filename ending in 'TestCase' or 'Test' 2. Run As->JUnit Test. 3. The test should execute and display output.  ### Deploying  1. In a Windows/DOS command window, navigate to the 'root' reference implementation directory. This directory is where the Apache Maven 'pom.xml'  file can be found.  2. Use the following command:          mvn install  A binary deployment 'zip' file will be created and expanded onto the current drive's root directory. The deployment directory will be called:  'fuml-[version string]-distribution'   ### Running  1. In a Windows/DOS command window, navigate to the 'bin' directory for the DEPLOYED implementation. The implementation can only be run from a DEPLOYED binary or source distribution (see above).  2. To load a UML model file (XMI 2.1 or 2.4.1/2.5 format) and execute one or more behaviors (activities), use the following command:          fuml <model-file> [<behavior-name> <behavior-name> <behavior-name> <...>]     Where:     * `<model-file>` is UML model file (XMI 2.1 or 2.4.1/2.5 format)          * `<behavior-name>` is a named behavior within the model-file     If no behavior name is given, then there should be only a single behavior in the top-level namespace of the model, and this is what is executed.  3. The execution trace will print to the console. This may be redirected to a file if desired (e.g., by appending '> trace.txt' to the command above).
TelluIoT/ThingML	![ThingML](Logotype_ThingML_100317_500px.png)  The ThingML approach is composed of *i*) a **modeling language**, *ii*) a set of **tools** and *iii*) a **methodology**. The modeling language combines well-proven software modeling constructs for the design and implementation of distributed reactive systems:  - statecharts and components (aligned with the UML) communicating through asynchronous message passing - an imperative platform-independent action language - specific constructs targeted at IoT applications.  The ThingML language is supported by a set of tools, which include editors, transformations (e.g. export to UML) and an advanced multi-platform code generation framework, which support multiple target programming languages (C, Java, Javascript). The [methodology](https://heads-project.github.io/methodology/) documents the development processes and tools used by both the IoT service developers and the platform experts.  > ThingML is distributed under the *[Apache 2.0 licence](https://www.apache.org/licenses/LICENSE-2.0)*, and has been developed by @ffleurey and @brice-morin of the Networked Systems and Services department of SINTEF in Oslo, Norway, together with a vibrant [open-source community](https://github.com/TelluIoT/ThingML/graphs/contributors). ThingML is now owned by [Tellu](http://www.tellucloud.com/), but remains open-source.  > **Issues, bug reports and feature requests should be submitted to the [issue tracker on GitHub](https://github.com/TelluIoT/ThingML/issues)**  ## &#x1F537; Prerequisites &#x2757;  ThingML can compile code for various platforms and languages. Please make sure you follow the required steps  ### &#x1F539; Java If you are going to compile Java code from ThingML, please:  - Make sure you have a proper [JDK](http://www.oracle.com/technetwork/java/javase/downloads/index.html) (a JRE is not sufficient) - Install [Maven](http://maven.apache.org/)  ### &#x1F539; Javascript If you are going to compile Javascript code from ThingML, for:  - NodeJS: Install [Node.JS](https://nodejs.org/en/) - Browser: Make sure you have a decent web browser (Chrome or Firefox should work fine, and probably some others)  ### &#x1F539; UML If you are going to compile UML Diagrams from ThingML, please:  - Install [Graphviz](http://www.graphviz.org/Download.php)  ### &#x1F539; Arduino If you are going to compile Arduino code from ThingML, please:  - Install [Arduino IDE](https://www.arduino.cc/en/Main/Software)  ### &#x1F539; Teensy If you are going to compile Teensy code from ThingML, please:  - Install [Teensyduino IDE](https://www.pjrc.com/teensy/td_download.html)  or - Install [cross compiled arm toochain](https://developer.arm.com/open-source/gnu-toolchain/gnu-rm/downloads) - Install [teensy command line loader](https://www.pjrc.com/teensy/loader_cli.html)  ### &#x1F539; C If you are going to compile C code from ThingML, please:  - Use a C-friendly OS (such as Linux) with a decent build toolchain (`make`, `gcc`), potentially in a Virtual Box  ## &#x1F537; Getting Started  ### &#x1F539; Installation  The easiest way to get started with ThingML is to use the ThingML plugins in the Eclipse IDE.  If you have docker, you can use the build container with Eclipse and ThingML at the [thingmleditor repository](https://github.com/madkira/thingmleditor) or the [thingmleditor docker hub](https://hub.docker.com/r/madkira/thingmleditor/)  Else 1. [Download Eclipse for Java Developers](https://eclipse.org/downloads/) 2. Install and Launch Eclipse 3. Install XText plugins: `Help -> Install New Software... -> Add...` and choose `XText` as a name and `Xtext - http://download.eclipse.org/modeling/tmf/xtext/updates/composite/releases/` as location, and then `OK`. Select `XText` and continue with the install procedure &#x23F3; 4. Install the ThingML plugins: Same procedure. Use `ThingML - http://thingml.org/dist/update2/` update site, and select ThingML &#x23F3;  You are now ready to use ThingML. &#x270C;  ### &#x1F539; Compiling ThingML code  Once you have created (or imported) ThingML files in your workspace, simply right click on a ThingML file in order to compile it. A `HEADS / ThingML` should be present in the menu and you can then select which compiler to use: Java, JavaScript, C, etc.  > The ThingML file you want to compile should contain a `configuration`  > The generated code will be located in a `thingml-gen` folder in your current project  #### How to compile and run generated Java code  &#x2757; Configure Eclipse so that it uses the JDK: `Window -> Preferences -> Java -> Installed JREs` (make sure it points to a JDK)  - Right click on `pom.xml` (in `thingml-gen/java/your-configuration`) - `Run as -> Maven build... ` - In `Goals` type: `clean install exec:java`  > If Maven claims it cannot find a `pom.xml` file, change the base directory in the `Run as -> Maven build...` window using the `Workspace...` button, so that it points to `thingml-gen/java/your-configuration`.  #### How to compile and run generated JavaScript (for the Browser) code  Nothing special. Open the generated `index.html` file in your System Browser (ideally Chrome or Firefox)  > Do not use the default web browser embedded into Eclipse!  #### How to compile and run generated JavaScript (Node.JS) code  &#x2757; In Eclipse, from this update site: `Node.JS - http://www.nodeclipse.org/updates/enide-2015/`, install `Features included in Enide Studio .Features Set` and `Nodeclipse Node.js .Features Set	1.0.2.201509250223`  - Right click on `package.json` (in `thingml-gen/nodejs/your-configuration`) - `Run as -> npm install ` - Right click on `main.js` - `Run as -> Node Application`  #### How to visualize generated UML (PlantUML) diagrams  &#x2757; Install PlantUML plugins in Eclipse using this update site: `http://files.idi.ntnu.no/publish/plantuml/repository/` (See below for how to install plugins in Eclipse)  - `Window -> Show View -> Other... -> PlantUML`  > Make sure you have Graphviz installed (see [Prerequisites](#-prerequisites-))  ####  How to compile and run generated C code  - Open a terminal at `...thingml-gen/posix/your-configuration` - `make` - `./your-configuration`  ####  How to compile and run generated Arduino code  - Open the generated file in the Arduino IDE - Compile - Upload to your board  > For more information about how to use the Arduino IDE and Arduino boards, have a look at [the Arduino documentation](https://www.arduino.cc/en/Guide/Environment).   ## &#x1F537; Compile ThingML from the sources  > You need Git, Maven, and a proper JDK8+  ```bash git clone https://github.com/TelluIoT/ThingML.git cd ThingML mvn clean install cd language mvn clean install ```  The command-line interface JAR (containing all you need to compile ThingML files) can be found here:  ```bash cd compilers/registry/target java -jar compilers.registry-2.0.0-SNAPSHOT-jar-with-dependencies.jar  --- ThingML help --- Typical usages:     java -jar your-jar.jar -t <tool> -s <source> [-o <output-dir>] [--options <option>][-d] Usage: <main class> [options]   Options:     --compiler, -c       Compiler ID (Mandatory unless --tool (-t) is used)     --create-dir, -d       Create a new directory named after the configuration for the output       Default: false     --help, -h       Display this message.       Default: false     --list-plugins       Display the list of available plugins       Default: false     --options       additional options for ThingML tools.     --output, -o       Optional output directory - by default current directory is used     --source, -s       A thingml file to compile (should include at least one configuration)     --tool, -t       Tool ID (Mandatory unless --compiler (-c) is used)  Compiler Id must belong to the following list:  ??     sintefboard     - Generates C++ based in code for Arduino.  ??     posixmt - Generates C code for Linux or other Posix runtime environments (GCC compiler).  ??     java    - Generates plain Java code.  ??     arduino - Generates C/C++ code for Arduino or other AVR microcontrollers (AVR-GCC compiler).  ??     UML     - Generates UML diagrams in PlantUML  ??     browser - Generates Javascript code that can run in common Web Browsers.  ??     nodejsMT        - Generates Multi-Process Javascript code (one nodejs process per instance) for the NodeJS platform.  ??     nodejs  - Generates Javascript code for the NodeJS platform.  ??     posix   - Generates C/C++ code for Linux or other Posix runtime environments (GCC compiler).  ??     debugGUI        - Generates html/js mock-up for other a ThingML external connector  Tool Id must belong to the following list:  ??     testconfigurationgen    - Generates test configuration for things annnotated with @test "input # output". ```  ## &#x1F537; FAQ  ### &#x1F539; Where can ThingML code run?  *Nowhere*! Or almost *everywhere*, from microcontrollers to the cloud!  A ThingML file *per se* is a design-time specification of the structure (components) and behavior (state machines) of a reactive system. It cannot be directly executed.  A ThingML file can however be compiled (or transformed) to Java/JavaScript/C/Arduino source code, which can in turn be compiled and executed on a platform. Code generated from ThingML has been successfully executed on a large number of platforms: PC Windows/Linux, Raspberry Pi 1, 2 and 3, Intel Edison, Arduino Uno/Mega/Yun/Mini, ESP8266/ESP32, Trinket, Teensy, and probably others.  ### &#x1F539; How to express *this* or *that* in ThingML?  A set of tutorials is available [here](https://github.com/HEADS-project/training/tree/master/1.ThingML_Basics). The tutorials describe the most common features of ThingML. In addition, [an extensive set of tests](testJar/src/main/resources/tests) describes pretty much all the concepts available. Have a look there is you wonder how to express something. Should this information be insufficient, have a look below.  ### &#x1F539; How is ThingML formalized?  The ThingML language is formalized into an EMF-based metamodel. The textual syntax is formalized as an [XText grammar](language/thingml/src/org/thingml/xtext/ThingML.xtext).  ### &#x1F539; All that code is wonderful, but I need some Science... &#x1F4DA;  ThingML is backed by a set of scientific publications (PDFs can easily be found on *e.g.* Google Scholar):  - **Model-Based Software Engineering to Tame the IoT Jungle**   Brice Morin, Nicolas Harrand and Franck Fleurey   In *IEEE Software, Special Issue on Internet of Things*, 2017. - **ThingML, A Language and Code Generation Framework for Heterogeneous Targets**   N. Harrand, F. Fleurey, B. Morin and K.E. Husa   In *MODELS’16: ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems. Practice and Innovation track*. St Malo, France, October 2-7, 2016 - **MDE to Manage Communications with and between Resource-Constrained Systems**   F. Fleurey, B. Morin, A. Solberg and O. Barais.   In *MODELS’11: ACM/IEEE 14th International Conference on Model Driven Engineering Languages and Systems*. Wellington, New Zealand, October 2011.  ThingML has also been used together with other approaches:  - **Agile Development of Home Automation System with ThingML**   A. Vasilevskiy, B. Morin, Ø. Haugen and P. Evensen.   In *INDIN’16: 14th IEEE International Conference on Industrial Informatics*. Poitiers, France, July 18-21, 2016 - **A Generative Middleware for Heterogeneous and Distributed Services**   B. Morin, F. Fleurey, K.E. Husa, and O. Barais.   In *CBSE’16: 19th International ACM Sigsoft Symposium on Component-Based Software Engineering*. Venice, Italy, April 5-8, 2016   ### &#x1F539; How to embed ThingML in my toolchain?  Embed the command-line inteface JAR [described previously in this readme](#-compile-thingml-from-the-sources) in your classpath.  You can also include ThingML as a Maven dependency in your project:  ```xml <dependency>      <groupId>org.thingml</groupId>      <artifactId>compilers.registry</artifactId>      <version>1.0.0-SNAPSHOT</version> </dependency>  ...  <repository>     <id>thingml-snapshot</id>     <name>thingml-snapshot</name>     <url>http://maven.thingml.org/thingml-snapshot/</url> </repository>  <repository>     <id>thingml-release</id>     <name>thingml-release</name>     <url>http://maven.thingml.org/thingml-release/</url> </repository> ```  ### &#x1F539; The code generated by ThingML for Java/JS/C/Arduino does not exactly fit my needs  Rather than being monolithic blobs, compilers are implemented in a modular way around a set of extension points defined in the [ThingML Code Generation Framework](compilers/README.md).  ### &#x1F539; Why can't I generate Go/Python/Lua/Ruby/*you-name-it*?  Well, it is up to you to implement a compiler for whatever language that is not supported by default. What are you waiting for?  ### &#x1F539; How can I programatically process ThingML models?  ```java File myFile = new File("source.thingml"); ThingMLModel myModel = ThingMLCompiler.loadModel(myFile); //Do something ThingMLCompiler.saveAsThingML(myModel, "target.thingml"); //or ThingMLCompiler.saveAsXMI(myModel, "target.xmi"); ```  >Protip1: Make sure you have a good understanding of the [ThingML metamodel](#-how-is-thingml-formalized)  >Protip2: Have a look at the [helper functions](language/thingml/src/org/thingml/xtext/helpers) which simplify some typical treatments  > Models saved this way will contain all the imports that the original file refered to in one big file  > This feature might currently be broken as we migrated to XText.  ## &#x1F537; More  **Visit [thingml.org](http://www.thingml.org) to find out more about ThingML !**   ![ThingML is released under OSI-compliant Apache 2.0 license](https://opensource.org/files/osi_keyhole_100X100_90ppi.png "ThingML is released under OSI-compliant Apache 2.0 license")
umlet/umlet	[![Build Status](https://api.travis-ci.org/umlet/umlet.svg?branch=master)](https://travis-ci.org/umlet/umlet) # UMLet UMLet is an open-source UML tool with a simple user interface: draw UML diagrams fast, export diagrams to eps, pdf, jpg, svg, and clipboard, share diagrams using Eclipse, and create new, custom UML elements.   * Please check out the [Wiki](https://github.com/umlet/umlet/wiki) for frequently asked questions  * Go to http://www.umlet.com to get the latest compiled versions or to http://www.umletino.com to use UMLet in your web browser
structurizr/java	![Structurizr](docs/images/structurizr-banner.png)  # Structurizr for Java  This GitHub repository is a collection of tooling to help you visualise, document and explore the software architecture of a software system. In summary, it allows you to create a software architecture model based upon Simon Brown's [C4 model](https://c4model.com) using Java code, and then export that model to be visualised using tools such as:  1. [Structurizr](https://structurizr.com): a web-based software as a service and on-premises product to render software architecture diagrams and supplementary Markdown/AsciiDoc documentation. 1. [PlantUML](docs/plantuml.md): a tool to create UML diagrams using a simple textual domain specific language. 1. [Graphviz](docs/graphviz-and-dot.md): a tool to render directed graphs using the DOT format.  As an example, the following Java code can be used to create a software architecture model that describes a user using a software system.  ```java public static void main(String[] args) throws Exception {     Workspace workspace = new Workspace("Getting Started", "This is a model of my software system.");     Model model = workspace.getModel();          Person user = model.addPerson("User", "A user of my software system.");     SoftwareSystem softwareSystem = model.addSoftwareSystem("Software System", "My software system.");     user.uses(softwareSystem, "Uses");          ViewSet views = workspace.getViews();     SystemContextView contextView = views.createSystemContextView(softwareSystem, "SystemContext", "An example of a System Context diagram.");     contextView.addAllSoftwareSystems();     contextView.addAllPeople(); } ```  If using [Structurizr](https://structurizr.com), the end-result, after adding some styling and positioning the diagram elements, is a system context diagram like this:  ![Getting Started with Structurizr for Java](docs/images/getting-started.png)  You can see the live workspace at [https://structurizr.com/share/25441](https://structurizr.com/share/25441).  [![Build Status](https://travis-ci.org/structurizr/java.svg?branch=master)](https://travis-ci.org/structurizr/java)  ## Table of contents  * Introduction     * [Getting started](docs/getting-started.md)     * [About Structurizr and how it compares to other tooling](https://structurizr.com/help/about)     * [Basic concepts](https://structurizr.com/help/concepts) (workspaces, models, views and documentation)     * [C4 model](https://c4model.com)     * [Binaries](docs/binaries.md)     * [API Client](docs/api-client.md)     * [Usage patterns](docs/usage-patterns.md) * Diagrams     * [System Context diagram](docs/system-context-diagram.md)     * [Container diagram](docs/container-diagram.md)     * [Component diagram](docs/component-diagram.md)     * [Dynamic diagram](docs/dynamic-diagram.md)     * [Deployment diagram](docs/deployment-diagram.md)     * [Enterprise Context diagram](docs/enterprise-context-diagram.md)     * [Styling elements](docs/styling-elements.md)     * [Styling relationships](docs/styling-relationships.md)     * [Filtered views](docs/filtered-views.md) * Documentation     * [Documentation overview](docs/documentation.md)     * [Structurizr](docs/documentation-structurizr.md)     * [arc42](docs/documentation-arc42.md)     * [Viewpoints and Perspectives](docs/documentation-viewpoints-and-perspectives.md)     * [Automatic template](docs/documentation-automatic.md) * Extracting software architecture information from code     * [Component finder](docs/component-finder.md)     * [Structurizr annotations](docs/structurizr-annotations.md)     * [Type matchers](docs/type-matchers.md)     * [Spring component finder strategies](docs/spring-component-finder-strategies.md)     * [Supplementing the model from source code](docs/supplementing-from-source-code.md)     * [Components and supporting types](docs/supporting-types.md)     * [The Spring PetClinic example](docs/spring-petclinic.md) * Exporting and visualising with other tools     * [PlantUML](docs/plantuml.md)     * [Graphviz and DOT](docs/graphviz-and-dot.md) * Other     * [Client-side encryption](docs/client-side-encryption.md)     * [Corporate branding](docs/corporate-branding.md)     * [Building from source](docs/building.md) * Related projects     * [cat-boot-structurizr](https://github.com/Catalysts/cat-boot/tree/master/cat-boot-structurizr): A way to apply dependency management to help modularise Structurizr code.     * [java-starter](https://github.com/structurizr/java-starter): A simple starting point for using Structurizr for Java     * [structurizr-groovy](https://github.com/tidyjava/structurizr-groovy): An initial version of a Groovy wrapper around Structurizr for Java.     * [structurizr-dotnet](https://github.com/structurizr/dotnet): Structurizr for .NET
galan/packtag	[![Build Status](https://img.shields.io/travis/galan/packtag.svg?style=flat)](https://travis-ci.org/galan/packtag)  [![Maven Central](https://img.shields.io/maven-central/v/de.galan.packtag/packtag-core.svg?style=flat)](https://maven-badges.herokuapp.com/maven-central/de.galan.packtag/packtag-core)  [![License](https://img.shields.io/github/license/galan/packtag.svg?style=flat)](https://www.apache.org/licenses/LICENSE-2.0.html)    # pack:tag    A JSP Taglib for delivering minified, combined and gzip-compressed resources (JavaScript and CSS).    # Usage    To use the taglib in a JSP, you have to declare it first:        <%@ taglib uri="https://github.com/d8bitr/packtag" prefix="pack" %>    (You can still use the old uri 'http://packtag.sf.net' for backward compatibility)    Now you can easily pack JavaScript by using the following tag:        <pack:script src="/js/myJavaScriptFile.js"/>    Accordingly for Cascading Style Sheets:        <pack:style src="/css/myCascadingStyleSheet.css"/>    You can enable and disable each tag individually by setting the attribute enabled to false, e.g.:        <pack:script src="/js/myJavaScriptFile.js" enabled="false"/>    You can combine resources simply by listing them up:        <pack:script>        <src>/js/myJavaScriptFile.js</src>        <src>/js/mySecondJavaScriptFile.js</src>      </pack:script>    That's it.    If you want to deepen your knowledge and learn about advanced techniques, I suggest reading "[pack:tag - a packed guide to website performance optimization](https://github.com/galan/packtag/raw/master/documentation/packtag%20-%20a%20packed%20guide%20to%20website%20performance%20optimization.pdf)".      # Integration    1. Add the maven dependency to your project:            <dependency>              <groupId>de.galan.packtag</groupId>              <artifactId>packtag-core</artifactId>              <version>3.13.2</version>          </dependency>    2. Copy the `<servlet>` and `<servlet-mapping>` from the [web.xml](https://github.com/galan/packtag/blob/master/packtag-testsite/src/main/webapp/WEB-INF/web.xml) into your  /WEB-INF/web.xml      # Note  pack:tag is still supported, and still encouraged to be used if it makes sense for your project. But, no further development beyond bug fixes is expected. pack:tag is over 8 years old, and has been stable since then. Thank you to the community for your support of this project over the last years.
MeteoGroup/jbrotli	jBrotli =========================================  Java bindings for [Brotli](https://github.com/google/brotli.git): a new compression algorithm for the internet.  ##### License  [![License](https://img.shields.io/:license-Apache%202.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)  ##### Supported operating systems and architectures  jbrotli provides platform dependant bindings for Google's brotli library. Thus each target platform which is provided here was compiled and tested for the following operating systems and architectures.  * Windows 7 or newer, x86 64bit * Windows 7 or newer, x86 32bit * Linux, x86 64bit * Raspberry Pi (Linux), ARMv6 32bit (hardware floating point)   ##### News  ###### 2016-06-13      With version 0.5.0 the compatibility for the Linux binaries was increased.     This is a pre-condition, to enable brotli compression in Hadoop, [HADOOP-13126](https://issues.apache.org/jira/browse/HADOOP-13126)   ###### 2016-03-20      With version 0.4.0 the Raspberry PI binaries where added as supported platform.     For easier adoption of Brotli, there is now a 'BrotliServletFilter' available.     Have a look at 'jbrotli-examples' on how to use it in Tomcat, Spring Boot or Dropwizard.     This 0.4.0 release is bundled with the LATEST (un-released) version of brotli,     because it contains a security fix CVE-2016-1968. The latest Google brotli-release 0.3.0     is still vulnerable to this issue.   ###### 2016-03-05      The problem with the crashing JVM is solved: it revealed itself as a memory leak in the      OutputStream class ;-)     Also high throughput and concurrent benchmarks are now running perfectly fine.     Monitoring with jConsole or jVisualVM shows that e.g. the BrotliServletFilter behaves     on par with GZIP compression in e.g. Tomcat web server.   ###### 2016-01-31      When I did high throughput and highly concurrent benchmarks with the HTTP servers (see below),     I observed peak memory usages over 32GByte, which killed my JVM. I've searched for memory leaks,     but couldn't found any orphan objects after comparing heap dumps.     Unfortunately there isn't that much documentation on memory usage available. Which makes it difficult     to understand why there are such peaks and if these are "normal".          I don't recommend to use jbrotli in projects with many and highly concurrent compression tasks.           ## About Brotli  Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression.  It was developed by Google and released in September 2015 via this blog post: [Introducing Brotli: a new compression algorithm for the internet](http://google-opensource.blogspot.de/2015/09/introducing-brotli-new-compression.html)   ## Using jbrotli  ##### Maven jbrotli releases are available at [bintray](https://bintray.com/nitram509/jbrotli/jbrotli/)  In order to use, simply add these lines to your project's **pom.xml**  ```xml   <dependencies>     <dependency>       <groupId>org.meteogroup.jbrotli</groupId>       <artifactId>jbrotli</artifactId>       <version>0.5.0</version>     </dependency>   </dependencies>    <repositories>     <repository>       <id>bintray-nitram509-jbrotli</id>       <name>bintray</name>       <url>http://dl.bintray.com/nitram509/jbrotli</url>     </repository>   </repositories> ```  jbrotli's pom.xml will automatically select the native library, depending on your platform's operating system family and arch type (Java property *os.arch*). For a list of supported platforms, look for released ```jbrotli-native-*``` artifacts at  [jbrotli's bintray repository](https://bintray.com/nitram509/jbrotli/jbrotli#files/com/meteogroup/jbrotli).   ##### Enabling your Java Web Application (WAR file)  In order to use BrotliServletFilter, simply add these lines to your project's **pom.xml**  ```xml   <dependencies>     <dependency>       <groupId>org.meteogroup.jbrotli</groupId>       <artifactId>jbrotli-servlet</artifactId>       <version>0.5.0</version>     </dependency>   </dependencies>    <repositories>     <repository>       <id>bintray-nitram509-jbrotli</id>       <name>bintray</name>       <url>http://dl.bintray.com/nitram509/jbrotli</url>     </repository>   </repositories> ```  Then finally activate the filter in your **web.xml**  ```xml   <filter>     <filter-name>BrotliFilter</filter-name>     <filter-class>org.meteogroup.jbrotli.servlet.BrotliServletFilter</filter-class>   </filter>    <filter-mapping>     <filter-name>BrotliFilter</filter-name>     <url-pattern>/*</url-pattern>   </filter-mapping> ```  See full example at [jbrotli-examples/simple-web-app](jbrotli-examples/simple-web-app)   ##### Enabling your [Spring Boot](http://projects.spring.io/spring-boot/) or [Dropwizard](http://www.dropwizard.io/) microservice  Enabling one of the famous microservice frameworks is very easy. Just have a look a the examples ...  * [jbrotli-examples/spring-boot](jbrotli-examples/spring-boot) * [jbrotli-examples/dropwizard](jbrotli-examples/dropwizard)    ##### Example of regular BrotliCompressor with custom dictionary  ```java BrotliLibraryLoader.loadBrotli();  byte[] inBuf = "Brotli: a new compression algorithm for the internet. Now available for Java!".getBytes(); byte[] compressedBuf = new byte[2048]; BrotliCompressor compressor = new BrotliCompressor(); int outLength = compressor.compress(Brotli.DEFAULT_PARAMETER, inBuf, compressedBuf); ```   ##### Example of BrotliStreamCompressor using default dictionary   ```java BrotliLibraryLoader.loadBrotli();  byte[] inBuf = "Brotli: a new compression algorithm for the internet. Now available for Java!".getBytes(); boolean doFlush = true; BrotliStreamCompressor streamCompressor = new BrotliStreamCompressor(Brotli.DEFAULT_PARAMETER); byte[] compressed = streamCompressor.compressBuffer(inBuf, doFlush); ```   ##### Clients examples, Apache HttpComponents and Jersey  * [Apache HTTP client](jbrotli-examples/http-client/src/main/java/org/meteogroup/jbrotli/httpclient/apache/HttpClientExample.java) * [Jersey HTTP client](jbrotli-examples/http-client/src/main/java/org/meteogroup/jbrotli/httpclient/jersey/JerseyClientExample.java)   ## Performance benchmark results  Some results are documented in this spreadsheet https://docs.google.com/spreadsheets/d/1y3t_NvXrD58tKCXMvNC49EtxBMQQOE_8SPQLS6c1cJo/edit?usp=sharing   ## How to build  If you want to compile & test this library yourself, please follow this [guideline](HOWTO-BUILD.md).
amplab/succinct	# Succinct  [![Build Status](https://amplab.cs.berkeley.edu/jenkins/buildStatus/icon?job=Succinct)](https://amplab.cs.berkeley.edu/jenkins/job/Succinct/) [![License](http://img.shields.io/:license-Apache%202-red.svg)](LICENSE)  [Succinct](http://succinct.cs.berkeley.edu) is a data store that enables queries directly on a compressed representation of data. This repository maintains the  Java implementations of Succinct's core algorithms, and applications that  exploit them, such as a [Apache Spark](http://spark.apache.org/) binding for Succinct.  ## Building Succinct  Succinct is built using [Apache Maven](http://maven.apache.org/). To build Succinct and its component modules, run:      mvn clean package  ## Succinct-Core  The Succinct-Core module contains Java implementation of Succinct's core algorithms. See a more descriptive description of the core module  [here](core/README.md).  ### Dependency Information  #### Apache Maven  To build your application with Succinct-Core, you can link against this library using Maven by adding the following dependency information to your pom.xml file:  ```xml <dependency>     <groupId>amplab</groupId>     <artifactId>succinct-core</artifactId>     <version>0.1.8</version> </dependency> ```  ## Succinct on Apache Spark We provide Apache Spark and Apache Spark SQL interfaces for Succinct, which expose a compressed, queryable RDD `SuccinctRDD`, enabling manipulation of  unstructured data, and a `SuccinctKVRDD` for querying semi-structured data (key-value pairs, text and json documents, etc.). We also expose Succinct as a DataSource in Apache Spark SQL as an experimental feature. More details on the integration with Apache Spark can be found [here](spark/README.md).  ### Dependency Information  #### Apache Maven  To build your application to run with Succinct on Apache Spark, you can link against this  library using Apache Maven by adding the following dependency information to your pom.xml file:  ```xml <dependency>     <groupId>amplab</groupId>     <artifactId>succinct-spark</artifactId>     <version>0.1.8</version> </dependency> ```  #### SBT and Spark-Packages  Add the dependency to your SBT project by adding the following to `build.sbt`  (see the [Spark Packages listing](http://spark-packages.org/package/amplab/succinct) for spark-submit and Maven instructions):  ``` resolvers += "Spark Packages Repo" at "http://dl.bintray.com/spark-packages/maven" libraryDependencies += "amplab" % "succinct" % "0.1.8" ```  The succinct-spark jar file can also be added to a Spark shell using the  `--jars` command line option. For example, to include it when starting the  spark shell:  ``` $ bin/spark-shell --jars succinct-0.1.8.jar ```
wro4j/wro4j	# <img src="http://code.google.com/p/wro4j/logo"> Web Resource Optimizer for Java [![Join the chat at https://gitter.im/wro4j/wro4j](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/wro4j/wro4j?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build Status](https://api.travis-ci.org/wro4j/wro4j.svg)](http://travis-ci.org/wro4j/wro4j) [![Coverage Status](https://codecov.io/github/wro4j/wro4j/coverage.png?branch=master)](https://codecov.io/github/wro4j/wro4j?branch=master) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/ro.isdc.wro4j/wro4j-core/badge.svg)](https://maven-badges.herokuapp.com/maven-central/ro.isdc.wro4j/wro4j-core) [![Stories in Ready](https://badge.waffle.io/wro4j/wro4j.png?label=ready&title=Ready)](http://waffle.io/wro4j/wro4j)    wro4j is a free and Open Source Java project which will help you to [easily improve](http://wro4j.github.com/wro4j) your web application page loading time. It can help you to keep your static resources (js & css) [well organized](http://wro4j.readthedocs.org/en/stable/WroFileFormat), merge & minify them at [run-time](http://wro4j.readthedocs.org/en/stable/Installation) (using a simple filter) or [build-time](http://wro4j.readthedocs.org/en/stable/MavenPlugin) (using maven plugin) and has a [dozen of features](http://wro4j.readthedocs.org/en/stable/Features) you may find useful when dealing with web resources.   # Getting Started  In order to get started with wro4j, you have to follow only 3 simple steps.  ## Step 1: Add WroFilter to web.xml ```xml <filter> 	<filter-name>WebResourceOptimizer</filter-name> 	<filter-class>ro.isdc.wro.http.WroFilter</filter-class> </filter> 		  <filter-mapping> 	<filter-name>WebResourceOptimizer</filter-name> 	<url-pattern>/wro/*</url-pattern> </filter-mapping> ```		 ## Step 2: Create wro.xml ```xml <groups xmlns="http://www.isdc.ro/wro"> 	<group name="all"> 		<css>/asset/*.css</css> 		<js>/asset/*.js</js> 	</group> </groups> 		 ``` ## Step 3: Use optimized resources ```html <html>   <head> 	<title>Web Page using wro4j</title> 	<link rel="stylesheet" type="text/css" href="/wro/all.css" /> 	<script type="text/javascript" src="/wro/all.js"/>   </head> <body>  </body> </html>		 ``` 		 # Documentation  The documentation for this project is located [here](http://wro4j.readthedocs.org/en/stable/)   # Issues  Found a bug? Report it to the [issue tracker](https://github.com/wro4j/wro4j/issues)   # Feedback  If you have any questions or suggestions, please feel free to post a comment to the [discussion group](https://groups.google.com/forum/#!forum/wro4j)  [Follow me](http://twitter.com/#!/wro4j) on Twitter.   # License  This project is available under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0.html).
Bilibili/boxing	## boxing --- Android multi-media selector based on MVP mode.[中文文档](README_CN.md)  [![Build Status](https://travis-ci.org/Bilibili/boxing.svg?branch=master)](https://travis-ci.org/Bilibili/boxing)  #### boxing Inside:  [![bili](screenshot/bili.webp)](https://play.google.com/store/apps/details?id=tv.danmaku.bili)  ### Feature --- - Custom UI - Multiple/single selection, preview and crop function - Gif support - Video selection - Image compression  ### Download --- Core version: only contain the core function.  UI version: contain UI implements base on core version.  - Maven  Core version  ```xml <dependency>                          	<groupId>com.bilibili</groupId>     	<artifactId>boxing</artifactId>     	<version>1.0.0</version>   	<type>pom</type>                 </dependency>                      		 ```  UI version  ```xml <dependency>                             	<groupId>com.bilibili</groupId>        	<artifactId>boxing-impl</artifactId>   	<version>1.0.0</version>      	<type>pom</type>                     </dependency>                          ```  - Gradle  Core version               ```java compile 'com.bilibili:boxing:1.0.0' ```  UI version ```java compile 'com.bilibili:boxing-impl:1.0.0' ```  ### Preview  ![multi_image](screenshot/multi_image.webp) ![single_image_crop](screenshot/single_image_crop.webp) ![video](screenshot/video.webp)  ### Getting Started  - Media loading initialization(required) ```java BoxingMediaLoader.getInstance().init(new IBoxingMediaLoader()); // a class implements IBoxingMediaLoader  ```  - Image cropping initialization(optional) ```java BoxingCrop.getInstance().init(new IBoxingCrop());  // a class implements IBoxingCrop  ``` - Build BoxingConfig   Specify the mode(Mode.SINGLE_IMG, Mode.MULTI_IMG, Mode.VIDEO) with camera and gif support.  ```java BoxingConfig config = new BoxingConfig(Mode); // Mode：Mode.SINGLE_IMG, Mode.MULTI_IMG, Mode.VIDEO config.needCamera(cameraRes).needGif().withMaxCount(9) // camera, gif support, set selected images count .withMediaPlaceHolderRes(resInt) // set the image placeholder, default 0 .withAlbumPlaceHolderRes(resInt) // set the album placeholder, default 0 .withVideoDurationRes(resInt) // set the video duration resource in video mode, default 0 ``` - Get Boxing, set Intent and call start ```java // start thumbnails Activity, need boxing-impl. Boxing.of(config).withIntent(context, BoxingActivity.class).start(callerActivity, REQUEST_CODE);    	 // start view raw image Activity, need boxing-impl. Boxing.of(config).withIntent(context, BoxingViewActivity.class).start(callerActivity, REQUEST_CODE);    	 // call of() use Mode.MULTI_IMG by default. Boxing.of().withIntent(context, class).start(callerActivity, REQUEST_CODE); ``` - Get Result ```java @Override protected void onActivityResult(int requestCode, int resultCode, Intent data) {   	List<BaseMedia> medias = Boxing.getResult(data);   	// avoid null } ``` ### Advanced usage Media loading and image cropping initialization are the same as Simple Usage.  - Customize Activity and Fragment   Extends AbsBoxingViewActivity and AbsBoxingViewFragment.   call `Boxing.of(config).withIntent(context, AbsBoxingViewActivity.class).start(callerActivity, REQUEST_CODE);` to start.  - Only customize Fragment   Extends AbsBoxingViewFragment,no AbsBoxingViewActivity.   call `Boxing.of(BoxingConfig).setupFragment(AbsBoxingViewFragment, OnFinishListener);` to start.   ### FileProvider                                                                                    Use camera in Android N, add this in AndroidManifest.xml                                                         ```xml <provider                                                  	android:name="android.support.v4.content.FileProvider" 	android:authorities="${applicationId}.file.provider" >                	<meta-data                                             		android:name="android.support.FILE_PROVIDER_PATHS" 		android:resource="@xml/boxing_file_provider"/> </provider>                  ```  ### Kotlin Support Stay hungry, stay foolish. checkout `feature/kotlin` for fun.  ### TODO Support different config at the same moment.  ### License ---- Copyright 2017 Bilibili Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0) Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
jpenninkhof/odata-boilerplate	# OData Boilerplate  The OData Boilerplate combines OpenUI5 with Spring Boot, OLingo and JPA and allows you to easily and quickly boot up a JVM based OData app based on modeling your data model in using JPA.  For convenience, a base-diagram for the JPA diagram editor has been provided as well. Once your data model is finished, the application with update the database schema of the connected database on first run. After every change and restart, the application will attempt to alter the database schema according to the changes detected in the JPA model.  By default, the application comes with just one entity: Members, that is automatically populated with a few names from Application.java  Once the application is running, you can browse to http://localhost:8080 to run the sample OpenUI5 application that is using the OData service. The service itself is available from http://localhost:8080/odata.svc  ## Features  - Deploys on any VM, including Tomcat, HANA Cloud Platform, Azure, but also on plain Java using built-in Tomcat - Full CRUD support on all JPA entities - Feature-complete OData v2 support, including $filter, $expand, $select, $count, $top, etc - Language dependent annotation support for Smart controls (through i18n.properties files) - Connects to most SQL databases, including SQL server, HANA, MySQL, HSQLDB, Derby - Automatically creates database schema based on JPA model  ## Prerequisites  To be able to run this application, you need Java JDK 1.7 (or higher) and Maven.  ## Installation  To install the boilerplate, you can just clone this repository to your computer.  After cloning it, set the connection parameters to connect to a database in `application.properties`  ## Usage  To run the application just enter `mvn spring-boot:run -P jar`  To stop the application press control-C  To build a jar run `mvn clean package`  To run the jar `mvn -jar target/odata-0.1.0.jar`  ## HANA Cloud Platform  The boilerplate is compatible with both the local as well as the cloud-runtime of HANA cloud platform and will compile to a WAR when Maven profile 'hcp' is selected (which happens to be the default ;).  When the application is run on the HCP cloud, it will automatically connect to the default database connection and start updating the database schema there. It is of couse still possible to connect the Java application to another data-source using the cockpit.  When you run the application from Eclipse on a local HANA Cloud Platform runtime, the site will be available on http://localhost:8080/odata. Also, through configuration of web.xml, authentication is switched on. When you run the application on a local HCP runtime without modifying web.xml, you need to add a user to your local server. You can do this by double clicking your HCP local server and adding a user to the 'users' tab.  ## Azure  To get the application running on Azure, it is probably easiest to compile the app to a jar using `mvn -P jar`, After that, you can just pick up the jar file from the target directory and FTP it into an Azure web application. To make Azure aware of the jar file, point the web.config to the jar file, as described on this page: https://azure.microsoft.com/documentation/articles/web-sites-java-custom-upload/#springboot  To make the logging go into the right directory, make sure this line is present in the application.properties file in the resources directory: `logging.path = D://home//LogFiles`. You may also want to remove the logback.xml from the resources directory, which is required for HANA Cloud Platform, but not for Azure.  ## Respository package  The only reason that the repository package is there, is because I liked the boilerplate to create some initial (demo/test) data on first launch. However, if you don't need this, it is safe to remove the repository package. It is not necessary to create a repository class for every entity in the model and only modelling entity classes will suffice when you just want to have tables in the database and expose them through OData.  ## Annotations  There are some limited (for now) features to add annotations allowing you to leverage smart controls and templates. At this moment it is possible to add annotations on attribute level by applying the `@Sap` annotation and/or @SAPLineItem just before the attribute definition, e.g.:  	@Sap(filterable=true, sortable=true) 	@SAPLineItem 	private String lastName;  The `sap:label` annotation with a language dependent label is automatically added, if the label has been defined in an `i18n` table in the resources directory. i18n-tags should be formatted according to <entity>.<attribut> notation, for example: `Member.LastName=Achternaam`. A sample i18n table has been provided in the boilerplate.  Applying the @SAP annotation  will result in SAP annotations being added in an almost similar to what SAP Gateway would do:  	<Property 		Name="LastName" Type="Edm.String" 		sap:label="Achternaam" 		sap:filterable="true" sap:updatable="false" sap:sortable="true" sap:creatable="false" 		xmlns:sap="http://www.sap.com/Protocols/SAPData"/>  Applying the @SAPLineItem annotation will add the annotated property to the UI.LineItem section, which contains the default fields show in the Smart Table control.  Please note that the annotations feature is not complete, but can very easily be extended when you leverage the boilerplate logic provided in `JPAEdmExtension`.  ## Contributing  1. Fork it! 2. Create your feature branch: `git checkout -b my-new-feature` 3. Commit your changes: `git commit -am 'Add some feature'` 4. Push to the branch: `git push origin my-new-feature` 5. Submit a pull request :D  ## Example  To have a look at the boilerplate output, please have a look at any of these urls:  - Sample OpenUI5 app: http://odataboilerplate.azurewebsites.net/ - OData endpoint: http://odataboilerplate.azurewebsites.net/odata.svc/ - OData metadata: http://odataboilerplate.azurewebsites.net/odata.svc/$metadata  ## License  This software is published under MIT license.  Please refer to LICENSE.TXT for more details
dhavaln/spring-rest-boilerplate	To run the integration tests included in this example with Maven use:  mvn verify  To deploy the web service on a jetty use:  mvn jetty:run  The web service is now available on http://localhost:9090  For test  1) Call http://localhost:9090/spring-rest-example/login with username, password and token (=test) parameter and it will create the security context
Saka7/spring-boot-angular4-boilerplate	# Spring Boot + Angular 4 Boilerplate  ![spring-boot-angular4-boilerplate](https://raw.githubusercontent.com/Saka7/spring-boot-angular4-boilerplate/master/frontend/src/assets/images/favicon.png)  [![Code Climate](https://codeclimate.com/github/Saka7/spring-boot-angular4-boilerplate/badges/gpa.svg)](https://codeclimate.com/github/Saka7/spring-boot-angular4-boilerplate) [![Issue Count](https://codeclimate.com/github/Saka7/spring-boot-angular4-boilerplate/badges/issue_count.svg)](https://codeclimate.com/github/Saka7/spring-boot-angular4-boilerplate)  Quick start for Spring Boot + Angular 4 projects with JWT auth  ## Includes:  Front-end:  - angular-cli boilerplate files - JWT authentication service  Back-end:  - gradle build file - boilerplate files - JWT authentication  ## Setup  You can use `setup.sh` script to change name and version of the app and database connection properties. Just run `sh setup.sh` and follow the instructions.  ## Build and Run  First of all you need to configure the database. Properties are located in `./backend/src/main/resources/application.properties` file.  > By default application is using PostgreSQL database(name: `test`, user: `test`, password: `test`).  Also you need to configure JWT secret in file listed above.  1. Run `npm install --prefix frontend` to install front-end dependencies. 2. Run `npm run build:prod --prefix frontend` to build angular application. 3. Run `./init_db` to create database, dbuser and dump default schema. 4. Run `gradle build -p backend` to build a spring boot application. 5. Run `gradle bootRun -p backend ` or `java -jar backend/build/libs/app-name-[version].jar` to start spring boot application on embedded server.  > By default server will be running on port `8080`.   ## Depelopment  - `npm start --prefix frontend` to start front-end server for development. - `npm run start:prod --prefix frontend` to start front-end server with service-workers. - `gradle bootRun -p backend ` to start spring boot application on embedded server.  > By default server will be running on port `4200`  ## Testing  - `npm test --prefix frontend` - to run front-end unit tests. - `npm run e2e --prefix frontend` - to run end to end tests. - `gradle test -p backend` - to run server tests.  ## Technologies used  - [spring-boot 1.5.3](https://projects.spring.io/spring-boot/) - [spring-mvc 4.3.6](https://docs.spring.io/spring/docs/current/spring-framework-reference/html/mvc.html) - [spring-data-jpa 1.11.0](http://projects.spring.io/spring-data-jpa/) - [spring-security 4.2.1](https://projects.spring.io/spring-security/) - [jjwt 0.7.0](https://github.com/jwtk/jjwt) - [lombok 1.16.12](https://projectlombok.org/) - [junit 4.11](http://junit.org/junit4/) - [gradle 3.3](https://gradle.org/) - [postgresql 9.5](https://www.postgresql.org/) - [h2 1.4](http://www.h2database.com/html/main.html) - [angular-cli 1.1.1](https://cli.angular.io/) - [angular 4.2.2](https://angular.io/) - [rxjs 5](http://reactivex.io/rxjs/) - [jasmine 2.5](https://jasmine.github.io/) - [karma 1.0](https://karma-runner.github.io/1.0/index.html) - [protractor 4](http://www.protractortest.org/#/)  ## License spring-boot-angular4-boilerplate is released under the [MIT License](https://opensource.org/licenses/MIT).
omerio/appstart	Appstart ========  Appstart is an opinionated Java boilerplate template for rapid development of multi-module [Google App Engine][1] applications based on [Google Guice][2], a lightweight dependency injection framework. Appstart uses the [App Engine Maven plugin][4], it has a parent pom and 3 maven projects. 2 of the projects (appstart-front and appstart-backend) are App Engine modules.  App Engine multi-module applications are organized as an unpacked [Java Enterprise Archive (EAR)][17] directory structure. This project contains the following directories:  - <b>appstart-ear</b> contains the EAR deployment descriptors `application.xml` and `appengine-application.xml` - <b>appstart-frontend</b> is configured to use a frontend instance `<instance-class>F1</instance-class>` and  - <b>appstart-backend</b> is configured to use a backend instance `<instance-class>B2</instance-class>`.  - The third project <b>appstart-common</b> is a common code project shared between the <b>appstart-frontend</b> and <b>appstart-backend</b> modules handy when you are developing multiple modules with shared code.  Appstart encourages the [Single Page Application (SPA)][13] pattern by providing backend services as APIs that can be invoked from any frontend, be it Web or Mobile. The philosophy of Appstart is to enable you to spend less or no time writing boilerplate code and more time focusing on your business logic.  ## Demo ##  A live demo is available here [https://appstart-web.appspot.com/][16].  Explore the Cloud Endpoint API for Appstart [here](https://apis-explorer.appspot.com/apis-explorer/?base=https://appstart-web.appspot.com/_ah/api#p/appstart/v1/).   ## Overall Architecture  ![Alt text](http://omerio.com/wp-content/uploads/2016/01/GAE-Appstart-Architecture.png "Architecture")   ## Key Technologies/Libraries  The following key technologies/libraries are used by Appstart. You can either use some or all of these technologies depending on your needs:  ### App Engine Services  - <b>cron.xml</b> for a cron job that run every midnight on the backend module and archives any old todos. This must be included with the default module (appstart-frontend) rather than any other modules as mentioned [here][18]. - <b>Cloud Endpoints</b> is configured on the default module (appstart-frontend).   ### Backend Technologies - <b>[Google Guice][2]</b>: App Engine friendly, lightweight dependency injection framework.  - <b>[Objectify][6]</b>: Convenient data access API for the App Engine datastore. - <b>[Cloud Endpoints][3]</b>: App Engine REST/RPC API to simplify clients (Android, iOS, etc..) access. - <b>[Jersey & Jersey Guice integration][7]</b>: Jersey framework is an implementation of the JAX-RS (JSR 311 & JSR 339) to provide RESTful Web services in Java.  - <b>[Lombok][8]</b>: A framework for boilerplate code generation such as getter, setter, constructors, equalTo, hashCode, etc... Saving you time to focus on your business logic. Note: for the code to compile in your IDE and not to show compile errors, you need to install Lombok in your IDE as explained [here][21]. - <b>[Google Guava][9], [Google Gson][11] and [Apache Commons Lang][10]</b>: Useful Java libraries that plays well with App Engine. - <b>[App Engine Unit Testing Libraries][14] & [jMockit][15]</b>: Creating unit tests using the App Engine local unit testing libraries. Mocking is achieved using the jMockit mocking library.    ### Frontend Technologies - <b>[AngularJS TodoMVC][12]</b>: Frontend page uses the TodoMVC AngularJS as an example. You can integrate your own frontend technology with Appstart.   ## Usage: You need to change the following values with your own:  - App Engine app-id (`<appengine.app.name>appstart-web</appengine.app.name>`) in appstart/pom.xml  - The module versions (`<appengine.app.version>v1</appengine.app.version>`) for appstart-front and appstart-backend located in the pom.xml for each module. - For [Google Cloud Endpoints][3] Update the values in [(`appstart/appstart-frontend/src/main/java/uk/co/inetria/appstart/Constants.java`)][19] and [(`appstart-frontend/src/main/webapp/js/endpoint.js`)][20] to reflect the respective client IDs you have registered in the [APIs Console][5].  To run or deploy the application (all modules): ```bash     git clone https://github.com/omerio/appstart.git     cd appstart     mvn install     cd appstart-ear     #to test it locally:     mvn appengine:devserver     #or to deploy it:     mvn appengine:update ```      To deploy or run individual modules: ```bash     cd appstart-frontend     #to test it locally:     mvn appengine:devserver     #or to deploy it:     mvn appengine:update ```    ### Local URLs: - Homepage (frontend module): [http://localhost:8888/](http://localhost:8888/) - API Explorer for Cloud Endpoints: [http://localhost:8888/_ah/api/explorer](http://localhost:8888/_ah/api/explorer) - Backend module (port number might be different, check you dev server output): [http://localhost:54423](http://localhost:54423)  ## TODO: * Create documentations. * Add sample unit tests that uses the App Engine local unit testing libraries. * ~~Create Cloud Endpoints frontend code to authenticate a user and refresh the OAuth token once it expires.~~ * ~~Provide an option on the frontend to use either the JAX-RS REST or Cloud Endpoints.~~ * Create a Maven archetype to easily scaffold projects.  ## Contributing:  Contributions are welcome and encouraged, simply fork the project make your changes and submit a pull request.  ## License  Open Source (Apache License 2.0)   [1]: https://developers.google.com/appengine [2]: https://github.com/google/guice/wiki/GoogleAppEngine [3]: https://developers.google.com/appengine/docs/java/endpoints/ [4]: https://developers.google.com/appengine/docs/java/tools/maven [5]: https://console.developers.google.com/ [6]: https://code.google.com/p/objectify-appengine/ [7]: https://jersey.java.net/ [8]: http://projectlombok.org/ [9]: https://code.google.com/p/guava-libraries/ [10]: http://commons.apache.org/proper/commons-lang/ [11]: https://code.google.com/p/google-gson/ [12]: http://todomvc.com/examples/angularjs/#/ [13]: http://omerio.com/2014/03/23/single-page-apps-a-bleeding-edge-new-concept-or-a-revived-old-one/ [14]: https://cloud.google.com/appengine/docs/java/tools/localunittesting [15]: http://jmockit.org/ [16]: https://appstart-web.appspot.com/ [17]: https://en.wikipedia.org/wiki/EAR_(file_format) [18]: https://cloud.google.com/appengine/docs/java/modules/#optional_configuration_files [19]: https://github.com/omerio/appstart/blob/master/appstart-frontend/src/main/java/uk/co/inetria/appstart/frontend/Constants.java [20]: https://github.com/omerio/appstart/blob/master/appstart-frontend/src/main/webapp/js/endpoint.js [21]: http://jnb.ociweb.com/jnb/jnbJan2010.html#installation
Prokky/ClearRecyclerAdapter	# ClearRecyclerAdapter  [![Build Status](https://travis-ci.org/Prokky/ClearRecyclerAdapter.svg?branch=master)](https://travis-ci.org/Prokky/ClearRecyclerAdapter) [![codecov](https://codecov.io/gh/Prokky/ClearRecyclerAdapter/branch/master/graph/badge.svg)](https://codecov.io/gh/Prokky/ClearRecyclerAdapter) [ ![Download](https://api.bintray.com/packages/prokky/maven/clearrecycleradapter/images/download.svg) ](https://bintray.com/prokky/maven/clearrecycleradapter/_latestVersion) [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-ClearRecyclerAdapter-brightgreen.svg?style=flat)](http://android-arsenal.com/details/1/3961)  Boilerplate code free Adapter for RecyclerView. Now supports different states for easy handling errors/progress.  ##Usage To include this library to your project add dependency in **build.gradle** file: ```java dependencies {     compile 'com.prokkypew:clearrecycleradapter:1.1.0' } ``` ###ClearRecyclerAdapter Create your Adapter class which extends ClearRecyclerAdapter and implement some functions. ```java  public class YourAdapter extends ClearRecyclerAdapter<YourObject> {     @Override     protected ClearRecyclerViewHolder getViewHolder(View view, int viewType) {         return new YourViewHolder(view);     }      @Override     protected int getItemLayoutId(int viewType) {         return R.layout.your_list_item;     }      public class YourViewHolder extends ClearRecyclerAdapter.ClearRecyclerViewHolder<YourObject> {         @BindView(R.id.title)         TextView titleText;          public YourViewHolder(View itemView) {             super(itemView);             Butterknife.bind(this, v); //just do it any way you like         }          @Override         public void bind(YourObject item) {             titleText.setText(item.name); //onBindViewHolder code here         }     } } ```  Modify data int the adapter with this: ```java adapter.set(List<T> items); adapter.addAll(List<T> items); adapter.add(T item); adapter.add(T item, int position); adapter.update(T item); adapter.remove(T item); adapter.clear(); List<T> items = adapter.getItems(); ``` If you need the View types for your adapter, feel free to use viewType parameter in getViewHolder(View, int) and getItemLayoutId(int) functions. Don't forget to override getItemViewType(int) int your adapter: ```java     @Override     protected ClearRecyclerViewHolder getViewHolder(View view, int viewType) {         switch(viewType){             case FIRST: return new FirstViewHolder(view);             case SECOND:             default: return new SecondViewHolder(view);         }     }      @Override     protected int getItemLayoutId(int viewType) {         switch(viewType){             case FIRST: return R.layout.first_layout;             case SECOND:             default: return R.layout.second_layout;         }     }     @Override     public int getItemViewType(int position) {         // return viewtype for position     } ``` ###StateRecyclerAdapter If you want easy progress/error states in your RecyclerView, simply extend StateRecyclerAdapter and implement its functions. ```java public class SampleStateAdapter extends StateRecyclerAdapter<SampleObject> {      @Override     public int getErrorLayoutId() {         return R.layout.sample_error_layout;     }      @Override     protected int getItemLayoutId(int viewType) {         return R.layout.sample_item_layout;     }      @Override     public int getProgressLayoutId() {         return R.layout.sample_progress_layout;     }      @Override     protected ClearRecyclerViewHolder getViewHolder(View view, int viewType) {         return new SampleViewHolder(view);     } } ``` Then just make some calls for changing state, and you'll see your progress/error/item view show. ```java     adapter.setState(StateRecyclerAdapter.AdapterState.ERROR);     adapter.setState(StateRecyclerAdapter.AdapterState.ITEMS);     adapter.setState(StateRecyclerAdapter.AdapterState.PROGRESS); ```  And you are done!  # License  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
lukaspili/Auto-Mortar	# Auto Mortar  Mortar / Flow / Dagger 2 requires to write a lot of boilerplate code.   Auto Mortar is an annotation processor that focuses on eliminating the maximum of that boilerplate. No magic tricks here, just some straightforward and human readable code generated for you.   ### "Traditional" way  ```java  // ShowUserScreen.java  @Layout(R.layout.screen_show_user) public class ShowUserScreen extends Path {      private String username;      public ShowUserScreen(String username) {         this.username = username;     }          // We also need a method that creates the dagger2 component     // either by reflection or by some method like the following     public Component createComponent(RootActivity.Component parent) { 	    return DaggerShowUserScreen.Component.builder() 	    	.component(parent) 	    	.module(new Module()) 	    	.build();     }       @dagger.Component(dependencies = RootActivity.Component.class, modules = Module.class)     @DaggerScope(ShowUserPresenter.class)     public interface Component extends AppDependencies {         void inject(ShowUserView view);     }      @dagger.Module     public class Module {          @Provides         @DaggerScope(ShowUserPresenter.class)         public Presenter providePresenter(RestClient restClient) {             return new Presenter(username, restClient);         }     }      public static class Presenter extends ViewPresenter<ShowUserView> {          private final String username;         private final RestClient restClient;          public Presenter(String username, RestClient restClient) {             this.username = username;             this.restClient = restClient;         }     } }  ```  ### With Auto Mortar  You only write the Presenter class.  ```java // ShowUserPresenter.java  @AutoScreen(         component = @AutoComponent(dependencies = RootActivity.class, superinterfaces = AppDependencies.class),         screenAnnotations = Layout.class ) @DaggerScope(ShowUserPresenter.class) @Layout(R.layout.screen_show_user) public class ShowUserPresenter extends ViewPresenter<ShowUserView> {      private final String username;     private final RestClient restClient;      public ShowUserPresenter(@ScreenParam String username, RestClient restClient) {         this.username = username;         this.restClient = restClient;     } }  ```  ### The big picture  1. Create the presenter class (recommanded name syntax is `XyzPresenter`) 2. Annotate the presenter with `@AutoScreen` 3. Rebuild the project in order to trigger the annotation processor 4. Use `XyzScreen` and `XyzScreenComponent` as you wish :)   ### How does it work  The annotation processor generates during compilation 3 classes for each `@AutoScreen` annotated presenter.   For the ShowUserPresenter class, it will generate:  - ShowUserScreen - ShowUserScreenComponent - ShowUserScreen.Module  All the generated code is readable and accessible in your IDE, in the same way Dagger 2 does.   #### Screen  The generated screen is named XyzScreen, while your presenter should be named XyzPresenter.   The screen contains the generated Module as a static subclass.    The generated screen can be annoted with your custom annotation.   In order to do so, you have to first annotate the presenter with that annotation, and then specify the annotation class in `@AutoScreen screenAnnotations` member.  For instance, if you have a `@Layout` annotation you want to apply on the generated screen:  ```java @AutoScreen( 	screenAnnotations = Layout.class ) @Layout(R.layout.my_layout) public class PostsPresenter {} ```  The generated screen will look like:  ```java @Layout(2130903043) // equals to R.layout.my_layout public final class PostsScreen {} ```  If you configured Auto Mortar to make the generated screens extend from `Path`, you can then use them with Flow.  ```java `Flow.get(context).set(new ViewPostScreen(1234))` ```  To see how to configure Auto Mortar and how to pass navigation parameters between screens, see below.   #### Component  The component generation relies on the Auto Dagger2 library.   See the readme of Auto Dagger2 for details: [https://github.com/lukaspili/Auto-Dagger2](https://github.com/lukaspili/Auto-Dagger2)   #### Module  The generated module is named XyzScreen.Module and declares a provider method for the presenter. The right dependencies will be injected.   **The presenter must use constructor injection for its dependencies. Field or setter injection is not supported.**  Generated module looks like:  ```java @dagger.Module   public class Module {     @Provides     @ScreenScope(YourNamePresenter.class)     public YourNamePresenter providePresenter(RestClient restClient) {       return new YourNamePresenter(restClient);     }   } ```   ### Auto Mortar configuration  You can also customize the code generation through a configuration annotation.   Create a new empty interface (or class), and annotate it with `@AutoMortarConfig`. All configuration options are exposed as members of `@AutoMortarConfig`.  ```java @AutoMortarConfig( 	screenSuperclass = Path.class // all generated screens will extend from Path ) interface Config { } ```  If you don't provide a configuration, the default will be used (see `automortar.config.DefaultAutoMortarConfig`). Only one configuration per project is supported.   ### Passing parameters between screens  When you navigate from one screen to another, you often want to pass some parameters.   `@AutoScreen` generates all for you. You just have do declare the navigation parameters in your presenter, like a normal dependecy injected by dagger.   The only difference is that you have to annotate it with `@ScreenParam` in the presenter constructor.  ```java public class ShowUserPresenter extends ViewPresenter<ShowUserView> {      private final String username; // username will be provided by another screen through Flow navigation     private final RestClient restClient; // restclient is provided by some dagger component      public ShowUserPresenter(@ScreenParam String username, RestClient restClient) {         this.username = username;         this.restClient = restClient;     } } ```  That's all! `@AutoScreen` will generate the following screen and module for you.  ```java public final class ShowUserScreen {   private String username;    public ShowUserScreen(String username) {     this.username = username;   }    @dagger.Module   public class Module {     @Provides     @ScreenScope(ShowUserPresenter.class)     public ShowUserPresenter providePresenter(RestClient restClient) {       return new ShowUserPresenter(username, restClient);     }   } } ```  Finally, navigate between screens like you would normally do:   `Flow.get(context).set(new ShowUserScreen("lukasz"))`   ### Component factory and helper  When using Mortar and Flow together, you would setup a context factory that setups the mortar context associated with the screen to display. You would use `DaggerService.createComponent()` to create the component associated to the screen, using reflection.   `@AutoScreen` generates for you the method that create the component without reflection. The generated screen implements a `ScreenComponentFactory` interface that declares the createComponent method. It takes the component dependency as parameter, and returns an instance of the component. It looks like:  ```java public final class ViewPostScreen implements ScreenComponentFactory<RootActivityComponent> {    @Override   public Object createComponent(RootActivityComponent dependency) {     return DaggerViewPostScreenComponent.builder()     	.component(dependency)     	.module(new Module())     	.build();   } ```  The generated screen provides also a helper static get method that retreives the component from the context:  ```java public final class ViewPostScreen implements ScreenComponentFactory {    public static ViewPostScreenComponent getComponent(Context context) {     return (ViewPostScreenComponent) context.getSystemService(AutoMortarConfig.DAGGER_SERVICE_NAME);   } ```   ### Dagger scope  In the same way as Auto Dagger2 works, you need to annotate your presenter with a dagger scope annotation (an annotation that is itself annotated with `@Scope`). Auto mortar will detect this annotation, and will apply it on the generated module and component.  If you don't provide scope annotation on the presenter, the generated module and component will be unscoped.   ## Demo  Check the sample project for demo.   You can also check a full demo project with Auto Mortar here:  [https://github.com/lukaspili/Power-Mortar-Flow-Dagger2-demo](https://github.com/lukaspili/Power-Mortar-Flow-Dagger2-demo)   ## Installation  Beware that the groupId changed to **com.github.lukaspili.automortar**  ```groovy buildscript {     repositories {         jcenter()     }     dependencies {         classpath 'com.android.tools.build:gradle:1.1.3'         classpath 'com.neenbedankt.gradle.plugins:android-apt:1.4'     } }  apply plugin: 'com.android.application' apply plugin: 'com.neenbedankt.android-apt'  repositories {     jcenter() }  dependencies {     apt 'com.github.lukaspili.automortar:automortar-compiler:1.1'     compile 'com.github.lukaspili.automortar:automortar:1.1'          apt 'com.github.lukaspili.autodagger2:autodagger2-compiler:1.1'     compile 'com.github.lukaspili.autodagger2:autodagger2:1.1'      apt 'com.google.dagger:dagger-compiler:2.0.1'     compile 'com.google.dagger:dagger:2.0.1'     provided 'javax.annotation:jsr250-api:1.0'      compile 'com.squareup.mortar:mortar:0.17'     compile 'com.squareup.flow:flow:0.10'     compile 'com.squareup.flow:flow-path:0.10' } ```   ## Status  Stable API.   ## Author  - Lukasz Piliszczuk ([@lukaspili](https://twitter.com/lukaspili))   ## License  Auto Mortar is released under the MIT license. See the LICENSE file for details.
lucko/helper	![alt text](https://i.imgur.com/zllxTFp.png "Banner") # helper [![Build Status](https://ci.lucko.me/job/helper/badge/icon)](https://ci.lucko.me/job/helper/) A utility to reduce boilerplate code in Bukkit plugins. It gets boring writing the same old stuff again and again. :)  ### Modules ##### [`helper`](https://github.com/lucko/helper/tree/master/helper): The main helper project [![Artifact](https://img.shields.io/badge/build-artifact-green.svg)](https://ci.lucko.me/job/helper/lastSuccessfulBuild/artifact/helper/target/helper.jar) [![Dependency Info](https://img.shields.io/badge/api-dependency_info-orange.svg)](https://github.com/lucko/helper#helper) [![JavaDoc](https://img.shields.io/badge/api-javadoc-blue.svg)](https://lucko.me/helper/javadoc/helper/)  ##### [`helper-sql`](https://github.com/lucko/helper/tree/master/helper-sql): Provides SQL datasources using HikariCP. [![Artifact](https://img.shields.io/badge/build-artifact-green.svg)](https://ci.lucko.me/job/helper/lastSuccessfulBuild/artifact/helper-sql/target/helper-sql.jar) [![Dependency Info](https://img.shields.io/badge/api-dependency_info-orange.svg)](https://github.com/lucko/helper#helper-sql) [![JavaDoc](https://img.shields.io/badge/api-javadoc-blue.svg)](https://lucko.me/helper/javadoc/helper-sql/)  ##### [`helper-redis`](https://github.com/lucko/helper/tree/master/helper-redis): Provides Redis clients and implements the helper Messaging system using Jedis. [![Artifact](https://img.shields.io/badge/build-artifact-green.svg)](https://ci.lucko.me/job/helper/lastSuccessfulBuild/artifact/helper-redis/target/helper-redis.jar) [![Dependency Info](https://img.shields.io/badge/api-dependency_info-orange.svg)](https://github.com/lucko/helper#helper-redis) [![JavaDoc](https://img.shields.io/badge/api-javadoc-blue.svg)](https://lucko.me/helper/javadoc/helper-redis/)  ##### [`helper-mongo`](https://github.com/lucko/helper/tree/master/helper-mongo): Provides MongoDB datasources. [![Artifact](https://img.shields.io/badge/build-artifact-green.svg)](https://ci.lucko.me/job/helper/lastSuccessfulBuild/artifact/helper-mongo/target/helper-mongo.jar) [![Dependency Info](https://img.shields.io/badge/api-dependency_info-orange.svg)](https://github.com/lucko/helper#helper-mongo) [![JavaDoc](https://img.shields.io/badge/api-javadoc-blue.svg)](https://lucko.me/helper/javadoc/helper-mongo/)  ##### [`helper-lilypad`](https://github.com/lucko/helper/tree/master/helper-lilypad): Implements the helper Messaging system using LilyPad. [![Artifact](https://img.shields.io/badge/build-artifact-green.svg)](https://ci.lucko.me/job/helper/lastSuccessfulBuild/artifact/helper-lilypad/target/helper-lilypad.jar)  ## Feature Overview  * [`Events`](#events) - functional event handling and flexible listener registration * [`Scheduler`](#scheduler) - easy access to the Bukkit scheduler * [`Promise`](#promise) - a chain of operations (Futures) executing between both sync and async threads * [`Metadata`](#metadata) - metadata with generic types, automatically expiring values and more * [`Messenger`](#messenger) - message channel abstraction * [`Commands`](#commands) - create commands using the builder pattern * [`Scoreboard`](#scoreboard) - asynchronous scoreboard using ProtocolLib * [`GUI`](#gui) - lightweight by highly adaptable and flexible menu abstraction * [`Menu Scheming`](#menu-scheming) - easily design menu layouts without having to worry about slot ids * [`Plugin Annotations`](#plugin-annotations) - automatically create plugin.yml files for your projects using annotations * [`Maven Annotations`](#maven-annotations) - download & install maven dependencies at runtime * [`Terminables`](#terminables) - a family of interfaces to help easily manipulate objects which can be unregistered, stopped, or gracefully halted * [`Serialization`](#serialization) - immutable and GSON compatible alternatives for common Bukkit objects * [`Bungee Messaging`](#bungee-messaging) - wrapper for BungeeCord's plugin messaging API  ... and much more!  * [**How to add helper to your project**](#using-helper-in-your-project) * [**Standalone plugin download**](https://ci.lucko.me/job/helper/)   ## Features ### [`Events`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/Events.java) helper adds a functional event handling utility. It allows you to dynamically register event listeners on the fly, without having to break out of logic, or define listeners as their own method.  Instead of *implementing Listener*, creating a *new method* annotated with *@EventHandler*, and *registering* your listener with the plugin manager, with helper, you can subscribe to an event with one simple line of code. This allows you to define multiple listeners in the same class, and register then selectively.  ```java Events.subscribe(PlayerJoinEvent.class).handler(e -> e.setJoinMessage("")); ```  It also allows for more advanced handling. You can set listeners to automatically expire after a set duration, or after they've been called a number of times. When constructing the listener, you can use Java 8 Stream-esque `#filter` predicates to refine the handling, as opposed to lines of `if ... return` statements.  ```java Events.subscribe(PlayerJoinEvent.class)         .expireAfter(2, TimeUnit.MINUTES) // expire after 2 mins         .expireAfter(1) // or after the event has been called 1 time         .filter(e -> !e.getPlayer().isOp())         .handler(e -> e.getPlayer().sendMessage("Wew! You were first to join the server since it restarted!")); ```  The implementation provides a selection of default filters. ```java Events.subscribe(PlayerMoveEvent.class, EventPriority.MONITOR)         .filter(Events.DEFAULT_FILTERS.ignoreCancelled())         .filter(Events.DEFAULT_FILTERS.ignoreSameBlock())         .handler(e -> {             // handle         }); ```  You can also merge events together into the same handler, without having to define the handler twice. ```java Events.merge(PlayerEvent.class, PlayerQuitEvent.class, PlayerKickEvent.class)         .filter(e -> !e.getPlayer().isOp())         .handler(e -> {             Bukkit.broadcastMessage("Player " + e.getPlayer() + " has left the server!");         }); ```  This also works when events don't share a common interface or super class. ```java Events.merge(Player.class)         .bindEvent(PlayerDeathEvent.class, PlayerDeathEvent::getEntity)         .bindEvent(PlayerQuitEvent.class, PlayerEvent::getPlayer)         .handler(e -> {             // poof!             e.getLocation().getWorld().createExplosion(e.getLocation(), 1.0f);         }); ```  Events handling can be done alongside the Cooldown system, allowing you to easily define time restrictions on certain events. ```java Events.subscribe(PlayerInteractEvent.class)         .filter(e -> e.getAction() == Action.RIGHT_CLICK_AIR)         .filter(PlayerInteractEvent::hasItem)         .filter(e -> e.getItem().getType() == Material.BLAZE_ROD)         .withCooldown(                 CooldownCollection.create(t -> t.getPlayer().getName(), Cooldown.of(10, TimeUnit.SECONDS)),                 (cooldown, e) -> {                     e.getPlayer().sendMessage("This gadget is on cooldown! (" + cooldown.remainingTime(TimeUnit.SECONDS) + " seconds left)");                 })         .handler(e -> {             // play some spooky gadget effect             e.getPlayer().playSound(e.getPlayer().getLocation(), Sound.CAT_PURR, 1.0f, 1.0f);             e.getPlayer().playEffect(EntityEffect.FIREWORK_EXPLODE);         }); ```   ### [`Scheduler`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/Scheduler.java) The scheduler class provides easy static access to the Bukkit Scheduler. All future methods return `Promise`s, allowing for easy use of callbacks.  It also exposes asynchronous and synchronous `Executor` instances.  ```java Scheduler.runLaterSync(() -> {     for (Player player : Bukkit.getOnlinePlayers()) {         if (!player.isOp()) {             player.sendMessage("Hi!");         }     } }, 10L); ```  It also provides a `Task` class, allowing for fine control over the status of repeating events. ```java Scheduler.runTaskRepeatingSync(task -> {     if (task.getTimesRan() >= 10) {         task.stop();         return;     }      // some repeating task }, 20L, 20L); ```   ### [`Promise`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/promise/Promise.java) A `Promise` is an object that acts as a proxy for a result that is initially unknown, usually because the computation of its value is yet incomplete.  The concept very closely resembles the Java [`CompletionStage`](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html) and [`CompletableFuture`](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html) APIs.  The main differences between CompletableFutures and Promises are:  * The ability to switch seamlessly between the main 'Server thread' and asynchronous tasks * The ability to delay an action by a number of game ticks  Promises are really easy to use. To demonstrate how they work, consider this simple reward system. The task flow looks a bit like this.  1. Announce to players that rewards are going to be given out. (sync) 2. Get a list of usernames who are due a reward (async) 3. Convert these usernames to UUIDs (async) 4. Get a set of Player instances for the given UUIDs (sync) 5. Give out the rewards to the online players (sync) 6. Notify the reward storage that these players have been given a reward  Using the Promise API, this might look something like this...  ```java RewardStorage storage = getRewardStorage();  Promise<List<Player>> rewardPromise = Promise.start()         .thenRunSync(() -> Bukkit.broadcastMessage("Getting ready to reward players in 10 seconds!"))         // wait 10 seconds, then start         .thenRunDelayedSync(() -> Bukkit.broadcastMessage("Starting now!"), 200L)         // get the players which need to be rewarded         .thenApplyAsync(n -> storage.getPlayersToReward())         // convert to uuids         .thenApplyAsync(storage::getUuidsFromUsernames)         // get players from the uuids         .thenApplySync(uuids -> {             List<Player> players = new ArrayList<>();             for (UUID uuid : uuids) {                 Player player = Bukkit.getPlayer(uuid);                 if (player != null) {                     players.add(player);                 }             }             return players;         });  // give out the rewards sync rewardPromise.thenAcceptSync(players -> {     for (Player player : players) {         storage.giveReward(player);     } });  // notify rewardPromise.thenAcceptSync(players -> {     for (Player player : players) {         storage.announceSuccess(player.getUniqueId());     } }); ```  However, then consider how this might look if you were just using nested runnables...  ```java RewardStorage storage = getRewardStorage();  Bukkit.getScheduler().runTask(this, () -> {      Bukkit.broadcastMessage("Getting ready to reward players in 10 seconds!");      Bukkit.getScheduler().runTaskLater(this, () -> {         Bukkit.broadcastMessage("Starting now!");          Bukkit.getScheduler().runTaskAsynchronously(this, () -> {             List<String> playersToReward = storage.getPlayersToReward();             Set<UUID> uuids = storage.getUuidsFromUsernames(playersToReward);              Bukkit.getScheduler().runTask(this, () -> {                 List<Player> players = new ArrayList<>();                 for (UUID uuid : uuids) {                     Player player = Bukkit.getPlayer(uuid);                     if (player != null) {                         players.add(player);                     }                 }                  for (Player player : players) {                     storage.giveReward(player);                 }                  Bukkit.getScheduler().runTaskAsynchronously(this, () -> {                     for (Player player : players) {                         storage.announceSuccess(player.getUniqueId());                     }                 });             });         });     }, 200L); }); ```  I'll leave it for you to decide which is better. :smile:   ### [`Metadata`](https://github.com/lucko/helper/tree/master/helper/src/main/java/me/lucko/helper/metadata) helper provides an alternate system to the Bukkit Metadata API. The main benefits over Bukkit are the use of generic types and automatically expiring, weak or soft values.  The metadata API can be easily integrated with the Event system, thanks to some default filters. ```java MetadataKey<Boolean> IN_ARENA_KEY = MetadataKey.createBooleanKey("in-arena");  Events.subscribe(PlayerQuitEvent.class)         .filter(Events.DEFAULT_FILTERS.playerHasMetadata(IN_ARENA_KEY))         .handler(e -> {             // clear their inventory if they were in an arena             e.getPlayer().getInventory().clear();         });  Events.subscribe(ArenaEnterEvent.class)         .handler(e -> Metadata.provideForPlayer(e.getPlayer()).put(IN_ARENA_KEY, true));  Events.subscribe(ArenaLeaveEvent.class)         .handler(e -> Metadata.provideForPlayer(e.getPlayer()).remove(IN_ARENA_KEY)); ```  MetadataKeys can also use generic types with guava's TypeToken. ```java MetadataKey<Set<UUID>> FRIENDS_KEY = MetadataKey.create("friends-list", new TypeToken<Set<UUID>>(){});  Events.subscribe(PlayerQuitEvent.class)         .handler(e -> {             Player p = e.getPlayer();              Set<UUID> friends = Metadata.provideForPlayer(p).getOrDefault(FRIENDS_KEY, Collections.emptySet());             for (UUID friend : friends) {                 Player pl = Bukkit.getPlayer(friend);                 if (pl != null) {                     pl.sendMessage("Your friend " + p.getName() + " has left!"); // :(                 }             }         }); ```  Values can automatically expire, or be backed with Weak or Soft references. ```java MetadataKey<Player> LAST_ATTACKER = MetadataKey.create("combat-tag", Player.class);  Events.subscribe(EntityDamageByEntityEvent.class)         .filter(e -> e.getEntity() instanceof Player)         .filter(e -> e.getDamager() instanceof Player)         .handler(e -> {             Player damaged = ((Player) e.getEntity());             Player damager = ((Player) e.getDamager());              Metadata.provideForPlayer(damaged).put(LAST_ATTACKER, ExpiringValue.of(damager, 1, TimeUnit.MINUTES));         });  Events.subscribe(PlayerDeathEvent.class)         .handler(e -> {             Player p = e.getEntity();             MetadataMap metadata = Metadata.provideForPlayer(p);              Optional<Player> player = metadata.get(LAST_ATTACKER);             player.ifPresent(pl -> {                 // give the last attacker all of the players experience levels                 pl.setTotalExperience(pl.getTotalExperience() + p.getTotalExperience());                 p.setTotalExperience(0);                 metadata.remove(LAST_ATTACKER);             });         }); ```  Unlike Bukkit's system, metadata will be removed automatically when a player leaves the server, meaning you need-not worry about creating accidental memory leaks from left over metadata. The API also supports attaching metadata to blocks, worlds and other entities.   ### [`Messenger`](https://github.com/lucko/helper/tree/master/helper/src/main/java/me/lucko/helper/messaging) helper provides a Messenger abstraction utility, which consists of a few key classes.  * [`Messenger`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/messaging/Messenger.java) - an object which manages messaging Channels * [`Channel`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/messaging/Channel.java) - represents an individual messaging channel. Facilitates sending a message to the channel, or creating a ChannelAgent * [`ChannelAgent`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/messaging/ChannelAgent.java) - an agent for interacting with channel messaging streams. Allows you to add/remove ChannelListeners to a channel * [`ChannelListener`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/messaging/ChannelListener.java) - an object listening to messages sent on a given channel  The system is very easy to use, and cuts out a lot of the boilerplate code which usually goes along with using PubSub systems.  As an example, here is a super simple global player messaging system.  ```java public class GlobalMessengerPlugin extends ExtendedJavaPlugin {      @Override     public void onEnable() {         // get the Messenger         Messenger messenger = getService(Messenger.class);          // Define the channel data model.         class PlayerMessage {             UUID uuid;             String username;             String message;              public PlayerMessage(UUID uuid, String username, String message) {                 this.uuid = uuid;                 this.username = username;                 this.message = message;             }         }          // Get the channel         Channel<PlayerMessage> channel = messenger.getChannel("pms", PlayerMessage.class);          // Listen for chat events, and send a message to our channel.         Events.subscribe(AsyncPlayerChatEvent.class, EventPriority.HIGHEST)                 .filter(Events.DEFAULT_FILTERS.ignoreCancelled())                 .handler(e -> {                     e.setCancelled(true);                     channel.sendMessage(new PlayerMessage(e.getPlayer().getUniqueId(), e.getPlayer().getName(), e.getMessage()));                 });          // Get an agent from the channel.         ChannelAgent<PlayerMessage> channelAgent = channel.newAgent();         channelAgent.register(this);          // Listen for messages sent on the channel.         channelAgent.addListener((agent, message) -> {             Scheduler.runSync(() -> {                 Bukkit.broadcastMessage("Player " + message.username + " says " + message.message);             });         });     } } ```  You can either integrate messenger into your own existing messaging system (using [`AbstractMessenger`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/messaging/AbstractMessenger.java), or, use **helper-redis**, which implements Messenger using Jedis and the Redis PubSub system.   ### [`Commands`](https://github.com/lucko/helper/tree/master/helper/src/main/java/me/lucko/helper/command) helper provides a very simple command abstraction, designed to reduce some of the boilerplate needed when writing simple commands.  Specifically: * Checking if the sender is a player/console sender, and then automatically casting. * Checking for permission status * Checking for argument usage * Checking if the sender is able to use the command. * Easily parsing arguments (not just a String[] like the Bukkit interface)  For example, a simple /msg command condenses down into only a few lines.  ```java Commands.create()         .assertPermission("message.send")         .assertPlayer()         .assertUsage("<player> <message>")         .handler(c -> {             Player other = c.arg(0).parseOrFail(Player.class);             Player sender = c.sender();             String message = c.args().subList(1, c.args().size()).stream().collect(Collectors.joining(" "));              other.sendMessage("[" + sender.getName() + " --> you] " + message);             sender.sendMessage("[you --> " + sender.getName() + "] " + message);         })         .register(this, "msg"); ```  All invalid usage/permission/argument messages can be altered when the command is built. ```java Commands.create()         .assertConsole("&cUse the console to shutdown the server!")         .assertUsage("[countdown]")         .handler(c -> {             ConsoleCommandSender sender = c.sender();             int delay = c.arg(0).parse(Integer.class).orElse(5);              sender.sendMessage("Performing graceful shutdown!");              Scheduler.runTaskRepeatingSync(task -> {                 int countdown = delay - task.getTimesRan();                  if (countdown <= 0) {                     Bukkit.shutdown();                     return;                 }                  Players.forEach(p -> p.sendMessage("Server restarting in " + countdown + " seconds!"));             }, 20L, 20L);         })         .register(this, "shutdown"); ```   ### [`Scoreboard`](https://github.com/lucko/helper/tree/master/helper/src/main/java/me/lucko/helper/scoreboard) helper includes a thread safe scoreboard system, allowing you to easily setup & update custom teams and objectives. It is written directly at the packet level, meaning it can be safely used from asynchronous tasks.  For example.... ```java MetadataKey<ScoreboardObjective> SCOREBOARD_KEY = MetadataKey.create("scoreboard", ScoreboardObjective.class);  BiConsumer<Player, ScoreboardObjective> updater = (p, obj) -> {     obj.setDisplayName("&e&lMy Server &7(" + Bukkit.getOnlinePlayers().size() + "&7)");     obj.applyLines(             "&7Hi and welcome",             "&f" + p.getName(),             "",             "&eRank: &f" + getRankName(p),             "&eSome data:" + getSomeData(p)     ); };  Events.subscribe(PlayerJoinEvent.class)         .handler(e -> {             // register a new scoreboard for the player when they join             Scoreboard sb = GlobalScoreboard.get();             ScoreboardObjective obj = sb.createPlayerObjective(e.getPlayer(), "null", DisplaySlot.SIDEBAR);             Metadata.provideForPlayer(e.getPlayer()).put(SCOREBOARD_KEY, obj);              updater.accept(e.getPlayer(), obj);         });  Scheduler.runTaskRepeatingAsync(() -> {     for (Player player : Bukkit.getOnlinePlayers()) {         MetadataMap metadata = Metadata.provideForPlayer(player);         ScoreboardObjective obj = metadata.getOrNull(SCOREBOARD_KEY);         if (obj != null) {             updater.accept(player, obj);         }     } }, 3L, 3L); ```   ### [`GUI`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/menu/Gui.java) helper provides a highly adaptable and flexible GUI abstraction class.  All you have to do is extend `Gui` and override the `#redraw` method.  To demonstrate how the class works, I wrote a simple typewriter menu. ```java public class TypewriterGui extends Gui {      // the display book     private static final MenuScheme DISPLAY = new MenuScheme().mask("000010000");      // the keyboard buttons     private static final MenuScheme BUTTONS = new MenuScheme()             .mask("000000000")             .mask("000000001")             .mask("111111111")             .mask("111111111")             .mask("011111110")             .mask("000010000");      // we're limited to 9 keys per line, so add 'P' one line above.     private static final String KEYS = "PQWERTYUIOASDFGHJKLZXCVBNM";      private StringBuilder message = new StringBuilder();      public TypewriterGui(Player player) {         super(player, 6, "&7Typewriter");     }      @Override     public void redraw() {          // perform initial setup.         if (isFirstDraw()) {              // when the GUI closes, send the resultant message to the player             bindRunnable(() -> getPlayer().sendMessage("Your typed message was: " + message.toString()));              // place the buttons             MenuPopulator populator = BUTTONS.newPopulator(this);             for (char keyChar : KEYS.toCharArray()) {                 populator.accept(ItemStackBuilder.of(Material.CLAY_BALL)                         .name("&f&l" + keyChar)                         .lore("")                         .lore("&7Click to type this character")                         .build(() -> {                             message.append(keyChar);                             redraw();                         }));             }              // space key             populator.accept(ItemStackBuilder.of(Material.CLAY_BALL)                     .name("&f&lSPACE")                     .lore("")                     .lore("&7Click to type this character")                     .build(() -> {                         message.append(" ");                         redraw();                     }));         }          // update the display every time the GUI is redrawn.         DISPLAY.newPopulator(this).accept(ItemStackBuilder.of(Material.BOOK)                 .name("&f" + message.toString() + "&7_")                 .lore("")                 .lore("&f> &7Use the buttons below to type your message.")                 .lore("&f> &7Hit ESC when you're done!")                 .buildItem().build());     } } ```  You can call the `#redraw` method from within click callbacks to easily change the menu structure as players interact with the menu.  ItemStackBuilder provides a number of methods for creating item stacks easily, and can be used anywhere. (not just in GUIs!)  The GUI class also provides a number of methods which allow you to * Define "fallback" menus to be opened when the current menu is closed * Setup ticker tasks to run whilst the menu remains open * Add invalidation tasks to be called when the menu is closed * Manipulate ClickTypes to only fire events when a certain type is used * Create automatically paginated views in a "dictionary" style   ### [`Menu Scheming`](https://github.com/lucko/helper/tree/master/helper/src/main/java/me/lucko/helper/menu/scheme) MenuScheme allows you to easily apply layouts to GUIs without having to think about slot ids. ```java @Override public void redraw() {     new MenuScheme(StandardSchemeMappings.STAINED_GLASS)             .mask("111111111")             .mask("110000011")             .mask("100000001")             .mask("100000001")             .mask("110000011")             .mask("111111111")             .scheme(14, 14, 1, 0, 10, 0, 1, 14, 14)             .scheme(14, 0, 0, 14)             .scheme(10, 10)             .scheme(10, 10)             .scheme(14, 0, 0, 14)             .scheme(14, 14, 1, 0, 10, 0, 1, 14, 14)             .apply(this); } ```  The above scheme translates into this menu.  ![](https://i.imgur.com/sERK75D.png)  The mask values determine which slots in each row will be transformed. The scheme values relate to the data values of the glass panes.  The scheming system can also be used alongside a `MenuPopulator`, which uses the scheme to add items to the Gui programatically.  ```java @Override public void redraw() {     MenuScheme scheme = new MenuScheme().mask("000111000");     MenuPopulator populator = scheme.newPopulator(this);      populator.accept(ItemStackBuilder.of(Material.PAPER).name("Item 1").buildItem().build());     populator.accept(ItemStackBuilder.of(Material.PAPER).name("Item 2").buildItem().build());     populator.accept(ItemStackBuilder.of(Material.PAPER).name("Item 3").buildItem().build()); } ```   ### [`Plugin Annotations`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/plugin/ap/Plugin.java) With helper, you can automagically create the standard `plugin.yml` files at compile time using annotation processing.  Simply annotate your main class with `@Plugin` and fill in the name and version. The processor will take care of the rest!  ```java @Plugin(name = "MyPlugin", version = "1.0.0") public class MyPlugin extends JavaPlugin {  } ```  The annotation also supports defining load order, setting a description and website, and defining (soft) dependencies. Registering commands and permissions is not necessary with helper, as `ExtendedJavaPlugin` provides a method for registering these at runtime.  ```java @Plugin(         name = "MyPlugin",         version = "1.0",         description = "A cool plugin",         load = PluginLoadOrder.STARTUP,         authors = {"Luck", "Some other guy"},         website = "www.example.com",         depends = {@PluginDependency("Vault"), @PluginDependency(value = "ASpecialPlugin", soft = true)},         loadBefore = {"SomePlugin", "SomeOtherPlugin"} ) public class MyPlugin extends JavaPlugin {  } ```   ### [`Maven Annotations`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/maven/MavenLibrary.java) helper includes a system which allows you to magically download dependencies for your plugins at runtime.  This means you don't have to shade MBs of libraries into your jar. It's as simple as adding an annotation to your plugins class.  ```java @MavenLibrary(groupId = "org.mongodb", artifactId = "mongo-java-driver", version = "3.4.2") @MavenLibrary(groupId = "org.postgresql", artifactId = "postgresql", version = "9.4.1212") public class ExamplePlugin extends JavaPlugin {      @Override     public void onLoad() {          // Downloads and installs all dependencies into the classloader!         // Not necessary if you extend helper's "ExtendedJavaPlugin" instead of "JavaPlugin"         LibraryLoader.loadAll(this);     } } ```   ### [`Terminables`](https://github.com/lucko/helper/tree/master/helper/src/main/java/me/lucko/helper/terminable) Terminables are a way to easily cleanup active objects in plugins when a shutdown or reset is needed.  The system consists of a few key interfaces.  * [`Terminable`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/terminable/Terminable.java) - The main interface. An object that can be unregistered, stopped, or gracefully halted. * [`TerminableConsumer`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/terminable/TerminableConsumer.java) - An object which binds with and registers Terminables. * [`CompositeTerminable`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/terminable/composite/CompositeTerminable.java) - An object which itself contains/has a number of Terminables, but does not register them internally. * [`CompositeTerminableConsumer`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/terminable/composite/CompositeTerminableConsumer.java) - A bit like a TerminableConsumer, just for CompositeTerminables * [`TerminableRegistry`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/terminable/registry/TerminableRegistry.java) - An object which is a Terminable itself, but also a TerminableConsumer & CompositeTerminableConsumer, all in one!  Terminables are a really important part of helper, as so much of the utility is accessible from a static context. Terminables are a way to tame these floating, globally built handlers, and register them with the plugin instance.  `ExtendedJavaPlugin` implements TerminableConsumer & CompositeTerminableConsumer, which lets you register Terminables and CompositeTerminables to the plugin. These are all terminated automagically when the plugin disables.  To demonstrate, I'll first define a new CompositeTerminable. Think of this as a conventional Listener class in a regular plugin. ```java public class DemoListener implements CompositeTerminable {      @Override     public void setup(@Nonnull TerminableConsumer consumer) {          Events.subscribe(PlayerJoinEvent.class)                 .filter(e -> e.getPlayer().hasPermission("silentjoin"))                 .handler(e -> e.setJoinMessage(null))                 .bindWith(consumer);          Events.subscribe(PlayerQuitEvent.class)                 .filter(e -> e.getPlayer().hasPermission("silentquit"))                 .handler(e -> e.setQuitMessage(null))                 .bindWith(consumer);      } } ```  Notice the `.bindWith(...)` calls? All Terminables have this method added via default in the interface. It lets you register that specific terminable with a consumer.  In order to setup our DemoListener, we need a CompositeTerminableConsumer. Luckily, ExtendedJavaPlugin implements this for us!  ```java public class DemoPlugin extends ExtendedJavaPlugin {      @Override     protected void enable() {          // either of these is fine (but don't use both!)         new DemoListener().bindWith(this);          bindComposite(new DemoListener());      } } ```  ### [`Serialization`](https://github.com/lucko/helper/tree/master/helper/src/main/java/me/lucko/helper/serialize) helper provides a few classes with are useful when trying to serialize plugin data. It makes use of Google's GSON to convert from Java Objects to JSON.  * [`Position`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/Position.java) - similar to Bukkit's location, but without pitch/yaw * [`BlockPosition`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/BlockPosition.java) - the location of a block within a world * [`ChunkPosition`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/ChunkPosition.java) - the location of a chunk within a world * [`Region`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/Region.java) - the area bounded by two Positions * [`BlockRegion`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/BlockRegion.java) - the area bounded by two BlockPositions * [`ChunkRegion`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/ChunkRegion.java) - the area bounded by two ChunkPositions * [`Direction`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/Direction.java) - the yaw and pitch * [`Point`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/Point.java) - a position + a direction  And finally, [`Serializers`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/Serializers.java), containing serializers for ItemStacks and Inventories.  There is also an abstraction for conducting file I/O. [`FileStorageHandler`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/serialize/FileStorageHandler.java) is capable of handling the initial creation of storage files, as well as automatically creating backups and saving when the server stops.  It's as simple as creating a class to handle serialization/deserialization, and then calling a method when you want to load/save data.  ```java public class DemoStorageHandler extends FileStorageHandler<Map<String, String>> {     private static final Splitter SPLITTER = Splitter.on('=');      public DemoStorageHandler(JavaPlugin plugin) {         super("demo", ",json", plugin.getDataFolder());     }      @Override     protected Map<String, String> readFromFile(Path path) {         Map<String, String> data = new HashMap<>();          try (BufferedReader reader = Files.newBufferedReader(path, StandardCharsets.UTF_8)) {             // read all the data from the file.             reader.lines().forEach(line -> {                 Iterator<String> it = SPLITTER.split(line).iterator();                 data.put(it.next(), it.next());             });         } catch (IOException e) {             e.printStackTrace();         }          return data;     }      @Override     protected void saveToFile(Path path, Map<String, String> data) {         try (BufferedWriter writer = Files.newBufferedWriter(path, StandardCharsets.UTF_8)) {             for (Map.Entry<String, String> e : data.entrySet()) {                 writer.write(e.getKey() + "=" + e.getValue());                 writer.newLine();             }         } catch (IOException e) {             e.printStackTrace();         }     } } ```  Then, to save/load, just create an instance of the handler, and use the provided methods. ```java DemoStorageHandler handler = new DemoStorageHandler(this); handler.save(ImmutableMap.of("some key", "some value"));  // or, to save a backup of the previous file too handler.saveAndBackup(ImmutableMap.of("some key", "some value")); ```  helper also provides a handler which uses Gson to serialize the data. ```java GsonStorageHandler<List<String>> gsonHandler = new GsonStorageHandler<>("data", ".json", getDataFolder(), new TypeToken<List<String>>(){}); gsonHandler.save(ImmutableList.of("some key", "some value")); ```   ### [`Bungee Messaging`](https://github.com/lucko/helper/blob/master/helper/src/main/java/me/lucko/helper/messaging/bungee/BungeeMessaging.java) helper provides a wrapper class for the BungeeCord Plugin Messaging API, providing callbacks to read response data.  It handles the messaging channels behind the scenes and simply runs the provided callback when the data is returned.  For example... ```java // sends the player to server "hub" Player player; BungeeMessaging.connect(player "hub"); ```  And for calls which return responses, the data is captured automatically and returned via the callback. ```java // requests the global player count and then broadcasts it to all players BungeeMessaging.playerCount(BungeeMessaging.ALL_SERVERS, count -> Bukkit.broadcastMessage("There are " + count + " players online!")); ```  The class also provides a way to use the "Forward" channel. ```java // prepare some data to send ByteArrayDataOutput buf = ByteStreams.newDataOutput(); buf.writeUTF(getServerName()); buf.writeUTF("Hey!");  // send the data BungeeMessaging.forward(BungeeMessaging.ONLINE_SERVERS, "my-special-channel", buf);  // listen for any messages sent on the special channel BungeeMessaging.registerForwardCallback("my-special-channel", buf -> {     String server = buf.readUTF();     String message = buf.readUTF();      Log.info("Server " + server + " says " + message);     return false; }); ```   ## Using helper in your project You can either install the standalone helper plugin on your server, or shade the classes into your project.  You will need to add my maven repository to your build script, or install helper locally. #### Maven ```xml <repositories>     <repository>         <id>luck-repo</id>         <url>http://repo.lucko.me/</url>     </repository> </repositories> ```  #### Gradle ```gradle repositories {     maven {         name "luck-repo"         url "http://repo.lucko.me/"     } } ```  Then, you can add dependencies for each helper module.  ### helper #### Maven ```xml <dependencies>     <dependency>         <groupId>me.lucko</groupId>         <artifactId>helper</artifactId>         <version>3.1.0</version>         <scope>provided</scope>     </dependency> </dependencies> ```  #### Gradle ```gradle dependencies {     compile ("me.lucko:helper:3.1.0") } ```  ### helper-sql #### Maven ```xml <dependencies>     <dependency>         <groupId>me.lucko</groupId>         <artifactId>helper-sql</artifactId>         <version>1.0.4</version>         <scope>provided</scope>     </dependency> </dependencies> ```  #### Gradle ```gradle dependencies {     compile ("me.lucko:helper-sql:1.0.4") } ```  ### helper-redis #### Maven ```xml <dependencies>     <dependency>         <groupId>me.lucko</groupId>         <artifactId>helper-redis</artifactId>         <version>1.0.5</version>         <scope>provided</scope>     </dependency> </dependencies> ```  #### Gradle ```gradle dependencies {     compile ("me.lucko:helper-redis:1.0.5") } ```  ### helper-mongo #### Maven ```xml <dependencies>     <dependency>         <groupId>me.lucko</groupId>         <artifactId>helper-mongo</artifactId>         <version>1.0.2</version>         <scope>provided</scope>     </dependency> </dependencies> ```  #### Gradle ```gradle dependencies {     compile ("me.lucko:helper-mongo:1.0.2") } ```
lukaspili/Power-Mortar-Flow-Dagger2-demo	# Mortar Demo with Power!  Demo project for Mortar / Flow / Dagger2 with **Auto Dagger2** and **Auto Mortar**.   It's the same project as: [https://github.com/lukaspili/Mortar-Flow-Dagger2-demo](https://github.com/lukaspili/Mortar-Flow-Dagger2-demo)   The only difference is that it uses Auto Mortar and Auto Dagger2 to reduce the boilerplate. See the difference!   ## Auto Dagger2  - [https://github.com/lukaspili/auto-dagger2](https://github.com/lukaspili/auto-dagger2)   ## Auto Mortar  - [https://github.com/lukaspili/auto-mortar](https://github.com/lukaspili/auto-mortar)    ## Showcase  - Mortar - Flow - Dagger2 - Screen (with Flow-path) - ScreenScoper : `ScreenScoper` - Subscreen (screen that is included in another screen) : `LoginScreen` - Additional custom view injected to screen : `BannerView`   ## You may also want to check out  ### Mortar Architect  Navigation stack for Mortar. Alternative to Flow. Focuses on Mortar scopes, simplicity, seamless integration and killing boilerplate code.   [https://github.com/lukaspili/Mortar-architect](https://github.com/lukaspili/Mortar-architect)  ### Mortar architect map demo  Demo application with Mortar / Dagger2 / Mortar architect. Showcase MapView and DrawerLayout.   [https://github.com/lukaspili/Mortar-architect-map-demo](https://github.com/lukaspili/Mortar-architect-map-demo)   ## Author  - Lukasz Piliszczuk ([@lukaspili](https://twitter.com/lukaspili))   ## License  Mortar demo is released under the MIT license. See the LICENSE file for details.
futuresimple/android-autoprovider	AutoProvider ============ Utility for creating ContentProviders without boilerplate and with heavy customization options.  Features ========  Usage ===== ```groovy dependencies {     compile 'com.getbase.android.autoprovider:library:0.2.0' } ```  Caveats =======  Credits =======  License =======      Copyright (C) 2015 Jerzy Chalupski      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at           http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
android-Infoedge/randomizer	#Randomizer  Annotation based library which is used to generate random data. This library uses reflection to parse annotations set on fields of model classes.This library makes it super easy to generate unlimited  random data for proper testing of your application.it requires  minimal code changes and no boilerplate code to generate random data. You just have to specify annotations on fields.  ## Features  * Primitive and Non Primitive fields can be annotated. * Reference type of some other class can be annotated. * Built in data is more than enough for all random data needs. * Custom data generator can be set for random data type for field. * Built in data generators can be reused to create custom data generator.    # Supported Annotations  * @AppBundleID * @AppName * @AppVersion * @Avatar * @BooleanValue * @City * @CollectionDescriptor * @ColorName * @CompanyName * @Country * @CountryCode * @CreditCardNumber * @CreditCardType * @Currency * @CurrencyCode * @CustomList * @DateValue * @DomainName * @DummyImageUrl * @Email * @Encrypt * @FileName * @FirstName * @Frequency * @FullName * @Gender * @Guid * @HexColor * @IpAddressV4 * @IpAddressV6 * @Isbn * @JobTitle * @Language * @LastName * @Latitude * @LinkedInSkill * @Longitude * @MacAddress * @Mapping * @MD5 * @MimeType * @Money * @Number * @Paragraphs * @Phone * @Race * @RowNumber * @Sentences * @Sequence * @SHA1 * @SHA256 * @ShirtSize * @SSN * @State * @StreetAddress * @Suffix * @Time * @TimeZone * @Title * @TopLevelDomain * @URL * @Username * @Words  Some of above annotations may have options to customize its behaviour.  ```java @DateValue( from = "01 Jan 2001",to = "31 Dec 2002" , customFormat = "dd MMM yyyy") String dateOfBirth;  @Number(min = 20,max = 30,decimals = 0) int age; ```      #Code Examples  ## Single class  ```java public class Person {      @FullName     String name;      @DateValue( from = "01 Jan 2001",to = "31 Dec 2002" , customFormat = "dd MMM yyyy")     String dateOfBirth;      @Number(min = 20,max = 30,decimals = 0)     int age;      @Avatar(width = 100,height = 100,format = Avatar.Format.PNG)     String profileAvatar;      @Phone(country = Phone.Country.INDIA)     @CollectionDescriptor(min = 1,max = 3)     List<String> phones; } ```  ## class Using Class (one to one)  ```java public class Student {      @FullName     public String name;      @ReferenceRecord(clazz = Address.class)     public Address address; } ``` ```java @ReferencedRecord public class Address {      @StreetAddress     public String streetAddress;      @State     public String state; } ``` Please note *@ReferencedRecord* at *Address* class.This is required.   ## class using class (one to many)  ```java public class Student {      @FullName     public String name;      @ReferenceRecord(clazz = Address.class)     @CollectionDescriptor(min = 1,max = 2)     public List<Address> address;      } ``` ```java @ReferencedRecord public class Address {      @StreetAddress     public String streetAddress;      @State     public String state;  } ```   ## Using custom generator  ```java //Please note that it extends GenerationRule public class CustomCreditCardNumberGenerator extends GenerationRule<CreditCardNumber,String> {      public CustomCreditCardNumberGenerator(CreditCardNumber annotation, ProviderFactory providerFactory) {         super(annotation, providerFactory);     }      //Will be called automatically to generate random data from source     @Override     public String generate() {         return CARD_NUMBERS[getRandom().nextInt(CARD_NUMBERS.length)];     }      //Provide your own data     private final String[] CARD_NUMBERS = {"0111-0666-9888-7666","0777-9453-5672-7765"}; } ``` ```java public class Person {      @CreditCardNumber     @CustomGenerator(generator = CustomCreditCardNumberGenerator.class)     public String creditCardNumber; } ```  ## Using existing Generator with Custom Generator  ``` //Please note that it extends DelegateGenerationRule rather than simple GenerationRule public class CustomEmailGenerator extends DelegateGenerationRule<Email,String> {      public CustomEmailGenerator(Email annotation, ProviderFactory providerFactory, GenerationRule<Email, String> mCoreGenerator) {         super(annotation, providerFactory, mCoreGenerator);     }      @Override     public String generate() {         return getCoreRandomData().toUpperCase();     } } ``` ```java public class Person {      @Email     @CustomGenerator(generator = CustomEmailGenerator.class,delegate = EmailGenerator.class)     @CollectionDescriptor(min = 2,max = 5)     public List<String> emails; } ``` Generator name follows convention Annotation name followed by Generator eg. **Annotation = @Email Generator = EmailGenerator**  # How to generate records ?  ```java Generator<Person> generator = new Generator<>(Person.class); List<Person> persons = generator.generate(5); ``` Simply two lines :)  ## For Android   This project has been divided into two parts *jrandomizer* and *arandomizer* . *arandomizer* simply contains wrapper class around **Generator** class that contains  **AsyncTask** for async data generation for android.  ```java DroidGenerator<Person> generator = new DroidGenerator<>(Person.class); List<Person> persons = generator.generate(5); ``` simply :)  # Include in build.gradle  ```gradle repositories {         maven {             url 'https://dl.bintray.com/android-infoedge/maven/'         }     } ```  For module  ```gradle //For android to take advantage of AsyncTask compile 'com.infoedge:arandomizer:0.1-beta1' ```  or  ```gradle //Pure java only compile 'com.infoedge:jrandomizer:0.1-beta1' ```  # Inspirations  https://www.mockaroo.com/  https://github.com/ragunathjawahar/android-saripaar  ##License  Copyright 2016 Info Edge India Limited  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
hitherejoe/AndroidTvBoilerplate	#Android TV Boilerplate [![Build Status](https://travis-ci.org/hitherejoe/Vineyard.svg?branch=master)](https://travis-ci.org/hitherejoe/AndroidTvBoilerplate) =======================  <p align="center">     <img src="images/web_banner.png" alt="Loading Card"/> </p>  This is an Android TV Boilerplate project which should make it easy for you to get started when wanting to create your own application for the Android TV platform!  The project is setup using:  - MVP architecture - Functional tests with [Espresso](http://google.github.io/android-testing-support-library/docs/espresso) - Unit tests with [Mockito](http://mockito.org/) - [Checkstyle](http://checkstyle.sourceforge.net/), [FindBugs](http://findbugs.sourceforge.net/) and [PMD](https://pmd.github.io/) - [Leanback Library](http://developer.android.com/tools/support-library/features.html#v17-leanback) - [Recommendation Library](http://developer.android.com/tools/support-library/features.html#recommendation) - [RxJava](https://github.com/ReactiveX/RxJava) and [RxAndroid](https://github.com/ReactiveX/RxAndroid) - [Retrofit](http://square.github.io/retrofit/) and [OkHttp](https://github.com/square/okhttp) - [Dagger 2](http://google.github.io/dagger/) - [Butterknife](https://github.com/JakeWharton/butterknife) - [Timber] (https://github.com/JakeWharton/timber) - [Mockito](http://mockito.org/) - [Glide](https://github.com/bumptech/glide)  The boilerplate currently has two core screens implemented and ready to feed data into:  ##Browse  <p align="center">     <img src="images/browse_fragment.png" alt="Loading Card"/> </p>  ##Search <p align="center">     <img src="images/search_fragment.png" alt="Loading Card"/> </p>  #Check  To check the code style and run unit tests:  ```./gradlew check```  #Building  To build, install and run a debug version, run this from the root of the project:  ```./gradlew assembleDebug```  #Unit Tests  To run the unit tests for the application:  ```./gradlew testDebugUnitTest```  #User Interface Tests  To run the user interface tests for the application:  ```./gradlew connectedDebugAndroidTest```
yatatsu/AutoBundle	# AutoBundle  [![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0) [![Build Status](https://travis-ci.org/yatatsu/AutoBundle.svg?branch=master)](https://travis-ci.org/yatatsu/AutoBundle) [ ![Download](https://api.bintray.com/packages/yatatsu/maven/autobundle/images/download.svg) ](https://bintray.com/yatatsu/maven/autobundle/_latestVersion) [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-AutoBundle-brightgreen.svg?style=flat)](http://android-arsenal.com/details/1/2600)  AutoBundle generates boilerplate code for field binding with ``android.os.Bundle``.  1. Usage 2. Advanced 3. Example 4. Principle 5. Download 6. Props 7. License  ## Usage  1. Generate builder method 2. Bind annotated fields 3. Store annotated fields  ### 1. Generate builder method  In your class which has state from `Bundle`  (`Activity`, `BroadcastReceiver`, `Service`, `Fragment` or others),  declare fields with `@AutoBundleField`.  Here is example for Activity.  ```java public class MyActivity extends Activity {     // field with @AutoBundleField, must not be private/protected.     @AutoBundleField     String title;      @AutoBundleField     int exampleId;      @AutoBundleField(required = false) // default is true     int optionalId; } ```  After build, `{YourClass}AutoBundle` class will be generated.  ```java public final class MyActivityAutoBundle {    // ~~~    public static final class Builder {     private final Bundle args;      public Builder(@NonNull String title, int exampleId) {       this.args = new Bundle();       this.args.putString("title", title);       this.args.putInt("exampleId", exampleId);     }      public @NonNull MyActivityAutoBundle.Builder optionalId(int optionalId) {       args.putInt("optionalId", optionalId);       return this;     }      public @NonNull Intent build(@NonNull Context context) {       Intent intent = new Intent(context, MyActivity.class);       intent.putExtras(args);       return intent;     }      public @NonNull Intent build(@NonNull Intent intent) {       intent.putExtras(args);       return intent;     }      public @NonNull Bundle bundle() {       return args;     }   } } ```  And you can create intent from builder.  ```java Intent intent = MyActivityAutoBundle.builder("hello, example!", 1)     .optionalId(2) // here is optional     .build(context); // your can also set bundle to other intent MyActivityAutoBundle.builder("hello, example!", 1)     .build(otherIntent); ```  If target class is subclass of these, builder can create intent.  - `Activity` - `Service` - `BroadcastReceiver`  Or if target is subclass of `android.app.Fragment` or `android.support.v4.app.Fragment`,  builder can create fragment.   ```java builder = ExampleFragmentAutoBundle.builder("hello, example!", 1)     .optionalId(2); ExampleFragment fragment = builder.build(); // you can also set bundle to other fragment builder.build(otherFragment); ```  And all builder has `bundle()`, which returns `Bundle`.  ### 2. Bind annotated fields  In target class, Call binding method in ``onCreate``.  ```java public class ExampleFragment extends DialogFragment {     // field with @AutoBundleField, must not be private/protected.     @AutoBundleField 	String title;      @AutoBundleField 	int exampleId;      @AutoBundleField(required = false) // default is true     int optionalId;      @Override     public void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         ExampleFragmentAutoBundle.bind(this, getArguments()); 		// `AutoBundle` is providing easier interface.  		// This code is equals to above. 		AutoBundle.bind(this);     } } ```  - ``bind(Object target, Intent intent)`` - ``bind(Object target, Bundle bundle)`` - ``bind(Activity target)`` (equals to ``bind(activity, activity.getIntent())``) - ``bind(Object target)`` (equals to ``bind(fragment, fragment.getArguments())``)  ### 3. Store annotated fields  AutoBundle bindings are also useful as restoring value in ``onSaveInstanceState(Bundle outState)``.  ``pack(Object object, Bundle bundle)`` stores field value to bundle. For example, store in ``onSaveInstanceState`` and restore in ``onCreate``.  ```java @Override public void onSaveInstanceState(Bundle outState) {     super.onSaveInstanceState(outState);     ExampleFragmentAutoBundle.pack(this, outState);     // or     AutoBundle.pack(this, outState); }  @Override public void onCreate(Bundle savedInstanceState) {     super.onCreate(savedInstanceState);     if (savedInstanceState != null) {         // restore         AutoBundle.bind(this, savedInstanceState);     } else {         AutoBundle.bind(this)     } } ```  ## Advanced  - custom key name - required - getter/setter - converter - use in library  ### Set key name  ``key`` is key for ``Bundle``. Default is field name. You cannot define duplicate key in one class.  ```java @AutoBundleField(key = "exampleId") int id; ```  ### Required  ``required`` option is true by default. If ``false``, Builder class has method which named key name, instead as contructor argument.  ```java @AutoBundleField(required = false) int optionalId; ```  then,  ```java ExampleFragment fragment = ExampleFragmentAutoBundle.builder()         .optionalId(2)         .build(); ```  ### Getter/Setter  You can use getter/setter for fields.  The method named `get/set{key}` As a default, But you can specify with `@AutoBundleGetter` and `@AutoBundleSetter`.  ```java @AutoBundleField private String userId;  // get{key} use as default. // no need for @AutoBundleGetter  public String getUserId() {     return userId; }  @AutoBundleGetter(key = "userId") public String getId() {    return userId; }  @AutoBundleSetter(key = "userId") public void setId(String id) {    this.userId = id; } ```  ### CustomConverter  ``converter`` option provide custom converter for storing to bundle. You can specify class which implements ``AutoBundleConverter<T, U>``.  ```java public class ExampleFragment extends Fragment {      @AutoBundleField(converter = DateArgConverter.class)     Date targetDate;      @Override     public void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         AutoBundle.bind(this);     }      public static class DateArgConverter implements AutoBundleConverter<Date, Long> {          @Override         public Long convert(Date o) {             return o.getTime();         }          @Override         public Date original(Long s) {             return new Date(s);         }      } } ```  ### Use in library module  If you use in library module, you need to pass the custom package for `AutoBundleBindingDispatcher`, which is generated by processor.  Set `autoBundleAsLibrary` option as following.  with annotationProcessor (in jack),  ```groovy android {     defaultConfig {         javaCompileOptions {             annotationProcessorOptions {                 // pachage name as you like                 arguments = [ autoBundleAsLibrary : 'com.yatatsu.example.library' ]             }         }     } } ```  or with android-apt,  ```groovy apt {     arguments {         autoBundleAsLibrary 'com.yatatsu.example.library'     } } ```  You can use library module's binding from app module by `subDispatchers` option. Pass package name of library's dispatcher.  (It maybe `your.package.AutoBundleBindingDispatcher`!)  ```groovy annotationProcessorOptions {     // pass package of library's dispatcher (multi dispatchers with comma)     arguments = [ subDispatchers : 'com.yatatsu.example.library.AutoBundleBindingDispatcher'             + ',com.yatatsu.example.library2.AutoBundleBindingDispatcher' ] } ```  ## Example  For more information or usage, see the sample application!  (See `example/` and generated codes are under `example/build`.)   ## Principle  Both Fragment and Intent are able to pass value by storing Bundle.  There is a good pattern, known as "createIntent" or "newInstance". Static method in called class (e.g. Fragment, Activity) promised the expected data will be passed. AutBundle provide this pattern as builder method instead of "createIntent".  ## Download  ```groovy dependencies {     compile 'com.github.yatatsu:autobundle:4.1.0'     annotationProcessor 'com.github.yatatsu:autobundle-processor:4.1.0' } ```  If your gradle plugin < 2.2, use `android-apt` plugin.  ```groovy buildscript {     dependencies {         classpath 'com.neenbedankt.gradle.plugins:android-apt:1.8'     } }  apply plugin: 'android-apt'  dependencies {     compile 'com.github.yatatsu:autobundle:4.1.0'     apt 'com.github.yatatsu:autobundle-processor:4.1.0' } ```  ## Props  - [sockeqwe/FragmentArgs](https://github.com/sockeqwe/fragmentargs) - [emilsjolander/IntentBuilder](https://github.com/emilsjolander/IntentBuilder)  Actually, AutoBundle is that just integrates above both great libraries.  ## License  ``` Copyright 2015 KITAGAWA, Tatsuya  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```
evernote/android-state	# Android-State  A utility library for Android to save objects in a `Bundle` without any boilerplate. It uses an annotation processor to wire up all dependencies.  ## Download  Download the latest [library](https://search.maven.org/#search%7Cga%7C1%7Ca%3A%22android-state%22) and [processor](https://search.maven.org/#search%7Cga%7C1%7Ca%3A%22android-state-processor%22) or grab via Gradle:  ```groovy dependencies {     compile 'com.evernote:android-state:1.1.5'     // Java only project     annotationProcessor 'com.evernote:android-state-processor:1.1.5'      // Kotlin with or without Java     kapt 'com.evernote:android-state-processor:1.1.5' } ```  You can read the [JavaDoc here](https://evernote.github.io/android-state/javadoc/).  ## Usage  Annotate any field with `@State` and use the `StateSaver` class to save those fields in a Bundle. This works from an `Activity` or `Fragment`, but also from anywhere else in your code. You can save any type which can be saved in a Bundle like the `String`, `Serializable`, and `Parcelable` data structures.  ```java public class MainActivity extends Activity {      @State     public int mValue;      @Override     protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         StateSaver.restoreInstanceState(this, savedInstanceState);     }      @Override     protected void onSaveInstanceState(Bundle outState) {         super.onSaveInstanceState(outState);         StateSaver.saveInstanceState(this, outState);     } } ```  ## Advanced  You can also save state in a `View` class.  ```java public class TestView extends View {      @State     public int mState;      public TestView(Context context) {         super(context);     }      @Override     protected Parcelable onSaveInstanceState() {         return StateSaver.saveInstanceState(this, super.onSaveInstanceState());     }      @Override     protected void onRestoreInstanceState(Parcelable state) {         super.onRestoreInstanceState(StateSaver.restoreInstanceState(this, state));     } } ```  It is recommended that saved properties not be `private`. If a property is `private`, then a non-private getter and setter method are required. This is especially useful for Kotlin, because properties are `private` by default and the aforementioned methods are generated by the compiler.  If you **want** your getter and setter to be used rather than the field value being used directly, the field **must** be private.  ```kotlin class DemoPresenter : Presenter<DemoView>() {      @State     var counter = 0      // ... }  ```  Of course, this also works in Java.  ```java public class TitleUpdater {      @State     private String mTitle;      public String getTitle() {         return mTitle;     }      public void setTitle(String title) {         mTitle = title;     } } ```  If you have a private field and don't want to provide a getter or setter method, then you can fallback to reflection. However, this method is not recommended.  ```java public class ImageProcessor {      @StateReflection     private byte[] mImageData;      // ... } ```  A custom bundler can be useful, if a class doesn't implement the `Parcelable` or `Serializable` interface, which oftentimes happens with third party dependencies.  ```java public class MappingProvider {      @State(PairBundler.class)     public Pair<String, Integer> mMapping;      public static final class PairBundler implements Bundler<Pair<String, Integer>> {         @Override         public void put(@NonNull String key, @NonNull Pair<String, Integer> value, @NonNull Bundle bundle) {             bundle.putString(key + "first", value.first);             bundle.putInt(key + "second", value.second);         }          @Nullable         @Override         public Pair<String, Integer> get(@NonNull String key, @NonNull Bundle bundle) {             if (bundle.containsKey(key + "first")) {                 return new Pair<>(bundle.getString(key + "first"), bundle.getInt(key + "second"));             } else {                 return null;             }         }     } } ```  #### ProGuard  This library comes with a ProGuard config. No further steps are required, but all necessary rules can be found [here](library/proguard.cfg).  ## [Icepick](https://github.com/frankiesardo/icepick)  This library is based on [Icepick](https://github.com/frankiesardo/icepick), a great library from Frankie Sardo. However, Icepick is missing some features important to us: it [doesn't support properties](https://github.com/frankiesardo/icepick/issues/81) which is a [bummer for Kotlin](https://github.com/frankiesardo/icepick/issues/47). Also, Icepick does not support private fields which may break encapsulation. A tool shouldn't force you into this direction.  Since Icepick is implemented in Clojure, we decided that it's better for us to rewrite the annotation processor in Java. Unfortunately, that makes it hard to push our features into Icepick itself. That's why we decided to fork the project.  There are also alternatives for Kotlin like [IceKick](https://github.com/tinsukE/icekick). We did not want to use two libraries to solve the same problem for two different languages; we wanted to have one solution for all scenarios.  Upgrading to this library from Icepick is easy. The API is the same; only the packages and the class name (i.e. from `Icepick` to `StateSaver`) have changed. If Icepick works well for you, then there's no need to upgrade.  ## License  ``` Copyright (c) 2017 Evernote Corporation. All rights reserved. This program and the accompanying materials are made available under the terms of the Eclipse Public License v1.0 which accompanies this distribution, and is available at  http://www.eclipse.org/legal/epl-v10.html  Files produced by Android-State code generator are not subject to terms of the Eclipse Public License 1.0 and can be used as set out in the copyright notice included in the generated files. ```
saintdan/spring-micro-services-boilerplate	# spring-micro-services-boilerplate  [中文版](README_zh.md)  ## <a name="index"></a>Index  - [Build and Run](#build) - [NOTICE](#notice) - [Usage](#usage)   - [Import init.sql](#init)   - [Access resources with Swagger](#swagger) - [Deploy](#deploy) - [License](#license) - [Version History](#version)  **spring-micro-services-boilerplate** is a boilerplate which is very helpful for java programmer and friendly to front end.  And build with:  - [Spring Boot](http://projects.spring.io/spring-boot/) - [Spring OAuth 2](http://projects.spring.io/spring-security-oauth/) - [Spring Security](http://projects.spring.io/spring-security/) - [Spring Data JPA](http://projects.spring.io/spring-data-jpa/)  And use [specification-arg-resolver](https://github.com/tkaczmarzyk/specification-arg-resolver) for filter.  > NOTE If you need RSA sign check, you can use `validateWithSignCheck` of [ValidateHelper](src/main/java/com/saintdan/framework/component/ValidateHelper.java)  ## <a name="build"></a>Build and Run [[TOP]](#index)  ``` $ cd <spring-micro-services-boilerplate root path> $ ./gradlew clean build bootRun ```  ## <a name="notice"></a>NOTICE [[TOP]](#index)  - Validate failed -> Response http status is **422**(Unprocessable Entity) - Server error -> Response http status is **500**(Internal Server Error)  ## <a name="usage"></a>Usage [[TOP]](#index)  ### <a name="init"></a>Step 1: Import the [init.sql](src/main/resources/init.sql) to your database, I suggest you to use [PostgreSQL](https://www.postgresql.org/) [[TOP]](#index)  ### <a name="swagger"></a>Step 2: Access resources with Swagger. [[TOP]](#index)  Run it, Open your browser and access [http://localhost:8080/swagger-ui.html#/](http://localhost:8080/swagger-ui.html#/). > **NOTE** Default port is 8080, you can modify it which key is `server.port` in [application.yml](src/main/resources/application.yml).  and then:  ![](imgs/swagger.png)  You can test API by the tips.  1. Get `access_token` and `refresh_token` with login API. ![](imgs/login.png) and result is: ![](imgs/token.png) 2. Or get new `access_token` with `refresh_token`. ![](imgs/refresh.png) 3. Access users. ![](imgs/users.png) and result is: ![](imgs/result.png)  ### <a name="deploy"></a>Deploy [[TOP]](#index)  1. Build **war** and use tomcat. 2. Build **jar** and run `java -jar foo.jar` 3. Use **Docker**. You can build your docker image by [Dockerfile](Dockerfile). And run it with [docker-compose.yml](docker-compose.yml).  ## <a name="license"></a>License [[TOP]](#index)  **[MIT](http://opensource.org/licenses/MIT)**  Copyright (c) since 2015 saintdan  ## <a name="version"></a>Version History [[TOP]](#index)  [Version history is here. ;)](VERSION_HISTORY.md)
lviggiano/owner	OWNER =====  OWNER, an API to ease Java property files usage.  [![Build Status](https://travis-ci.org/lviggiano/owner.png?branch=master)](https://travis-ci.org/lviggiano/owner) [![Coverage Status](https://coveralls.io/repos/lviggiano/owner/badge.png)](https://coveralls.io/r/lviggiano/owner) [![security status](https://www.meterian.com/badge/gh/lviggiano/owner/security)](https://www.meterian.com/report/gh/lviggiano/owner) [![stability status](https://www.meterian.com/badge/gh/lviggiano/owner/stability)](https://www.meterian.com/report/gh/lviggiano/owner) [![Built with Maven](http://maven.apache.org/images/logos/maven-feather.png)](http://owner.newinstance.it/maven-site/)  Donate ------------------------------------------------------------------------  [![Flattr this](https://button.flattr.com/flattr-badge-large.png)](https://flattr.com/submit/auto?fid=pqvxnq&url=https%3A%2F%2Fgithub.com%2Flviggiano%2Fowner%2F)  ![Donate bitcoin:1DARNDYjjKhNF8j2DFwT3zrtUsMQvV2am1](http://i.imgur.com/BC7sVAG.png)   bitcoin:1DARNDYjjKhNF8j2DFwT3zrtUsMQvV2am1  [![Donate with PayPal](https://www.paypalobjects.com/webstatic/en_US/i/btn/png/gold-rect-paypal-26px.png)](https://www.paypal.me/lviggiano)    INTRODUCTION ------------  The goal of OWNER API is to minimize the code required to handle application configuration through Java properties files.  Full documentation available on [project website][website].  BASIC USAGE -----------  The approach used by OWNER APIs, is to define a Java interface associated to a properties file.  Suppose your properties file is defined as `ServerConfig.properties`:  ```properties port=80 hostname=foobar.com maxThreads=100 ```  To access this property you need to define a convenient Java interface in `ServerConfig.java`:  ```java public interface ServerConfig extends Config {     int port();     String hostname();     int maxThreads(); } ```  We'll call this interface the *Properties Mapping Interface* or just *Mapping Interface* since its goal is to map Properties into a an easy to use piece of code.  Then, you can use it from inside your code:  ```java public class MyApp {     public static void main(String[] args) {         ServerConfig cfg = ConfigFactory.create(ServerConfig.class);         System.out.println("Server " + cfg.hostname() + ":" + cfg.port() +                            " will run " + cfg.maxThreads());     } } ```  But this is just the tip of the iceberg.  Continue reading here: [Basic usage](http://owner.aeonbits.org/docs/usage/).  DOWNLOAD --------  Public Releases can be downloaded from [GitHub Releases](https://github.com/lviggiano/owner/releases) page or [Maven Central Repository](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.aeonbits.owner%22%20AND%20a%3A%22owner%22).   DOCUMENTATION -------------  Make sure to have a look at the documentation on [project website][website] to learn how flexible and powerful OWNER is, and why you may need it!  Chinese documentation is provided by [Yunfeng Cheng](https://github.com/cyfonly) via a GitHub independent project at [this address][chinese-docs].    [website]: http://owner.aeonbits.org   [chinese-docs]: https://github.com/cyfonly/owner-doc   LICENSE -------  OWNER is released under the BSD license. See [LICENSE][] file included for the details.    [LICENSE]: https://raw.github.com/lviggiano/owner/master/LICENSE
johncarl81/parceler	= Parceler  image:https://badges.gitter.im/johncarl81/parceler.svg[link="https://gitter.im/johncarl81/parceler?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge"] image:https://travis-ci.org/johncarl81/parceler.png?branch=master["Build Status", link="https://travis-ci.org/johncarl81/parceler"] image:https://maven-badges.herokuapp.com/maven-central/org.parceler/parceler-api/badge.svg["Maven Central", link="https://maven-badges.herokuapp.com/maven-central/org.parceler/parceler-api"]  Have a question?  http://stackoverflow.com/questions/ask?tags=parceler[Ask it on StackOverflow.]  Found an issue?  https://github.com/johncarl81/parceler/issues/new[Please report it.]  In Android, http://developer.android.com/reference/android/os/Parcelable.html[Parcelables] are a great way to serialize Java Objects between Contexts. http://www.developerphil.com/parcelable-vs-serializable/[Compared] with traditional Serialization, Parcelables take on the order of 10x less time to both serialize and deserialize. There is a major flaw with Parcelables, however. Parcelables contain a ton of boilerplate code. To implement a Parcelable, you must mirror the `writeToParcel()` and `createFromParcel()` methods such that they read and write to the Parcel in the same order. Also, a Parcelable must define a `public static final Parcelable.Creator CREATOR` in order for the Android infrastructure to be able to leverage the serialization code.  Parceler is a code generation library that generates the Android Parcelable boilerplate source code. No longer do you have to implement the Parcelable interface, the `writeToParcel()` or `createFromParcel()` or the `public static final CREATOR`. You simply annotate a POJO with `@Parcel` and Parceler does the rest. Because Parceler uses the Java JSR-269 Annotation Processor, there is no need to run a tool manually to generate the Parcelable code. Just annotate your Java Bean, compile and you are finished. By default, Parceler will serialize the fields of your instance directly:  [source,java] ---- @Parcel public class Example {     String name;     int age;      public Example() {}      public Example(int age, String name) {         this.age = age;         this.name = name;     }      public String getName() { return name; }      public int getAge() { return age; } } ----  Be careful not to use private fields when using the default field serialization strategy as it will incur a performance penalty due to reflection.  To use the generated code, you may reference the generated class directly, or via the `Parcels` utility class:  [source,java] ---- Parcelable wrapped = Parcels.wrap(new Example("Andy", 42)); ----  To dereference the `@Parcel`, just call the `Parcels.unwrap()` method:  [source,java] ---- Example example = Parcels.unwrap(wrapped); example.getName(); // Andy example.getAge(); // 42 ----  Of course, the wrapped `Parcelable` can be added to an Android Bundle to transfer from Activity to Activity:  [source,java] ---- Bundle bundle = new Bundle(); bundle.putParcelable("example", Parcels.wrap(example)); ----  And dereferenced in the `onCreate()` method:  [source,java] ---- Example example = Parcels.unwrap(getIntent().getParcelableExtra("example")); ----  This wrapping and unwrapping technique plays well with the Intent Factory pattern. In addition, Parceler is supported by the following libraries:   * http://androidtransfuse.org/documentation.html#parcel[Transfuse] - Allows `@Parcel` annotated beans to be used with the `@Extra` injection.  * https://github.com/sockeqwe/fragmentargs#argsbundler[FragmentArgs] - Uses the `ParcelerArgsBundler` adapter to wrap and unwrap `@Parcel` annotated beans with fragment parameters.  * https://github.com/f2prateek/dart[Dart] - Autodetects `@Parcel` annotated beans and automatically unwraps them when using `@InjectExtra`.  * http://androidannotations.org/[AndroidAnnotations] - Autodetects `@Parcel` annotated beans and https://github.com/excilys/androidannotations/wiki/ParcelerIntegration[automatically wraps/unwraps] them when using `@Extra`, `@FragmentArg`, `@InstanceState` and other `Bundle` related annotations.  * https://github.com/MarcinMoskala/ActivityStarter/wiki/Parceler-Arg-Converter-usage[ActivityStarter] - Supports natively Parceler objects as arguments to Activities, Fragments, Services, etc.  === Parcel attribute types Only a select number of types may be used as attributes of a `@Parcel` class. The following list includes the mapped types:   * `byte`  * `double`  * `float`  * `int`  * `long`  * `char`  * `boolean`  * `String`  * `IBinder`  * `Bundle`  * `SparseArray` of any of the mapped types*  * `SparseBooleanArray`  * `ObservableField`  * `List`, `ArrayList` and `LinkedList` of any of the mapped types*  * `Map`, `HashMap`, `LinkedHashMap`, `SortedMap`, and `TreeMap` of any of the mapped types*  * `Set`, `HashSet`, `SortedSet`, `TreeSet`, `LinkedHashSet` of any of the mapped types*  * `Parcelable`  * `Serializable`  * Array of any of the mapped types  * Any other class annotated with `@Parcel`  *Parcel will error if the generic parameter is not mapped.  Parceler also supports any of the above types directly. This is especially useful when dealing with collections of classes annotated with `@Parcel`:  [source,java] ---- Parcelable listParcelable = Parcels.wrap(new ArrayList<Example>()); Parcelable mapParcelable = Parcels.wrap(new HashMap<String, Example>()); ----  ==== Polymorphism Note that Parceler does not unwrap inheritance hierarchies, so any polymorphic fields will be unwrapped as instances of the base class. This is because Parceler opts for performance rather than checking `.getClass()` for every piece of data.  [source,java] ---- @Parcel public class Example {     public Parent p;     @ParcelConstructor Example(Parent p) { this.p = p; } }  @Parcel public class Parent {} @Parcel public class Child extends Parent {} ----  [source,java] ---- Example example = new Example(new Child()); System.out.println("%b", example.p instanceof Child); // true example = Parcels.unwrap(Parcels.wrap(example)); System.out.println("%b", example.p instanceof Child); // false ----  Refer to the <<custom-serialization,Custom Serialization>> section for an example of working with polymorphic fields.  === Serialization techniques  Parceler offers several choices for how to serialize and deserialize an object in addition to the field-based serialization seen above.  ==== Getter/setter serialization Parceler may be configured to serialize using getter and setter methods and a non-empty constructor. In addition, fields, methods and constructor parameters may be associated using the `@ParcelProperty` annotation. This supports a number of bean strategies including immutability and traditional getter/setter beans.  To configure default method serialization, simply configure the `@Parcel` annotation with `Serialization.BEAN`:  [source,java] ---- @Parcel(Serialization.BEAN) public class Example {     private String name;     private int age;     private boolean enabled;      public String getName() { return name; }     public void setName(String name) { this.name = name; }      public int getAge() { return age; }     public void setAge(int age) { this.age = age; }          public boolean isEnabled() { return enabled; }     public void setEnabled(boolean enabled) { this.enabled = enabled; } } ----  To use a constructor with serialization, annotate the desired constructor with the `@ParcelConstructor` annotation:  [source,java] ---- @Parcel(Serialization.BEAN) public class Example {     private final String name;     private final int age;     private boolean enabled;      @ParcelConstructor     public Example(int age, String name, boolean enabled) {         this.age = age;         this.name = name;         this.enabled = enabled;     }      public String getName() { return name; }      public int getAge() { return age; }          public boolean isEnabled() { return enabled; } } ----  If an empty constructor is present, Parceler will use that constructor unless another constructor is annotated.  ==== Mixing getters/setters and fields You may also mix and match serialization techniques using the `@ParcelProperty` annotation. In the following example, `firstName` and `lastName` are written to the bean using the constructor while `firstName` is read from the bean using the field and `lastName` is read using the `getLastName()` method. The parameters `firstName` and `lastName` are coordinated by the parameter names `"first"` and `"last"` respectfully.  [source,java] ---- @Parcel public class Example {     @ParcelProperty("first")     String firstName;     String lastName;      @ParcelConstructor     public Example(@ParcelProperty("first") String firstName, @ParcelProperty("last") String lastName){         this.firstName = firstName;         this.lastName = lastName;     }      public String getFirstName() { return firstName; }      @ParcelProperty("last")     public String getLastName() { return lastName; } } ----  For attributes that should not be serialized with Parceler, the attribute field, getter or setter may be annotated by `@Transient`.  Parceler supports many different styles centering around the POJO. This allows `@Parcel` annotated classes to be used with other POJO based libraries, including the following:   * https://code.google.com/p/google-gson/[GSON]  * https://realm.io/docs/java/latest/#parceler[Realm]  * https://bitbucket.org/littlerobots/cupboard[Cupboard]  * http://simple.sourceforge.net/[Simple XML]  * https://github.com/Raizlabs/DBFlow[DBFlow]  ==== Static Factory support As an alternative to using a constructor directly, Parceler supports using an annotated Static Factory to build an instance of the given class. This style supports Google's https://github.com/google/auto/tree/master/value[AutoValue] annotation processor / code generation library for generating immutable beans. Parceler interfaces with AutoValue via the `@ParcelFactory` annotation, which maps a static factory method into the annotated `@Parcel` serialization:  [source,java] ---- @AutoValue @Parcel public abstract class AutoValueParcel {      @ParcelProperty("value") public abstract String value();      @ParcelFactory     public static AutoValueParcel create(String value) {         return new AutoValue_AutoValueParcel(value);     } } ----  AutoValue generates a different class than the annotated `@Parcel`, therefore, you need to specify which class Parceler should build in the `Parcels` utility class:  [source,java] ---- Parcelable wrappedAutoValue = Parcels.wrap(AutoValueParcel.class, AutoValueParcel.create("example")); ---- And to deserialize: [source,java] ---- AutoValueParcel autoValueParcel = Parcels.unwrap(wrappedAutoValue); ----  ==== Custom serialization `@Parcel` includes an optional parameter to include a manual serializer `ParcelConverter` for the case where special serialization is necessary. This provides a still cleaner option for using Parcelable classes than implementing them by hand.  The following code demonstrates using a `ParcelConverter` to unwrap the inheritance hierarchy during deserialization.  [source,java] ---- @Parcel public class Item {     @ParcelPropertyConverter(ItemListParcelConverter.class)     public List<Item> itemList; } @Parcel public class SubItem1 extends Item {} @Parcel public class SubItem2 extends Item {}  public class ItemListParcelConverter implements ParcelConverter<List<Item>> {     @Override     public void toParcel(List<Item> input, Parcel parcel) {         if (input == null) {             parcel.writeInt(-1);         }         else {             parcel.writeInt(input.size());             for (Item item : input) {                 parcel.writeParcelable(Parcels.wrap(item), 0);             }         }     }      @Override     public List<Item> fromParcel(Parcel parcel) {         int size = parcel.readInt();         if (size < 0) return null;         List<Item> items = new ArrayList<Item>();         for (int i = 0; i < size; ++i) {             items.add((Item) Parcels.unwrap(parcel.readParcelable(Item.class.getClassLoader())));         }         return items;     } } ----  Parceler is also packaged with a series of base classes to make Collection conversion easier located under the `org.parceler.converter` package of the api. These base classes take care of a variety of difficult or verbose jobs dealing with Collections including null checks and collectin iteration. For instance, the above `ParcelConverter` could be written using the `ArrayListParcelConverter':  [source,java] ---- public class ItemListParcelConverter extends ArrayListParcelConverter<Item> {     @Override     public void itemToParcel(Item item, Parcel parcel) {         parcel.writeParcelable(Parcels.wrap(item), 0);     }      @Override     public Item itemFromParcel(Parcel parcel) {         return Parcels.unwrap(parcel.readParcelable(Item.class.getClassLoader()));     } } ----  === Classes without Java source For classes whose corresponding Java source is not available, one may include the class as a Parcel by using the `@ParcelClass` annotation. This annotation may be declared anywhere in the compiled source that is convenient. For instance, one could include the `@ParcelClass` along with the Android Application:  [source,java] ---- @ParcelClass(LibraryParcel.class) public class AndroidApplication extends Application{     //... } ----  Multiple `@ParcelClass` annotations may be declared using the `@ParcelClasses` annotation.  In addition, classes referenced by `@ParcelClass` may be configured using the `@Parcel` annotation. This allows the serialization configuration through any parameter available on the `@Parcel` annotation including the serialization technique or classes to analyze.  One useful technique is the ability to define global custom converters for a type: [source,java] ---- @ParcelClass(     value = LibraryParcel.class,     annotation = @Parcel(converter = LibraryParcelConverter.class)) class SomeClass{} ---- This allows for fine grained control over a class that isn't available for direct modification.  === Advanced configuration  ==== Skipping analysis It is a common practice for some libraries to require a bean to extend a base class. Although it is not the most optimal case, Parceler supports this practice by allowing the configuration of what classes in the inheritance hierarchy to analyze via the analyze parameter:  [source, java] ---- @Parcel(analyze = {One.class, Three.class}) class One extends Two {} class Two extends Three {} class Three extends BaseClass {} ----  In this example, only fields of the `One` and `Three` classes will be serialized, avoiding both the `BaseClass` and `Two` class parameters.  ==== Specific wrapping  The Parcels utility class looks up the given class for wrapping by class. For performance reasons this ignores inheritance, both super and base classes. There are two solutions to this problem. First, one may specify additional types to associate to the given type via the `implementations` parameter:  [source, java] ---- class ExampleProxy extends Example {} @Parcel(implementations = {ExampleProxy.class}) class Example {}  ExampleProxy proxy = new ExampleProxy(); Parcels.wrap(proxy);  // ExampleProxy will be serialized as a Example ----  Second, one may also specify the class type when using the `Parcels.wrap()` method:  [source, java] ---- ExampleProxy proxy = new ExampleProxy(); Parcels.wrap(Example.class, proxy); ----  ==== Configuring Proguard  To configure Proguard, add the following lines to your proguard configuration file.  These will keep files related to the `Parcels` utilty class and the `Parcelable` `CREATOR` instance: er ---- # Parceler library -keep interface org.parceler.Parcel -keep @org.parceler.Parcel class * { *; } -keep class **$$Parcelable { *; } ----  == Getting Parceler  You may download Parceler as a Maven dependency:  [source,xml] ---- <dependency>     <groupId>org.parceler</groupId>     <artifactId>parceler</artifactId>     <version>1.1.9</version>     <scope>provided</scope> </dependency> <dependency>     <groupId>org.parceler</groupId>     <artifactId>parceler-api</artifactId>     <version>1.1.9</version> </dependency> ----  or Gradle: [source,groovy] ---- compile 'org.parceler:parceler-api:1.1.9' annotationProcessor 'org.parceler:parceler:1.1.9' ----  Or from http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.parceler%22[Maven Central].  == License ---- Copyright 2011-2015 John Ericksen  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ----
AndroidBootstrap/android-bootstrap	# Release Notes  ### 2015-10-15  * All fragments have been updated to support lib fragments. * All activities are now AppCompat * Dagger is updated to Dagger 2 * ButterKnife has been updated to 7.0.1 * Timer notification now shows the timer in the notification (useful for when someone does not have the app open) * No ETA on the Material and RxJava implementation other than "when there is time". That magical time is wild stuff.  # UPDATE: 2014-11-18  * Android Bootstrap is now API 15+ only. * I will also be implementing RxJava, Material design and a few other things into this app as well. There will be *no upgrade path* as Android Bootstrap is mean to act as a starting point for Android Applications.  # Android Bootstrap App  [![Build Status](https://travis-ci.org/AndroidBootstrap/android-bootstrap.svg?branch=master)](https://travis-ci.org/AndroidBootstrap/android-bootstrap)  This repository contains the source code for the [Android Bootstrap](http://www.androidbootstrap.com/) Android app available from [Google Play](https://play.google.com/store/apps/details?id=com.donnfelker.android.bootstrap).  Please see the [issues](https://github.com/androidbootstrap/android-bootstrap/issues) section to report any bugs or feature requests and to see the list of known issues.  Have a questions about Android Bootstrap? Ask away on the [android-bootstrap discussion forum](https://groups.google.com/forum/#!forum/android-bootstrap).  <a href="https://play.google.com/store/apps/details?id=com.donnfelker.android.bootstrap" alt="Download from Google Play">   <img src="http://f.cl.ly/items/3V0K1s1i402W0c193v2w/Image%202013.07.08%201%3A45%3A25%20PM.png"> </a>  <a href="https://play.google.com/store/apps/details?id=com.donnfelker.android.bootstrap" alt="Download from Google Play">   <img src="http://f.cl.ly/items/0e3T2F2x3M0K2l1X0A0u/Image%202013.07.08%201%3A46%3A09%20PM.png"> </a>  ## HOW TO Learn how to develop with IntelliJ and Gradle.  ## Authentication Log into this demo app with the following credentials:  user: demo@androidbootstrap.com  password: android   ## Generating your Bootstrap App Why generate? Simple ... renaming files, folders, copy and pasting is SUPER error prone and well... it sucks overall. This can easily take a few days with debugging if you run into issues and perform a lot of typo's. Using the generator on [AndroidBootstrap.com](http://www.androidbootstrap.com) you can generate your application with your application name as well as the package (and folder structure) that you want to work with.  As an example, you know that you want your app name and package to the following:    - *App Name*: Notify   - *Package Name*: com.notify.app.mobile  After generating the app on [AndroidBootstrap.com](http://www.androidbootstrap.com) the folder structure of the source code for the app will change:    - From: __com/donnfelker/android/bootstrap__   - To: __com/notify/app/mobile__  At that point all the source files that were located in ____com/donnfelker/android/bootstrap__ will be moved to the new folder __com/notify/app/mobile__.  All import statments that reference the old resources (__R.com.donnfelker.android.bootstrap.R__) will now be renamed to the correct package. The artifact id's in the *pom.xml* (and various other places) will be replaced. The App Name will be replaced in the strings/etc.  The end result is that you will be given a zip file with the correct structure. Open the zip and then execute the *./gradlew* command and your app should be ready for development.  Enjoy!  The application  ## License  * [Apache Version 2.0](http://www.apache.org/licenses/LICENSE-2.0.html)   Copyright 2014 Donn Felker Copyright 2014 GitHub Inc.  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at   http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.   ## Building  The build requires [Gradle](http://www.gradle.org/downloads) v1.10+ and the [Android SDK](http://developer.android.com/sdk/index.html) to be installed in your development environment. In addition you'll need to set the `ANDROID_HOME` environment variable to the location of your SDK:      export ANDROID_HOME=/path/to/your/android-sdk  After satisfying those requirements, the build is pretty simple:  * Run `gradlew` or `gradle assembleDebug` or `gradle assembleRelease` from the `app` directory to build the APK only * Run one of the commands above from the root directory to build the app and also run   the integration tests, this requires a connected Android device or running   emulator.  You might find that your device doesn't let you install your build if you already have the version from the Android Market installed.  This is standard Android security as it it won't let you directly replace an app that's been signed with a different key.  Manually uninstall Android Bootstrap from your device and you will then be able to install your own built version.  ## Building in Eclipse  Why are you using Eclipse still? :) Please use Android Studio, we do not support Eclipse.   ## Acknowledgements  Android Bootstrap is a result of a template project I've developed over the years as well as a combination of a lot of great work that the [GitHub Gaug.es](http://www.github.com/github/gauges-android) app and [GitHub Android](http://www.github.com/github/android) app showcased. Some of the code in this project is based on the GitHub Gaug.es and GitHub Android app.  Android Bootstrap is built on the awesome [Parse.com API](http://www.parse.com/) and uses many great open-source libraries from the Android dev community:  * [AppCompat](http://www.youtube.com/watch?v=6TGgYqfJnyc) for a   consistent, great looking header across all Android platforms,   [ViewPagerIndicator](https://github.com/JakeWharton/Android-ViewPagerIndicator)   for swiping between fragments and   [NineOldAndroids](https://github.com/JakeWharton/NineOldAndroids) for   view animations - all from [Jake Wharton](http://jakewharton.com/). * [NavigationDrawer](http://developer.android.com/design/patterns/navigation-drawer.html) for the menu drawer navigation. * [Dagger](https://github.com/square/dagger) for dependency-injection. * [ButterKnife](https://github.com/JakeWharton/butterknife) for view injection * [Otto](https://github.com/square/otto) as the event bus * [Robotium](http://code.google.com/p/robotium/)   for driving our app during integration tests. * [android-maven-plugin](https://github.com/jayway/maven-android-plugin)   for automating our build and producing release-ready APKs. * [Retrofit](http://square.github.io/retrofit/) for interacting with   remote HTTP resources (API's in this case). * [google-gson](http://code.google.com/p/google-gson/) for consuming JSON and hydrating   POJO's for use in the app.   ## Contributors Thank you to all the [contributors](http://www.github.com/androidbootstrap/android-bootstrap/contributors) on this project. Your help is much appreciated.   ## Contributing  Please fork this repository and contribute back using [pull requests](https://github.com/androidbootstrap/android-bootstrap/pulls).  Any contributions, large or small, major features, bug fixes, additional language translations, unit/integration tests are welcomed and appreciated but will be thoroughly reviewed and discussed.  I hope this helps you in building your next android app.
uwolfer/gerrit-rest-java-client	gerrit-rest-java-client ======================  [![Linux Build](https://travis-ci.org/uwolfer/gerrit-rest-java-client.svg?branch=master)](https://travis-ci.org/uwolfer/gerrit-rest-java-client) [![Windows Build](https://ci.appveyor.com/api/projects/status/ctm64o74lxdri26s/branch/master?svg=true)](https://ci.appveyor.com/project/uwolfer/gerrit-rest-java-client/branch/master) [![Coverage Status](https://img.shields.io/coveralls/uwolfer/gerrit-rest-java-client.svg)](https://coveralls.io/r/uwolfer/gerrit-rest-java-client) [![Quality Gate](https://sonarcloud.io/api/badges/gate?key=com.urswolfer.gerrit.client.rest:gerrit-rest-java-client)](https://sonarcloud.io/dashboard/index/com.urswolfer.gerrit.client.rest:gerrit-rest-java-client) [![Technical Dept](https://sonarcloud.io/api/badges/measure?key=com.urswolfer.gerrit.client.rest:gerrit-rest-java-client&metric=sqale_debt_ratio)](https://sonarcloud.io/dashboard/index/com.urswolfer.gerrit.client.rest:gerrit-rest-java-client) [![Dependency Status](https://www.versioneye.com/java/com.urswolfer.gerrit.client.rest:gerrit-rest-java-client/badge)](https://www.versioneye.com/java/com.urswolfer.gerrit.client.rest:gerrit-rest-java-client) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.urswolfer.gerrit.client.rest/gerrit-rest-java-client/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.urswolfer.gerrit.client.rest/gerrit-rest-java-client)  Introduction -----------  Java implementation of the [Gerrit Code Review Tool] REST API.  Only Gerrit 2.6 or newer is supported (missing / incomplete REST API in older versions).  This implementation is used for example as base for the [Gerrit IntelliJ Plugin].  Many different authentication-methods are supported (HTTP basic, HTTP digest, LDAP with form, HTTP password from Gerrit setting, ...).  [Gerrit Code Review Tool]: https://www.gerritcodereview.com/ [Gerrit IntelliJ Plugin]: https://github.com/uwolfer/gerrit-intellij-plugin   Usage ------- This library implements <code>[com.google.gerrit.extensions.api.GerritApi]</code>.  You just need a few lines to get it working: ```java GerritRestApiFactory gerritRestApiFactory = new GerritRestApiFactory(); GerritAuthData.Basic authData = new GerritAuthData.Basic("http://localhost:8080"); // or: authData = new GerritAuthData.Basic("https://example.com/gerrit", "user", "password"); GerritApi gerritApi = gerritRestApiFactory.create(authData); List<ChangeInfo> changes = gerritApi.changes().query("status:merged").withLimit(10).get(); ```  If you like to write a script instead of a full Java application, you might want to use [Groovy]. There is a [basic Groovy example] available.  _Note:_ It is not guaranteed that all interfaces are implemented. If an implementation is missing, you get a <code>com.google.gerrit.extensions.restapi.NotImplementedException</code>. Feel free to implement it and create a pull request at GitHub - it is quite easy! :)  _Note:_ The source of <code>com.google.gerrit.extensions</code> is included in this repository at the moment because not all extensions to this API are merged into Gerrit repository yet.  [com.google.gerrit.extensions.api.GerritApi]: https://gerrit.googlesource.com/gerrit/+/HEAD/gerrit-extension-api/src/main/java/com/google/gerrit/extensions/api/GerritApi.java [Groovy]: http://www.groovy-lang.org/ [basic Groovy example]: https://github.com/uwolfer/gerrit-rest-java-client/blob/master/examples/Basic.groovy  Maven Artifact -------------- Releases are available with Maven: ```xml <dependency>     <groupId>com.urswolfer.gerrit.client.rest</groupId>     <artifactId>gerrit-rest-java-client</artifactId>     <version>0.8.12</version> </dependency> ```  Android Support --------------- Apache HttpClient causes problems on Android platform. There is a workaround by using [HttpClient for Android]. Android support builds are not officially released, but you should be able to create your own build by using the [httpclient-android branch]. You probably want to merge master branch into this branch before building it.  [HttpClient for Android]: https://hc.apache.org/httpcomponents-client-4.3.x/android-port.html [httpclient-android branch]: https://github.com/uwolfer/gerrit-rest-java-client/tree/httpclient-android  Dependencies ------------ This library depends on [Apache HttpClient], [Gson] and [Guava].  [Apache HttpClient]: https://hc.apache.org/httpcomponents-client-ga/ [Gson]: https://github.com/google/gson [Guava]: https://github.com/google/guava  Your Support ------------ If you like this library, you can support it: * Star it: [Star it at GitHub](https://github.com/uwolfer/gerrit-rest-java-client). GitHub account required. * Improve it: Report bugs or feature requests. Or even fix / implement them by yourself - everything is open source! * Donate: You can find donation-possibilities at the bottom of this file.   Donations -------- If you like this work, you can support it with [this donation link](https://www.paypal.com/webscr?cmd=_s-xclick&hosted_button_id=8F2GZVBCVEDUQ). If you don't like Paypal (Paypal takes 2.9% plus $0.30 per transaction fee from your donation), please contact me. Please only use the link from github.com/uwolfer/gerrit-intellij-plugin to verify that it is correct.   Copyright and license --------------------  Copyright 2013 - 2017 Urs Wolfer  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this work except in compliance with the License. You may obtain a copy of the License in the LICENSE file, or at:    [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
docusign/docusign-java-client	# The Official DocuSign Java Client  [![Build status][travis-image]][travis-url] [![Maven Central status][maven-image]][maven-url]  You can sign up for a free [developer sandbox](https://www.docusign.com/developer-center).  Requirements ============  Java 1.6 or later.    Installation ============  ### Maven users  Add this dependency to your project's POM:  ```xml <dependency>    <groupId>com.docusign</groupId>    <artifactId>docusign-esign-java</artifactId>    <version>2.3.0</version> </dependency> ```  ### Gradle users  Add this dependency to your project's build file:  ```groovy compile "com.docusign:docusign-esign-java:2.3.0" ```  #### Dependencies  This client has the following external dependencies:  * io.swagger:swagger-annotations:jar:1.5.8 * com.sun.jersey:jersey-client:jar:1.19.1 * com.sun.jersey.contribs:jersey-multipart:jar:1.19.1 * com.fasterxml.jackson.core:jackson-core:jar:2.7.0 * com.fasterxml.jackson.core:jackson-annotations:jar:2.7.0 * com.fasterxml.jackson.core:jackson-databind:jar:2.7.0 * com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider:jar:2.7.0 * com.fasterxml.jackson.datatype:jackson-datatype-joda:jar:2.1.5 * joda-time:joda-time:jar:2.9.3 * com.brsanthu:migbase64:jar:2.2 * junit:junit:jar:4.12  #### Note for Android Developers   If you encounter build errors due to duplicate definitions, include the following in your build.gradle module:  ``` android {    packagingOptions {       pickFirst 'META-INF/services/javax.ws.rs.ext.MessageBodyReader’       pickFirst 'META-INF/services/javax.ws.rs.ext.MessageBodyWriter’       pickFirst 'META-INF/services/com.sun.jersey.spi.inject.InjectableProvider’       pickFirst 'META-INF/jersey-module-version' pickFirst 'META-INF/NOTICE’       pickFirst 'META-INF/LICENSE’       pickFirst 'META-INF/services/com.fasterxml.jackson.databind.Module’       pickFirst 'META-INF/services/com.fasterxml.jackson.core.ObjectCodec’       pickFirst 'META-INF/services/com.fasterxml.jackson.core.JsonFactory’    } } ```  ### Package Managers  This client is available through the following Java package managers:  - [Nexus Repository Manager](https://oss.sonatype.org/#nexus-search;quick~docusign-esign-java) (oss.sonatype.org). You can search for com.docusign or docusign-esign-java. The current version is 2.3.0. - [JFrog Bintray](https://bintray.com/dsdevcenter/maven/docusign-esign-java) (bintray.com). You can search for com.docusign or docusign-esign-java. The current version is 2.3.0.  ### Others  Or you can manually download and add the following JARs to your project:  * The [docusign-esign-java-2.3.0](https://github.com/docusign/docusign-java-client/releases/latest) JAR. * The [Dependency JARs](/target/lib) in /lib folder.   Usage =====  To send a signature request from a Template using 3-legged OAuth:  ```java import com.docusign.esign.api.*; import com.docusign.esign.client.*; import com.docusign.esign.model.*; import com.docusign.esign.client.auth.AccessTokenListener;  import org.apache.oltu.oauth2.common.token.BasicOAuthToken; import java.awt.Desktop;  public class DocuSignExample {   public static void main(String[] args) {     String RedirectURI = "[REDIRECT_URI]";     String ClientSecret = "[CLIENT_SECRET]";     String IntegratorKey = "[INTEGRATOR_KEY]";     String BaseUrl = "https://demo.docusign.net/restapi";     String OAuthBaseUrl = "https://account-d.docusign.com";          ApiClient apiClient = new ApiClient(OAuthBaseUrl, "docusignAccessCode", IntegratorKey, ClientSecret);     apiClient.setBasePath(BaseUrl);     // make sure to pass the redirect uri     apiClient.configureAuthorizationFlow(IntegratorKey, ClientSecret, RedirectURI);     Configuration.setDefaultApiClient(apiClient);     try     {       // get DocuSign OAuth authorization url       String oauthLoginUrl = apiClient.getAuthorizationUri();       // open DocuSign OAuth login in the browser       System.out.println(oauthLoginUrl);       Desktop.getDesktop().browse(URI.create(oauthLoginUrl));       // IMPORTANT: after the login, DocuSign will send back a fresh       // authorization code as a query param of the redirect URI.       // You should set up a route that handles the redirect call to get       // that code and pass it to token endpoint as shown in the next       // lines:       String code = "<once_you_get_the_oauth_code_put_it_here>";       // assign it to the token endpoint       apiClient.getTokenEndPoint().setCode(code);       // optionally register to get notified when a new token arrives       apiClient.registerAccessTokenListener(new AccessTokenListener() {         @Override         public void notify(BasicOAuthToken token) {           System.out.println("Got a fresh token: " + token.getAccessToken());         }       });       // ask to exchange the auth code with an access code       apiClient.updateAccessToken();            /////////////////////////////////////////////////////////////////////////////////////////////////////////       // STEP 1: AUTHENTICATE TO RETRIEVE ACCOUNTID & BASEURL                /////////////////////////////////////////////////////////////////////////////////////////////////////////        AuthenticationApi authApi = new AuthenticationApi(apiClient);       LoginInformation loginInfo = authApi.login();              // parse first account ID (user might belong to multiple accounts) and baseUrl       String accountId = loginInfo.getLoginAccounts().get(0).getAccountId();        String baseUrl = loginInfo.getLoginAccounts().get(0).getBaseUrl();       String[] accountDomain = baseUrl.split("/v2");              // below code required for production, no effect in demo (same domain)        apiClient.setBasePath(accountDomain[0]);       Configuration.setDefaultApiClient(apiClient);              /////////////////////////////////////////////////////////////////////////////////////////////////////////       // *** STEP 2: CREATE ENVELOPE FROM TEMPLATE              /////////////////////////////////////////////////////////////////////////////////////////////////////////              // create a new envelope to manage the signature request       EnvelopeDefinition envDef = new EnvelopeDefinition();       envDef.setEmailSubject("DocuSign Java SDK - Sample Signature Request");              // assign template information including ID and role(s)       envDef.setTemplateId("[TEMPLATE_ID]");              // create a template role with a valid templateId and roleName and assign signer info       TemplateRole tRole = new TemplateRole();       tRole.setRoleName("[ROLE_NAME]");       tRole.setName("[SIGNER_NAME]");       tRole.setEmail("[SIGNER_EMAIL]");            // create a list of template roles and add our newly created role       java.util.List<TemplateRole> templateRolesList = new java.util.ArrayList<TemplateRole>();       templateRolesList.add(tRole);            // assign template role(s) to the envelope        envDef.setTemplateRoles(templateRolesList);              // send the envelope by setting |status| to "sent". To save as a draft set to "created"       envDef.setStatus("sent");            // instantiate a new EnvelopesApi object       EnvelopesApi envelopesApi = new EnvelopesApi(apiClient);            // call the createEnvelope() API       EnvelopeSummary envelopeSummary = envelopesApi.createEnvelope(accountId, envDef);     }     catch (com.docusign.esign.client.ApiException ex)     {       System.out.println("Exception: " + ex);     }   } }  ```  To send a signature request from a Template using service integration:  ```java import com.docusign.esign.api.*; import com.docusign.esign.client.*; import com.docusign.esign.model.*;  import java.util.List; import java.io.IOException;  public class DocuSignExample {   public static void main(String[] args) {     String OAuthBaseUrl = "account-d.docusign.com";     String BaseUrl = "https://demo.docusign.net/restapi";     String RedirectURI = "[OAUTH_REDIRECT_URI]";     String IntegratorKey = "[INTEGRATOR_KEY]";     String UserId = "[USER_ID_TO_SEND_ON_BEHALF]";     String publicKeyFilename = "[PATH_TO_RSA265_PUBLIC_KEY]";     String privateKeyFilename = "[PATH_TO_RSA265_PRIVATE_KEY]";          ApiClient apiClient = new ApiClient(BaseUrl);          try {       // IMPORTANT NOTE:       // the first time you ask for a JWT access token, you should grant access by making the following call       // get DocuSign OAuth authorization url:       //String oauthLoginUrl = apiClient.getJWTUri(IntegratorKey, RedirectURI, OAuthBaseUrl);       // open DocuSign OAuth authorization url in the browser, login and grant access       //Desktop.getDesktop().browse(URI.create(oauthLoginUrl));       // END OF NOTE              apiClient.configureJWTAuthorizationFlow(publicKeyFilename, privateKeyFilename, OAuthBaseUrl, IntegratorKey, UserId, 3600); // request for a fresh JWT token valid for 1 hour       Configuration.setDefaultApiClient(apiClient);              /////////////////////////////////////////////////////////////////////////////////////////////////////////       // STEP 1: AUTHENTICATE TO RETRIEVE ACCOUNTID & BASEURL                /////////////////////////////////////////////////////////////////////////////////////////////////////////        AuthenticationApi authApi = new AuthenticationApi();       LoginInformation loginInfo = authApi.login();              // parse first account ID (user might belong to multiple accounts) and baseUrl       String accountId = loginInfo.getLoginAccounts().get(0).getAccountId();        String baseUrl = loginInfo.getLoginAccounts().get(0).getBaseUrl();       String[] accountDomain = baseUrl.split("/v2");              // below code required for production, no effect in demo (same domain)        apiClient.setBasePath(accountDomain[0]);       Configuration.setDefaultApiClient(apiClient);              /////////////////////////////////////////////////////////////////////////////////////////////////////////       // *** STEP 2: CREATE ENVELOPE FROM TEMPLATE              /////////////////////////////////////////////////////////////////////////////////////////////////////////              // create a new envelope to manage the signature request       EnvelopeDefinition envDef = new EnvelopeDefinition();       envDef.setEmailSubject("DocuSign Java SDK - Sample Signature Request");              // assign template information including ID and role(s)       envDef.setTemplateId("[TEMPLATE_ID]");              // create a template role with a valid templateId and roleName and assign signer info       TemplateRole tRole = new TemplateRole();       tRole.setRoleName("[ROLE_NAME]");       tRole.setName("[SIGNER_NAME]");       tRole.setEmail("[SIGNER_EMAIL]");            // create a list of template roles and add our newly created role       java.util.List<TemplateRole> templateRolesList = new java.util.ArrayList<TemplateRole>();       templateRolesList.add(tRole);            // assign template role(s) to the envelope        envDef.setTemplateRoles(templateRolesList);              // send the envelope by setting |status| to "sent". To save as a draft set to "created"       envDef.setStatus("sent");            // instantiate a new EnvelopesApi object       EnvelopesApi envelopesApi = new EnvelopesApi();            // call the createEnvelope() API       EnvelopeSummary envelopeSummary = envelopesApi.createEnvelope(accountId, envDef);     } catch (ApiException ex) {       System.out.println("Exception: " + ex);     } catch (Exception e) {       System.out.println("Exception: " + e.getLocalizedMessage());     }   } }  ```  See [SdkUnitTests.java](https://github.com/docusign/docusign-java-client/blob/master/src/test/java/SdkUnitTests.java) for more examples.  # Authentication  ## Service Integrations that use Legacy Header Authentication  ([Legacy Header Authentication](https://docs.docusign.com/esign/guide/authentication/legacy_auth.html) uses the X-DocuSign-Authentication header.)  1. Use the [Authentication: login method](https://docs.docusign.com/esign/restapi/Authentication/Authentication/login/) to retrieve the account number **and the baseUrl** for the account. The url for the login method is www.docusign.net for production and demo.docusign.net for the developer sandbox. The `baseUrl` field is part of the `loginAccount` object. See the [docs and the loginAccount object](https://docs.docusign.com/esign/restapi/Authentication/Authentication/login/#/definitions/loginAccount) 2. The baseUrl for the selected account, in production, will start with na1, na2, na3, eu1, or something else. Use the baseUrl that is returned to create the *basePath* (see the next step.) Use the basePath for all of your subsequent API calls. 3. As returned by login method, the baseUrl includes the API version and account id. Split the string to obtain the *basePath*, just the server name and api name. Eg, you will receive `https://na1.docusign.net/restapi/v2/accounts/123123123`. You want just `https://na1.docusign.net/restapi`  4. Instantiate the SDK using the basePath. Eg `ApiClient apiClient = new ApiClient(basePath);` 5. Set the authentication header as shown in the examples by using `Configuration.Default.AddDefaultHeader`  ## User Applications that use OAuth Authentication 1. After obtaining a Bearer token, call the [OAuth: Userinfo method](https://docs.docusign.com/esign/guide/authentication/userinfo.html). Obtain the selected account's `base_uri` (server name) field. The url for the Userinfo method is account-d.docusign.com for the demo/developer environment, and account.docusign.com for the production environment. 1. Combine the base_uri with "/restapi" to create the basePath. The base_uri will start with na1, na2, na3, eu1, or something else. Use the basePath for your subsequent API calls. 4. Instantiate the SDK using the basePath. Eg `ApiClient apiClient = new ApiClient(basePath);` 5. Create the `authentication_value` by combining the `token_type` and `access_token` fields you receive from either an [Authorization Code Grant](https://docs.docusign.com/esign/guide/authentication/oa2_auth_code.html) or [Implicit Grant](https://docs.docusign.com/esign/guide/authentication/oa2_implicit.html) OAuth flow.  5. Set the authentication header by using `Configuration.Default.AddDefaultHeader('Authorization', authentication_value)`   Testing =======  You must have Maven installed. To run the tests:      mvn test  Support =======  Feel free to log issues against this client through GitHub.  We also have an active developer community on Stack Overflow, search the [DocuSignAPI](http://stackoverflow.com/questions/tagged/docusignapi) tag.  License =======  The DocuSign Java Client is licensed under the following [License](LICENSE).   [travis-image]: https://img.shields.io/travis/docusign/docusign-java-client.svg?style=flat [travis-url]: https://travis-ci.org/docusign/docusign-java-client [maven-image]: https://img.shields.io/maven-central/v/com.docusign/docusign-esign-java.svg?style=flat [maven-url]: https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.docusign%22
Jaspersoft/jrs-rest-java-client	Rest Client for JasperReports Server [![Build Status](https://travis-ci.org/Jaspersoft/jrs-rest-java-client.svg?branch=master)](https://travis-ci.org/Jaspersoft/jrs-rest-java-client) [![Coverage Status](https://coveralls.io/repos/Jaspersoft/jrs-rest-java-client/badge.png?branch=master)](https://coveralls.io/r/Jaspersoft/jrs-rest-java-client?branch=master) =========================================  With this library you can easily write Java applications which can interact with one or more JasperReports servers simultaneously in a very simple way. Library provides very friendly API for user, it minimizes possibility of building wrong requests.  Table of Contents ------------------ 1. [Introduction](#introduction). 2. [Configuration](#configuration).   * [Loading configuration from file](#loading-configuration-from-file).   * [Creation of manual configuration](#creation-of-manual-configuration).   * [HTTPS configuration](#https-configuration).   * [X-HTTP-Method override](#x-http-method-override).   * [Switching of authentication type](#switching-authentication-type).   * [Exception handling](#exception-handling).   * [Logging](#logging).   * [Switching between JSON and XML](#switching-between-json-and-xml).   * [Client instantiation](#client-instantiation). 3. [Authentication](#authentication).   * [Anonymous session](#anonymous-session).   * [Invalidating session](#invalidating-session). 4. [Report services](#report-services).   * [Running a report](#running-a-report).   * [Requesting report execution status](#requesting-report-execution-status).   * [Requesting report execution details](#requesting-report-execution-details).   * [Requesting Report Output](#requesting-report-output).   * [Download file attachments for report output](#download-file-attachments-for-report-output).   * [Exporting a Report Asynchronously](#exporting-a-report-asynchronously).   * [Polling Export Execution](#polling-export-execution).   * [Finding Running Reports and Jobs](#finding-running-reports-and-jobs).   * [Stopping Running Reports and Jobs](#stopping-running-reports-and-jobs). 5. [Input controls service](#input-controls-service).   * [Listing input controls structure](#listing-input-controls-structure).   * [Reordering input controls structure](#reordering-input-controls-structure).   * [Listing input controls values](#listing-input-controls-values).   * [Setting input controls values](#setting-input-controls-values). 6. [Administration services](#administration-services).   1. [Organizations service](#organizations-service).     * [Searching for Organizations](#searching-for-organizations).     * [Viewing an Organization](#viewing-an-organization).     * [Creating an Organization](#creating-an-organization).     * [Modifying Organization Properties](#modifying-organization-properties).     * [Deleting an Organization](#deleting-an-organization).   2. [Users service](#users-service).     * [Searching for Users](#searching-for-users).     * [Viewing a User](#viewing-a-user).     * [Creating a User](#creating-a-user).     * [Modifying User Properties](#modifying-user-properties).     * [Deleting a User](#deleting-a-user).   3. [Attributes service](#attributes-service).     * [Viewing User Attributes](#viewing-user-attributes).     * [Setting User Attributes](#setting-user-attributes).     * [Deleting User Attributes](#deleting-user-attributes).     * [Viewing Organization Attributes](#viewing-organization-attributes).     * [Setting Organization Attributes](#setting-organization-attributes).     * [Deleting Organization Attributes](#deleting-organization-attributes).     * [Viewing Server Attributes](#viewing-server-attributes).     * [Setting Server Attributes](#setting-server-attributes).     * [Deleting Server Attributes](#deleting-server-attributes).     * [Getting attributes permissions](#getting-attributes-permissions).     * [Searching attributes ](#searching-attributes).   4. [The Roles Service](#the-roles-service).     * [Searching for Roles](#searching-for-roles).     * [Viewing a Role](#viewing-a-role).     * [Creating a Role](#creating-a-role).     * [Modifying a Role](#modifying-a-role).     * [Setting Role Membership](#setting-role-membership).     * [Deleting a Role](#deleting-a-role).   5. [The Settings Service](#settings-service).     * [Getting server specific settings](#getting-server-specific-settings). 7. [Repository Services](#repository-services).   1. [Resources Service](#resources-service).     * [Searching the Repository](#searching-the-repository).     * [Viewing Resource Details](#viewing-resource-details).     * [Downloading File Resources](#downloading-file-resources).     * [Creating a Resource](#creating-a-resource).     * [Modifying a Resource](#modifying-a-resource).     * [Copying a Resource](#copying-a-resource).     * [Moving a Resource](#moving-a-resource).     * [Uploading SemanticLayerDataSource](#uploading-semanticlayerdatasource).        * [Uploading MondrianConnection](#uploading-mondrianconnection).     * [Uploading SecureMondrianConnection](#uploading-securemondrianconnection).     * [Uploading ReportUnit](#uploading-reportunit).     * [Uploading File Resources](#uploading-file-resources).     * [Deleting Resources](#deleting-resources).   2. [The Permissions Service](#the-permissions-service).     * [Viewing Multiple Permissions](#viewing-multiple-permissions).     * [Viewing a Single Permission](#viewing-a-single-permission).     * [Setting Multiple Permissions](#setting-multiple-permissions).     * [Setting a Single Permission](#setting-a-single-permission).     * [Deleting Permissions in Bulk](#deleting-permissions-in-bulk).     * [Deleting a Single Permission](#deleting-a-single-permission). 8. [Jobs service](#jobs-service).   * [Listing Report Jobs](#listing-report-jobs).   * [Viewing a Job Definition](#viewing-a-job-definition).   * [Extended Job Search](#extended-job-search).   * [Scheduling a Report](#scheduling-a-report).   * [Viewing Job Status](#viewing-job-status).   * [Editing a Job Definition](#editing-a-job-definition).   * [Updating Jobs in Bulk](#updating-jobs-in-bulk).   * [Pausing Jobs](#pausing-jobs).   * [Resuming Jobs](#resuming-jobs).   * [Restarting Failed Jobs](#restarting-failed-jobs). 9. [Calendars service](#calendars-service).   * [Listing All Registered Calendar Names](#listing-all-registered-calendar-names).   * [Viewing an Exclusion Calendar](#viewing-an-exclusion-calendar).   * [Adding or Updating an Exclusion Calendar](#adding-or-updating-an-exclusion-calendar).   * [Deleting an Exclusion Calendar](#deleting-an-exclusion-calendar). 10. [Import/Export](#importexport).   1. [Export service](#export-service).     * [Checking the Export State](#checking-the-export-state).     * [Fetching the Export Output](#fetching-the-export-output).   2. [Import service](#import-service).     * [Checking the Import State](#checking-the-import-state). 11. [Domain metadata service](#domainmetadata-service). 12. [Thumbnail Search Service](#thumbnail-search-service). 13. [Diagnostic Service](#diagnostic-service). 14. [Query Executor Service](#query-executor-service). 15. [Server Information Service](#server-information-service). 16. [Bundles service](#bundles-service). 17. [Asynchronous API](#asynchronous-api). 18. [Getting serialized content from response](#getting-serialized-content-from-response). 19. [Possible issues](#possible-issues). 20. [Maven dependency to add jasperserver-rest-client to your app](#maven-dependency-to-add-jasperserver-rest-client-to-your-app). 21. [License](#license).  Introduction ------------- With this library you can easily write Java applications which can interact with one or more JasperReports servers simultaneously in a very simple way. Library provides very friendly API for user, it minimizes possibility of building wrong requests. To use library in your maven-based application you need just to specify dependency and repository which are given below or download jar file manually from ``` http://jaspersoft.artifactoryonline.com/jaspersoft/repo/com/jaspersoft/jrs-rest-java-client/{version}/jrs-rest-java-client-{version}.jar ```  Configuration ------------- To start working with the library you should firstly configure one ore more instances of `JasperserverRestClient`. To do this you should create instance of `RestClientConfiguration`. It can be done in two ways: - loading configuration from file; - creation of manual configuration in java code.  ###Loading configuration from file: ```java RestClientConfiguration configuration = RestClientConfiguration.loadConfiguration("configuration.properties"); ``` Here is example of `configuration.properties` file: ```java // required content url=http://localhost:8080/jasperserver-pro // optional content connectionTimeout=20                         readTimeout=20 jasperserverVersion=v6_0_0 authenticationType=SPRING logHttp=true logHttpEntity=true restrictedHttpMethods=false handleErrors=true contentMimeType=JSON acceptMimeType=JSON ``` File must contain at least URL which is entry point to your server's REST services and it is needed to URL  corresponds to this pattern `{protocol}://{host}:{port}/{contextPath}`. Please notice, configuration settings may be changed after loading manually in java code.  ###Creation of manual configuration To configure `JasperserverRestClient` manually, use the constructor of `RestClientConfiguration` and properties: ```java RestClientConfiguration configuration = new RestClientConfiguration("http://localhost:8080/jasperserver"); configuration.setAcceptMimeType(MimeType.JSON).setContentMimeType(MimeType.JSON).setJrsVersion(JRSVersion.v6_0_0).setLogHttp(true); ``` ####HTTPS configuration **To use HTTPS you need:**  1. Configure your server to support HTTPS  2. Download [InstallCert](http://miteff.com/files/InstallCert-bin.zip) util and follow  [InstallCert-Guide](http://www.mkyong.com/webservices/jax-ws/suncertpathbuilderexception-unable-to-find-valid-certification-path-to-requested-target/) instructions.  3. Set HTTPS as your protocol in server URL, e.g. `https://localhost:8443/jasperserver`  4. Configure trusted certificates if needed  ```java RestClientConfiguration configuration = new RestClientConfiguration("https://localhost:8443/jasperserver"); X509TrustManager x509TrustManager = ... TrustManager[] trustManagers = new TrustManager[1]; trustManagers[0] = x509TrustManager; configuration.setTrustManagers(trustManagers); ``` ####X-HTTP-Method override To avoid situation, when your proxies or web services do not support arbitrary HTTP methods or newer HTTP methods, you can use “restricted mode”. In this mode `JaperserverRestClient` sends requests through POST method and set the `X-HTTP-Method-Override` header with value of intended HTTP method. To use this mode you should set flag `RestrictedHttpMethods`: ```java configuration.setRestrictedHttpMethods(true); ``` Or in configuration file: ```java restrictedHttpMethods=false ```` If you do not use the "restricted mode", POST or GET methods and server returns  the response with 411 error code, `JaperserverRestClient` resend this request through POST method with the X-HTTP-Method-Override header automatically. ####Switching authentication type `JasperserverRestClient` supports two authentication types: SPRING and BASIC.  `SPRING` type of authentication means that your credentials are sent as a form  to `/j_security_check directly/` uri. Using these types you obtain JSESSIONID cookie of authenticated session after sending credentials. In the `BASIC` mode `JasperserverRestClient` uses basic authentication (sends encrypted credentials with each request). Client uses `SPRING` authentication by default but you can specify authentication type in RestClientConfiguration instance: ```java configuration.setAuthenticationType(AuthenticationType.SPRING); ``` Or set authentication type in configuration file:  ```java  authenticationType=SPRING  or  authenticationType=BASIC  ``` Please notice, the basic authentication is not stateless and it is valid till method logout() is called or the application is restarted and you can not use this authentication type for Report Service, because all operations must be executed in the same session (for details, read section [Report services](https://github.com/Jaspersoft/jrs-rest-java-client/blob/master/README.md#report-services)). ####Exception handling You can choose strategy of errors that are specified by status code of server response: 1. handling of errors directly. This mode is allowed by default. 2. getting operation result in any case with null entity and handling error after calling `getEntity()` method: ```java OperationResult<InputStream> result = session                 .thumbnailsService()                 .thumbnail()                 .report("/")                 .get(); // response status is 406, but exception won't be thrown result.getEntity();     // the error will be handled and an exception will be thrown ``` To apply the second strategy set `handleErrors` property of `RestCleintConfiguration` to `false`: ```java configuration.setHandleErrors(false); ``` or specify this property in configuration file: ```java handleErrors=false ``` You can customize exception handling for each endpoint. To do this you need to pass `com.jaspersoft.jasperserver.jaxrs.client.core.exceptions.handling.ErrorHandler` implementation to `JerseyRequestBuilder.buildRequest()` factory method.  JRS REST client exception handling system is based on `com.jaspersoft.jasperserver.jaxrs.client.core.exceptions.handling.ErrorHandler` interface. Its `void handleError(Response response)` method is responsible for all error handling logic. You can use existed handlers, define your own handlers or extend existed handlers.   1. Existed handlers:    * `com.jaspersoft.jasperserver.jaxrs.client.core.exceptions.handling.DefaultExceptionHandler` - this implementation is suitable for most of the JRS errors, but sometimes you can meet some not standart errors and here such implementations as `com.jaspersoft.jasperserver.jaxrs.client.apiadapters.jobs.JobValidationErrorHandler`, `com.jaspersoft.jasperserver.jaxrs.client.apiadapters.reporting.RunReportErrorHandler`, etc. take responsibility.  2. You can create your own handler by implementing `com.jaspersoft.jasperserver.jaxrs.client.core.exceptions.handling.ErrorHandler`.  3. You can extend `com.jaspersoft.jasperserver.jaxrs.client.core.exceptions.handling.DefaultExceptionHandler` or any other handler and override its methods `void handleBodyError(Response response)` and/or `void handleStatusCodeError(Response response, String overridingMessage)`.  ####Logging It is possible to log outgoing requests and incoming responses using `logHttp` property of `RestCleintConfiguration`: ```java config.setLogHttp(true); ``` Also, you are able to log entities using `logHttpEntity` option: ```java config.setLogHttpEntity(true). ``` In configuration file: ```java logHttp=true logHttpEntity=true ``` ####Switching between JSON and XML You can configure a client to make request either with JSON or XML content. ```java RestClientConfiguration configuration = new RestClientConfiguration("http://localhost:4444/jasperserver"); configuration.setContentMimeType(MimeType.XML); configuration.setAcceptMimeType(MimeType.XML); ``` Or in `configuration.properties`: ``` contentMimeType=JSON acceptMimeType=JSON or  contentMimeType=XML acceptMimeType=XML ``` ####Client instantiation: After configuration you need just to pass `configuration` instance to `JasperserverRestClient` constructor. ```java JasperserverRestClient client = new JasperserverRestClient(configuration); ```  Authentication --------------- This library automatically encrypts your password before send it if encryption is on, so to authenticate you need just specify login and password (not encrypted) in `authenticate()` method. ```java Session session = client.authenticate("jasperadmin", "jasperadmin"); //authentication with multitenancy enabled Session session = client.authenticate("jasperadmin|organization_1", "jasperadmin"); ``` If you need to set user time zone different from default system timezone (for example, for running reports) use the code below: ```java Session session = client.authenticate("jasperadmin", "jasperadmin", TimeZone.getTimeZone("America/Los_Angeles")); // or Session session = client.authenticate("jasperadmin", "jasperadmin", "America/Los_Angeles"); ``` Also you can set user locale at authentication: ```java Session session = client.authenticate("jasperadmin", "jasperadmin", new Locale("de"), TimeZone.getTimeZone("America/Los_Angeles")); // or Session session = client.authenticate("jasperadmin", "jasperadmin", "de", America/Los_Angeles"); ``` ###Anonymous session For some Jasperserver services authentication is not required (for example, settings service, bundles service or server info service), so you can use anonymous session:  ```java AnonymousSession session = client.getAnonymousSession(); ``` ####Invalidating session Not to store session on server you can invalidate it with `logout()` method. ```java session.logout(); ```   Report services =============== After you've configured the client you can easily use any of available services. For reporting service there is one feature that should be noted - when you are running a report all subsequent operations must be executed in the same session. Here's the code: ```java Session session = client.authenticate("jasperadmin", "password"); ``` We've authenticated as `jasperadmin` user an got a session for this user, all subsequent operations must be done through this session instance. ####Running a report: There are two approaches to run a report - in synchronous and asynchronous modes. To run report in synchronous mode you can use the code below: ```java OperationResult<InputStream> result = client         .authenticate("jasperadmin", "jasperadmin")         .reportingService()         .report("/reports/samples/Cascading_multi_select_report")         .prepareForRun(ReportOutputFormat.HTML, 1)         .parameter("Cascading_name_single_select", "A & U Stalker Telecommunications, Inc")         .run(); InputStream report = result.getEntity(); ``` You can set format of report as String as well(name of format is case insensitive): ```java OperationResult<InputStream> result = client         .authenticate("jasperadmin", "jasperadmin")         .reportingService()         .report("/reports/samples/Cascading_multi_select_report")         .prepareForRun("HTML", 1)         .parameter("Cascading_name_single_select", "A & U Stalker Telecommunications, Inc")         .run(); ``` Also you can use this method to run report with several values for the same parameter. In this case new values of the parameter are added to the previous ones (new values do not replace previous values of the parameter):  ```java OperationResult<InputStream> result = client         .authenticate("superuser", "superuser")         .reportingService()         .report("/reports/samples/Cascading_multi_select_report")         .prepareForRun(ReportOutputFormat.PDF, 1)         .parameter("Cascading_state_multi_select", "CA")         .parameter("Cascading_state_multi_select",  "OR", "WA")         .parameter("Cascading_name_single_select", "Adams-Steen Transportation Holdings")         .parameter("Country_multi_select", "USA")         .run(); ``` Please notice, if you pass zero as number of page, you  will get all  pages of report. In this mode you don't need to work in one session. In the above code we specified report URI, format in which we want to get a report and some report parameters. As we a result we got `InputStream` instance. In synchronous mode as a response you get a report itself while in asynchronous you get just a descriptor with report ID which you can use to download report afer it will be ready. If you need run report in another time zone specify it using `forTimeZone()` method: ```java OperationResult<InputStream> result = session                 .reportingService()                 .report("/public/Samples/Reports/12g.PromotionDetailsReport")                 .prepareForRun(ReportOutputFormat.PDF, 1)                 .forTimeZone(TimeZone.getTimeZone("America/Los_Angeles"))                 .run(); InputStream report = result.getEntity();   // or set time zones as string  OperationResult<InputStream> result = session                 .reportingService()                 .report("/public/Samples/Reports/12g.PromotionDetailsReport")                 .prepareForRun(ReportOutputFormat.PDF, 1)                 .forTimeZone("America/Los_Angeles")                 .run(); ``` In order to run a report in asynchronous mode, you need firstly build `ReportExecutionRequest` instance and specify all the parameters needed to launch a report. The response from the server is the `ReportExecutionDescriptor` instance which contains the request ID needed to track the execution until completion and others report parameters. Here's the code to run a report: ```java //instantiating request and specifying report parameters ReportExecutionRequest request = new ReportExecutionRequest(); request.setReportUnitUri("/reports/samples/StandardChartsReport"); request         .setAsync(true)                         //this means that report will be run on server asynchronously         .setOutputFormat(ReportOutputFormat.HTML);               //report can be requested in different formats e.g. html, pdf, etc.  OperationResult<ReportExecutionDescriptor> operationResult =         session                                 //pay attention to this, all requests are in the same session!!!                 .reportingService()                 .newReportExecutionRequest(request);  reportExecutionDescriptor = operationResult.getEntity(); ``` In the above code we've created `ReportExecutionRequest` instance and sent it to JR server through the `newReportExecutionRequest` method. As a response we've got `OperationResult` instance which contains HTTP response wrapper and instance of `ReportExecutionDescriptor` which we can get with `operationResult.getEntity()`. Also you can set output format as String: ```java ReportExecutionRequest request = new ReportExecutionRequest(); request.setReportUnitUri("/reports/samples/StandardChartsReport"); request         .setAsync(true)                                  .setOutputFormat("html");                ``` As in sync mode you can set report time zone: ```java ReportExecutionRequest request = new ReportExecutionRequest(); request                 .setOutputFormat(ReportOutputFormat.PDF)                 .setPages("1")                 .setTimeZone(TimeZone.getTimeZone("America/Los_Angeles"))                 .setReportUnitUri("/public/Samples/Reports/12g.PromotionDetailsReport")                 .setAsync(true);             ``` ####Requesting report execution status: After you've got `ReportExecutionDescriptor` you can request for the report execution status: ```java OperationResult<ReportExecutionStatusEntity> operationResult =         session                                 //pay attention to this, all requests are in the same session!!!                 .reportingService()                 .reportExecutionRequest(reportExecutionDescriptor.getRequestId())                 .status();  ReportExecutionStatusEntity statusEntity = operationResult.getEntity(); ``` In the above code we've just specified request ID and got its status as a `ReportExecutionStatusEntity` instance. ####Requesting report execution details: Once the report is ready, your client must determine the names of the files to download by requesting the reportExecution descriptor again. ```java OperationResult<ReportExecutionDescriptor> operationResult =         session                                 //pay attention to this, all requests are in the same session!!!                 .reportingService()                 .reportExecutionRequest(reportExecutionDescriptor.getRequestId())                 .executionDetails();  ReportExecutionDescriptor descriptor = operationResult.getEntity(); ``` ####Requesting Report Output After requesting a report execution and waiting synchronously or asynchronously for it to finish, you are ready to download the report output. Every export format of the report has an ID that is used to retrieve it. For example, the HTML export has the ID html. To download the main report output, specify this export ID in the `export` method. For example, to download the main HTML of the report execution response above, use the following code: ```java OperationResult<InputStream> operationResult =         session                                 //pay attention to this, all requests are in the same session!!!                 .reportingService()                 .reportExecutionRequest(reportExecutionDescriptor.getRequestId())                 .export("html")                 .outputResource();  InputStream file = operationResult.getEntity(); ``` As a response you'll get an `InputStream` instance. ####Download file attachments for report output: To download file attachments for HTML output, use the following code. You must download all attachments to display the HMTL content properly. ```java ExportDescriptor htmlExportDescriptor = ... //retrieving htmlExportDescriptor from reportExecutionDescriptor  for(AttachmentDescriptor attDescriptor : htmlExportDescriptor.getAttachments()){     OperationResult<InputStream> operationResult =             session                             //pay attention to this, all requests are in the same session!!!                     .reportingService()                     .reportExecutionRequest(reportExecutionDescriptor.getRequestId())                     .export(htmlExportDescriptor.getId())                     .attachment(attDescriptor.getFileName());      InputStream file = operationResult.getEntity();     //doing something with file } ``` ####Exporting a Report Asynchronously After running a report and downloading its content in a given format, you can request the same report in other formats. As with exporting report formats through the user interface, the report does not run again because the export process is independent of the report. ```java ExportExecutionOptions exportExecutionOptions = new ExportExecutionOptions()         .setOutputFormat(ReportOutputFormat.PDF)         .setPages("3");  OperationResult<ExportExecutionDescriptor> operationResult =         session                 .reportingService()                 .reportExecutionRequest(reportExecutionDescriptor.getRequestId())                 .runExport(exportExecutionOptions);  ExportExecutionDescriptor statusEntity = operationResult.getEntity(); ``` ####Polling Export Execution As with the execution of the main report, you can also poll the execution of the export process. For example, to get the status of the HTML export in the previous example, use the following code: ```java OperationResult<ReportExecutionStatusEntity> operationResult =         session                                 //pay attention to this, all requests are in the same session!!!                 .reportingService()                 .reportExecutionRequest(reportExecutionDescriptor.getRequestId())                 .export("html")                 .status();  ReportExecutionStatusEntity statusEntity = operationResult.getEntity(); ``` ####Finding Running Reports and Jobs You can search for reports that are running on the server, including report jobs triggered by the scheduler. To search for running reports, use the search arguments from `ReportAndJobSearchParameter` enumeration. ```java OperationResult<ReportExecutionListWrapper> operationResult =         session                 .reportingService()                 .runningReportsAndJobs()                 .parameter(ReportAndJobSearchParameter.REPORT_URI, "/reports/samples/AllAccounts")                 .find();  ReportExecutionListWrapper entity = operationResult1.getEntity(); ``` ####Stopping Running Reports and Jobs To stop a report that is running and cancel its output, use the code below: ```java OperationResult<ReportExecutionStatusEntity> operationResult1 =         session                 .reportingService()                 .reportExecutionRequest(executionDescriptor.getRequestId())                 .cancelExecution();  ReportExecutionStatusEntity statusEntity = operationResult1.getEntity(); ```  ###Input controls service: The reports service includes methods for reading and setting input controls of any input controls container, i.e. reportUnit, reportOptions, dashboard, adhocDataView ####Listing Report Parameters Structure The following code returns a description of the structure of the input controls for a given container. ```java  OperationResult<ReportInputControlsListWrapper> operationResult = session                 .inputControlsService()                 .inputControls()                 .container("/reports/samples/Cascading_multi_select_report")                 .get();  ReportInputControlsListWrapper result = operationResult.getEntity(); ``` The response contains the structure of the input controls for the container. It contains the information needed by your application to display the report parameters to your users and allow them to make a selection. In particular, this includes any cascading structure as a set of dependencies between container parameters. Each input control also has a type that indicates how the user should be allowed to make a choice: `bool, singleSelect, singleSelectRadio, multiSelectCheckbox, multiSelect, singleValue, singleValueText, singleValueNumber, singleValueDate, singleValueDatetime, singleValueTime`. The structure includes a set of validation rules for each report parameter. These rules indicate what type of validation your client should perform on input control values it receives from your users, and if the validation fails, the message to display. Depending on the type of the report parameter, the following validations are possible: * mandatoryValidationRule – This input is required and your client should ensure the user enters a value. * dateTimeFormatValidation – This input must have a data time format and your client should ensure the user enters a valid date and time. To skip input controls state generation use `excludeState(true)` setting: ```java OperationResult<ReportInputControlsListWrapper> operationResult = session                 .inputControlsService()                 .inputControls()                 .container("/reports/samples/Cascading_multi_select_report")                 .excludeState(true)                 .get(); ReportInputControlsListWrapper result = operationResult.getEntity(); ``` ####Reordering input controls structure You can change structure of input controls according to client demands using the next code: ```java OperationResult<ReportInputControlsListWrapper> reorderedOperationResult = session                 .inputControlsService()                 .inputControls()                 .container("/reports/samples/Cascading_multi_select_report")                 .reorder(inputParameters); ``` It is impossible to change input controls except change of theirs order. Sent to server structure MUST be the same as it received from there, except order. You cannot modify some values, add or remove control, etc. ####Listing input controls values The following code returns a description of the possible values of all report parameters for the report. Among these choices, it shows which ones are selected. ```java OperationResult<InputControlStateListWrapper> operationResult = session                 .inputControlsService()                 .inputControls()                 .container("/reports/samples/Cascading_multi_select_report")                 .values()                 .get();  InputControlStateListWrapper result = operationResult.getEntity(); ``` The response contains the structure of the report parameters for the report. If a selection-type report parameter has a null value, it is given as `NULL`. If no selection is made, its value is given as `NOTHING`. Use setting `useCashedData(false)` to avoid getting cashed data: ```java  OperationResult<InputControlStateListWrapper> operationResult = session                 .inputControlsService()                 .inputControls()                 .container("/reports/samples/Cascading_multi_select_report")                 .values()                 .useCashedData(false)                 .get(); InputControlStateListWrapper result = operationResult.getEntity(); ``` ####Setting input controls values The following code updates the state of specified input controls values, so they are set for the next run of the report. ```java OperationResult<InputControlStateListWrapper> operationResult = session                 .inputControlsService()                 .inputControls()                 .container("/reports/samples/Cascading_multi_select_report")                 .values()                 .parameter("Country_multi_select", "Mexico")                 .parameter("Cascading_state_multi_select", "Guerrero", "Sinaloa")                 .run(); InputControlStateListWrapper result = operationResult.getEntity(); ``` In response you get updated values for specified input controls. If you want to get updated values with full structure of input controls, you should use `includeFullStructure(true)` setting: ```java OperationResult<InputControlStateListWrapper> operationResult = session                 .inputControlsService()                 .inputControls()                 .container("/reports/samples/Cascading_multi_select_report")                 .values()                 .parameter("Country_multi_select", "USA")                 .parameter("Cascading_state_multi_select", "CA", "OR", "WA")                 .includeFullStructure(true)                 .run(); InputControlStateListWrapper result = operationResult.getEntity(); ``` Administration services: ======================== Only administrative users may access the REST services for administration.  ###Organizations service It provides methods that allow you to list, view, create, modify, and delete organizations (also known as tenants). Because the organization ID is used in the URL, this service can operate only on organizations whose ID is less than 100 characters long and does not contain spaces or special symbols. As with resource IDs, the organization ID is permanent and cannot be modified for the life of the organization. ####Searching for Organizations The service searches for organizations by ID, alias, or display name. If no search is specified, it returns a list of all organizations. Searches and listings start from but do not include the logged-in user’s organization or the specified base. ```java OperationResult<OrganizationsListWrapper> result = session         .organizationsService()         .allOrganizations()         .parameter(OrganizationParameter.INCLUDE_PARENTS, "true")         .get(); ``` ####Viewing an Organization The `organization()` method with an organization ID retrieves a single descriptor containing the list of properties for the organization. When you specify an organization, use its unique ID, not its path. ```java OperationResult<ClientTenant> result = session         .organizationsService()         .organization("myOrg1")         .get(); ``` Also you may specify organization as object: ```java ClientTenant organization = new ClientTenant(); organization.setId("test_Id");  OperationResult<ClientTenant> result = session         .organizationsService()         .organization(organization)         .get(); ``` ####Creating an Organization To create an organization, put all information in an organization descriptor, and include it in a request to the `rest_v2/organizations` service, with no ID specified. The organization is created in the organization specified by the `parentId` value of the descriptor. ```java OperationResult<Organization> result = session         .organizationsService()         .organization(organization)         .create(); ``` The another way to create organization is to use `createOrUpdate()` method: ```java OperationResult<Organization> result = session         .organizationsService()         .organization(organization)         .createOrUpdate(organization); ``` Be carefully using this method because you can damage existing organization if the `organizationId` of new organization is already used. The descriptor is sent in the request should contain all the properties you want to set on the new organization. Specify the `parentId` value to set the parent of the organization, not the `tenantUri` or `tenantFolderUri` properties. However, all properties have defaults or can be determined based on the alias value. The minimal descriptor necessary to create an organization is simply the alias property. In this case, the organization is created as child of the logged-in user’s home organization. ####Modifying Organization Properties To modify the properties of an organization, use the `update` method and specify the organization ID in the URL. The request must include an organization descriptor with the values you want to change. You cannot change the ID of an organization, only its name (used for display) and its alias (used for logging in). ```java Organization organization = new Organization(); organization.setAlias("lalalaOrg");  OperationResult<ClientTenant> result = session         .organizationsService()         .organization("myOrg1")         .createOrUpdate(organization); ``` ####Deleting an Organization To delete an organization, use the `delete()` method and specify the organization ID in the `organization()` method. When deleting an organization, all of its resources in the repository, all of its sub-organizations, all of its users, and all of its roles are permanently deleted. ```java OperationResult<ClientTenant> result = session         .organizationsService()         .organization("myOrg1")         .delete(); ``` ###Users service It provides methods that allow you to list, view, create, modify, and delete user accounts, including setting role membership. Because the user ID is used in the URL, this service can operate only on users whose ID is less than 100 characters long and does not contain spaces or special symbols. As with resource IDs, the user ID is permanent and cannot be modified for the life of the user account. ####Searching for Users You can search for users by name or by role. If no search is specified, service returns all users. ```java OperationResult<UsersListWrapper> operationResult =         session                 .usersService()                 .allUsers()                 .param(UsersParameter.REQUIRED_ROLE, "ROLE_USER")                 .get();  UsersListWrapper usersListWrapper = operationResult.getEntity(); ``` ####Viewing a User Method `username()` with a user ID (username) retrieves a single descriptor containing the full list of user properties and roles. ```java OperationResult<ClientUser> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .usersService()                 .user("jasperadmin")                 .get();  ClientUser user = operationResult.getEntity(); ``` Also you may specify user as object: ```java ClientUser userObject = new ClientUser()                 .setUsername("test_user")                 .setPassword("test_password")                 .setEmailAddress("john.doe@email.net")                 .setEnabled(true)                 .setExternallyDefined(false)                 .setFullName("John Doe"); OperationResult<ClientUser> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .usersService()                 .user(userObject)                 .get();  ClientUser user = operationResult.getEntity(); ``` The full user descriptor includes detailed information about the user account, including any roles. ####Creating a User To create a user account, put all required information in a user descriptor `ClientUser`, and include it in a request to the users service (`createOrUpdate()` method), with the intended user ID (username) specified in the `username()` method. To create a user, the user ID in the `username()` method must be unique on the server. If the user ID already exists, that user account will be modified. The descriptor sent in the request should contain all the properties you want to set on the new user, except for the username that is specified in the `username()` method. To set roles on the user, specify them as a list of roles. ```java //Creating a user ClientUser user = new ClientUser()         .setUsername("john.doe")         .setPassword("12345678")         .setEmailAddress("john.doe@email.net")         .setEnabled(true)         .setExternallyDefined(false)         .setFullName("John Doe");  client     .authenticate("jasperadmin", "jasperadmin")     .usersService()     .user(user.getUsername())     .createOrUpdate(user);  //Granting new user with admin role ClientRole role = client         .authenticate("jasperadmin", "jasperadmin")         .rolesService()         .rolename("ROLE_ADMINISTRATOR")         .get()         .getEntity();  Set<ClientRole> roles = new HashSet<ClientRole>(); roles.add(role); user.setRoleSet(roles);  client     .authenticate("jasperadmin", "jasperadmin")     .usersService()     .user(user.getUsername())     .createOrUpdate(user); ``` ####Modifying User Properties To modify the properties of a user account, put all desired information in a user descriptor (`ClientUser`), and include it in a request to the users service (`createOrUpdate()` method), with the existing user ID (username) specified in the `username()` method. To modify a user, the user ID must already exist on the server. If the user ID doesn’t exist, a user account will be created. To add a role to the user, specify the entire list of roles with the desired role added. To remove a role from a user, specify the entire list of roles without the desired role removed. ```java ClientUser user = new ClientUser()         .setUsername("john.doe")         .setPassword("12345678")         .setEmailAddress("john.doe@email.net")         .setEnabled(true)         .setExternallyDefined(false)         .setFullName("Bob");                    //field to be updated  client     .authenticate("jasperadmin", "jasperadmin")     .usersService()     .user("john.doe")     .createOrUpdate(user); ``` ####Deleting a User To delete a user, call the `delete()` method and specify the user ID in the `username()` method. ```java client     .authenticate("jasperadmin", "jasperadmin")     .usersService()     .user(user.getUsername())     .delete(); ```  ###Attributes service Attributes, also called profile attributes, are name-value pairs associated with a user, organization or server. Certain advanced features such as Domain security and OLAP access grants use profile attributes in addition to roles to grant certain permissions. Unlike roles, attributes are not pre-defined, and thus any attribute name can be assigned any value at any time. Attributes service provides methods for reading, writing, and deleting attributes on any given holder (server, organization or user account). All attribute operations apply to a single specific holder; there are no operations for reading or searching attributes from multiple holders. As the holder's id is used in the URL, this service can operate only on holders whose ID is less than 100 characters long and does not contain spaces or special symbols. In addition, both attribute names and attribute values being written with this service are limited to 255 characters and may not be empty (null) or not contain only whitespace characters. ####Viewing User Attributes The code below allow you to retrieve single attribute defined for the user: ```java    HypermediaAttribute userAttribute = session                    .attributesService()                    .forUser("jasperadmin")                    .attribute(attribute.getName())                    .get()                    .getEntity();  ```  You may work work with user as object: ```java     CleintUser userObject = new ClientUser();     userObject.setName("jasperadmin");     HypermediaAttribute userAttribute = session                        .attributesService()                        .forUser(userObject)                        .attribute(attribute.getName())                        .get()                        .getEntity(); ```  If user belong to organization you may specify it by name or as object: ```java  HypermediaAttribute userAttribute = session                  .attributesService()                  .forOrganization("organization_1")                  .forUser("jasperadmin")                  .attribute(attribute.getName())                  .get()                  .getEntity();    ClientTenant orgObject = new CleintTenant();   orgObject.setId("someId");    HypermediaAttribute userAttribute = session                  .attributesService()                  .forOrganization(orgObject)                  .forUser("jasperadmin")                  .attribute(attribute.getName())                  .get()                  .getEntity(); ```    The code below retrieves the list of attributes defined for the user. ```java    HypermediaAttribute userAttribute = session                    .attributesService()                    .forUser("jasperadmin")                    .attribute("attributeName", "attributeName1", "attributeName2")                    .get()                    .getEntity();    ```  If user belong to organization you may specify organization: ```java  HypermediaAttribute userAttribute = session                  .attributesService()                  .forOrganization("organization_1")                  .forUser("jasperadmin")                  .allAttributes()                  .get()                  .getEntity();  ``` You can get the list of all attributes that includes the name and value of each attribute:  ```java  List<HypermediaAttribute> attributes = session                  .attributesService()                  .forOrganization("organization_1")                  .forUser("jasperadmin")                  .allAttributes()                  .get()                  .getEntity()                  .getProfileAttributes(); ```  Each attribute may only have one value, however that value may contain a comma-separated list that is interpreted by the server as being multi-valued.  ####Setting User Attributes The `createOrUpdate()` method of the attributes service adds or replaces attributes on the specified user. The list of attributes defines the name and value of each attribute. Each attribute may only have one value, however, that value may contain a comma separated list that is interpreted by the server as being multi-valued. There are two syntaxes, the following one is for adding or replacing all attributes ```java HypermediaAttributesListWrapper serverAttributes = new HypermediaAttributesListWrapper(); serverAttributes.setProfileAttributes(asList(new HypermediaAttribute(new ClientUserAttribute().setName("test_attribute_1").setValue("test_value_1")),                                              new HypermediaAttribute(new ClientUserAttribute().setName("test_attribute_2").setValue("test_value_2"))));        session                        .attributesService()                        .forOrganization("organization_1")                        .forUser("jasperadmin")                        .attributes("test_attribute_1","test_attribute_2")                        .createOrUpdate(serverAttributes); ```  Be careful with definition of attribute names because the server uses different strategies for creating or updating attributes depending on list of attribute names, list of attributes and existing attributes on the server:  1. if requested attributes' names in `attributes()` method matches with attribute name of object defined in `createOrUpdate()` method and the attribute does not exist on the server it will be *created* on the server;  2. if requested attributes' names in `attributes()` method matches with attribute name of object defined in `createOrUpdate()` method and the attribute exists on the server it will be *updated* on the server;  3. if requested attributes' names in `attributes()` method does not  match with any attribute names of object defined in `createOrUpdate()` method and the attribute exists on the server it will be *deleted* on the server;  4. if requested attribute in `createOrUpdate()` method  method does not  match with any attribute names in `attributes()` it will be *ignored* and will not be sent to the server;  5. if requested list of attributes' names  in `attributes()` method is empty or you use `allAttributes()` method and pass attributes in `createOrUpdate()` method  -  the existing attributes on the server it will be complitely *replaced* with passed ones:  ```java   session                        .attributesService()                        .forOrganization("organization_1")                        .forUser("jasperadmin")                        .allAttributes()                        .createOrUpdate(serverAttributes);  ``` The second way of using the attributes service is adding or replacing individual attribute: ```java         HypermediaAttribute attribute = new HypermediaAttribute();                 attribute.setName("test_attribute");                 attribute.setValue("test_value");          session                         .attributesService()                         .forOrganization("organization_1")                         .forUser("jasperadmin")                         .attribute("test_attribute")                         .createOrUpdate(attribute)                         .getEntity(); ``` ####Deleting User Attributes The `delete()` method of the attributes service removes attributes from the specified user. When attributes are removed, both the name and the value of the attribute are removed, not only the value. There are two syntaxes, the following one is for deleting multiple attributes or all attributes at once. ```java  session                 .attributesService()                 .forOrganization("organization_1")                 .forUser("jasperadmin")                 .attributes("test_attribute_1","test_attribute_2")                 .delete();  // or  session                 .attributesService()                 .forOrganization("organization_1")                 .forUser("jasperadmin")                 .allAttributes()                 .delete();   ``` The second syntax deletes a single attribute for the specified user: ```java session                 .attributesService()                 .forOrganization("organization_1")                 .forUser("jasperadmin")                 .attribute("attributeName")                 .delete(); ``` ####Viewing Organization Attributes The code below retrieves the list of attributes, if any, defined for the organization. ```java List<HypermediaAttribute> attributes = session                 .attributesService()                 .forOrganization("organization_1")                 .allAttributes()                 .get()                 .getEntity()                 .getProfileAttributes(); ``` You can retrieve any specified attributes. In this case all you need is to define required attributes. See code snippet below: ```java List <HypermediaAttribute> attributes = session         .attributesService()         .forOrganization("organization_1")         .attributes("number_of_employees", "number_of_units", "country_code")         .get()         .getEntity()         .getAttributes(); ``` Or to get a single organization attribute. ```java HypermediaAttribute attributes = session         .attributesService()         .forOrganization("organization_1")         .attribute("industry")         .get()         .getEntity(); ``` ####Setting Organization Attributes Service allows you to create new organization attributes. See code below: ```java HypermediaAttributesListWrapper attributes = new HypermediaAttributesListWrapper(); attributes.setProfileAttributes(asList(         new HypermediaAttribute(new ClientUserAttribute().setName("test_attribute_1").setValue("test_value_1")),         new HypermediaAttribute(new ClientUserAttribute().setName("test_attribute_2").setValue("test_value_2"))));                  OperationResult<HypermediaAttributesListWrapper> attributes = session                 .attributesService()                 .forOrganization("organization_1")                 .attributes(asList("test_attribute_1", "test_attribute_2")                 .createOrUpdate(serverAttributes); ``` If you want to replace all existing attributes with new ones: ```java OperationResult<HypermediaAttributesListWrapper> attributes = session                 .attributesService()                 .forOrganization("organization_1")                 .allAttributes()                 .createOrUpdate(serverAttributes); ``` Be careful with definition of attribute names because the server uses different strategies for creating or updating attributes depending on list of attribute names, list of attributes and existing attributes on the server (see section [Setting User Attributes] (https://github.com/Jaspersoft/jrs-rest-java-client#setting-user-attributes)). Or to create a single organization attribute code below: ```java HypermediaAttribute attribute = new HypermediaAttribute(new ClientTenantAttribute().setName("industry").setValue("IT")); OperationResult<HypermediaAttribute> retrieved = session         .attributesService()         .forOrganization("organization_1")         .attribute("industry")         .createOrUpdate(attribute); ``` Attribute name should not exist on the server and match with `name` field of `attribute` object, otherwise the attribute will be deleted.  ####Deleting Organization Attributes You can also delete a single organization attribute. ```java OperationResult<HypermediaAttribute> operationResult = session                 .attributesService()                 .forOrganization("organization_1")                 .attribute("industry")                 .delete(); ``` Or to delete list of attributes: ```java OperationResult<HypermediaAttributesListWrapper> operationResult = session                 .attributesService()                 .forOrganization(orgName)                 .attributes("number_of_employees", "country_code")                 .delete(); ``` ####Viewing Server Attributes We have also provided service to get server attributes. Code below return available server attributes.  ```java List<HypermediaAttribute> attributes = session         .attributesService()         .allAttributes()         .get()         .getEntity()         .getProfileAttributes(); ``` Or you can specify any concrete attribute. ```java HypermediaAttribute entity = session                 .attributesService()                 .attribute("attribute_name")                 .get()                 .getEntity(); ``` ####Setting Server Attributes It is possible to create new server attributes. ```java HypermediaAttributesListWrapper serverAttributes = new HypermediaAttributesListWrapper();         serverAttributes.setProfileAttributes(asList(                 new HypermediaAttribute(new ClientUserAttribute().setName("max_threads").setValue("512")),                 new HypermediaAttribute(new ClientUserAttribute().setName("admin_cell_phone").setValue("03"))));  OperationResult<HypermediaAttributesListWrapper> attributes = session                 .attributesService()                 .attributes("max_threads", "admin_cell_phone")                 .createOrUpdate(newServerAttributes); ``` If you want to replace all existing attributes with new ones: ```java OperationResult<HypermediaAttributesListWrapper> attributes = session                 .attributesService()                 .allAttributes()                 .createOrUpdate(serverAttributes); ``` Be careful with definition of attribute names because the server uses different strategies for creating or updating attributes depending on list of attribute names, list of attributes and existing attributes on the server (see section [Setting User Attributes] (https://github.com/TanyaEf/jrs-rest-java-client#setting-user-attributes)). To create a single server attribute: ```java HypermediaAttribute attribute = new HypermediaAttribute(new ClientUserAttribute().setName("latency").setValue("5700"));          session                 .attributesService()                 .attribute("latency")                 .createOrUpdate(attribute); ``` Attribute name should not exist on the server and match with `name` field of `attribute` object, otherwise the attribute will be deleted.  ####Deleting Server Attributes You can also delete all server attribute. ```java         session                 .attributesService()                 .allAttributes()                 .delete()                 .getEntity(); ``` You can also delete a single server attribute. ```java         session                 .attributesService()                 .attribute("latency")                 .delete(); ``` Or any specified attributes. ```java session                 .attributesService()                 .attributes("max_threads", "admin_cell_phone")                 .delete(); ``` ###Getting attributes permissions Since `6.1` version of `JaspersoftReportServer` you can obtain attributes with permissions using additional parameter `setIncludePermissions()`: ```java   HypermediaAttribute entity = session                  .attributesService()                  .attribute("attribute_name")                  .setIncludePermissions(true)                  .get()                  .getEntity(); ``` Pay attention, the setting `setIncludePermission()` specify only the **server response format**, you can not set any permissions with this setting. ####Seasching Attributes To get full list of attributes with specified parameters use the next code: ```java         session             .attributesService()             .allAttributes()             .parameter(AttributesSearchParameter.HOLDER, "/")             .parameter(AttributesSearchParameter.GROUP, AttributesGroupParameter.AWS)             .parameter(AttributesSearchParameter.OFFSET, 20)             .parameter(AttributesSearchParameter.INCLUDE_INHERITED, Boolean.TRUE)             .search();     HypermediaAttributesListWrapper attributes = operationResult.getEntity(); ``` Supported parameters are: **holder** - represent the target holder, attributes should be fetched from; **group** - attribute group; **custom** - custom attributes(doesn't affect on server); **log4j** - logger specific attributes; **mondrian** - server attributes that make affect on Mondrian engine; **aws** - aws specific server attributes; **jdbc** - jdbc drivers specific attributes; **adhoc** - adhoc specific attributes; **ji** - profiling attributes; **customServerSettings** - updated server settings(changed log4j, mondrian, aws, jdbc, adhoc, ji server setting). **recursive** - flag indicates if attributes will be fetched also from lower level; **includeInherited** - flag indicates if search should include also higher level attributes, relatively to target holder; **offset** - pagination, start index for requested pate; **limit** - pagination, resources count per page.  You can also specified names of attributes:  ```java         session             .attributesService()             .attributes("attrName1", "attrName2")             .parameter(AttributesSearchParameter.HOLDER, "/")             .parameter(AttributesSearchParameter.GROUP, AttributesGroupParameter.CUSTOM)             .parameter(AttributesSearchParameter.OFFSET, 20)             .parameter(AttributesSearchParameter.INCLUDE_INHERITED, Boolean.TRUE)             .search();     HypermediaAttributesListWrapper attributes = operationResult.getEntity(); ``` Notice, for root 'HOLDER` is `/`, for organization - `organizationId`, for user in organization - 'organizationId/userName'. To specify the holder you can use the existing API: ```java         session             .attributesService()             .forOrganization("/")             .forUser("jasperadmin")             .attributes("attrName1", "attrName2")             .parameter(AttributesSearchParameter.GROUP, AttributesGroupParameter.CUSTOM)             .parameter(AttributesSearchParameter.OFFSET, 20)             .parameter(AttributesSearchParameter.INCLUDE_INHERITED, Boolean.TRUE)             .search();     HypermediaAttributesListWrapper attributes = operationResult.getEntity(); ``` ###Getting attributes permissions Since `6.1` version of `JaspersoftReportServer` you can obtain attributes with permissions using additional parameter `setIncludePermissions()`: ```java   HypermediaAttribute entity = session                  .attributesService()                  .attribute("attribute_name")                  .setIncludePermissions(true)                  .get()                  .getEntity(); ``` Pay attention, the setting `setIncludePermission()` specify only the **server response format**, you can not set any permissions with this setting.  ###The Roles Service It provides similar methods that allow you to list, view, create, modify, and delete roles. The new service provides improved search functionality, including user-based role searches. Because the role ID is used in the URL, this service can operate only on roles whose ID is less than 100 characters long and does not contain spaces or special symbols. Unlike resource IDs, the role ID is the role name and can be modified. ####Searching for Roles The `allRoles()` method searches for and lists role definitions. It has options to search for roles by name or by user (`param()` method) that belong to the role. If no search is specified, it returns all roles. ```java OperationResult<RolesListWrapper> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .rolesService()                 .allRoles()                 .param(RolesParameter.USER, "jasperadmin")                 .get();  RolesListWrapper rolesListWrapper = operationResult.getEntity(); ``` ####Viewing a Role The `rolename()` method with a role ID retrieves a single role descriptor containing the role properties. ```java OperationResult<ClientRole> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .rolesService()                 .rolename("ROLE_ADMINISTRATOR")                 .get();  ClientRole role = operationResult.getEntity(); ``` ####Creating a Role To create a role, send the request via `createOrUpdate()` method to the roles service with the intended role ID (name) specified in the URL. Roles do not have any properties to specify other than the role ID, but the request must include a descriptor that can be empty. ```java ClientRole role = new ClientRole()         .setName("ROLE_HELLO");  OperationResult<ClientRole> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .rolesService()                 .rolename(role.getName())                 .createOrUpdate(role);  Response response = operationResult.getResponse(); ``` ####Modifying a Role To change the name of a role, send a request via `createOrUpdate()` to the roles service and specify the new name in the role descriptor. The only property of a role that you can modify is the role’s name. After the update, all members of the role are members of the new role name, and all permissions associated with the old role name are updated to the new role name. ```java ClientRole roleHello = new ClientRole()         .setName("ROLE_HELLO_HELLO");  OperationResult<ClientRole> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .rolesService()                 .rolename("ROLE_HELLO")                 .createOrUpdate(roleHello);  Response response = operationResult.getResponse(); ``` ####Setting Role Membership To assign role membership to a user, set the roles property on the user account with the PUT method of the rest_ v2/users service. For details, see section [creating a user](https://github.com/Jaspersoft/jrs-rest-java-client/blob/master/README.md#creating-a-user). ####Deleting a Role To delete a role, send the DELETE method and specify the role ID (name) in the URL. When this method is successful, the role is permanently deleted. ```java OperationResult<ClientRole> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .rolesService()                 .rolename("ROLE_HELLO")                 .delete(); Response response = operationResult.getResponse(); ``` Settings Service ================  It provides method that allow you to get server specific settings, required by UI to work with the server in sync. There can be formats and patterns, modes for some modules etc.  ####Getting server specific settings To get settings, use the `getEntity()` method and specify the group of settings in the `group()` method and class of entity as shown below. The method `getEntity()` returns instance of specified class: ```java  final Map settings = session                 .settingsService()                     .settings()                         .group(group, Map.class)                             .getEntity();  ```  Please notice, you can get settings of user’s time zones  in this way as List only: ```java final List settings = session         .settingsService()         .settings()         .group("userTimeZones", List.class)         .getEntity(); ``` Supported groups of settings are:   1.	“request”. Settings related to current AJAX request configuration. Returned settings are: maxInactiveInterval, contextPath;  2.	“dataSourcePatterns”. Validation patterns for data source UI. Returned settings are: dbHost, dbPort, dbPort, dbName, sName, driverType, schemaName, informixServerName, dynamicUrlPartPattern;  3.	“userTimeZones”. Time zones of current user. Returned settings are pairs of code and description of time zone;  4.	“globalConfiguration”. AWS specific settings. Returned settings are : paginatorItemsPerPage, paginatorPagesRange, reportLevelConfigurable, paginationForSinglePageReport, calendarInputJsp, userItemsPerPage, roleItemsPerPage, tenantItemsPerPage, userNameNotSupportedSymbols, roleNameNotSupportedSymbols, userNameSeparator, defaultRole, passwordMask, viewReportsFilterList, outputFolderFilterList, outputFolderFilterPatterns, tenantNameNotSupportedSymbols, tenantIdNotSupportedSymbols, resourceIdNotSupportedSymbols, publicFolderUri, themeDefaultName, themeFolderName, themeServletPrefix, dateFormat, currentYearDateFormat, timestampFormat, timeFormat, entitiesPerPage, tempFolderUri, organizationsFolderUri, jdbcDriversFolderUri, emailRegExpPattern, enableSaveToHostFS, allFileResourceTypes, dataSourceTypes;  5.	“awsSettings”. AWS specific settings. Returned settings are: productTypeIsEc2, isEc2Instance, productTypeIsJrsAmi, awsRegions, productTypeIsMpAmi, suppressEc2CredentialsWarnings;  6.	“decimalFormatSymbols”. Response is locale dependent;  7.	“dateTimeSettings”. All settings related to client date-time formatting. Response is locale dependent;   8.	“dashboardSettings”. Settings depend on configuration of Jaspersoft server;  9.	“inputControls”. Different settings for input controls. Configuration of settings depend on configuration of Jaspersoft server;  10.	“metadata”. Configuration of settings depends on configuration of Jaspersoft server;  11.	“adhocview”.  Different configuration dictionary values and lists for ad hoc. Configuration of settings depends on configuration of Jaspersoft server.  There is another way to get settings using specified methods for groups of settings that return specific object of settings:  ```java  Final RequestSettings settings = session                .settingsService()                 .settings()                 .ofRequestGroup()                 .getEntity(); ``` Pleace notice, you should use List interface to get user’s time zones setting in this way: ```java final List<UserTimeZone> settings = session         .settingsService()         .settings()         .ofUserTimeZonesGroup()         .getEntity(); ``` Or you can get List of specified DTO for user’s time zones using GenericType class: ```java final List<UserTimeZone> settings = session         .settingsService()         .settings()         .group("userTimeZones", new GenericType<List<UserTimeZone>>() {})         .getEntity(); ``` Supported specified methods are: ```java OperationResult<RequestSettings> ofRequestGroup(); OperationResult<DataSourcePatternsSettings> ofDataSourcePatternsGroup(); OperationResult<List<UserTimeZone>> ofUserTimeZonesGroup(); OperationResult<AwsSettings> ofAwsGroup(); OperationResult<DecimalFormatSymbolsSettings> ofDecimalFormatSymbolsGroup(); OperationResult<DashboardSettings> ofDashboardGroup(); OperationResult<GlobalConfigurationSettings> ofGlobalConfigurationGroup(); OperationResult<DateTimeSettings> ofDateTimeGroup(); OperationResult<InputControlsSettings> ofInputControlsGroup(); ```  Repository Services ===================== ###Resources Service This new service provides greater performance and more consistent handling of resource descriptors for all repository resource types. The service has two formats, one takes search parameters to find resources, the other takes a repository URI to access resource descriptors and file contents. ####Searching the Repository The resources service, when `resources()` method used without specifying any repository URI, is used to search the repository. The various parameters let you refine the search and specify how you receive search results. ```java OperationResult<ClientResourceListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resources()         .search(); ClientResourceListWrapper resourceListWrapper = result.getEntity(); //OR OperationResult<ClientResourceListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resources()         .parameter(ResourceSearchParameter.FOLDER_URI, "/reports/samples")         .parameter(ResourceSearchParameter.LIMIT, "5")         .search(); ClientResourceListWrapper resourceListWrapper = result.getEntity(); ``` The response of a search is a set of shortened descriptors showing only the common attributes of each resource. One additional attribute specifies the type of the resource. This allows you to quickly receive a list of resources for display or further processing. ####Viewing Resource Details Use the `resource()` method and a resource URI with `details()` method to request the resource's complete descriptor. ```java OperationResult<ClientResource> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource("/properties/GlobalPropertiesList")         .details(); ``` ####Downloading File Resources There are two operations on file resources: * Viewing the file resource details to determine the file format * Downloading the binary file contents  To view the file resource details, specify the URL of the file in `resource()` method and use the code form [Viewing Resource Details](https://github.com/Jaspersoft/jrs-rest-java-client/blob/master/README.md#viewing-resource-details) section. To download file binary content, specify the URL of the file in `resource()` method and use the code below ```java OperationResult<InputStream> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource("/themes/default/buttons.css")         .downloadBinary();  InputStream inputStream = result.getEntity(); ``` To get file MIME type yo can get `Content-Type` header from the `Response` instance. ####Creating a Resource The `createNew()` and `createOrUpdate()` methods offer alternative ways to create resources. Both take a resource descriptor but each handles the URL differently. With the `createNew()` method, specify a folder in the URL, and the new resource ID is created automatically from the label attribute in its descriptor. With the `createOrUpdate()` method, specify a unique new resource ID as part of the URL in `resource()` method. ```java ClientFolder folder = new ClientFolder(); String parentUri = "/reports"; folder         .setUri("/reports/testFolder")         .setLabel("Test Folder")         .setDescription("Test folder description")         .setPermissionMask(0)         .setCreationDate("2014-01-24 16:27:47")         .setUpdateDate("2014-01-24 16:27:47")         .setVersion(0);  OperationResult<ClientResource> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource(folder.getUri())         .createOrUpdate(folder); //OR OperationResult<ClientResource> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource(parenUri)         .createNew(folder); ``` ####Modifying a Resource Use the `createOrUpdate()` method above to overwrite an entire resource. Specify the path of the target resource in the `resource()` method and specify resource of the same type. Use `parameter(ResourceServiceParameter.OVERWRITE, "true")` to replace a resource of a different type. The resource descriptor must completely describe the updated resource, not use individual fields. The descriptor must also use only references for nested resources, not other resources expanded inline. You can update the local resources using the hidden folder _file. The `patchResource()` method updates individual descriptor fields on the target resource. It also accept expressions that modify the descriptor in the Spring Expression Language. This expression language lets you easily modify the structure and values of descriptors. ```java PatchDescriptor patchDescriptor = new PatchDescriptor(); patchDescriptor.setVersion(0); patchDescriptor.field("label", "Patch Label");  OperationResult<ClientFolder> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource("/reports/testFolder")         .patchResource(ClientFolder.class, patchDescriptor); ``` Note that you must explicitly set the type of resource to update because of server issue. ####Copying a Resource To copy a resource, specify in `copyFrom()` method its URI and in `resource()` method URI of destination location. ```java OperationResult<ClientResource> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource("/reports")         .copyFrom("/datasources/testFolder"); ``` ####Moving a Resource To move a resource, specify in `moveFrom()` method its URI and in `resource()` method URI of destination location. ```java OperationResult<ClientResource> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource("/datasources")         .moveFrom("/reports/testFolder"); ``` ####Uploading File Resources To upload file you must specify the MIME type that corresponds with the desired file type, you can take it from `ClientFile.FileType` enumeration. ```java OperationResult<ClientFile> result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource("/reports/testFolder")         .uploadFile(imageFile, ClientFile.FileType.img, "fileName", "fileDescription"); ``` ####Uploading SemanticLayerDataSource RestClient also supports a way to create complex resources and their nested resources in a single multipart request. One of such resources is `SemanticLayerDataSource`.   ```java ClientSemanticLayerDataSource domainEntity = session         .resourcesService()             .resource(domain)                 .withBundle(defBundle, newDefaultBundle)                 .withBundle(enUSBundle, newEnUsBundle)                 .withSecurityFile(securityFile, securityFile)                 .withSchema(schemaFile, schema)             .inFolder("/my/new/folder/")                 .create()                     .entity();         ``` ####Uploading MondrianConnection REST Client allows you to create `MondrianConnection` Resource with mondrian schema XML file. You can specify the folder in which the resource will be placed. Provided API allows to add XML schema as `String` or `InputStream`.     ```java ClientMondrianConnection connection = session     .resourcesService()         .resource(mondrianConnection)             .withMondrianSchema(schema, schemaRef)         .createInFolder("my/olap/folder")             .entity(); ``` ####Uploading SecureMondrianConnection To upload `SecureMondrianConnection` Resource with a bunch of support files such as Mondrian schema XML file and AccessGrantSchemas files you can use our new API ```java ClientSecureMondrianConnection entity = session.resourcesService()     .resource(secureMondrianConnection)         .withMondrianSchema(mondrianSchema)         .withAccessGrantSchemas(Arrays.asList(accessGrantSchema))     .createInFolder("/my/new/folder/")         .entity(); ``` ####Uploading ReportUnit To upload `ReportUnit` resource to the server you can use next API, which allows you to do it in a very simple way. You can add JRXML file and a bunch of various files like images and others as well. ```java ClientReportUnit entity = session.resourcesService()     .resource(reportUnit)         .withJrxml(file, descriptor)         .withNewFile(imgFile, "myFile", imgDescriptor)                     .createInFolder("/my/new/folder/")                 .entity(); ``` ####Deleting Resources You can delete resources in two ways, one for single resources and one for multiple resources. To delete multiple resources at once, specify multiple URIs with the `ResourceSearchParameter.RESOURCE_URI` parameter. ```java //multiple OperationResult result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resources()         .parameter(ResourceSearchParameter.RESOURCE_URI, "/some/resource/uri/1")         .parameter(ResourceSearchParameter.RESOURCE_URI, "/some/resource/uri/2")         .delete(); //OR //single OperationResult result = client         .authenticate("jasperadmin", "jasperadmin")         .resourcesService()         .resource("/reports/testFolder")         .delete(); ``` ###The Permissions Service In the permissions service, the syntax is expanded so that you can specify the resource, the recipient (user name or role name) and the permission value within the URL. This makes it simpler to set permissions because you don’t need to send a resource descriptor to describe the permissions. In order to set, modify, or delete permissions, you must use credentials or login with a user that has “administer” permissions on the target resource. Because a permission can apply to either a user or a role, the permissions service uses the concept of a “recipient”. A recipient specifies whether the permission applies to a user or a role, and gives the ID of the user or role. There are two qualities of a permission: * The assigned permission is one that is set explicitly for a given resource and a given user or role. Not all permissions are assigned, in which case the permission is inherited from the parent folder. * The effective permission is the permission that is being enforced, whether it is assigned or inherited.  ####Viewing Multiple Permissions  ```java OperationResult<RepositoryPermissionListWrapper> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .permissionsService()                 .resource("/datasources")                 .get(); ``` ####Viewing a Single Permission Specify the recipient in the URL to see a specific assigned permission. ```java OperationResult<RepositoryPermission> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .permissionsService()                 .resource("/datasources")                 .permissionRecipient(PermissionRecipient.ROLE, "ROLE_USER")                 .get();  RepositoryPermission permission = operationResult.getEntity(); ``` ####Setting Multiple Permissions The `createNew()` method assigns any number of permissions to any number of resources specified in the body of the request. All permissions must be newly assigned, and the request will fail if a recipient already has an assigned (not inherited) permission. Use the `createOrUpdate()` method to update assigned permissions. The `createOrUpdate()` method modifies exiting permissions (already assigned). ```java List<RepositoryPermission> permissionList = new ArrayList<RepositoryPermission>(); permissionList.add(new RepositoryPermission("/themes", "user:/joeuser", 30));  RepositoryPermissionListWrapper permissionListWrapper = new RepositoryPermissionListWrapper(permissionList);  OperationResult operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .permissionsService()                 .createNew(permissionListWrapper);  Response response = operationResult.getResponse(); ``` ####Setting a Single Permission The `createNew()` method accepts a single permission descriptor. ```java RepositoryPermission permission = new RepositoryPermission(); permission         .setUri("/")         .setRecipient("user:/joeuser")         .setMask(PermissionMask.READ_WRITE_DELETE);  OperationResult operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .permissionsService()                 .createNew(permission);  Response response = operationResult.getResponse(); ``` ####Deleting Permissions in Bulk The `delete()` method removes all assigned permissions from the designated resource. After returning successfully, all effective permissions for the resource are inherited. ```java OperationResult operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .permissionsService()                 .resource("/themes")                 .delete();  Response response = operationResult.getResponse(); ``` ####Deleting a Single Permission Specify a recipient in the `permissionRecipient()` method and call the `delete()` method to remove only that permission. ```java OperationResult operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .permissionsService()                 .resource("/")                 .permissionRecipient(PermissionRecipient.USER, "joeuser")                 .delete();  Response response = operationResult.getResponse(); ``` Jobs service ================== The jobs service provides the interface to schedule reports and manage scheduled reports (also called jobs). In addition, this service provides an API to scheduler features that were introduced in JasperReports Server 4.7, such as bulk updates, pausing jobs, FTP output and exclusion calendars. ####Listing Report Jobs Use the following method to list all jobs managed by the scheduler. ```java OperationResult<JobSummaryListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .jobs()         .get();  JobSummaryListWrapper jobSummaryListWrapper = result.getEntity(); ``` The jobs are described in the `JobSummary` element. ####Viewing a Job Definition The following piece of code with a specific job ID specified in `job()` method retrieves the detailed information about that scheduled job. ```java OperationResult<Job> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .job(8600)         .get();  Job job = result.getEntity(); ``` This code returns a job element that gives the output, scheduling, and parameter details, if any, for the job. ####Extended Job Search The `search()` method is used for more advanced job searches. Some field of the jobsummary descriptor can be used directly as parameters, and fields of the job descriptor can also be used as search criteria. You can also control the pagination and sorting order of the reply. ```java Job criteria = new Job); criteria.setLabel("updatedLabel"); criteria.setAlert(new JobAlert());  OperationResult<JobSummaryListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .jobs()         .parameter(JobsParameter.SEARCH_LABEL, "hello")         .search(criteria);  JobSummaryListWrapper jobSummaryListWrapper = result.getEntity(); ``` The `criteria` parameter lets you specify a search on fields in the job descriptor, such as output formats. Some fields may be specified in both the search parameter and in a dedicated parameter, for example label. In that case, the search specified in the parameter takes precedence.   For example, you can search for all jobs that specify output format of PDF. The criteria to specify this field is: ```java List<String> outputFormats = new ArrayList<String>(); outputFormats.add("PDF"); OutputFormatsListWrapper wrapper = new OutputFormatsListWrapper(outputFormats); JobExtension criteria = new JobExtension(); criteria.setOutputFormats(wrapper); ``` Currently the code is a little bit littered, in futere versions it will be eliminated. ####Scheduling a Report To schedule a report, create its job descriptor similar to the one returned by the `job(id).get();` method, and use the `scheduleReport()` method of the jobs service. Specify the report being scheduled inside the job descriptor. You do not need to specify any job IDs in the descriptor, because the server will assign them. ```java job.setLabel("NewScheduledReport"); job.setDescription("blablabla"); JobSource source = job.getSource(); source.setReportUnitURI("/reports/samples/Employees");  OperationResult<Job> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .scheduleReport(job);  job = result.getEntity(); ``` The body contains the job descriptor of the newly created job. It is similar to the one that was sent but now contains the jobID for the new job. ####Viewing Job Status The following method returns the current runtime state of a job: ```java OperationResult<JobState> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .job(8600)         .state();  JobState jobState = result.getEntity(); ``` Response contains the `JobState` status descriptor. ####Editing a Job Definition To modify an existing job definition, use the `job(id).get()` method to read its job descriptor, modify the descriptor as required, and use the `update()` method of the jobs service. The `update()` method replaces the definition of the job with the given job ID. ```java String label = "updatedLabel"; Long jobId = job.getId(); job.setLabel(label);  OperationResult<Job> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .job(jobId)         .update(job);  Job job = result.getEntity(); ``` ####Updating Jobs in Bulk To update several jobs at once you should specify jobs IDs as parameters, and send a descriptor with filled fields to update. ```java Job jobDescriptor = new Job(); jobDescriptor.setDescription("Bulk update description");  OperationResult<JobIdListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .jobs()         .parameter(JobsParameter.JOB_ID, "8600")         .parameter(JobsParameter.JOB_ID, "8601")         .update(jobDescriptor); ``` The code above will update the `description` field of jobs with IDs `8600` and `8601`. ####Pausing Jobs The following method pauses currently scheduled job execution. Pausing keeps the job schedule and all other details but prevents the job from running. It does not delete the job. ```java OperationResult<JobIdListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .jobs()         .parameter(JobsParameter.JOB_ID, "8600")         .pause(); ``` ####Resuming Jobs Use the following method to resume any or all paused jobs in the scheduler. Resuming a job means that any defined trigger in the schedule that occurs after the time it is resumed will cause the report to run again. Missed schedule triggers that occur before the job is resumed are never run. ```java OperationResult<JobIdListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .jobs()         .parameter(JobsParameter.JOB_ID, "8600")         .resume(); ``` ####Restarting Failed Jobs Use the following method to rerun failed jobs in the scheduler. For each job to be restarted, the scheduler creates an immediate single-run copy of job, to replace the one that failed. Therefore, all jobs listed in the request body will run once immediately after issuing this command. The single-run copies have a misfire policy set so that they do not trigger any further failures (`MISFIRE_ INSTRUCTION_IGNORE_MISFIRE_POLICY`). If the single-run copies fail themselves, no further attempts are made automatically. ```java OperationResult<JobIdListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .jobs()         .parameter(JobsParameter.JOB_ID, "8600")         .restart(); ``` ###Calendars service The scheduler allows a job to be defined with a list of excluded days or times when you do not want the job to run. For example, if you have a report scheduled to run every business day, you want to exclude holidays that change every year. The list for excluded days and times is defined as a calendar, and there are various ways to define the calendar.  The scheduler stores any number of exclusion calendars that you can reference by name. When scheduling a report, reference the name of the calendar to exclude, and the scheduler automatically calculates the correct days to trigger the report. The scheduler also allows you to update an exclusion calendar and update all of the report jobs that used it. Therefore, you can update the calendar of excluded holidays every year and not need to modify any report jobs. ####Listing All Registered Calendar Names The following method returns the list of all calendar names that were added to the scheduler. ```java OperationResult<CalendarNameListWrapper> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .calendars();  //OR .calendars(CalendarType.HOLIDAY); //to specify the desired calendar type  CalendarNameListWrapper calendarNameListWrapper = result.getEntity(); ``` ####Viewing an Exclusion Calendar The following method takes the name of an exclusion calendar and returns the definition of the calendar: ```java OperationResult<Calendar> result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .calendar("testCalendar")         .get();  Calendar jobCalendar = result.getEntity(); ``` As a result we have common caledar descriptor `ReportJobCalendar`. ####Adding or Updating an Exclusion Calendar This method creates a named exclusion calendar that you can use when scheduling reports. If the calendar already exists, you have the option of replacing it and updating all the jobs that used it. ```java WeeklyCalendar calendar = new WeeklyCalendar(); calendar.setDescription("lalala"); calendar.setTimeZone("GMT+03:00"); calendar.setExcludeDaysFlags(new boolean[]{true, false, false, false, false, true, true});  OperationResult result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .calendar("testCalendar")         .createOrUpdate(calendar); ``` Unlike common `ReportJobCalendar` which we receive as result of GET operation here we need create the calendar instance of desired type and path it to the `createOrUpdate()` method. ####Deleting an Exclusion Calendar Use the following method to delete a calendar by name. ```java OperationResult result = client         .authenticate("jasperadmin", "jasperadmin")         .jobsService()         .calendar("testCalendar")         .delete(); ``` Import/Export ============= ###Export service The export service works asynchronously: first you request the export with the desired options, then you monitor the state of the export, and finally you request the output file. You must be authenticated as the system admin (superuser)or jasperadmin for the export services. ```java OperationResult<State> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .exportService()                 .newTask()                 .user("jasperadmin")                 .role("ROLE_USER")                 .parameter(ExportParameter.EVERYTHING)                 .create();  State state = operationResult.getEntity();  The export parameters you can specify are:  `everything `- export everything except audit data: all repository resources, permissions, report jobs, users, and roles. This option is equivalent to:--uris --repository-permissions --report-jobs --users --roles (default value is false).   `repository-permissions `- when this option is present, repository permissions are exported along with each exported folder and resource. This option should only be used in conjunction with uris (default value is false).  `role-users `- when this option is present, each role export triggers the export of all users belonging to the role. This option should only be used in conjunction with --roles (default value is false).  `include-access-events `- access events (date, time, and user name of last modification) are exported (default value is false).  `include-audit-events `- include audit data for all resources and users in the export (default value is false).   `include-monitoring-events `- include monitoring events (default value is false).  `include-attributes `- include attributes in export (default value is false).  `skip-attribute-values `- skip attributes values to be exported (default value is false).  `include-server-settings`	- include server settings in export(default value is false).  `skip-suborganizations `- if the parameter is set to true, the system will omit all the items(e.g. resources, user, roles, organizations) which belong to "sub organizations" even they are directly specified using corresponding options (default value is false).  `skip-dependent-resources `- skip dependent resources (domain, datasource etc.) to be exported (default value is false).  //TODO task Also you can specify: `uris` - list of folder or resource URIs in the repository  to export. `scheduledJobs` - list of repository report unit and folder URIs for which report unit jobs should be exported. For a folder URI, this option exports the scheduled jobs of all reports in the folder and all subfolders.	 `roles` - list of roles to export. `users` - list of users to export. `resourceTypes` - list of resource types, that will be included in export. If the parameter is null or empty then will include all resource types. `organization` - identifier of organization to export together with its sub organizations. If it is specified it also will be the root organization, starting from it system will export all resources, users, roles e.t.c.  ```java         OperationResult<State> stateOperationResult = session                 .exportService()                 .newTask()                 .uri("/temp/supermartDomain")                 .user("jasperadmin")                 .role("ROLE_USER")                 .resourceTypes(asList("jdbcDataSource", "reportUnit", "file"))                 .parameter(ExportParameter.EVERYTHING)                 .create(); ``` ``` ####Checking the Export State After receiving the export ID, you can check the state of the export operation. ```java OperationResult<State> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .exportService()                 .task(state.getId())                 .state();  State state = operationResult.getEntity(); ``` The body of the response contains the current state of the export operation. ####Fetching the Export Output When the export state is ready, you can download the zip file containing the export catalog. ```java OperationResult<InputStream> operationResult1 =         client                 .authenticate("jasperadmin", "jasperadmin")                 .exportService()                 .task(state.getId())                 .fetch();  InputStream inputStream = operationResult1.getEntity(); ``` ###Import service Use the following service to upload a catalog as a zip file and import it with the given options. Specify options as arguments from `com.jaspersoft.jasperserver.jaxrs.client.apiadapters.importexport.importservice.ImportParameter`. Arguments that are omitted are assumed to be false. You must be authenticated as the system admin (superuser) or jasperadmin for the import service. Jaspersoft does not recommend uploading files greater than 2 gigabytes. ```java URL url = ImportService.class.getClassLoader().getResource("testExportArchive.zip"); OperationResult<State> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .importService()                 .newTask()                 .parameter(ImportParameter.INCLUDE_ACCESS_EVENTS, true)                 .create(new File(url.toURI()));  State state = operationResult.getEntity(); ``` Available parameters are: `includeAccessEvents` - access events (date, time, and user name of last modification) are exported (default value is false). `includeAuditEvents` - include audit data for all resources and users in the export (default value is false). `update` - resources in the catalog replace those in the repository if their URIs and typesmatch (default value is false). `skipUserUpdate` - when used with --update, users in the catalog are not imported or updated. Use this option to import catalogs without overwriting currently defined user  (default value is false). `includeMonitoringEvents` - include monitoring events (default value is false). `includeServerSettings` - include server settings  (default value is false). `mergeOrganization` - allows merging of exported organization/resource into organization with different identifier. In the case if it is false, then system will throw an exception, if exportedOrganizationId != organizationId_we_import_Into (default value is false). `brokenDependencies` - defines strategy with broken dependencies. Available values are - fail, skip, include (default value is fail).   Also you can set: `brokenDependencies` - defines strategy with broken dependencies. Available values are:      fail - server will give an error (errorCode=import.broken.dependencies) if import archive contain broken dependent resources.     skip - import will skip from import broken resources. 	include - import will proceed with broken dependencies. In this case server will try to import broken dependent resources. a) In the case when in target environment there are already dependent resources import of target resource will be success, and resource will be skipped from import if there are no dependent resources to recover dependency chain. `parameters` - list of import parameters.  `organization` - organization identifier we import into. ```java  ``` ####Checking the Import State After receiving the import ID, you can check the state of the import operation. ```java OperationResult<State> operationResult =         client                 .authenticate("jasperadmin", "jasperadmin")                 .importService()                 .task(state.getId())                 .state();  State state = operationResult.getEntity(); ``` ####Getting and restarting import task  To get import task metadata you can use next code example: ```java         OperationResult<State> operationResult = session                 .importService()                 .newTask()                 .parameter(ImportParameter.INCLUDE_ACCESS_EVENTS, true)                 .parameter(ImportParameter.UPDATE, true)                 .create(new File("\\import.zip"));         State state = operationResult.getEntity();          ImportTask task = session                 .importService()                 .task(state.getId())                 .getTask()                 .getEntity(); ``` Also you can restart import task: ```java          ImportTask task = importService.                 task(state.getId())                 .restartTask(new ImportTask().setBrokenDependencies("false"))                 .getEntity(); ```  ####DomainMetadata Service The DomainMetadata Service gives access to the sets and items exposed by a Domain for use in Ad Hoc reports. Items are database fields exposed by the Domain, after all joins, filters, and calculated fields have been applied to the database tables selected in the Domain. Sets are groups of items, arranged by the Domain creator for use by report creators.  A limitation of the DomainMetadata Service only allows it to operate on Domains with a single data island. A data island is a group of fields that are all related by joins between the database tables in the Domain. Fields that belong to tables that are not joined in the Domain belong to separate data islands.  The following code retrieves metadata of Domain. ```java DomainMetaData domainMetaData = session.domainService()         .domainMetadata("/Foodmart_Sales")         .retrieve()         .getEntity(); ``` ####Thumbnail Search Service This service is used for requesting a thumbnail image of an existing resource. You can get a single resource. See code below. ```java InputStream entity = session.thumbnailsService()         .thumbnail()         .report("/public/Samples/Reports/08g.UnitSalesDetailReport")         .defaultAllowed(true)         .get()         .getEntity(); ``` Or to get multiple resources thumbnails. ```java List<ResourceThumbnail> entity = session.thumbnailsService()         .thumbnails()         .reports(asList("/public/Samples/Reports/08g.UnitSalesDetailReport",                          "/public/Samples/Reports/11g.SalesByMonthReport"))         .defaultAllowed(true)         .get()         .getEntity()         .getThumbnails(); ``` By default you obtain multiple resources using POST HTTP method, but you can switch to GET method using `requestMethod(RequestMethod.GET)` method: ```java List<ResourceThumbnail> entity = session.thumbnailsService()                 .thumbnails()                 .reports(asList("/public/Samples/Reports/08g.UnitSalesDetailReport",                         "/public/Samples/Reports/11g.SalesByMonthReport"))                 .defaultAllowed(true)                 .requestMethod(RequestMethod.GET)                 .get()                 .getEntity()                 .getThumbnails();                 ``` Please notice that ResourceThumbnail class (DTO) contains the content in Base64 string format (not InputStream).  ###Diagnostic Service  The service is used to create, update, stop log collectors and get logs and data snapshots. To create log collector use the code below: ```java OperationResult<CollectorSettings> operationResult1 = session                                                             .diagnosticService()                                                             .forCollector(collector1)                                                             .create();  collector1 = operationResult1.getEntity(); ``` New collector gets ID and status "RUNNING" automatically. After creation you can get metadata  of all collectors: ```java OperationResult<CollectorSettingsList> operationResult = session                 .diagnosticService()                 .allCollectors()                 .collectorsSettings();  CollectorSettingsList result = operationResult                 .getEntity(); ``` or for single log collector: ```java OperationResult<CollectorSettings> operationResult = session                 .diagnosticService()                 .forCollector(collector1.getId())                 .collectorSettings();  CollectorSettings result = operationResult                 .getEntity(); ``` You can change collector's metadata or stop it using `.updateCollectorSettings()` method: ```java PatchDescriptor patchDescriptor = new PatchDescriptor(); List<PatchItem> items = new ArrayList<PatchItem>(); items.add(new PatchItem().setField("status").setValue("STOPPED")); patchDescriptor.setItems(items);  OperationResult<CollectorSettings> operationResult = session                 .diagnosticService()                 .forCollector(collector1)                 .updateCollectorSettings(patchDescriptor);  CollectorSettings result = operationResult                 .getEntity(); ``` Using similar method for batch operation you can update the same field of all collectors: ```java OperationResult<CollectorSettingsList> operationResult = session                 .diagnosticService()                 .allCollectors()                 .updateCollectorsSettings(patchDescriptor); CollectorSettingsList result = operationResult                 .getEntity(); ``` Also you can update whole collector: ```java collector1.setStatus("STOPPED");  OperationResult<CollectorSettings> operationResult = session                 .diagnosticService()                 .forCollector(collector1)                 .updateCollectorSettings(collector1);  CollectorSettings result = operationResult                 .getEntity();  ``` To get collectors' content use code below: ```java OperationResult<InputStream> operationResult = session                 .diagnosticService()                 .allCollectors()                 .collectorsContent();                           // or for single collector            OperationResult<InputStream> operationResult = session                           .diagnosticService()                           .forCollector(collector1)                           .collectorContent();            InputStream result = operationResult                           .getEntity(); ```` Please notice, you should stop them previously using `.updateCollectorSettings()` methods.  Stopping the collector will turn off logging and begin resource export (if "includeDataSnapshots" is `true` and resourceUri not empty). Once stopped, collectors can't be run again. When App Server (e.g. Tomcat) is restarted, all collectors must change to stopped state. Before getting collectors' content check them status with method `.collectorsSettings()`.  Delete log collectors you can as single or as batch operation: ```java OperationResult<CollectorSettings> operationResult = session                 .diagnosticService()                 .forCollector(collector1)                 .delete();                  OperationResult<CollectorSettingsList> operationResult = session                                 .diagnosticService()                                 .allCollectors()                                 .delete(); ```  ###Query Executor Service In addition to running reports, JasperReports Server exposes queries that you can run through the QueryExecutor service. For now the only resource that supports queries is a Domain.  The following code executes query and retrieves a result of execution as QueryResult entity. ```java QueryResult queryResult = session.queryExecutorService()         .query(queryFromXmlFile, "/organizations/organization_1/Domains/Simple_Domain")         .execute()         .getEntity(); ```  ###Server Information Service Use the following service to verify the server information, the same as the `About JasperReports Server` link in the user interface. ```java OperationResult<ServerInfo> result = client         .authenticate("jasperadmin", "jasperadmin")         .serverInfoService()         .details();  ServerInfo serverInfo = result.getEntity(); ``` The server returns a `ServerInfo` instance containing the requested information. You can access each value separately with the following code: ```java OperationResult<String> result = client         .authenticate("jasperadmin", "jasperadmin")         .serverInfoService()         .edition();         //.version();         //.licenseType();         //.features();         //.expiration();         //.editionName();         //.dateTimeFormatPattern();         //.dateFormatPattern();         //.build();  String edition = result.getEntity(); ``` ###Bundles service Use bundles service to get bundles of internalization properties for particular or default user’s locale as JSON.  By default service use default system locale where the application was stared. If user specified locale at authentication, the service will use it as default locale. To get all bundles for particular locale(foe example, "de") different from default locale and locale specified at authentication use the code below: ```java final Map<String, Map<String, String>> bundles = session         .bundlesService()         .forLocale("de")         .allBundles()         .getEntity(); ``` or specify locale as instance of `java.util.Locale.class` or as constant of this class: ```java final Map<String, Map<String, String>> bundles = session         .bundlesService()         .forLocale(Locale.US)         .allBundles()         .getEntity();          final Map<String, Map<String, String>> bundles = session                 .bundlesService()                 .forLocale(new Locale("en_US"))                 .allBundles()                 .getEntity(); ``` Please notice, locale specified in `.forLocale()` method has the highest priority for the service. If you do not call`.forLocale()` method, you will get bundles for your default locale (locale specified at authentication or locale of system where the client was stared if the others were not specified): ```java final Map<String, Map<String, String>> bundles = session         .bundlesService()         .allBundles()         .getEntity(); ``` To get bundle by name you should specified locale in `.forLocale()` method and name of the bundle in `.bundles()` method: ```java final Map<String, String> bundle = session         .bundlesService()         .forLocale("en_US")         .bundle("jasperserver_messages")         .getEntity(); ```  ###Asynchronous API Each operation which requests server has its asynchronous brother which has same name with `async` prefix, e. g. `get() -> asyncGet()`. Each of these operations take a `com.jaspersoft.jasperserver.jaxrs.client.core.Callback` implementation with `execute()` method implemented. `execute()` takes an `OperationResult` instance as a parameter. The `execute` method is called when the response from server came. Each of these `async` operations returns `com.jaspersoft.jasperserver.jaxrs.client.core.RequestExecution` instance which gives you ability to cancel execution. Example: ```java RequestExecution requestExecution = session         .organizationsService()         .organizations()         .parameter(OrganizationParameter.CREATE_DEFAULT_USERS, "false")         .asyncCreate(new Organization().setAlias("asyncTestOrg"), new Callback<OperationResult<Organization>, Void>() {             @Override             public Void execute(OperationResult<Organization> data) {                 System.out.println(data.getEntity());             }         });          requestExecution.cancel(); ```  ###Getting serialized content from response If you need to get a plain response body, either JSON, XML, HTML or plain text, you gen get it it with code below: ```java OperationResult<UsersListWrapper> result = ... result.getSerializedContent(); ```  ###Possible issues 1. <strong>Deploying jrs-rest-client within web app to any Appplication Server, e.g. JBoss, Glassfish, WebSphere etc.</strong> jrs-rest-client uses the implementation of JAX-RS API of version 2.0 and if your application server does not support this version you will get an error. To solve this problem you need to add to your application a deployment configuration specific for your AS where you need to exclude modules with old JAX-RS API version. Example of such descriptor for JBoss AS you can find below:  ```xml <jboss-deployment-structure>     <deployment>         <exclusions>              <!-- Exclude JAVA EE of JBOSS (javax.ws..) => Add dependency javax.annotation -->             <module name="javaee.api" />             <!-- Exclude RestEasy conflict (javax.ws.rs.ext.RunDelegate) -->             <module name="javax.ws.rs.api"/>             <module name="org.codehaus.jackson.jackson-core-asl" />             <module name="org.jboss.resteasy.resteasy-atom-provider" />             <module name="org.jboss.resteasy.resteasy-cdi" />             <module name="org.jboss.resteasy.resteasy-crypto" />             <module name="org.jboss.resteasy.resteasy-jackson-provider" />             <module name="org.jboss.resteasy.resteasy-jaxb-provider" />             <module name="org.jboss.resteasy.resteasy-jaxrs" />             <module name="org.jboss.resteasy.resteasy-jettison-provider" />             <module name="org.jboss.resteasy.resteasy-jsapi" />             <module name="org.jboss.resteasy.resteasy-json-p-provider" />             <module name="org.jboss.resteasy.resteasy-multipart-provider" />             <module name="org.jboss.resteasy.resteasy-validator-provider-11" />             <module name="org.jboss.resteasy.resteasy-yaml-provider" />         </exclusions>     </deployment> </jboss-deployment-structure> ```  ###Maven dependency to add jasperserver-rest-client to your app: ```xml     <dependencies>         <dependency>             <groupId>com.jaspersoft</groupId>             <artifactId>jrs-rest-java-client</artifactId>             <version>6.3.1</version>         </dependency>     </dependencies>      <repositories>                  <repository>             <id>jaspersoft-clients-snapshots</id>             <name>Jaspersoft clients snapshots</name>             <url>https://jaspersoft.jfrog.io/jaspersoft/jaspersoft-clients-releases</url>         </repository>      </repositories> ```  License -------- Copyright &copy; 2005 - 2014 Jaspersoft Corporation. All rights reserved. http://www.jaspersoft.com.  Unless you have purchased a commercial license agreement from Jaspersoft, the following license terms apply:  This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.  You should have received a copy of the GNU Lesser General Public License along with this program. If not, see <http://www.gnu.org/licenses/>.
darko1002001/android-rest-client	[Follow to public page for more info](http://darko1002001.github.com/android-rest-client/)  android-rest-client ===================  A simple rest API client library  Check for the latest version on [Maven Central](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22com.aranea-apps.android.libs%22%20AND%20a%3A%22android-rest%22)    used libraries ================  Main Rest client uses OKHTTP from square https://github.com/square/okhttp  to provide HTTP layer compatibility across android OS versions and manufacturers  Additions library uses:      <dependency> 			<groupId>com.fasterxml.jackson.core</groupId> 			<artifactId>jackson-core</artifactId> 			<version>2.3.0</version> 		</dependency> 		<dependency> 			<groupId>com.fasterxml.jackson.core</groupId> 			<artifactId>jackson-annotations</artifactId> 			<version>2.3.0</version> 		</dependency> 		<dependency> 			<groupId>com.fasterxml.jackson.core</groupId> 			<artifactId>jackson-databind</artifactId> 			<version>2.3.0</version> 		</dependency> 		<dependency> 			<groupId>commons-io</groupId> 			<artifactId>commons-io</artifactId> 			<version>2.0.1</version> 		</dependency>    Overview ================  Can be used to run synchronous or asynchronous requests towards your API. There is a service class that handles request execution by 2 kinds of thread pools (choice can specified in the request implementation) either a single thread executor or a fixed size executor. Each of the executors is backed by a priority queue which means each individual request can have an execution priority set.  You can also provide your own service class which will handle the requests, extend from the current ones etc...  By default authorization is handled by setting an OAuth token in the header, but can be replaced with a custom implementation.  Note: The process of getting an oauth token from the server is not a part of this library implementation and have to be set according to the specification for your webservice.   Usage: ======  Look at the demo project included in this repo for details.  There is another sample project with an older version of android-rest-client found here: https://github.com/darko1002001/sync-notes-android   ## Steps to setup the library  1. Import the lib project into your workspace and add it as a library project 2. Look into the demo project manifest and copy the definitions 3. Check the App.java class inside the Demo project on how to configure the library.   ## Adding just the JAR files  The library can be added inside /libs as a JAR. The dependencies include a version of Jackson for parsing JSON requests. you can use the regular jar and add your own parsers if the responses aren't JSON or you want to use a different library for parsing them.  ## Manifest Declarations        <uses-permission android:name="android.permission.INTERNET" />            <uses-sdk android:minSdkVersion="14" />            <application>       <service android:name="com.araneaapps.android.libs.asyncrunners.services.ExecutorService"></service>       </application>  ## Configuration  The library needs to be configured in a class extending Application (look into the demo project for specifics).       RestClientConfiguration builder = new RestClientConfiguration.ConfigurationBuilder()         .setAuthenticationProvider(new AuthenticationProvider() {           @Override           public void authenticateRequest(RestClientRequest client) {             // YOu can add parameters or headers which will be attached to each request           }         })         .create();      RestClientConfiguration.init(this, builder);  ## Note - If including this library with gradle will automatically merge your manifest with the one with the library. The lib is published as AAR.   ## Calling requests (From the demo project)      ```      //----------------------------------          TwitterService.getUsersTwitterRequest("android").setCallback(new HttpCallbackImplementation())         .executeAsync();          //----------------------------------       // Callback returning your defined type      private final class HttpCallbackImplementation implements HttpCallback<UserModel> {          @Override         public void onSuccess(UserModel responseData, ResponseStatus responseCode) {             textViewResponse.setText(responseData.toString());         }          @Override         public void onHttpError(ResponseStatus responseCode) {             Toast.makeText(getApplicationContext(),                     responseCode.getStatusCode() + " " + responseCode.getStatusMessage(),                     Toast.LENGTH_LONG).show();          }     }     ```  Two callback methods are created each executing by default in the Android UI Thread so you don't have to implement the logic for switching back to UI from another thread.   ## SDK that uses This library:        Chute https://github.com/chute/Chute-SDK-V2-Android    ## Use of Authentication Provider  Look at the demo project to see how the Authentication Provider can be set. Use this if you need to define a custom logic (Parameters and/or headers that will be appended on each request). You can also specify a provider for individual requests.   # Changelog:  ## Release 2.2.0  Updated version of OKHTTP to 2.+ Changed the requests to RestClientRequest.class which handles all the request types. Added the option to run requests without a parser or a callback The callback can be set externally when creating the request  ## Release 1.7.0    Added RestClientConfiguration so all the global configuration is done through it.  ## Release 1.6.0  ### Breaking changes    Added response status to success callback implementation  ## Release 1.4.0    Created a new project additions which now has some helper libraries and classes such as Jackson JSON parser, IOUtils from apache etc...   Moved the async part (Service with executors) of the library to a new more general project on github not specific to just HTTP.   HttpRequestStore.init(context) is now replaced with AsyncRunners.init(context) which will provide a wrapping layer around the code that should be initialized in the App class   HttpRequest now extends Runnable interface   Added the option to override a handleStatus method in BaseHttpRequestImpl to be able to do custom result handing based on the status (see demo for sample)  ## Release 1.3.5    Fix issue with add param with token provider   Added socket timeout and connection timeout setters by calling `getClient().set...` inside any of the request classes.   ## Release 1.3.0     Added Entity body requests which are now the base class for the String body request.    Opens options to add FileEntity body which enables you to upload streams from files.    File Requests also include a progress listener which you can set if you want to track the progress.    Added File Parser which save the response stream to a specified file location.  ## Release 1.1.0    Reworked the HttpRequestParser to return an InputStream instead of a String.    Use StringResponseParser (extend or use directly) to get the input stream into a String
sakurajiang/RestAPP	![Github](https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/ic_launcher2.0.png)    **gank.io Material Design, RxJava & Retrofit & MVVM**  这是2.0版本，如果想看1.0版本，请在分支处选择1.0，从1.0版本到2.0版本是一个大更新，所有界面都更改了，因为 我比较喜欢网易云音乐的界面，所以2.0版本是仿网易云音乐界面，并且增加了很多功能，比如，**摇一摇**,**搜索**等， 整个项目看起来焕然一新！   然后，我又重新**封装**了一下项目，并且将原来的摇一摇**加载图片的bug**给消除了，同时增加了**保存和分享**的功能，这里是原来的[2.0版本](https://github.com/sakurajiang/OriginalRestAPP)，我之所以另开一个仓库来存储原来的版本，而没有直接将新代码提交到这个的仓库中，是因为这样可以直接在网页上看出两次封装的不同，可以更好的理解我为什么这么封装。特别是对于Adapter的重新封装。     相关文章:https://sakurajiang.github.io/     进去之后点击右边的Tags，然后选择#RestApp，就能找到关于这个项目的文章了。    直接下载:https://fir.im/sakurajiangAPP     界面预览：  <img src="https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/Screenshot_2016-09-19-16-46-14-236_com.jdk.gank.restapp.png"  width="300" height="500" />    <img src="https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/Screenshot_2016-09-19-16-46-20-464_com.jdk.gank.restapp.png"  width="300" height="500" />  <img src="https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/Screenshot_2016-09-19-16-46-28-725_com.jdk.gank.restapp.png"  width="300" height="500" />   <img src="https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/Screenshot_2016-09-19-16-46-38-850_com.jdk.gank.restapp.png"  width="300" height="500" />  <img src="https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/Screenshot_2016-09-19-16-52-33-905_com.jdk.gank.restapp.png"  width="300" height="500" />    <img src="https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/Screenshot_2016-09-19-16-52-40-976_com.jdk.gank.restapp.png"  width="300" height="500" />  <img src="https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/Screenshot_2016-09-19-16-54-27-207_com.jdk.gank.restapp.png"  width="300" height="500" />     <img src="https://raw.githubusercontent.com/sakurajiang/Picture/master/RestAPP/Screenshot_2016-09-19-16-54-29-662_com.jdk.gank.restapp.png"  width="300" height="500" />  ### about me I am a junior student ( now a senior student) , I learn Android almost a year , Compared with a year ago, I made great progress in programming , and feel a sense of achievement, if you have any questions or love programming , please send an email to wssakurajiang@gmail.com or 546759734@qq.com    In addition, my blog: https://sakurajiang.github.io/  If you like this open source projects, please star: https://github.com/sakurajiang/RestAPP
openshift/openshift-java-client	OpenShift Java Client [![Build Status](https://travis-ci.org/openshift/openshift-java-client.svg)](https://travis-ci.org/openshift/openshift-java-client)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.openshift/openshift-java-client/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.openshift/openshift-java-client) ===========================  Java client for the OpenShift REST API.  It pretty much offers all features that are currently available in the rhc-* command line tools  (create/rename a domain, create/destroy applications, list applications, list available cartridges, add cartridges, etc.).   Only for Version 2 ----------------  This client is used by JBoss Tools for OpenShift 2.x.  For later OpenShift versions see https://github.com/openshift/openshift-restclient-java/.  Usage ----- An exemplary usage of the client may look like the following:  		IOpenShiftConnection connection =  				new OpenShiftConnectionFactory().getConnection("myApplicationId", "user", "password"); 		IUser user = connection.getUser(); 		IDomain domain = user.createDomain("myDomain"); 		IApplication as7Application = domain.createApplication("myApplication", LatestVersionOf.jbossAs().get(user)); 		IEmbeddedCartridge mySqlCartridge = as7Application.addEmbeddableCartridge(LatestVersionOf.mySQL().get(user)); 		String unstructuredCredentials = mySqlCartridge.getCreationLog(); 		String mySqlConnectionUrl = mySqlCartridge.getUrl();  There are also 2 blog posts on jboss.org which discuss the API in more details:  * [show-domain-info: openshift-java-client in a nutshell](http://planet.jboss.org/post/show_domain_info_openshift_java_client_in_a_nutshell) * [enable-openshift-ci: full example using openshift-java-client](https://community.jboss.org/wiki/Enable-openshift-ciFullExampleUsingOpenshift-java-client)  Most insightful are the integration tests within the library which pretty much use the API in all details:  * ApplicationResourceIntegrationTest * DomainResourceIntegrationTest * EmbeddedCartridgeResourceIntegrationTest * etc.   Download -------- You may either build from source using maven (mvn clean package) or get the prebuilt artifact from the maven central.
darren-fu/RestyPass	# RestyPass >High performance Restful services call client library, support service discovery, load balance, circuit breaker, service fallback, retry.  automatically proxy client HTTP requests based on interfaces and annotations and compatible with Spring MVC annotations。  # import jar   ```xml   <dependency>     <groupId>com.github.darren-fu</groupId>     <artifactId>resty-pass</artifactId>     <version>1.4.3</version> </dependency> ```    ## Purpose   Project can be used with spring cloud / spring boot, solve the interface call between services in the micro service architecture. Welcome to contribute ideas and code.   ## compare with SpringCloud：Feign + Hystrix + Ribbon + ApacheHttpClient - Http connection pool performance upgrade, RestyPass based on Netty implementation of the AsyncHttpClient connection pool, performance testing than ApacheHttpClient 30% higher. - Reduce object generation, Feign+Hystrix+Ribbon+ApacheHttpClient, multiple library combinations to complete a complete http client request, a request chain to create a lot of redundant objects。 - Reduce thread switching, Such as Hystrix, ApacheHttpClient have their own thread pool, a request is often completed through a number of thread switching, loss of performance. - Easier configuration, RestyPass uses annotations to configure individual interface requests. - Real-time update configuration, RestyPass support real-time update part of the configuration, such as  disable / enable fallback services, disable / enable circuit breaker, and granularity can be accurate to the interface level。 - Easy to develop, free to implement most of the core interface of the custom implementation, and direct injection can be enabled（base on Spring context）。  - Support filter, and feel free to define a new one.  - Support traffic limit configuration. - Support service discovery automatically. - Support versionInfo route. ## Demo（demo[client]+demo-serverside[server]）   ### client side  #### enable resty service  RestyPass will use DiscoveryClient in spring cloud automatically while you enable server discovery,otherwise it will use resty-server.yaml to get server list. Implement ServerContext interface, you can define your own way to find server, just inject it as a normal bean will be fine.   ```java // use @EnableRestyPass to enable RestyPass @SpringBootApplication @EnableRestyPass(basePackages = {"com.github.df"}) @RestController  public class TestClientApplication {     public static void main(String[] args) {         SpringApplication.run(TestClientApplication.class, args);     }      @Autowired     private ProxyService proxyService;      @RequestMapping(value = "nothing")     public String callNothing() {         proxyService.getNothing();         return "OK";     } } ```   #### define client service ```java   //RestyService define service @RestyService(serviceName = "server",         fallbackEnabled = true,         fallbackClass = ProxyServiceImpl.class,         forceBreakEnabled = false,         circuitBreakEnabled = false,         loadBalancer = RandomLoadBalancer.class,         retry = 1,         requestTimeout = 10000,         limit = 1000, //traffic limit         versionInfo = {"<=1.3.1-RELEASE","<2.0-RC"} // versionInfo route ) @RequestMapping(value = "/resty") public interface ProxyService extends ApplicationService {          // RestyMethod define interface     // 同步调用     @RestyMethod(retry = 2,             fallbackEnabled = "false",             circuitBreakEnabled = "false",             forceBreakEnabled = "false",             limit = 10)     @RequestMapping(value = "/get_nothing", method = RequestMethod.GET, headers = "Client=RestyProxy", params = "Param1=val1")     void getNothing();          //use spring mvc annotations     @RestyMethod()     @RequestMapping(value = "/get_age/{name}", method = RequestMethod.GET)     Response<String> getAge(@RequestParam("id") Long id, @PathVariable(value = "name") String name, @RequestHeader(value="TOKEN") String token);      // Async call： (outcome)parameter type is Future<?>     @RestyMethod     @RequestMapping(value = "/get_status", method = RequestMethod.GET)     String getStatus(RestyFuture<String> future);      // Async call ： return type is Future<?>      @RestyMethod     @RequestMapping(value = "/get_user", method = RequestMethod.GET)     Future<User> getUser(); }  ``` #### define server instances we use yaml to define instances in demo, usually, we use CloudDiscoveryServerContext to discover instances automatically.   ```yaml # resty-server.yaml # define server list servers:   - serviceName: server     instances:       - host: localhost         port: 9201       - host: localhost         port: 9202 ``` ### server side  ```java   @RestController @RequestMapping("/resty") public class TestController {     @RequestMapping(value = "/get_nothing", method = RequestMethod.GET)     public void nothing() {         System.out.println("############nothing");     }   } ``` ### versionInfo route rules 1. concept versionInfo will be parse into versionInfo number and versionInfo stage.   eg. 1.2.2-RELEASE, versionInfo number is 1.22, versionInfo stage is RELEASE.   3.4SNAPSHOT, versionInfo number is 3.4, versionInfo stage is SNAPSHOT.   2. configuration   versionInfo number support operations: >, >=, <, <=, =,!,!= (! same with !=)   versionInfo stage support operations:!, !=, =(! same with !=)  eg. versionInfo = {"<=1.3.1-RELEASE","<2.0-RC"}   the config is math when the versionInfo of server instance is 1.2.5-RELEASE or 1.8.1-RC, but 1.3.2-RELEASE is not match.     ## core conception  - RestyCommand: contain all information in one resty request. - RestyCommandExecutor: execute resty command and return the result. - ServerContext: server instances container, find and refresh the server instances. - LoadBalancer: load balancer, you can define one technical LB for every resty service. - CommandFilter: filter the command as your want. - FallbackExecutor: execute the fallback Impl.  ## config and inject You can use your own Impl, just inject it will be fine.    ```java @Configuration public class RestyPassConfig {      @Bean     public FallbackExecutor fallbackExecutor() {         return new RestyFallbackExecutor();     }      @Bean     public ServerContext serverContext() {         return new ConfigurableServerContext();     }      @SuppressWarnings("SpringJavaAutowiringInspection")     @Bean     public CommandExecutor commandExecutor(RestyCommandContext commandContext) {         return new RestyCommandExecutor(commandContext);     }      @Bean     public CommandFilter CustomCommandFilter() {         return new CustomCommandFilter();     }       private static class CustomCommandFilter implements CommandFilter {         @Override         public int order() {             return 0;         }          @Override         public boolean shouldFilter(RestyCommand restyCommand) {             return true;         }          @Override         public CommandFilterType getFilterType() {             return CommandFilterType.BEFOR_EXECUTE;         }          @Override         public void before(RestyCommand restyCommand) throws FilterException {              System.out.println("custom command filter");         }          @Override         public void after(RestyCommand restyCommand, Object result) {          }          @Override         public void error(RestyCommand restyCommand, RestyException ex) {          }     }  }   ```       # License  RestyPass is Open Source software released under the Apache 2.0 license.
ameizi/elasticsearch-jest-example	jest ====  ElasticSearch Java Rest Client Examples  ### 高亮查询(highlight)  ``` POST http://127.0.0.1:9200/news/_search?q=李克强 {     "query" : {         match_all:{}     },     "highlight" : {         "pre_tags" : ["<font color='red'>", "<b>", "<em>"],         "post_tags" : ["</font>", "<b>", "</em>"],         "fields" : [             {"title" : {}},             {"content" : {                 "fragment_size" : 350,                 "number_of_fragments" : 3,                 "no_match_size": 150             }}         ]     } } ```  ``` POST http://127.0.0.1:9200/news/_search?q=李克强 {     "query" : {         match_all:{}     },     "highlight" : {         "pre_tags" : ["<font color='red'><b><em>"],         "post_tags" : ["</font><b></em>"],         "fields" : [             {"title" : {}},             {"content" : {                 "fragment_size" : 350,                 "number_of_fragments" : 3,                 "no_match_size": 150             }}         ]     } } ```  ### 删除索引  https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-delete-index.html  ``` DELETE http://127.0.0.1:9200/news ```  ### 创建索引  https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html  ``` PUT http://127.0.0.1:9200/news ```  ### 创建或修改mapping  https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html  ``` PUT /{index}/_mapping/{type} ```  ``` PUT http://127.0.0.1:9200/news/_mapping/article {   "article": {     "properties": {       "pubdate": {         "type": "date",         "format": "dateOptionalTime"       },       "author": {         "type": "string"       },       "content": {         "type": "string"       },       "id": {         "type": "long"       },       "source": {         "type": "string"       },       "title": {         "type": "string"       },       "url": {         "type": "string"       }     }   } } ```  ### 查看mapping  https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html   ``` GET http://127.0.0.1:9200/_all/_mapping  GET http://127.0.0.1:9200/_mapping ```  ``` GET http://127.0.0.1:9200/news/_mapping/article ```  输出:  ``` {   "news": {     "mappings": {       "article": {         "properties": {           "author": {             "type": "string"           },           "content": {             "type": "string"           },           "id": {             "type": "long"           },           "pubdate": {             "type": "date",             "store": true,             "format": "yyyy-MM-dd HH:mm:ss"           },           "source": {             "type": "string"           },           "title": {             "type": "string"           },           "url": {             "type": "string"           }         }       }     }   } } ```  ### 删除mapping  https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-delete-mapping.html  ``` [DELETE] /{index}/{type}  [DELETE] /{index}/{type}/_mapping  [DELETE] /{index}/_mapping/{type} ```  ``` DELETE http://127.0.0.1:9200/news/_mapping/article ```  ### ansj分词器测试  http://127.0.0.1:9200/news/_analyze?analyzer=ansj_index&text=习近平  http://127.0.0.1:9200/news/_analyze?analyzer=ansj_index&text=我是中国人  http://127.0.0.1:9200/news/_analyze?analyzer=ansj_index&text=汪东兴同志遗体在京火化汪东兴同志病重期间和逝世后，习近平李克强张德江俞正声刘云山王岐山张高丽江泽民胡锦涛等同志，前往医院看望或通过各种形式对汪东兴同志逝世表示沉痛哀悼并向其亲属表示深切慰问新华社北京8月27日电中国共产党的优秀党员  ### ansj分词器查询  * 普通查询  http://127.0.0.1:9200/news/_search?q=习近平&analyzer=ansj_index&size=50  * 指定term查询  http://127.0.0.1:9200/news/_search?q=content:江泽民&analyzer=ansj_index&size=50  http://127.0.0.1:9200/news/_search?q=title:江泽民&analyzer=ansj_index&size=50  http://127.0.0.1:9200/news/_search?q=source:新华网&analyzer=ansj_index&size=50  * 其中`ansj_index`为在`elasticsearch.yml`文件中配置的`ansj`分词器  [elasticsearch rest api 快速上手](https://github.com/sxyx2008/elasticsearch/issues/5)  ### elasticsearch-jdbc  ```bash @echo off  set DIR=%~dp0 set LIB="%DIR%\..\lib\*" set BIN="%DIR%\..\bin\*"  REM ??? echo {^     "type" : "jdbc",^     "jdbc" : {^         "url" : "jdbc:mysql://localhost:3306/news",^         "user" : "root",^         "password" : "root",^         "schedule" : "0 0/15 * ? * *",^         "sql" :  [^              {"statement":"SELECT title,content,url,source,author,pubdate FROM news"},^              {^                 "statement":"SELECT title,content,url,source,author,pubdate FROM news where pubdate > ?",^                 "parameter" : [ "$metrics.lastexecutionstart" ]^              }^ 	],^ 	"autocommit" : true,^         "treat_binary_as_string" : true,^         "elasticsearch" : {^              "cluster" : "elasticsearch",^              "host" : "localhost",^              "port" : 9300^         },^         "index" : "news",^         "type" : "article"^       }^ }^ | "%JAVA_HOME%\bin\java" -cp "%LIB%" -Dlog4j.configurationFile="file://%DIR%\log4j2.xml" "org.xbib.tools.Runner" "org.xbib.tools.JDBCImporter" ```  [elasticsearch-jdbc 插件的使用](https://github.com/sxyx2008/elasticsearch/issues/3)
hubrick/vertx-rest-client	# Vert.x Rest client  A fully functional Vert.x REST client with RxJava support and request caching.  The design is inspired by Spring's RestTemplate.  The handling of MimeTypes and HttpMessageConverters is taken directly from Spring.   ## Compatibility - Java 8+ - Vert.x 3.x.x   Vert.x version     | Library version  ------------------ | ----------------  3.0.0 - 3.3.3      | 2.0.x  3.0.0 - 3.3.3      | 2.1.0  3.0.0 - 3.3.3      | 2.2.1  3.4.x              | 2.3.1   ## Dependencies  ### Dependency ### Maven ```xml <dependency>     <groupId>com.hubrick.vertx</groupId>     <artifactId>vertx-rest-client</artifactId>     <version>2.3.1</version> </dependency> ```  ## How to use #### Simple example   ```java public class ExampleVerticle extends Verticle {      @Override     public void start() {         final ObjectMapper objectMapper = new ObjectMapper();         final List<HttpMessageConverter> httpMessageConverters = ImmutableList.of(             new FormHttpMessageConverter(),              new StringHttpMessageConverter(),              new JacksonJsonHttpMessageConverter(objectMapper)         );              final RestClientOptions restClientOptions = new RestClientOptions()             .setConnectTimeout(500)             .setGlobalRequestTimeout(300)             .setDefaultHost("example.com")             .setDefaultPort(80)             .setKeepAlive(true)             .setMaxPoolSize(500);          final RestClient restClient = RestClient.create(vertx, restClientOptions, httpMessageConverters);                                               // GET example         final RestClientRequest getRestClientRequest = restClient.get("/api/users/123", SomeReturnObject.class, getRestResponse -> {             final SomeReturnObject someReturnObject = getRestResponse.getBody();             // TODO: Handle response         });         getRestClientRequest.exceptionHandler(exception -> {             // TODO: Handle exception         });         getRestClientRequest.end();                  // POST example         final RestClientRequest postRestClientRequest = restClient.post("/api/users/123", SomeReturnObject.class, portRestResponse -> {             final SomeReturnObject someReturnObject = portRestResponse.getBody();             // TODO: Handle response         });         postRestClientRequest.exceptionHandler(exception -> {             // TODO: Handle exception         });         postRestClientRequest.setContentType(MediaType.TEXT_PLAIN);         postRestClientRequest.end("Some data");     } } ```  #### RxJava example ```java public class ExampleVerticle extends Verticle {      @Override     public void start() {         final ObjectMapper objectMapper = new ObjectMapper();         final List<HttpMessageConverter> httpMessageConverters = ImmutableList.of(             new FormHttpMessageConverter(),              new StringHttpMessageConverter(),              new JacksonJsonHttpMessageConverter(objectMapper)         );                  final RestClientOptions restClientOptions = new RestClientOptions()             .setConnectTimeout(500)             .setGlobalRequestTimeout(300)             .setDefaultHost("example.com")             .setDefaultPort(80)             .setKeepAlive(true)             .setMaxPoolSize(500);          final RxRestClient rxRestClient = RxRestClient.create(vertx, restClientOptions, httpMessageConverters);                                               // GET example         final Observable<RestClientResponse> getRestClientResponse = rxRestClient.get("/api/users/123", SomeReturnObject.class, restClientRequest -> restClientRequest.end());         getRestClientResponse.subscribe(             getRestClientResponse -> {                  // Handle response             },             error -> {                 // Handle exception             }         );     } } ```  ### Request caching The request cache can be enabled by setting the RequestCacheOptions in the RestClientOptions or setting it on the RestClientRequest object itself which either enables caching on the whole client or per request. The intentation of this cache is to make RxJava flows simpler which means the same service can be called multiple times in the same RxJava flow without worrying that it will be really called multiple times.   The cache caches not only the results but also the calls itself in case the data from the cache can't be retrieve since the first fired request which should cache the data still didn't finish. When the first fired request is finished it will distribute the data to all requests which are interested in it.  The cache key is calculated using the uri, headers and body. Which means it will only hit the cache in case it's a identical request. To seperate the same request from different callers it's a good idea to introduce a unique requestId in the HTTP Header.  #### Example ```java     final RequestCacheOptions requestCacheOptions = new RequestCacheOptions()         .withExpiresAfterWriteMillis(1000)         .withExpiresAfterAccessMillis(1000); ```   ### How to set exception handlers (non RxJava) Exception handlers are inherited but can be overridden on every level. You can set exception handlers on: - RestClient - RestClientRequest - RestClientResponse  ```java public class ExampleVerticle extends Verticle {      @Override     public void start() {         ...         // GET example         final RestClientRequest getRestClientRequest = restClient.get("/api/users/123", SomeReturnObject.class, getRestResponse -> {             getRestResponse.exceptionHandler(exception -> {                 // TODO: Handle exception             });                          final SomeReturnObject someReturnObject = getRestResponse.getBody();             // TODO: Handle response         });         getRestClientRequest.exceptionHandler(exception -> {             // TODO: Handle exception         });         restClient.exceptionHandler(exception -> {             // TODO: Handle exception         });         getRestClientRequest.end();         ...     } } ```  They will be inherited in the following way: RestClient -> RestClientRequest -> RestClientResponse. If you want to have a RestClient per Verticle (the desired way) set an exception handler per RestClientRequest and reuse the RestClient instance.   ## Supported HttpMessageConverters  Name                               | Description  ---------------------------------- | --------------------------------------------------------------------------------------  FormHttpMessageConverter           | Url-encodes the params. Content-Type: application/x-www-form-urlencoded  JacksonJsonHttpMessageConverter    | Encodes the object to JSON. Content-Type: application/json  StringHttpMessageConverter         | Outputs the plain string in the desired charset. Content-Type: plain/text  ByteArrayHttpMessageConverter      | Propagates the byte array without copying it to the http body. Activated by the presence of a byte[]. Content-Type: default application/octet-stream  ByteBufHttpMessageConverter        | Propagates the ByteBuf without copying it to the http body. Activated by the presence of a ByteBuf. Content-Type: default application/octet-stream  ByteBufferHttpMessageConverter     | Propagates the ByteBuffer without copying it to the http body. Activated by the presence of a ByteBuffer. Content-Type: default application/octet-stream  MultipartHttpMessageConverter      | Adds support for mutipart uploads. Content-Type: multipart/form-data   ## Exceptions  Name                               | Description  ---------------------------------- | --------------------------------------------------------------------------------------  RestClientException                | Base exception  InvalidMediaTypeException          | When the media type is not valid  InvalidMimeTypeException           | When the mime type is not valid  HttpStatusCodeException            | Base class for exceptions when status code is 4xx or 5xx  HttpClientErrorException           | Thrown in case of a 4xx  HttpServerErrorException           | Thrown in case of a 5xx   ## License Apache License, Version 2.0
CUTR-at-USF/SiriRestClient	SIRI REST Client for Android  ============================    Check out more information on the wiki:  https://github.com/CUTR-at-USF/SiriRestClient/wiki
dschulten/spring-hateoas-rest-service-sample	# Spring Hateoas ReST Service Sample (Experimental)  Example server and client based on spring-hateoas, showcasing the capabilities of spring-hateoas.    The goal is to have a framework which supports not only the creation of a hateoas enabled server api, but also a client framework capable of following the links which are embedded in the documents it receives, without knowing any URIs except for the entry point of the API.    The client is heavily inspired by Jon Moore's Oredev talk.    ## Running Standalone  - Build the sample using mvn install  - Change to rest-service-war  - Run mvn jetty:run  - Open your favorite rest client and go to http://localhost:8080/    ## Running from IDE    In your IDE, run jetty, e.g. in Eclipse I recommend the run-jetty-run plugin. Depending on your launch configuration, the service URL will be    http://localhost:8080/rest-service-war    Be sure to pass Accept: application/json from your rest client in order to see the json output.     We want to do ReST with HATEOAS. The resources come with the URLs leading to the next step in the application logic, the client does not need to know how to construct URLs. So let us browse the API like a HATEOAS enabled client would.     ## Use Browser    Make sure your browser understands and shows json (e.g. with Jsonovich for Firefox). Now you can start at the URL above. A simple HTML page shows a people URL and a search URL. Hit the search link. The search resource asks you for a personId (any id from 1 to 100 is valid). If you pass a personId, the server responds with a json object containing information about the person, and some additional links, such as the products owned by the person.    The point is that this form can be generated automatically from the Controller RequestMappings, and that a ReST client can recognize and submit the form. No out-of-band information is necessary for a client that is looking, let's say, for the products of a customer. If the URL structure changes, the client adapts as long as the rels, the form names and the parameter names remain stable.    ## Use Hateoas-enabled ReST-Client    Use the rest-client to browse the api programmatically.    The rest-client subproject features a client that attempts to follow the HATEOAS constraint. It is still in early stages, but it *is* able to follow rels, and submit forms. In order to run it, just execute the App binary. It will retrieve the products of two persons as json and print them out.    To give you an idea how the client does this, consider the following snippet from the rest-client:  ```      URI entryPoint = new URI("http://localhost:8080/");      CommonsHttpClient4Browser browser = new CommonsHttpClient4Browser(entryPoint);      // ...    	browser.followRel("search")  			.submitForm("searchPerson", Args.of("personId", 1))  			.followRel("products");  		System.out.println(browser.getCurrentResource().toString());  ```  with an output of    ```  starting at URI http://localhost:8080/  following rel search to http://localhost:8080/people/search  submitting form searchPerson with FormRequest [method=GET, requestBody=personId=1, uri=/people/person, contentType=application/x-www-form-urlencoded, encoding=UTF-8]  following rel products to http://localhost:8080/products/persons/1    [{"links":[{"rel":"self","href":"http://localhost:8080/products/1"}],"productName":"Starfleet Standard Tricorder"}]  ```  In followRel, the browser looks at rels in the Link header, in hmtl link and href elements and in json links.   The submitForm method saves the client the effort to build its own query links. Links are strictly created by the server.    Note that the URLs can be changed by the server as needed. If the server chooses to change the search rel to /persons/query or whatever, the client adapts automatically.    Currently the client is still constrained to a fixed chain of rels and forms. I am currently looking into ways to make the client more flexible here, based on the Follow Your Nose (http://www.w3.org/QA/2010/07/new_opportunities_for_linked_d.html) pattern. Another goal is to have some 'real' application state, not just a drill down on data.  Feedback is most welcome.
codegist/crest	CodeGist - CRest  CRest - Client Representational State Transfer  More information at http://crest.codegist.org
excelsior-oss/restler	[![Build Status](https://travis-ci.org/excelsior-oss/restler.svg?branch=master)](https://travis-ci.org/excelsior-oss/restler) [![Maven Central](https://img.shields.io/maven-central/v/org.restler/restler-core.svg)](https://maven-badges.herokuapp.com/maven-central/org.restler/restler-core)  Restler =======   ### Overview *Restler* is a library that automatically generates a client for a web service at run time, by analyzing the respective annotated Spring controller interface. *Restler* may help you remove HTTP-specific boilerplate from your integration tests, microservices and [thirdparty HTTP API clients](https://github.com/excelsior-oss/restler/wiki/GitHub-client).  **EPA warning: Restler currently is in early public access stage and it is neither feature complete, tested in production or backward compatible**  ### Features  * Easily extensible architecture  * Custom authentication, authorization and errors mapping strategies  * Support of form-based, http basic, cookie-based and generic header-based authentication  * Support of async controllers through methods returning `Future`, `DefferedResult` or `Callable` objects  * Experemental Spring Data REST support  ### Simple Usage Example  Suppose you have the following interface on the server: ```java /**    * An annotated Spring controller interface   */ @Controller @RequestMapping("greeter") public interface Greeter {  	@RequestMapping("greetings/{language}")	 	String getGreeting(@PathVariable String language, @RequestParam(defaultValue = "Anonymous") String name);   } ```  Then in your client you can invoke the `getGreeting` method of the remote service using the following code snippet: ```java Service service = new Restler("https://www.example.com/api", new SpringMvcSupport()).build(); Greeter greeter = service.produceClient(Greeter.class); String greeting = greeter.getGreeting("en","Buddy"); // the result of https://www.example.com/api/greeter/greetings/en?name=Buddy call ```
bluebreezecf/SparkJobServerClient	## Spark-Job-Server-Client  ### Backgroud People always use curl or HUE to upload jar and run spark job in Spark Job Server. But the Spark Job Server official only presents the rest apis to upload job jar and  run a job, doesn't give client lib with any language implementation.  Now there is another option to communicate with spark job server in Java, that is Spark-Job-Server-Client, the Java Client of the Spark Job Server implementing the arranged Rest APIs.  Spark-Job-Server-Client is a open-source program of **com.bluebreezecf** under Apache License v2. It aims to make the java applications use the spark more easily.  ### How to compile You can execute the following commands to compile this client: ```shell git clone https://github.com/bluebreezecf/SparkJobServerClient.git cd SparkJobServerClient mvn clean package ``` Then you can find`spark-job-server-client-1.0.0.jar`in SparkJobServerClient/target, it is the main jar of spark-job-server-client. Besides, `spark-job-server-client-1.0.0-sources.jar`is the java source jar, and `spark-job-server-client-1.0.0-javadoc.jar` is the java doc api jar.  ### How to set dependency Except the DIY way of getting the latest codes and compiling it, now the real and original **spark-job-server** has been published to [http://repo1.maven.org/maven2/](http://repo1.maven.org/maven2/com/bluebreezecf/spark-job-server-client/). So everyone, who has set the url as one of the remote maven repository, can download or get it via `mvn package` after setting the following dependency into your pom.xml. ```xml <dependency>     <groupId>com.bluebreezecf</groupId>     <artifactId>spark-job-server-client</artifactId>     <version>1.0.0</version> </dependency> ``` One can find all the releases of `spark-job-server-client` via [search.maven.org](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.bluebreezecf%22%20AND%20a%3A%22spark-job-server-client%22`).  ### How to use The following sample codes shows how to use spark-job-server-client:  ```java import java.io.File; import java.util.HashMap; import java.util.List; import java.util.Map;  import com.bluebreezecf.tools.sparkjobserver.api.ISparkJobServerClient; import com.bluebreezecf.tools.sparkjobserver.api.ISparkJobServerClientConstants; import com.bluebreezecf.tools.sparkjobserver.api.SparkJobConfig; import com.bluebreezecf.tools.sparkjobserver.api.SparkJobInfo; import com.bluebreezecf.tools.sparkjobserver.api.SparkJobJarInfo; import com.bluebreezecf.tools.sparkjobserver.api.SparkJobResult; import com.bluebreezecf.tools.sparkjobserver.api.SparkJobServerClientException; import com.bluebreezecf.tools.sparkjobserver.api.SparkJobServerClientFactory;  /**  * A sample shows how to use spark-job-server-client.  *   * @author bluebreezecf  * @since 2014-09-16  *  */ public class SparkJobServerClientTest { 	 	public static void main(String[] args) { 		ISparkJobServerClient client = null; 		try { 			client = SparkJobServerClientFactory.getInstance().createSparkJobServerClient("http://localhost:8090/"); 			//GET /jars 			List<SparkJobJarInfo> jarInfos = client.getJars(); 			for (SparkJobJarInfo jarInfo: jarInfos) { 				System.out.println(jarInfo.toString()); 			}  			//POST /jars/<appName> 			client.uploadSparkJobJar(new File("d:\\spark-examples_2.10-1.0.2.jar"), "spark-test"); 			 			//GET /contexts 			List<String> contexts = client.getContexts(); 			System.out.println("Current contexts:"); 			for (String cxt: contexts) { 				System.out.println(cxt); 			} 			 			//POST /contexts/<name>--Create context with name ctxTest and null parameter 			client.createContext("ctxTest", null); 			//POST /contexts/<name>--Create context with parameters 			Map<String, String> params = new HashMap<String, String>(); 			params.put(ISparkJobServerClientConstants.PARAM_MEM_PER_NODE, "512m"); 			params.put(ISparkJobServerClientConstants.PARAM_NUM_CPU_CORES, "10"); 			client.createContext("cxtTest2", params); 			 			//DELETE /contexts/<name> 			client.deleteContext("ctxTest"); 			 			//GET /jobs 			List<SparkJobInfo> jobInfos = client.getJobs(); 			System.out.println("Current jobs:"); 			for (SparkJobInfo jobInfo: jobInfos) { 				System.out.println(jobInfo); 			} 			 			//Post /jobs---Create a new job 			params.put(ISparkJobServerClientConstants.PARAM_APP_NAME, "spark-test"); 			params.put(ISparkJobServerClientConstants.PARAM_CLASS_PATH, "spark.jobserver.WordCountExample"); 			//1.start a spark job asynchronously and just get the status information 			SparkJobResult result = client.startJob("input.string= fdsafd dfsf blullkfdsoflaw fsdfs", params); 			System.out.println(result); 			 			//2.start a spark job synchronously and wait until the result 			params.put(ISparkJobServerClientConstants.PARAM_CONTEXT, "cxtTest2"); 			params.put(ISparkJobServerClientConstants.PARAM_SYNC, "true"); 			result = client.startJob("input.string= fdsafd dfsf blullkfdsoflaw fsdffdsfsfs", params); 			System.out.println(result); 			 			//GET /jobs/<jobId>---Gets the result or status of a specific job 			result = client.getJobResult("fdsfsfdfwfef"); 			System.out.println(result); 			 			//GET /jobs/<jobId>/config - Gets the job configuration 			SparkJobConfig jobConfig = client.getConfig("fdsfsfdfwfef"); 			System.out.println(jobConfig); 		} catch (SparkJobServerClientException e1) { 			e1.printStackTrace(); 		} catch (Exception e) { 			e.printStackTrace(); 		} 	} } ``` ### How to contribute Anyone interested in this program can do the following things:  1. `Fork` it to your own git repository.  2. Create a new branch for your feature via `git checkout -b your-new-feature`.  3. Add or modify new codes.  4. Commit the modifications through `git commit -am 'add your new feature'`.  5. Push the new branch by `git push origin your-new-feature`.  6. Create a new `pull request`.  Any questions and discussions can be added in [SparkJobServerClient/issues] (https://github.com/bluebreezecf/SparkJobServerClient/issues)
bowyer-app/ParseSendClient	ParseSendClient ============================== [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-ParseSendClient-green.svg?style=flat)](https://android-arsenal.com/details/1/2227)  Parse push send client.  Use Parse REST API Push Notifications  [Parse REST API](https://parse.com/docs/rest/guide/#quick-reference-push-notifications)  ![Demo send now](./art/sendnow.gif) ![Demo sckeduling](./art/schedule.gif)  Usage ==== Create notification Object  ```java ParsePushModel model = ParsePushModel.to().setTitle(pushTitle).setMessage(                                        pushMessage).setUrl(pushUrl); ```  Create channel  ```java String[] channel = new String[1]; channel[0] = "demo"; ```  Send push now.  ```java PushSendLogic.sendPush(model, channel, new PushSendLogic.PushSendCallBack() {     @Override     public void onSuccess() {      }      @Override     public void onFailure(String message) {      }  }); ```  Send scheduling push.  ```java Calendar calendar = Calendar.getInstance(); //set push date calendar.set(year, monthOfYear, dayOfMonth); //set push time calendar.set(Calendar.HOUR_OF_DAY, hourOfDay); calendar.set(Calendar.MINUTE, minute); //call sendSchedulingPush PushSendLogic.sendSchedulingPush(model, calendar, channel,     new PushSendLogic.PushSendCallBack() {          @Override          public void onSuccess() {                  }          @Override         public void onFailure(String message) {         } }); ```  Here's Custom push Object  ```java public class ParsePushModel {      String title;      String message;      String url;      public ParsePushModel to() {         return new ParsePushModel();     }      public ParsePushModel setTitle(String title) {         this.title = title;         return this;     }      public ParsePushModel setMessage(String message) {         this.message = message;         return this;     }      public ParsePushModel setUrl(String url) {         this.url = url;         return this;     }      public String getTitle() {         return title;     }      public String getMessage() {         return message;     }      public String getUrl() {         return url;     }  } ```  Download ==== Download via Gradle:  ``` repositories {     maven {        jcenter()     } }  dependencies {    compile 'com.bowyer.app:parsesendclient:0.1.0@aar'    compile 'com.squareup.retrofit:retrofit:1.9.0'    compile 'com.google.code.gson:gson:1.7.2'    compile 'com.squareup.okhttp:okhttp:2.2.0'    compile 'com.squareup.okhttp:okhttp-urlconnection:2.2.0' } ```  # parsepush.properties  add parsepush.properties into resources dir  | property  | description | | ------------- | ------------- | | PARSE_APPLICATION_ID | your Application ID | | PARSE_REST_API_KEY | your Client Key | | PARSE_REST_API_KEY | your REST API Key |  ![Parse](./art/properties.png) License -------- ``` Copyright (c) 2015 Bowyer Released under the MIT license http://opensource.org/licenses/mit-license.php ```
mmazi/rescu	ResCU - a lightweight Rest client utility for Java (JAX-RS) ========================================================  ResCU enables the user to create a proxy Rest client in run-time directly from a JAX-RS annotated interface. ResCU is mostly focused on json-based services, and uses Jackson for json-to-object mapping.  Several other libraries do this (eg. Jersey and RESTEasy); the benefit of ResCU is that it is very lightweight with minimal dependencies. This makes it useful for quickly creating REST clients in Android apps etc.   Dependencies ---------------  - jackson (json parser) - slf4j (logging interface) - jsr-311 (JAX-RS) (a set of REST-service specific annotations) - jsr-305 (a set of annotations) - net.iharder Base64   Features and benefits ---------------  - Lightweight, minimal dependencies. - JAX-RS-annotated server-side interfaces may be reused to create clients; basic support   is provided for @GET, @POST, @PUT, @DELETE, @HEAD, @OPTIONS, @Path, @QueryParam, @FormParam, @HeaderParam, @PathParam,  @Consumes and @Produces (`application/json` and `text/plain` only). - Support for basic HTTP authentication and some common request signing paradigms. See the [Basic HTTP Authentication wiki](https://github.com/mmazi/rescu/wiki/Basic-HTTP-Authentication). - Support for custom exceptions on API methods: on an exceptional HTTP response, rescu can deserialize the response body as an exception. See [Exception handling wiki](https://github.com/mmazi/rescu/wiki/Exception-handling). - Suppoft for custom interceptors.   Limitations ---------------  - Rescu is meant mostly for json-based REST services. The response body is always interpreted as json or plain text. No XML, and no plans to add it. - JAX-RS: No support yet for the following annotations: @MatrixParam, @CookieParam, @ApplicationPath, @HttpMethod, @Encoded, @DefaultValue.   Logging ---------------  ResCU uses slf4j for logging. For best results, a supported logging implementation (eg. log4j, JUL, logback, ...) should be provided in runtime, though this is not required. See slf4j's documentation for more info.  See [logback.xml](/src/test/resources/logback.xml) in test sources for example configuration.  Set the logging level for rescu to `debug` or `trace` in `logback.xml` for debugging:      <logger name="si.mazi.rescu" level="trace"/>  Usage ---------------  #### Maven  Rescu is hosted in Maven Central so all you need to do is add this dependency to your pom:      <dependency>         <groupId>com.github.mmazi</groupId>         <artifactId>rescu</artifactId>         <version>1.6.0</version>     </dependency>  #### Usage in code  1. Create a JAX-RS-annotated interface (or get it from the REST service developer), eg. `ExampleService.java`. 2. Call `ExampleService service = RestProxyFactory.createProxy(ExampleService.class, "http://www.example.com/")`. 3. That's it! Just use the `service` object you just got.  #### Examples  See the [tests](/src/test) for some examples. [ExampleService](/src/test/java/si/mazi/rescu/ExampleService.java) is an example of a JAX-RS-annotated interface.  For more working examples, see [XChange](https://github.com/timmolter/XChange), eg. [BitstampTradeServiceRaw.java](https://github.com/timmolter/XChange/blob/develop/xchange-bitstamp/src/main/java/com/xeiam/xchange/bitstamp/service/polling/BitstampTradeServiceRaw.java).  #### Settings  Rescu can be configured by adding a `rescu.properties` file in your classpath.  Supported settings with example values (copy this into your `rescu.properties` file):      rescu.http.readTimeoutMillis = 5000             # Read timeout in milliseconds when performing HTTP requests. The default is 30000 (30 seconds).     rescu.http.readProxyHost = www.example.com      # HTTP proxy host. Both host and port must be set in order to use a proxy.     rescu.http.readProxyPort = 80                   # HTTP proxy port. Both host and port must be set in order to use a proxy.     rescu.http.ignoreErrorCodes = true              # If set to true, the HTTP response body never be parsed as Exception but always as the method response type. Defaults to false.  License ---------------  Rescu is released under the MIT License. Please see [LINCESE.txt](LICENSE.txt) for the full text.  An important note ---------------  I am very open to new suggestions, change requests etc. If ResCU seems almost right for you, but not quite, do write me a note, eg. by [opening an issue](https://github.com/mmazi/rescu/issues/new). Documentation and clarification requests are welcome too!
yongjhih/drupalfit	drupalfit =========  [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-drupalfit-brightgreen.svg?style=flat)](https://android-arsenal.com/details/1/1250)  [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/yongjhih/drupalfit?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [![Build Status](https://travis-ci.org/yongjhih/drupalfit.png?branch=master)](https://travis-ci.org/yongjhih/drupalfit) [![Stories in Ready](https://badge.waffle.io/yongjhih/drupalfit.png)](http://waffle.io/yongjhih/drupalfit)  [![Bountysource](https://www.bountysource.com/badge/team?team_id=43965&style=bounties_posted)](https://www.bountysource.com/teams/8tory/bounties?utm_source=8tory&utm_medium=shield&utm_campaign=bounties_posted) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.infstory/drupalfit/badge.svg?style=flat)](https://maven-badges.herokuapp.com/maven-central/com.infstory/drupalfit)  ![square drupal](drupal-webtreatsetc.png "Square drupal")  A drupal services rest client with retrofit for android  ![Screenshot](app/screenshot.png "Sign-up sample")  Usage =====  * Initialization  ```java DrupalManager.get().setEndpoint("https://example.com/api"); ```  * register  ```java DrupalManager.get().register("foo", "foo@example.com", "password", new Callback<User>() {     @Override     public void success(User user, Response response) {     }     @Override     public void failure(RetrofitError error) {     } }); ```  * login  ```java DrupalManager.get().login("foo", "password", new Callback<Login>() {     @Override     public void success(Login login, Response response) {     }     @Override     public void failure(RetrofitError error) {     } }); ```  * logout (after login)  ```java DrupalManager.get().logout(new Callback<Logout>() {     @Override     public void success(Logout logout, Response response) {     }     @Override     public void failure(RetrofitError error) {     } }); ```  * getNode  ```java DrupalManager.get().getNode(1, new Callback<Node>() {     @Override     public void success(Node node, Response response) {     }     @Override     public void failure(RetrofitError error) {     } }); ```  * addComment  ```java String commentContent = "Hello"; int nodeId = 1; DrupalManager.get().addComment(commentContent, nodeId, new Callback<Comment>() {     @Override     public void success(Comment comment, Response response) {     }     @Override     public void failure(RetrofitError error) {     } }); ```  * getComment  ```java String commentId = 1; DrupalManager.get().getComment(commentId, new Callback<Comment>() {     @Override     public void success(Comment comment, Response response) {     }     @Override     public void failure(RetrofitError error) {     } }); ```  Bonus =====  * Sign-in with facebook access token (Depend on [yongjhih/drupal-hybridauth](https://github.com/yongjhih/drupal-hybridauth)) and getProfile with.  It's easy to integrate facebook sdk.  ```java DrupalManager.get().setProvider(this, DrupalManager.FACEBOOK, "faceb00k_t0ken");  DrupalManager.get().getProfile(new Callback<User>() {     @Override     public void success(User user, Response response) {     }     @Override     public void failure(RetrofitError error) {     } }); ```  Installation ============  build.gradle:  ```gradle dependencies {     compile "com.infstory:drupalfit:+" } ```  [License] (LICENSE) ===================  ``` The MIT License (MIT)  Copyright (c) 2014 Andrew Chen  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```
noveogroup-amorgunov/spring-mvc-react	# Spring 4 MVC + ReactJS  ![alt tag](src/main/webapp/resources/preview.png)  Very light version of [stackoverflow](http://stackoverflow.com/) build by [ReactJS](https://facebook.github.io/react/) (client-side) and [Spring4](https://spring.io/) (server-side).  ## Features  - Authorization system (by [json web token](https://jwt.io/)) - Questions, answers, users, reputation, tags and votes! - Localization in react using [localizify](https://github.com/noveogroup-amorgunov/localizify)  ## Intallation  **0** Clone repository!  ```shell $ git clone https://github.com/noveogroup-amorgunov/spring-mvc-react.git ```  **1** Change database driver (by default set for MySQL) and connections parameters (url, user and password) in `src/main/resources/app.properties`  **2** Change `jwt` secret key in `src/main/resources/app.properties` too (not nessasary)  **3** Create schema. After run application table will be created in auto mode. Follow example for MySQL  ```sql CREATE SCHEMA `spring-mvc-react` DEFAULT CHARACTER SET utf8 ; ```  **4** Install and build frontend dependencies   ```shell $ cd src/main/webapp $ npm install $ npm install webpack -g # intstall webpack globally $ npm run build # build bundle.js file ```  Use `npm run watch` for work in watch-mode. When you change some javascript file, here will be build new bundle.js  **5** Run server  ```shell $ mvn jetty:run ``` Access ```http://localhost:4017/spring4ajax```  To import this project into Eclipse IDE:  1. ```$ mvn eclipse:eclipse``` 2. Import into Eclipse via **existing projects into workspace** option. 3. Done.
rest-driver/rest-driver	#REST-driver  [![Build Status](https://secure.travis-ci.org/rest-driver/rest-driver.png?branch=master)](http://travis-ci.org/rest-driver/rest-driver)  Test Driver to test your RESTful services and clients.  There are two libraries here:  * REST server driver - for testing your RESTful service * REST client driver - for testing your RESTful client & mocking remote services  **Latest version: 2.0.0**  Please note: version `2.0.0` onwards will require Java 8.  [Downloads](https://github.com/rest-driver/rest-driver/wiki/Downloads) - [Server Driver](https://github.com/rest-driver/rest-driver/wiki/Server-Driver) - [Client Driver](https://github.com/rest-driver/rest-driver/wiki/Client-driver)  ##Goals  Everyone knows that testing is good for your health and REST is good for your sanity.  Now it is easy to keep both in check by allowing you to write _quick_, _readable_, _fluid_ tests which are easy to refactor and give excellent error handling/reporting.  So we provide these libraries to test from both ends of the pipe.  ##REST server driver  In order to do thorough testing of your RESTful service, you'll have to make actual HTTP requests and check the actual HTTP responses from your service.  REST driver makes this as easy as:  ```java Response response = get( "http://www.example.com" );  assertThat(response, hasStatusCode(200)); assertThat(response.asJson(), hasJsonPath("$.name", equalTo("jeff"))); ```  More info: [Server Driver](https://github.com/rest-driver/rest-driver/wiki/Server-Driver)  ##REST client driver  If you have a client for a RESTful service, it's not ideal to have an actual service running somewhere to test against.  This is difficult to keep on top of, makes for brittle/flickering tests, and tightly couples your tests to someone else's code.  We provide a mock-like interface which launches a real HTTP server and allows setup like this:  ```java clientDriver.addExpectation(onRequestTo("/").withMethod(Method.GET),                              giveResponse("some wonderful content", "text/plain")); ```  The server will listen for a requests and respond with the replies you set.  Any unexpected action or unmet expectation will fail your tests.  You can also use the client-driver to mock out any remote services which your RESTful service uses.  More info: [Client Driver](https://github.com/rest-driver/rest-driver/wiki/Client-driver)  ## Other languages  * [rest-cljer](https://github.com/whostolebenfrog/rest-cljer), a convenient Clojure wrapper for REST client driver.
vojtechhabarta/typescript-generator	[![Maven Central](https://img.shields.io/maven-central/v/cz.habarta.typescript-generator/typescript-generator-core.svg)](https://repo1.maven.org/maven2/cz/habarta/typescript-generator/typescript-generator-core/) [![Appveyor](https://img.shields.io/appveyor/ci/vojtechhabarta/typescript-generator/master.svg)](https://ci.appveyor.com/project/vojtechhabarta/typescript-generator) [![Stars](https://img.shields.io/github/stars/vojtechhabarta/typescript-generator.svg?style=social)](https://github.com/vojtechhabarta/typescript-generator)  Quick links: [Configuration parameters](http://www.habarta.cz/typescript-generator/maven/typescript-generator-maven-plugin/generate-mojo.html) | [Breaking changes](https://github.com/vojtechhabarta/typescript-generator/wiki/Breaking-Changes) | [Release notes](https://github.com/vojtechhabarta/typescript-generator/releases)  typescript-generator ==================== typescript-generator is a tool for generating TypeScript definition files (.d.ts) from Java JSON classes. If you have REST service written in Java using object to JSON mapping you can use typescript-generator to generate TypeScript interfaces from Java classes.  For example for this Java class:  ``` java public class Person {     public String name;     public int age;     public boolean hasChildren;     public List<String> tags;     public Map<String, String> emails; } ```  typescript-generator outputs this TypeScript interface: ``` typescript interface Person {     name: string;     age: number;     hasChildren: boolean;     tags: string[];     emails: { [index: string]: string }; } ```  Supported types include: - all Java primitive types with their corresponding wrappers (for example `int` and `Integer`, `boolean` and `Boolean`, etc.) - `String` - `Date` - enum - array - `List` and `Map` (including derived interfaces and implementation classes) - customized type mapping  For more details see [Type Mapping Wiki page](../../wiki/Type-Mapping).  > Note: typescript-generator works with compiled classes using Java reflection. It doesn't use source files (except for Javadoc feature). In Maven plugin this means either classes compiled from source files in the same module or classes added using `<dependency>` element.  Maven -----  In Maven build you can use `typescript-generator-maven-plugin` like this: ``` xml <plugin>     <groupId>cz.habarta.typescript-generator</groupId>     <artifactId>typescript-generator-maven-plugin</artifactId>     <version>x.y.z</version>     <executions>         <execution>             <id>generate</id>             <goals>                 <goal>generate</goal>             </goals>             <phase>process-classes</phase>         </execution>     </executions>     <configuration>         <jsonLibrary>jackson2</jsonLibrary>         <classes>             <class>cz.habarta.typescript.generator.Person</class>         </classes>         <outputKind>module</outputKind>     </configuration> </plugin> ```  More complete sample can be found [here](sample-maven). Detailed description how to configure typescript-generator-maven-plugin is on generated [site](http://vojtechhabarta.github.io/typescript-generator/maven/typescript-generator-maven-plugin/generate-mojo.html).   Gradle ------  In Gradle build you can use `cz.habarta.typescript-generator` plugin like this: ```groovy apply plugin: 'cz.habarta.typescript-generator' buildscript {     repositories {         mavenCentral()     }     dependencies {         classpath group: 'cz.habarta.typescript-generator', name: 'typescript-generator-gradle-plugin', version: 'x.y.z'     } } generateTypeScript {     jsonLibrary = 'jackson2'     classes = [         'cz.habarta.typescript.generator.sample.Person'     ]     outputFile = 'build/sample.d.ts'     outputKind = 'global'     namespace = 'Rest'; } ```  More complete sample can be found [here](sample-gradle). Gradle plugin has the same features as Maven plugin, for detailed description see Maven generated [site](http://vojtechhabarta.github.io/typescript-generator/maven/typescript-generator-maven-plugin/generate-mojo.html).    Direct invocation ----------------- If you do not use Maven or Gradle you can invoke typescript-generator directly using `TypeScriptGenerator.generateTypeScript()` method.   Input classes ------------- Input classes can be specified using several parameters: - **`classes`** - list of fully qualified class names, includes all listed classes and their dependencies, `$` character is used for nested classes like `com.example.ClassName$NestedClassName` - **`classPatterns`** - list of glob patterns like `com.example.*Json`, includes all classes matched by the pattern, supported are `*` and `**` wildcards - **`classesFromJaxrsApplication`** - fully qualified name of JAX-RS application class, all classes used by application resources will be included, recommended if you have JAX-RS application class - **`classesFromAutomaticJaxrsApplication`** - value `true` will include classes from automatically discovered REST resources, recommended if you have JAX-RS application without `Application` subclass - **`excludeClasses`** - list of fully qualified class names, excluded classes will be mapped to TypeScript `any` type, if exluded class is a resource then this resource will not be scanned for used classes  > Note: it is possible to use multiple parameters at the same time.  For more details see [Class Names Glob Patterns](../../wiki/Class-Names-Glob-Patterns) and [JAX RS Application](../../wiki/JAX-RS-Application) Wiki pages.   Output parameters ----------------- Output is configured using several parameters: - `outputKind` (required parameter) - determines if and how module will be generated     - values are: `global`, `module`, `ambientModule` - `outputFileType` - specifies TypeScript file type     - values are: `declarationFile` (.d.ts) or `implementationFile` (.ts) - `outputFile` - specifies path and name of output file  For more details see [Modules and Namespaces](http://vojtechhabarta.github.io/typescript-generator/doc/ModulesAndNamespaces.html) page.   Download -------- Releases are available from Maven Central Repository. [Search](http://search.maven.org/#search%7Cga%7C1%7Ccz.habarta.typescript-generator) for dependency information for your build tool or download [typescript-generator-core](https://repo1.maven.org/maven2/cz/habarta/typescript-generator/typescript-generator-core) directly.   Wiki ---- For more detailed description of some topics see [Wiki pages](../../wiki).   Architecture ------------  `TypeScriptGenerator` has 3 main parts (`ModelParser`, `ModelCompiler` and `Emitter`) which work together to produce TypeScript declarations for specified Java classes.  ```            (Model)            (TsModel) ModelParser  ==>  ModelCompiler  ==>  Emitter          |         |           V         V         TypeProcessor ```  - `ModelParser` reads Java JSON classes and their properties using Java reflections and creates `Model`.   It uses `TypeProcessor`s for finding used classes.   For example if property type is `List<Person>` it discovers that `Person` class should be also parsed.   `ModelParser`s are specific for each JSON library (for example `Jackson2Parser`). - `ModelCompiler` transforms Java model to TypeScript model (`Model` class to `TsModel` class).   It uses `TypeProcessor`s for mapping Java types to TypeScript types (for example for `int` returns `number`). - `Emitter` takes `TsModel` and produces TypeScript declaration file.   Links -----  - http://www.rainerhahnekamp.com/type-safe-endpoints-with-typescript-and-java - blog post about using typescript-generator not only with Spring MVC - http://www.jsweet.org/10-reasons-to-use-jsweet - blog post about JSweet transpiler mentions typescript-generator - https://github.com/raphaeljolivet/java2typescript - tool similar to typescript-generator   Contributing ------------  - this project targets Java 7 - keep pull requests small and focused ([10 tips for better Pull Requests](http://blog.ploeh.dk/2015/01/15/10-tips-for-better-pull-requests/)) - do not add dependencies unless previously discussed in issue  ### Code formatting  - use 4 spaces for indentation in Java files - sort java imports alphabetically, you can use wildcards - please do not reformat whole files in IDE
ModeShape/modeshape	[![License](http://img.shields.io/:license-apache%202.0-brightgreen.svg)](http://www.apache.org/licenses/LICENSE-2.0.html) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.modeshape/modeshape-parent/badge.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.modeshape%22) [![Build Status](https://travis-ci.org/ModeShape/modeshape.svg?branch=master)](http://travis-ci.org/ModeShape/modeshape)  Copyright 2008-2016 ModeShape Project. Licensed under the Apache License, Version 2.0.  # The ModeShape project  This is the official Git repository for the ModeShape project.  ModeShape is an open source implementation of the JCR 2.0 ([JSR-283](http://www.jcp.org/en/jsr/detail?id=283])) specification and standard API. To your applications, ModeShape looks and behaves like a regular JCR repository. Applications can search, query, navigate, change, version, listen for changes, etc. But ModeShape can store that content in a variety of back-end stores (including relational databases, the filesystem, etc.), or it can access and update existing content from *other* kinds of systems (including file systems, Git repositories, JDBC database metadata, and other JCR repositories which support CMIS). ModeShape's connector architecture means that you can write custom connectors to access any kind of system. And ModeShape can even federate multiple back-end systems into a single, unified virtual repository.  ModeShape repositories can be used in a variety of applications. One of the most obvious ones is in provisioning and management, where it's critical to  understand and keep track of the metadata for models, database, services, components, applications, clusters, machines, and other systems used in an enterprise.  Governance takes that a step farther, by also tracking the policies and expectations against which performance can be verified. In these cases, a repository  is an excellent mechanism for managing this complex and highly-varied information. But a ModeShape repository doesn't have to be large and complex: it  could just manage configuration information for an application, or it could just provide a JCR interface on top of a couple of non-JCR systems.  For more information on ModeShape, including getting started guides, reference guides, and downloadable binaries, visit the project's website at [http://www.modeshape.org](http://www.modeshape.org) or follow us on our [blog](http://modeshape.wordpress.org) or on [Twitter](http://twitter.com/modeshape). Or hop into our [IRC chat room](http://www.jboss.org/modeshape/chat) and talk our community of contributors and users.  ## Get the code  The easiest way to get started with the code is to [create your own fork](http://help.github.com/forking/) of this repository, and then clone your fork:  	$ git clone git@github.com:<you>/modeshape.git 	$ cd modeshape 	$ git remote add upstream git://github.com/ModeShape/modeshape.git 	 At any time, you can pull changes from the upstream and merge them onto your master:  	$ git checkout master               # switches to the 'master' branch 	$ git pull upstream master          # fetches all 'upstream' changes and merges 'upstream/master' onto your 'master' branch 	$ git push origin                   # pushes all the updates to your fork, which should be in-sync with 'upstream'  The general idea is to keep your 'master' branch in-sync with the 'upstream/master'.  ## Building ModeShape  Then, we use Maven 3.x to build our software. The following command compiles all the code, installs the JARs into your local Maven repository, and run all of the unit tests:  	$ mvn clean install -s settings.xml  BTW, that '-s settings.xml' argument uses the 'settings.xml' file in our codebase, which is set up to use the JBoss Maven repository  If you are interested in building & contributing to the Wildfly kit as well, you need to run the following command (instead of the earlier command)  to build all modules, including the Wildfly kit, using our "integration" profile:      $ mvn clean install -s settings.xml -Pintegration  To build everything, including the ModeShape kit for Wildfly, our JavaDoc, and our other assemblies, use the "assembly" profile instead:      $ mvn clean install -s settings.xml -Passembly  As an alternative to always passing the "-s settings.xml" parameter, you can modify your local ~/.m2/settings.xml file and add the above mentioned repositories, making sure they are active by default during a build.  That command takes a while -- we do have over 12K unit tests. So if need be, your builds can skip the tests:  	$ mvn clean install -s settings.xml -DskipTests 	 If you have *any* trouble building (or don't like the '-s settings.xml' usage), check the [detailed build instructions and tips](http://community.jboss.org/wiki/ModeShapeAndMaven).  ## Contribute fixes and features  ModeShape is licensed under the Apache License 2.0 (see the file LICENSE.txt or [http://www.apache.org/licenses/LICENSE-2.0.html]() for details).  Contributions to ModeShape are welcome. They must be completely authored by you, and you must have the right to contribute them (for example, if you are employed, you must have received the necessary permissions from your employer to make the contribution). All contributions to ModeShape must be licensed under the Apache License 2.0, just like the project itself.  Before committing anything, PLEASE make sure you have set up all of the development tools (see http://community.jboss.org/wiki/ModeShapeDevelopmentTools), are following the project's guidelines (see http://community.jboss.org/wiki/ModeShapeDevelopmentGuidelines), and are  using our accepted workflow (see http://community.jboss.org/wiki/ModeShapeDevelopmentWorkflow).  If you want to fix a bug or make any changes, please log an issue in the [ModeShape JIRA](https://issues.jboss.org/browse/MODE) describing the bug or new feature. Then we highly recommend making the changes on a topic branch named with the JIRA issue number. For example, this command creates a branch for the MODE-1234 issue:  	$ git checkout -b mode-1234  After you're happy with your changes and a full build (with unit tests) runs successfully, commit your changes on your topic branch (using [really good comments](http://community.jboss.org/wiki/ModeShapeDevelopmentGuidelines#Commits)). Then it's time to check for and pull any recent changes that were made in the official repository:  	$ git checkout master               # switches to the 'master' branch 	$ git pull upstream master          # fetches all 'upstream' changes and merges 'upstream/master' onto your 'master' branch 	$ git checkout mode-1234            # switches to your topic branch 	$ git rebase master                 # reapplies your changes on top of the latest in master 	                                      (i.e., the latest from master will be the new base for your changes)  If the pull grabbed a lot of changes, you should rerun your build to make sure your changes are still good. You can then either [create patches](http://progit.org/book/ch5-2.html) (one file per commit, saved in `~/mode-1234`) with   	$ git format-patch -M -o ~/mode-1234 orgin/master  and upload them to the JIRA issue, or you can push your topic branch and its changes into your public fork repository  	$ git push origin mode-1234         # pushes your topic branch into your public fork of ModeShape  and [generate a pull-request](http://help.github.com/pull-requests/) for your changes.   We prefer pull-requests, because we can review the proposed changes, comment on them, discuss them with you, and likely merge the changes right into the official repository.
eBay/parallec	<a href="http://www.parallec.io"><img alt="Parallec-logo" src="http://www.parallec.io/images/parallec-logo.png" width="325"></a>   ![build status](https://img.shields.io/badge/build-info=>-green.svg) [![Build Status](https://travis-ci.org/eBay/parallec.svg?branch=master)](https://travis-ci.org/eBay/parallec) [![Coverage Status](https://img.shields.io/codecov/c/github/eBay/parallec.svg)](https://codecov.io/github/eBay/parallec) [![Apache V2.0 License](http://www.parallec.io/images/apache2.svg) ](https://github.com/eBay/parallec/blob/master/LICENSE)   ![latest 0.10.x](http://img.shields.io/badge/latest_stable-0.10.x=>-green.svg) [ ![latest beta  maven central](https://maven-badges.herokuapp.com/maven-central/io.parallec/parallec-core/badge.svg?style=flat)](http://search.maven.org/#artifactdetails|io.parallec|parallec-core|0.10.6|) [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/eBay/parallec?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)  [![Javadoc](http://www.parallec.io/images/parallec-javadoc-blue.svg)](http://www.parallec.io/javadoc/index.html?io/parallec/core/ParallelClient.html) [![Documentation](http://www.parallec.io/images/parallec-documentation-red.svg)](http://www.parallec.io/docs/) [![Samples](http://www.parallec.io/images/parallec-samples-brightgreen.svg)](https://github.com/eBay/parallec-samples) [![Chinese](http://www.parallec.io/images/parallec-cnbrief-blue.svg)](https://github.com/eBay/parallec/blob/master/README-brief-cn.md)   [ [Get-Started](https://github.com/eBay/parallec/blob/master/README.md#get-started) | [Features](https://github.com/eBay/parallec/blob/master/README.md#features) | [Use Cases](https://github.com/eBay/parallec/blob/master/README.md#use-cases) | [Samples](https://github.com/eBay/parallec-samples) | [Change Log](https://github.com/eBay/parallec/blob/master/CHANGELOG.md) / [What's New](https://github.com/eBay/parallec/blob/master/README.md#whats-new) / [Versions](https://github.com/eBay/parallec/blob/master/README.md#versions) |  [User Group](https://github.com/eBay/parallec/blob/master/README.md#user-group) |  [Motivation](https://github.com/eBay/parallec/blob/master/README.md#motivation) | [Demos](https://github.com/eBay/parallec/blob/master/README.md#demos) | [Performance](https://github.com/eBay/parallec/blob/master/README.md#performance) | [Compare](https://github.com/eBay/parallec/blob/master/README.md#compare) | [Contributors](https://github.com/eBay/parallec/blob/master/README.md#contributors) | [About](https://github.com/eBay/parallec/blob/master/README.md#authors) | [News](http://www.parallec.io/blog/) | [Plugin](https://github.com/eBay/parallec-plugins) | [中文介绍](https://github.com/eBay/parallec/blob/master/README-brief-cn.md) ]   [ [API Overview](http://www.parallec.io/docs/api-overview/) | [Generate & Submit Task](http://www.parallec.io/docs/submit-task/) | [Track Status & Examine Responses](http://www.parallec.io/docs/track-status/) | [Configurations](http://www.parallec.io/docs/configurations/) ]  [Tweeted](https://twitter.com/jboner/status/663618652063813632) by the Creator of Akka & Featured in [ [This Week in #Scala](http://www.cakesolutions.net/teamblogs/this-week-in-scala-16/11/2015) | [OSChina](http://www.oschina.net/p/parallec) - [2015 Top 100](http://www.oschina.net/news/69808/2015-annual-ranking-top-100-new-open-source-software) ]  ### Overview  Parallec is a fast parallel async HTTP(S)/SSH/TCP/UDP/Ping client java library based on [Akka](http://akka.io). Scalably aggregate and handle API responses **anyway** and send it **anywhere** by writing [20 lines](https://www.youtube.com/watch?v=QcavegPMDms) of code. A super convenient **response context** let you pass in/out any object when handling the responses. Now you can conduct scalable API calls, then effortlessly pass aggregated data anywhere to elastic search, kafka, MongoDB, graphite, memcached, etc. Flexible task level concurrency control without creating a 1,000 threads thread pool. Parallec means  **Paralle**l **C**lient (pronounced as "Para-like"). Visit [www.parallec.io](http://www.parallec.io)  **[Watch Demo](https://github.com/eBay/parallec/wiki/Parallec-Aggregates-HTTP-Responses-from-8000-Servers)**: **8,000** web server HTTP response aggregation to memory in **12** seconds / to ElasticSearch in **16** seconds.  **Aggregated error messages - Debug friendly with full visibility**: Having trouble debugging in concurrent environment? Not any more! All exceptions, timeout, stack traces, request sent and response received time are **captured and aggregated** in the [response](http://www.parallec.io/javadoc/index.html?io/parallec/core/ResponseOnSingleTask.html) map. It is available in [ParallelTask](http://www.parallec.io/javadoc/index.html?io/parallec/core/ParallelTask.html) for polling right after you execute a task asynchronously.  Multi-level (worker/manager) timeout guarantees tasks return even for 100,000s of requests.     **[Production Use Cases](http://www.parallec.io/#testimonial-ebay)**: widely used in infrastructure software as [the polling and aggregation engine](https://medium.com/@jeffpeiyt/a-reusable-part-in-many-cloud-projects-2151f42572e9#.31x5rt6j2)  1. **Application Deployment / PaaS**: Parallec has been [integrated](http://www.parallec.io/#testimonial-ebay) in eBay main production application deployment system (PaaS). Parallec orchestrates 10+ API tasks, with each task targeting 10s to 1,000s servers over 1,000+ application pools in production. Parallec has been used with work flow engine [Winder](https://github.com/eBay/Winder) to handle work flows more complex but similar to [this one](http://stackoverflow.com/questions/39081350/parallel-ssh-with-executorservice/39671381#39671381). 2. **Data Extraction / ETL**: Parallec has been [used](http://www.parallec.io/#testimonial-ebay) by eBay Israel's web intelligence team for executing 10k-100k API parallel calls to a single 3rd party server with dramatic improved performance and reduced resources.  3. **Network Troubleshooting via Probing**:  In eBay's network / cloud team, Parallec is instrumental to ensure an extremely low false alert rates to accurately detect switch soft failures. Parallec serves as the core polling engine in the master component to check agent healths and mark down agents to effectively and timely eliminate noises. 4. **[Agent Management / Agent Master](https://medium.com/@jeffpeiyt/a-reusable-part-in-many-cloud-projects-2151f42572e9#.31x5rt6j2)**: In eBay's site operation / tools team, Parallec serves as the core engine to manage and monitor a puppet agent/salt minion/kubernetes kubelet like agent on 100,000+ production servers to ensure scalable operations.  ![Workflow Overview](http://www.parallec.io/images/parallec-flow.svg)    ### Get Started Donwload [the latest JAR](https://search.maven.org/remote_content?g=io.parallec&a=parallec-core&v=LATEST) or grab from Maven:  ```xml <dependency> 	<groupId>io.parallec</groupId> 	<artifactId>parallec-core</artifactId> 	<version>0.10.6</version> </dependency> ``` Snapshots of the development version are available in [Sonatype's `snapshots` repository](https://oss.sonatype.org/content/repositories/snapshots/io/parallec/parallec-core/).  or Gradle: ```xml compile 'io.parallec:parallec-core:0.10.6' ```   **6 Line Example**   In the example below,  simply changing **prepareHttpGet()** to **prepareSsh()**, **prepareTcp()**, **prepareUdp()**, **preparePing()** enables you to conduct parallel SSH/TCP/Ping. Details please refer to the [Java Doc](http://www.parallec.io/javadoc/index.html?io/parallec/core/ParallelClient.html) and [Example Code](https://github.com/ebay/parallec-samples).   ```java import io.parallec.core.*; import java.util.Map;  ParallelClient pc = new ParallelClient();  pc.prepareHttpGet("").setTargetHostsFromString("www.google.com www.ebay.com www.yahoo.com") .execute(new ParallecResponseHandler() {     public void onCompleted(ResponseOnSingleTask res,         Map<String, Object> responseContext) {         System.out.println( res.toString() );  } }); ```	 **20 Line Example**  Now that you have learned the basics, check out how easy to pass an elastic search client using the convenient **response context** to aggregate data anywhere you like. You can also pass a hash map to the `responseContext`, save the processed results to the map during `onCompleted`, and use the map outside for further work.   ```java ParallelClient pc = new ParallelClient(); org.elasticsearch.node.Node node = nodeBuilder().node(); //elastic client initialize HashMap<String, Object> responseContext = new HashMap<String, Object>(); responseContext.put("Client", node.client()); pc.prepareHttpGet("")         .setConcurrency(1000).setResponseContext(responseContext)         .setTargetHostsFromLineByLineText("http://www.parallec.io/userdata/sample_target_hosts_top100_old.txt", HostsSourceType.URL)         .execute( new ParallecResponseHandler() {             public void onCompleted(ResponseOnSingleTask res,                     Map<String, Object> responseContext) {                 Map<String, Object> metricMap = new HashMap<String, Object>();                 metricMap.put("StatusCode", res.getStatusCode().replaceAll(" ", "_"));                 metricMap.put("LastUpdated",PcDateUtils.getNowDateTimeStrStandard());                 metricMap.put("NodeGroupType", "Web100");                 Client client = (Client) responseContext.get("Client");                 client.prepareIndex("local", "parallec", res.getHost()).setSource(metricMap).execute();             }         }); node.close(); pc.releaseExternalResources(); ```  **Different Requests to the Same Target**  Now see **how easy** to use the request template to send multiple different requests to the same target. Variable replacement is allowed in post body, url and headers. [Read more..](http://www.parallec.io/docs/submit-task/#apis-on-variable-replacement-for-heterogeneous-requests)  ```java pc.prepareHttpGet("/userdata/sample_weather_$ZIP.txt")     .setReplaceVarMapToSingleTargetSingleVar("ZIP",         Arrays.asList("95037","48824"), "www.parallec.io")     .execute(new ParallecResponseHandler() {...}... ```  - [http://www.parallec.io/userdata/sample_weather_48824.txt](http://www.parallec.io/userdata/sample_weather_48824.txt) - [http://www.parallec.io/userdata/sample_weather_95037.txt](http://www.parallec.io/userdata/sample_weather_95037.txt)   ### What's New  * 06/2017 Add dynamic response encoding according to response content type. * 09/2016 Add option to save response headers in HTTP [#24](https://github.com/eBay/parallec/issues/24). * 08/2016 Support Parallel async UDP (via Netty) [#41](https://github.com/eBay/parallec/issues/41). * 07/2016 Support replacing different ports in different requests.  * 06/2016 Parallel SSH add run sudo with password for commands.  More details please check the [Change Log](https://github.com/eBay/parallec/blob/master/CHANGELOG.md).  ### Versions  * The latest production-ready version is `0.10.x`, where we use in production. * **On async-http-client 2.x** The Parallec.io version using more up-to-date `async-http-client` (currently using AHC version `2.0.15`) is `0.20.0-SNAPSHOT`. This version has passed comprehensive unit tests but has not been used yet in production. This version **requires JDK8** due to AHC 2.x and can be used with the parallec-plugins with the same version `0.20.0-SNAPSHOT`, details please check [#37](https://github.com/eBay/parallec/issues/37).   ### More Readings  - [**More Examples**](https://github.com/ebay/parallec-samples#http) on setting context, send to Elastic Search / Kafka, async running, auto progress polling, track progress, TCP/SSH/Ping.  UDP example is [here](https://github.com/eBay/parallec/blob/master/src/test/java/io/parallec/core/main/udp/ParallelClientUdpBasicTest.java), with more to come.  - [**Set Target Hosts**](http://www.parallec.io/docs/submit-task/#set-target-hosts) from list, string, line by line text, json path, from local or remote URLs. - [**Full Documentation**](http://www.parallec.io/docs/) - [**Javadoc**](http://www.parallec.io/javadoc/index.html?io/parallec/core/package-summary.html) - [**Ping Demo**](https://github.com/eBay/parallec/blob/master/README.md#demos) Ping 8000 Servers within 11.1 Seconds, performance test vs. [FPing](http://fping.org/).   ### User Group  * Ask a question, and keep up to date on the library development by joining the discussion group / forum: [Parallec.io Google Group](https://groups.google.com/forum/#!forum/parallec).  * Feel free to submit a [Github Issue](https://github.com/eBay/parallec/issues/new) for any questions and suggestions too. * Check [FAQ](http://www.parallec.io/docs/faq/).   ### Use Cases  1. Scalable web server monitoring, management, and configuration push, ping check. 1. Asset / server status discovery, remote task execution in agent-less(parallel SSH) or agent based (parallel HTTP/TCP) method. 1. Scalable API aggregation and processing with flexible destination with your favorate message queue / storage / alert engine. 1. Orchestration and work flows on multiple web servers.  1. Parallel different requests with controlled concurrency to a single server: as a parallec client for REST API enabled Database / Web Server CRUD operations. Variable replacement allowed in post body, url and headers. 1. Load testing with request template.  1. Network monitoring with active probing via UDP/Ping etc.     ## Features<a name="features"></a>  Parallec is built on Akka actors and [Async HTTP Client](https://github.com/AsyncHttpClient/async-http-client) / [Netty](http://netty.io/) / [Jsch](http://www.jcraft.com/jsch/).  The library focuses on HTTP while also enables scalable communication over SSH/Ping/TCP.  **90%+ Test coverage** assures you always find an example of each of feature.  1. **Exceedingly intuitive** interface with builder pattern similar to that in [Async HTTP Client](https://github.com/AsyncHttpClient/async-http-client), but handles concurrency behind the scenes. 1. **Generic response handler with context**. Special response context enables total freedom and convenience of processing each response your way. Process and aggregate data **anywhere** to Kafka, Redis, Elastic Search, mongoDB, CMS and etc.   1. **Flexible on when to invoke the handler**:  before (in worker thread) or after the aggregation (in master/manager thread). 1. **Flexible Input of target hosts**: Input target hosts from a list, string, JSON Path from local files or a remote URL 1. **Scalable and fast**, **infinitely scalable** with built-in [**Concurrency control**](http://www.ebaytechblog.com/2014/03/11/rest-commander-scalable-web-server-management-and-monitoring/#akka). 1. **Auto-progress polling** to enable task level concurrency with **Async API** for long jobs and  orchestrations. 1. **Request template** to handle non-uniform requests. 1. **Convenient single place handling success and failure cases**. Handle in a single function where you can get the response including the actual response if success; or stacktrace and error details if failures. 1. **Capacity aware task scheduler** helps you to auto queue up and fire tasks when capacity is insufficient. (e.g. submit consecutively 5 tasks each hitting 100K websites with default concurrency will result in a queue up) 1. **Fine-grained task progress tracking** helps you track the the progress each individual task status. Of a parallel task on 1000 target hosts, you may check status on any single host task, and percentage progress on how many are completed. 1. **Fine-grained task cancelation** on whole/individual request level. Of a parallel task on 1000 target hosts, you may cancel a subset of target hosts or cancel the whole parallel task anytime. 1. **Status-code-aggregation** is provided out of the box. 1. **Parallel Ping** supports both InetAddress.reachable ICMP (requires root) and Process based ping with retries.  Performance testing shows it is **2x the speed of** than best-effort tuned FPing on pinging on 1500 targets. (2.2 vs 4.5 sec) 1. **Parallel SSH** supports both key and password based login and task cancellation. 1. **Parallel TCP/UDP** supports idle timeout based channel closes.    ## Motivation - Flexible response handling and immediate processing embedded in other applications. - Handle async APIs with auto progress polling for task level concurrency control. - Support of other protocols, and [more](https://github.com/eBay/parallec/blob/master/README.md#features).. - Anyone can write 20 lines to make his/her application become [REST Commander](http://www.restcommander.com/).  With the feedbacks, lessons, and improvements from the past year of internal usage and open source of **[REST Commander](http://www.restcommander.com)**, we now made the core of REST Commander as an easy to use standalone library. We added [**15+ new**](https://github.com/eBay/parallec/blob/master/README.md#compare) features, rewritten 70%+ of the code, with [**90%+ test coverage**](https://codecov.io/github/eBay/parallec) for confident usage and contribution. This time we also structure it better so that most internal development can be directly made here.   ## Watch Parallec in Action<a name="demos"></a>  [**Watch Demo**](https://www.youtube.com/watch?v=QcavegPMDms"Parallec demo - Click to Watch!"): Parallec Aggregates 100 websites status to elastic search and visualized with [20 lines of code](https://github.com/eBay/parallec-samples/blob/master/sample-apps/src/main/java/io/parallec/sample/app/http/Http100WebAggregateToElasticSearchApp.java).  <a title="Click to Watch HD version in Youtube" href="https://www.youtube.com/watch?v=QcavegPMDms"><img alt="20 lines parallec to elastic search demo" src="http://www.parallec.io/demos/elastic-web100-v3.gif" /></a>  **[Watch Demo on HTTP Calls on 8000 Servers](https://github.com/eBay/parallec/wiki/Parallec-Aggregates-HTTP-Responses-from-8000-Servers)**: **8,000** web server HTTP response aggregation to memory in **12** seconds / to ElasticSearch in **16** seconds.  [**Watch Ping Demo**](https://www.youtube.com/watch?v=9m1TFuO1Mys"Parallec Ping vs FPing demo - Click to Watch!"): Parallec is **2x Speed** of best-efforted tuned [FPing](http://fping.org) with same accurate results and pings 8000 servers within 11.1 seconds, details check [here](https://github.com/eBay/parallec/wiki/Parallec-pings-8000-servers-in-11.1-seconds).  <a title="Click to Watch HD version in Youtube" href="https://www.youtube.com/watch?v=9m1TFuO1Mys"><img alt="parallec pings 8000 servers in 11.1 seconds" src="http://www.parallec.io/demos/parallec-vs-fping-v1.gif" /></a>   ## Performance  Note that speed varies based on network speed, API response time, the slowest servers, timeout, and concurrency settings.  ##### HTTP  We conducted remote task execution API on 3,000 servers with response aggregated to elastic search, visualized within 15 seconds, by writing 25 lines of code.  With another faster API, calls to 8,000 servers in the same datacenter with response aggregated in memory in 12 seconds.   ##### Ping Parallec 2.2 seconds vs FPing 4.5 seconds on 1500 servers. Parallec is 2x the speed of [FPing](http://fping.org/) (after best-effort tuning : -i 1 -r 0 v3.12)  of pinging 1500 servers while getting the same ping results.  Parallec pings 8000 servers within 11.1 seconds with breeze.  As usual, don't rely on these numbers and perform your own benchmarks.  ## Compare Parallec vs REST Commander vs ThreadPools+Async Client<a name="compare"></a>  - Compared with java thread pool based solution, parallec gives you worry free concurrency control without constraints on thread size. Thread pools do not fit well when need to have a concurrency of 1000 (1000 threads..)  or need a different concurrency setting for each request. - Compared with single-threaded Node.js solutions, Parallec enables parallel computation-intensive response handling with multiple-cores.  - Similar issues with Python's global interpreter lock, and to use multiple CPU you will need to use costly multi-process. These are more suited for I/O only but no cpu intensive response processing.    In Parallec, you may handle response either in Worker (before aggregation: in parallel) or in Manager (after aggregation: single thread). [Read More..](http://www.parallec.io/docs/submit-task/#apis-on-response-handling)   For more related work review, please visit [here](http://www.ebaytechblog.com/2014/03/11/rest-commander-scalable-web-server-management-and-monitoring/#relatedwork).  |                                                        Features                                                       | Parallec | REST Commander | Thread Pools + Async Client | |:---------------------------------------------------------------------------------------------------------------------:|:--------:|:--------------:|:---------------------------:| |                               Embedded library with intuitive builder pattern interface                               |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |               Ready to use application with GUI wizard based request submission and response aggregation              |    <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >    |       <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                                 Simple concurrency control not limited by thread size                                 |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                             Immediate response handler without waiting all response return                            |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |             <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >             | |                               Capacity aware task scheduler and global capacity control                               |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | | Total freedom of response processing and API aggregation: Pluggable and generic response handler and response context |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |             <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                                        1 line plugin to enable SSL Client auth                                        |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                                                   90% Test Coverage                                                   |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                     Load target hosts from CMS query, JSON Path, text, list, string from URL/local                    |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                  Task level concurrency and orchestration for Async APIs: auto polling task progress                  |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                                          Task level configuration on timeout and replacing Async HTTP Client                                          |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                           Async and sync task control with progress polling and cancellation                          |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                                Scalable Parallel SSH with password and key based login                                |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >       |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |                    Proven scalability and speed on 100,000+ target hosts in Production environment                    |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |   Generic request template with variable replacement for sending different requests to same/different target hosts    |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |   Scalable Ping with Retries    |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |   Scalable TCP/UDP with idle timeout    |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |   Flexible handler location at either worker (in parallel) or manager thread    |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |   Out-of-the-box two-level response aggregation on status code|    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |  Configurable response log trimming on intervals|    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             | |  Cancel task on a list of target hosts |    <img alt="Parallec-logo" src="http://www.parallec.io/images/yes.png" >   |       <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >      |              <img alt="Parallec-logo" src="http://www.parallec.io/images/no.png" >             |   ## Plugins  #### [SSL Client Auth Plugin](https://github.com/eBay/parallec-plugins)  ## [Change Log](https://github.com/eBay/parallec/blob/master/CHANGELOG.md)   ## [Contributors](https://github.com/eBay/parallec/blob/master/AUTHORS.txt)  We deeply thank all contributors for their effort.  - Lukasz Kryger  [http://stackoverflow.com/users/1240557/kryger](http://stackoverflow.com/users/1240557/kryger) - [billzwu](https://github.com/billzwu) - [faisal-hameed](https://github.com/faisal-hameed) - [fivesmallq](https://github.com/fivesmallq) - [bryant1410](https://github.com/bryant1410) - [djKooks](https://github.com/djKooks)  ## Authors  Parallec is served to you by [Yuanteng (Jeff) Pei](https://www.linkedin.com/in/peiyuant) and [Teng Song](https://www.linkedin.com/pub/teng-song/49/763/713), [Cloud Infrastructure & Platform Services (CIPS)](https://helpusbuild.ebayc3.com/) at eBay Inc. (original authors)  ## Credits & Acknowledgement  - We thanks our manager [Andy Santosa](https://www.linkedin.com/pub/andy-santosa/0/230/305), project manager [Marco Rotelli](https://www.linkedin.com/pub/marco-rotelli/2/25/54), [Cloud Infrastructure & Platform Services (CIPS)](https://helpusbuild.ebayc3.com/) and legal for the big support on this project and the open source effort. - The auto-progress polling is inspired by [lightflow](https://github.com/yubin154/lightflow). - We thank [openpojo](https://github.com/oshoukry/openpojo) and the author Osman Shoukry for his help on making the openpojo more accessible for us to use in our project. - We thank [AsyncHttpClient](https://github.com/AsyncHttpClient/async-http-client) and Stephane Landelle for guidance.    ## Contributions <div style="text-align:right">   <img src="https://github.com/eBay/Winder/blob/master/docs/ebaysf-open-x.png" width="150px"/> </div> Any helpful feedback is more than welcome. This includes feature requests, bug reports, pull requests, constructive feedback, and etc. You must agree on [this](https://github.com/eBay/parallec/blob/master/CONTRIBUTING.md) before submitting a [pull](https://github.com/eBay/parallec/pulls) request.   ## Licenses  Code licensed under Apache License v2.0  © 2015-2017 eBay Software Foundation
searchbox-io/Jest	JEST ====  [![Build Status](https://travis-ci.org/searchbox-io/Jest.svg?branch=master)](https://travis-ci.org/searchbox-io/Jest) [![Coverage Status](https://coveralls.io/repos/searchbox-io/Jest/badge.svg?branch=master)](https://coveralls.io/r/searchbox-io/Jest?branch=master) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.searchbox/jest/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.searchbox/jest)   Jest is a Java HTTP Rest client for [ElasticSearch][es].  ElasticSearch is an Open Source (Apache 2), Distributed, RESTful, Search Engine built on top of Apache Lucene.  ElasticSearch already has a Java API which is also used by ElasticSearch internally, [but Jest fills a gap, it is the missing client for ElasticSearch Http Rest interface](#comparison-to-native-api).  >Read great [introduction][ibm] to ElasticSearch and Jest from IBM Developer works.   Documentation --------------------- For the usual Jest Java library, that you can use as a maven dependency, please refer to [the README at jest module][readme].  For the Android port please refer to [the README at jest-droid module][droidreadme].   Compatibility ------------ Jest Version | Elasticsearch Version --- | --- >= 2.0.0 | 2.0 0.1.0 - 1.0.0 | 1.0 <= 0.0.6 | < 1.0  Also see [changelog][changelog] for detailed version history.   Support and Contribution ------------ All questions, bug reports and feature requests are handled via the [GitHub issue tracker][issuetracker] which also acts as the knowledge base. Please see the [Contribution Guidelines][contributing] for more information.   <a id="comparison"></a>Comparison to native API --------------------- >There are several alternative clients available when working with ElasticSearch from Java, like Jest that provides a POJO marshalling mechanism on indexing and for the search results. In this example we are using the Client that is included in ElasticSearch. By default the client doesn't use the REST API but connects to the cluster as a normal node that just doesn't store any data. It knows about the state of the cluster and can route requests to the correct node but supposedly consumes more memory. For our application this doesn't make a huge difference but for production systems that's something to think about. ><cite>-- [Florian Hopf](http://blog.florian-hopf.de/2013/05/getting-started-with-elasticsearch-part.html)</cite>  <!-- --> >So if you have several ES clusters running different versions, then using the native (or transport) client will be a problem, and you will need to go HTTP (and Jest is the main option I think). If versioning is not an issue, the native client will be your best option as it is cluster aware (thus knows how to route your queries and does not need another hop), and also moves some computation away from your ES cluster (like merging search results that will be done locally instead of on the data node). ><cite>-- [Rotem Hermon](http://www.quora.com/ElasticSearch/What-is-the-best-client-library-for-elasticsearch)</cite>  <!-- --> >ElasticSearch does not have Java rest client. It has only native client comes built in. That is the gap. You can add security layer to HTTP but native API. That is why none of SAAS offerings can be used with native api. ><cite>-- [Searchly](https://twitter.com/searchboxio)</cite>   Thanks --------------------- Thanks to [JetBrains][jetbrains] for providing a license for [IntelliJ IDEA][idea] to develop this project.  <a href="https://www.jetbrains.com/idea/features/"><img src="https://rawgit.com/searchbox-io/Jest/master/logo_IntelliJIDEA.svg" height="50px" /></a>  We also would like to thank the following people for their significant contributions. * [Andrea Turli](https://github.com/andreaturli) * [Andrej Kazakov](https://github.com/andrejserafim) * [asierdelpozo](https://github.com/asierdelpozo) * [Clayton Stout](https://github.com/cfstout) * [Dominic Tootell](https://github.com/tootedom)       * [Erik Dreyer](https://github.com/edreyer) * [Erik Schuchmann](https://github.com/eschuchmann) * [Fernandez Ludovic](https://github.com/ldez) * [Filippo Rossoni](https://github.com/filippor) * [FrancoisThareau](https://github.com/FrancoisThareau) * [happyprg](https://github.com/happyprg) * [Igor Kupczyński](https://github.com/puszczyk) * [Kristoffer Renholm](https://github.com/renholm) * [Mark Woon](https://github.com/markwoon) * [Martin W. Kirst](https://github.com/nitram509) * [Matthew Bogner](https://github.com/matthewbogner) * [Min Cha](https://github.com/MinCha) * [Neil Gentleman](https://github.com/nigelzor) * [Pavel Zalunin](https://github.com/whiter4bbit) * [phospodka](https://github.com/phospodka) * [Steven Rapp](https://github.com/srapp) * [Stig Brautaset](https://github.com/stig) * [Thomas Mühlfriedel](https://github.com/tomsen-san)   Copyright and License ---------------------  Copyright 2013 www.searchly.com  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this work except in compliance with the License. You may obtain a copy of the License in the LICENSE file, or at:  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.    [es]: http://www.elasticsearch.org [ibm]: http://www.ibm.com/developerworks/java/library/j-javadev2-24/index.html?ca=drs- [readme]: https://github.com/searchbox-io/Jest/tree/master/jest [droidreadme]: https://github.com/searchbox-io/Jest/tree/master/jest-droid [changelog]: https://github.com/searchbox-io/Jest/wiki/Changelog [issuetracker]: https://github.com/searchbox-io/Jest/issues [contributing]: https://github.com/searchbox-io/Jest/blob/master/CONTRIBUTING.md [jetbrains]: http://www.jetbrains.com/ [idea]: http://www.jetbrains.com/idea/
Dreampie/Resty	Resty 一款极简的restful轻量级的web框架 ===========  更新说明  ----  - [x] feature/20170203 强化Stringer工具和让日志支持彩色输出方便开发者调试 [@t-baby](https://github.com/t-baby)  ----  [![Join the chat at https://gitter.im/Dreampie/Resty](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Dreampie/Resty?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [![Issue Stats](http://issuestats.com/github/Dreampie/Resty/badge/pr?style=flat)](http://issuestats.com/github/Dreampie/Resty)  [![Issue Stats](http://issuestats.com/github/Dreampie/Resty/badge/issue?style=flat)](http://issuestats.com/github/Dreampie/Resty) <a href="http://dreampie.gitbooks.io/resty-chs/content/index.html" target="_blank">开发文档</a>  如果你还不是很了解restful，或者认为restful只是一种规范不具有实际意义，推荐一篇osc两年前的文章：[RESTful API 设计最佳实践](http://www.oschina.net/translate/best-practices-for-a-pragmatic-restful-api)  和 Infoq的一篇极其理论的文章  [理解本真的REST架构风格](http://www.infoq.com/cn/articles/understanding-restful-style) 虽然有点老，介绍的也很简单，大家权当了解，restful的更多好处，还请google  拥有jfinal/activejdbc一样的activerecord的简洁设计，使用更简单的restful框架  restful的api设计，是作为restful的服务端最佳选择（使用场景：客户端和服务端解藕，用于对静态的html客户端（mvvm等），ios，andriod等提供服务端的api接口）  Java开发指南:[Java style guide](https://github.com/Dreampie/java-style-guide)  Api设计指南:[Http api design](https://github.com/Dreampie/http-api-design-ZH_CN)  Resty例子： [resty-samples](https://github.com/Dreampie/resty-samples)(纯接口) [resty-demo](https://github.com/Dreampie/resty-demo)(带界面)  如果你在考虑前后端分离方案，推荐resty+vuejs，https://github.com/Dreampie/vuejs2-demo  开发群: <a target="_blank" href="http://shang.qq.com/wpa/qunwpa?idkey=8fc9498714ebbc3675cc5a5035858004154ef4645ebc9c128dfd76688d32179b"><img border="0" src="http://pub.idqqimg.com/wpa/images/group.png" alt="极简Restful框架 - Resty" title="极简Restful框架 - Resty"></a>  其他开发者贡献的插件:[Beetl扩展(大鹏)](https://github.com/zhaopengme/Resty-ext)  [Shiro扩展(zhoulieqing)](http://git.oschina.net/zhoulieqing/resty-shiro) [MongoPlugin(T-baby)](https://github.com/T-baby/MongoDB-Plugin)   > 有兴趣一起维护该框架的，可以联系我，进入合作开发  > 规范：提前说明功能，新建分支 `feature/日期` 功能 `fix/日期` 修复 在readme里添加一个TODO list描述  > - [x] feature/20161228 a task list item done [@Dreampie](https://github.com/Dreampie)  > - [ ] feature/20161229 a task list item todo [@Dreampie](https://github.com/Dreampie)  > 注意代码2格缩进，最后所有合作者一起代码review，合格之后合并到master  maven使用方式：  1. 添加依赖包 ```xml <dependency>     <groupId>cn.dreampie</groupId>     <artifactId>resty-route</artifactId>     <version>1.3.0.RELEASE</version> </dependency> ```  2.如果使用带有SNAPSHOT后缀的包，请添加该仓库 ```xml <repositories>     <repository>       <id>oss-snapshots</id>       <url>https://oss.sonatype.org/content/repositories/snapshots</url>       <releases>         <enabled>true</enabled>       </releases>       <snapshots>         <enabled>true</enabled>       </snapshots>     </repository>   </repositories> ```  一、独有优点： -----------  重大更新：  1.3.0更新内容： 使用jetty作为嵌入式热加载默认实现(只要java文件进行编译就会重新加载)，resty-captcha验证码功能...  1.2.0更新内容：使用header来控制api版本，基于数据源的读写分离，更简单的tableSetting.[详情查看](http://www.oschina.net/news/68791/resty-1-2-0-snapshot)  1.1.0版本重大更新：快速接入spring，缓存，加密，header，XForwardedSupports等，[详情查看](http://www.oschina.net/news/67001/resty-1-1-0-snapshot)   Record的时代已经到来，你完全不用使用任何的model来执行你的数据 ```java //创建record的执行器  针对sec_user表 并开启缓存 Record recordDAO = new Record("sec_user"); //使用当前数据源和表数据 new一个对象来保存数据 recordDAO.reNew().set("属性", "值").save(); Record r1 = recordDAO.reNew().set("属性", "值"); Record r2 = recordDAO.reNew().set("属性", "值"); //批量保存 recordDAO.save(r1, r2); //更新 r2.set("属性", "值").update() //查询全部 List<Record> records = recordDAO.findAll(); //条件查询 recordDAO.findBy(where,paras) //分页查询 Page<Record> records = recordDAO.paginateAll(); //根据id删除 recordDAO.deleteById("1");  //本次查询放弃使用cache  recordDAO.unCache().findBy(where,paras); //把record的数据源切换到dsmName数据源上 recordDAO.useDS(dsmName).findBy(where,paras);  //等等，完全摆脱model，实现快速操作数据  ```  Model支持动态切换数据源和本次查询放弃使用cache ```java User dao=new User(); //本次查询放弃使用cache  dao.unCache().findBy(where,paras); //把model的数据源切换到dsmName数据源上 dao.useDS(dsmName).findBy(where,paras);  ```   //数据库和全局参数配置移植到application.properties  详情参看resty-example  ```java #not must auto load app.encoding=UTF-8 app.devEnable=true app.showRoute=false app.cacheEnabled=true #默认使用ehcacheProvider #app.cacheProvider=cn.dreampie.cache.redis.RedisProvider  ##druid plugin auto load db.default.url=jdbc:mysql://127.0.0.1/example?useUnicode=true&characterEncoding=UTF-8 db.default.user=dev db.default.password=dev1010 db.default.dialect=mysql  #c3p0配置 c3p0.default.minPoolSize=3 c3p0.default.maxPoolSize=20  #druid配置 #druid.default.initialSize=10 #druid.default.maxPoolPreparedStatementPerConnectionSize=20 #druid.default.timeBetweenConnectErrorMillis=1000 #druid.default.filters=slf4j,stat,wall  #flyway database migration auto load flyway.default.valid.clean=true flyway.default.migration.auto=true flyway.default.migration.initOnMigrate=true   db.demo.url=jdbc:mysql://127.0.0.1/demo?useUnicode=true&characterEncoding=UTF-8 db.demo.user=dev db.demo.password=dev1010 db.demo.dialect=mysql #druid druid.demo.initialSize=10 druid.demo.maxPoolPreparedStatementPerConnectionSize=20 druid.demo.timeBetweenConnectErrorMillis=1000 druid.demo.filters=slf4j,stat,wall #flyway flyway.demo.valid.clean=true flyway.demo.migration.auto=true flyway.demo.migration.initOnMigrate=true    //数据库的配置精简  自动从文件读取参数  只需配置model扫描目录 和dsmName public void configPlugin(PluginLoader pluginLoader) {   //第一个数据库   ActiveRecordPlugin activeRecordPlugin = new ActiveRecordPlugin(new DruidDataSourceProvider("default"), true);   activeRecordPlugin.addIncludePaths("cn.dreampie.resource");   pluginLoader.add(activeRecordPlugin); }  ```    1.极简的route设计，完全融入普通方法的方式，方法参数就是请求参数，方法返回值就是数据返回值  ```java   @GET("/users/:name")   //在路径中自定义解析的参数 如果有其他符合 也可以用 /users/{name}   // 参数名就是方法变量名  除路径参数之外的参数也可以放在方法参数里  传递方式 user={json字符串}   public Map find(String name,User user) {     // return Lister.of(name);     return Maper.of("k1", "v1,name:" + name, "k2", "v2");     //返回什么数据直接return   } ```  2.极简的activerecord设计，数据操作只需短短的一行,支持批量保存对象  ```java   //批量保存   User u1 = new User().set("username", "test").set("providername", "test").set("password", "123456");   User u2 = new User().set("username", "test").set("providername", "test").set("password", "123456");   User.dao.save(u1,u2);    //普通保存   User u = new User().set("username", "test").set("providername", "test").set("password", "123456");   u.save();    //更新   u.update();   //条件更新   User.dao.updateBy(columns,where,paras);   User.dao.updateAll(columns,paras);    //删除   u.deleted();   //条件删除   User.dao.deleteBy(where,paras);   User.dao.deleteAll();    //查询   User.dao.findById(id);   User.dao.findBy(where,paras);   User.dao.findAll();    //分页   User.dao.paginateBy(pageNumber,pageSize,where,paras);   User.dao.paginateAll(pageNumber,pageSize); ```  3.极简的客户端设计，支持各种请求，文件上传和文件下载（支持断点续传）  ```java   Client httpClient=null;//创建客户端对象   //启动resty-example项目，即可测试客户端   String apiUrl = "http://localhost:8081/api/v1.0";   //如果不需要 使用账号登陆   //httpClient = new Client(apiUrl);   //如果有账号权限限制  需要登陆   httpClient = new Client(apiUrl, "/tests/login", "u", "123");    //该请求必须  登陆之后才能访问  未登录时返回 401  未认证   ClientRequest authRequest = new ClientRequest("/users");   ClientResult authResult = httpClient.build(authRequest).get();   System.out.println(authResult.getResult());    //get   ClientRequest getRequest = new ClientRequest("/tests");   ClientResult getResult = httpClient.build(getRequest).get();   System.out.println(getResult.getResult());    //post   ClientRequest postRequest = new ClientRequest("/tests");   postRequest.addParam("test", Jsoner.toJSONString(Maper.of("a", "谔谔")));   ClientResult postResult = httpClient.build(postRequest).post();   System.out.println(postResult.getResult());    //put   ClientRequest putRequest = new ClientRequest("/tests/x");   ClientResult putResult = httpClient.build(putRequest).put();   System.out.println(putResult.getResult());     //delete   ClientRequest deleteRequest = new ClientRequest("/tests/a");   ClientResult deleteResult = httpClient.build(deleteRequest).delete();   System.out.println(deleteResult.getResult());     //upload   ClientRequest uploadRequest = new ClientRequest("/tests/resty");   uploadRequest.addUploadFiles("resty", ClientTest.class.getResource("/resty.jar").getFile());   uploadRequest.addParam("des", "test file  paras  测试笔");   ClientResult uploadResult = httpClient.build(uploadRequest).post();   System.out.println(uploadResult.getResult());     //download  支持断点续传   ClientRequest downloadRequest = new ClientRequest("/tests/file");   downloadRequest.setDownloadFile(ClientTest.class.getResource("/resty.jar").getFile().replace(".jar", "x.jar"));   ClientResult downloadResult = httpClient.build(downloadRequest).get();   System.out.println(downloadResult.getResult()); ```   4.支持多数据源和嵌套事务（使用场景：需要访问多个数据库的应用，或者作为公司内部的数据中间件向客户端提供数据访问api等）  ```java   // 在resource里使用事务,也就是controller里，rest的世界认为所以的请求都表示资源，所以这儿叫resource   @GET("/users")   @Transaction(name = {"default", "demo"}) //多数据源的事务，如果你只有一个数据库  直接@Transaction 不需要参数   public User transaction() {   //TODO 用model执行数据库的操作  只要有操作抛出异常  两个数据源 都会回滚  虽然不是分布式事务  也能保证代码块的数据执行安全   }    // 如果你需要在service里实现事务，通过java动态代理（必须使用接口，jdk设计就是这样）   public interface UserService {     @Transaction(name = {"demo"})//service里添加多数据源的事务，如果你只有一个数据库  直接@Transaction 不需要参数     public User save(User u);   }   // 在resource里使用service层的 事务   // @Transaction(name = {"demo"})的注解需要写在service的接口上   // 注意java的自动代理必须存在接口   // TransactionAspect 是事务切面 ，你也可以实现自己的切面比如日志的Aspect，实现Aspect接口   // 再private UserService userService = AspectFactory.newInstance(new UserServiceImpl(), new TransactionAspect(),new LogAspect());   private UserService userService = AspectFactory.newInstance(new UserServiceImpl(), new TransactionAspect()); ```  5.极简的权限设计,可以通过cache支持分布式session，你只需要实现一个简单接口和添加一个拦截器，即可实现基于url的权限设计  ```java   public void configInterceptor(InterceptorLoader interceptorLoader) {     //权限拦截器 放在第一位 第一时间判断 避免执行不必要的代码     interceptorLoader.add(new SecurityInterceptor(new MyAuthenticateService()));   }    //实现接口   public class MyAuthenticateService implements AuthenticateService {     //登陆时 通过name获取用户的密码和权限信息     public Principal findByName(String name) {       DefaultPasswordService defaultPasswordService = new DefaultPasswordService();        Principal principal = new Principal(name, defaultPasswordService.hash("123"), new HashSet<String>() {{         add("api");       }});       return principal;     }     //基础的权限总表  所以的url权限都放在这儿  你可以通过 文件或者数据库或者直接代码 来设置所有权限     public Set<Credential> loadAllCredentials() {       Set<Credential> credentials = new HashSet<Credential>();       credentials.add(new Credential("GET", "/api/v1.0/users**", "users"));       return credentials;     }   } ```  6.极简的缓存设计，可扩展，非常简单即可启用model的自动缓存功能  ```java   //启用缓存并在要自动使用缓存的model上   //config application.properties  app.cacheEnabled=true   //开启缓存@Table(name = "sec_user", cached = true)    @Table(name = "sec_user", cached = true)   public class User extends Model<User> {     public static User dao = new User();    } ```  7.下载文件，只需要直接return file  ```java   @GET("/files")   public File file() {     return new File(path);   } ```  8.上传文件，注解配置把文件写到服务器  ```java   @POST("/files")   @FILE(dir = "/upload/") //配置上传文件的相关信息   public UploadedFile file(UploadedFile file) {     return file;   } ```  9.当然也是支持传统的web开发，你可以自己实现数据解析，在config里添加自定义的解析模板  ```java   public void configConstant(ConstantLoader constantLoader) {     // 通过后缀来返回不同的数据类型  你可以自定义自己的  render  如：FreemarkerRender     //默认已添加json和text的支持，只需要把自定义的Render add即可     // constantLoader.addRender("json", new JsonRender());   } ```  二、运行example示例： -----------------  1.在本地mysql数据库里创建demo,example数据库，对应application.properties的数据库配置  2.运行resty-example下的pom.xml->flyway-maven-plugin:migrate，自动根具resources下db目录下的数据库文件生成数据库表结构  3.运行resty-example下的pom.xml->tomcat6-maven-plugin:run,启动example程序  提醒:推荐idea作为开发ide，使用分模块的多module开发  License <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License V2</a>  捐赠： [支付宝](https://raw.githubusercontent.com/Dreampie/Resty/master/alipay.png)
Nextome/GeoJsonify	# GeoJsonify ## Simply add GeoJson layers to your Maps [![](https://jitpack.io/v/Nextome/GeoJsonify.svg)](https://jitpack.io/#Nextome/GeoJsonify) <br> ### Supported Map Services:  * [Google Maps](https://maps.google.com/);  * [Open Street Map](https://www.openstreetmap.org);  * [MapBox](https://www.mapbox.com/);  ### How to use 1. Add JitPack in your **root build.gradle** at the end of repositories: ```gradle 	allprojects { 		repositories { 			... 			maven { url 'https://jitpack.io' } 		} 	} ``` 2. Add the dependency in your **module build.gradle**: ```gradle 	dependencies { 	        compile 'com.github.Nextome:GeoJsonify:v1.0.0' 	}  ```  3. Simply use the method: ```java GeoJsonify.geoJsonifyMap(map, List<Uri> jsonUris, int color, Context context); ```  * **map** is the map where the layers will be added. It can be a *GoogleMap* (Google Maps) / *MapboxMap* (Mapbox) / *MapView* (OSM)  * **jsonUris** is a list of URIs, each one with a different .geojson file to parse. * **color** is the color of the lines that will be rendered.  Alternativly, you can also specify a different color for each layer using ```java GeoJsonify.geoJsonifyMap(map, List<Uri> jsonUris, List<Integer> colors, Context context); ```  ### Javadoc Available [here](https://nextome.github.io/GeoJsonify/index.html).  ### Example (Google Maps) Here's a full Google Maps implementation: ```java public class GoogleMapsActivity extends MapBaseActivity implements OnMapReadyCallback {      @Override     protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         this.getIntentExtras(getIntent());         setContentView(R.layout.activity_maps);          SupportMapFragment mapFragment = (SupportMapFragment) getSupportFragmentManager()                 .findFragmentById(R.id.map);         mapFragment.getMapAsync(this);     }      @Override     public void onMapReady(GoogleMap googleMap) {         Uri uri1 = Uri.fromFile(new File(Environment.getExternalStorageDirectory().getPath() + "italy.geojson"));         Uri uri2 = Uri.fromFile(new File(Environment.getExternalStorageDirectory().getPath() + "puglia.geojson"));          List<Uri> URIs = new ArrayList<>();         URIs.add(uri1);         URIs.add(uri2);                  try {             GeoJsonify.geoJsonifyMap(googleMap, URIs, Color.BLACK, this.getContext());         } catch (IOException e) {             e.printStackTrace();             Toast.makeText(this.getContext(), "Unable to read file", Toast.LENGTH_SHORT).show();         } catch (JSONException e) {             e.printStackTrace();             Toast.makeText(this.getContext(), "Unable to parse file", Toast.LENGTH_SHORT).show();         }     } } ```  ![](https://lh3.googleusercontent.com/IOziKkwBfPIyOLsQhWddI36wqQJs2lHB34g8A2JyrYrnTNp6Q3HCrtkIkfAdB8qWppgA=h900-rw)   For more examples with all services, see our example app **GeoJson Viewer** <br> <br> <br> # GeoJson Viewer ## View GeoJson files on your Android Device  [![](https://github.com/Nextome/geojson-viewer/blob/master/resources/cover.jpg)](https://youtu.be/qo7hc_iLI6s)  <a href='com.nextome.geojsonviewer?pcampaignid=MKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1'><img width="320" alt='Get it on Google Play' src='https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png'/></a>   ## How to use Simply choose a _GeoJson_ file on your device and select a map provider from the list.  Supported Map Services:  * [Google Maps](https://maps.google.com/);  * [Open Street Map](https://www.openstreetmap.org);  * [MapBox](https://www.mapbox.com/);   ## Examples #### Map with Polygons, Lines and Points <img src="https://github.com/Nextome/geojson-viewer/blob/master/resources/example_path.png" width="480">  #### Different Services Same _.geojson_ file opened with Google Maps, Open Street Map and MapBox  <img src="https://github.com/Nextome/geojson-viewer/blob/master/resources/example_gmaps.png" width="200"> <img src="https://github.com/Nextome/geojson-viewer/blob/master/resources/example_osm.png" width="200"> <img src="https://github.com/Nextome/geojson-viewer/blob/master/resources/example_mapbox.png" width="200">  ## How to build the project  * Clone the project;  * Open Android Studio and select Import Project;  * Add your own Google Maps and Mapbox API keys in _strings.xml_;   We'll also be happy to accept your pull requests.   ## Read More Read more about adding a GeoJson layer on maps on our [blog](https://medium.com/nextome/show-a-geojson-layer-on-google-maps-osm-mapbox-on-android-cd75b8377ba).    ## Install the app GeoJson Viewer is available for free on [Google Play](https://play.google.com/store/apps/details?id=com.nextome.geojsonviewer).  ## Licence GeoJson Viewer is licensed under the [Apache License 2.0](https://github.com/Nextome/geojson-viewer/blob/master/LICENSE).   Made at [Nextome](http://nextome.org/).
aalmiray/Json-lib	Json-lib with auto-expansion of properties =============  The XML serialization is extended to handle automatic expansion of properties for arrays that contains objects.  Source file to be serialized to and from JSON ------------- ```xml <?xml version="1.0" encoding="UTF-8" standalone="yes"?> <Document DOMVersion="8.0" Self="d">     <x:xmpmeta xmlns:x="adobe:ns:meta/" x:xmptk="Adobe XMP Core 5.3-c011 66.145661, 2012/02/06-14:56:27">         <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">             <rdf:Description rdf:about=""                              xmlns:dc="http://purl.org/dc/elements/1.1/">                 <dc:format>application/x-indesign</dc:format>             </rdf:Description>             <rdf:Description rdf:about=""                              xmlns:xmp="http://ns.adobe.com/xap/1.0/">                 <xmp:CreatorTool>Adobe InDesign CS6 (Macintosh)</xmp:CreatorTool>             </rdf:Description>             <rdf:Description rdf:about=""                              xmlns:xmpMM="http://ns.adobe.com/xap/1.0/mm/">                 <xmpMM:InstanceID>xmp.iid:D093CC710A2068118083AFA2F0AAE3ED</xmpMM:InstanceID>             </rdf:Description>         </rdf:RDF>     </x:xmpmeta> </Document> ```  file.xml  Original json-lib -------  This code  ```java XMLSerializer serializer = new XMLSerializer(); serializer.setTypeHintsEnabled(false);  JSON jsonRepresentation = serializer.readFromFile( "file.xml" );  String xml = serializer.write( jsonRepresentation ); System.out.writeToReadme(xml) ```  will render:  ```xml <o DOMVersion="8.0" Self="d">     <x:xmpmeta xmptk="Adobe XMP Core 5.3-c011 66.145661, 2012/02/06-14:56:27" xmlns:x="adobe:ns:meta/">         <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">             <rdf:Description>                 <e about="" xmlns:dc="http://purl.org/dc/elements/1.1/">                     <dc:format>application/x-indesign</dc:format>                 </e>                 <e about="" xmlns:xmp="http://ns.adobe.com/xap/1.0/">                     <xmp:CreatorTool>Adobe InDesign CS6 (Macintosh)</xmp:CreatorTool>                 </e>                 <e about="" xmlns:xmpMM="http://ns.adobe.com/xap/1.0/mm/">                     <xmpMM:InstanceID>xmp.iid:D093CC710A2068118083AFA2F0AAE3ED</xmpMM:InstanceID>                 </e>             </rdf:Description>         </rdf:RDF>     </x:xmpmeta> </o> ```   AutoExpand -------  This code  ```java XMLSerializer serializer = new XMLSerializer(); serializer.setTypeHintsEnabled(false); serializer.setPerformAutoExpansion(true);  JSON jsonRepresentation = serializer.readFromFile( "file.xml" );  String xml = serializer.write( jsonRepresentation ); System.out.writeToReadme(xml) ```  will render:  ```xml <o DOMVersion="8.0" Self="d"> <x:xmpmeta xmptk="Adobe XMP Core 5.3-c011 66.145661, 2012/02/06-14:56:27" xmlns:x="adobe:ns:meta/">     <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">         <rdf:Description about="" xmlns:dc="http://purl.org/dc/elements/1.1/">             <dc:format>application/x-indesign</dc:format>         </rdf:Description>         <rdf:Description about="" xmlns:xmp="http://ns.adobe.com/xap/1.0/">             <xmp:CreatorTool>Adobe InDesign CS6 (Macintosh)</xmp:CreatorTool>         </rdf:Description>         <rdf:Description about="" xmlns:xmpMM="http://ns.adobe.com/xap/1.0/mm/">             <xmpMM:InstanceID>xmp.iid:D093CC710A2068118083AFA2F0AAE3ED</xmpMM:InstanceID>         </rdf:Description>     </rdf:RDF> </x:xmpmeta> </o> ```
otto-de/jsonhome	# JSONHOME  Libraries to publish and use [json-home](http://tools.ietf.org/html/draft-nottingham-json-home-02) documents.  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/de.otto/jsonhome-core/badge.svg)](https://maven-badges.herokuapp.com/maven-central/de.otto/jsonhome-core)  [![Dependency Status](https://www.versioneye.com/user/projects/55ba7bcf65376200200018c5/badge.svg?style=flat)](https://www.versioneye.com/user/projects/55ba7bcf65376200200018c5)  ## 0. Draft 03  As you have possibly noticed, Marc Nottingham published [Draft 03](http://tools.ietf.org/html/draft-nottingham-json-home-03) a few days ago, containing a number of incompatible changes to the application/json-home format.  Beside of beeing incompatible, there is one problem: the 'representations' hint is now replaced by a 'formats' hint,  containing 'representation hints' - but these representation hints are  [currently](http://tools.ietf.org/html/draft-nottingham-json-home-03#section-5) not specified ('TBD').  Because of this, an implementation of draft 03 would not be an improvement compared to the  current implementation.   As soon as this problem is solved (possibly with draft 04?), I will implement support for the new format.  In order to avoid incompatibilities, the support will include feature toggles, so you can: * configure the desired format (draft 02, draft 03, ...) * publish a format containing old and new attributes * consume both versions on the client-side.  ## 1. Json-Home?  Think of an machine-readable alternative to an index.html in json format, describing the REST resources of an application. If the caller knows the format, no URIs must be constructed using string-magic. The resources of the application will become navigable.  An example from the draft specification:  ```    GET / HTTP/1.1    Host: example.org    Accept: application/json-home     HTTP/1.1 200 OK    Content-Type: application/json-home    Cache-Control: max-age=3600    Connection: close ``` ```json    {      "resources": {        "http://example.org/rel/widgets": {          "href": "/widgets/"        },        "http://example.org/rel/widget": {          "href-template": "/widgets/{widget_id}",          "href-vars": {            "widget_id": "http://example.org/param/widget"          },          "hints": {            "allow": ["GET", "PUT", "DELETE", "PATCH"],            "representations": ["application/json"],            "accept-patch": ["application/json-patch"],            "accept-post": ["application/xml"],            "accept-ranges": ["bytes"]          }        }      }    } ```  ## 2. Usage  Module jsonhome-spring is used to serve json-home documents from your application. Section 2.1 illustrates how to do this.  There is also a module jsonhome-client, used to access remote or local json-home documents. You may find an example in Section 2.2.  ### 2.1 Serving json-home documents  A simple Spring MVC controller is looking like this: ```java @Controller @RequestMapping(value = "/helloworld") public class HelloWorldController {      @RequestMapping(produces = "text/plain")     @ResponseBody     public String sayHello() {         return "Hello World!";     }      } ```  If you want to have a json-home document for the different entry-point resources of your application, you have to do the following:  ### 2.1.1 Add a dependency to jsonhome-spring to your application.  ### 2.1.2 Have a look at the hello-world example. You will have to:    * Specify the properties used by jsonhome (helloworld.properties):    ```   jsonhome.applicationBaseUri = <base uri of the resources of your application>   jsonhome.relationTypeBaseUri = <base uri of the link-relation types of your application>   ```    * Import the jsonhome-beans and add a property-placeholder configurer in your Spring bean config:    ```xml   <import resource="classpath*:jsonhome/jsonhome-beans.xml" />   <context:property-placeholder location="/WEB-INF/properties/helloworld.properties" />   ```  ### 2.1.3 Identify the entry-point resources of your RESTful application. If your resources are using hypermedia intensively, these will be only a few. If not, you will end up with many entry-point resources (resources that are not reachable by following links contained in other resources). If you find lots of them, this is most likely a hint that your REST API is not at level 3 of Richardsons Maturity Model (see http://martinfowler.com/articles/richardsonMaturityModel.html).  ### 2.1.4 Add a @Rel annotation to all controller methods, dealing with your entry-point resources: ```java @Controller @RequestMapping(value = "/helloworld") public class HelloWorldController {      @Rel("/rel/hello-world")     @RequestMapping(produces = "text/plain")     @ResponseBody     public String sayHello() {         return "Hello World!";     }  } ```  ### 2.1.5 Result That's basically all you have to do to get a json-home document like this: ```json { "resources" : {     "http://localhost:8080/jsonhome-example/rel/hello-world" : {       "href" : "http://localhost:8080/jsonhome-example/helloworld",       "hints" : {         "allow" : [           "GET"         ],         "representations" : [           "text/plain"         ]       }    } }} ``` The "http://localhost:8080" part of the URI is surely not a good idea in practice. In a real application,  you should use absolute URIs like "http://mycompany.com/rel/hello-world".  The example application is using the RelController in order to resolve the link-relation URIs. Just open the resource from /jsonhome-example/json-home using your browser, and you will see the result.  The RelController is able to serve a human-readable representation of the json-home document (as HTML).  This is especially important for developers using your REST resources, because it is easier to read than  JSON and - in contrast to hand-written documentation - never out-to-date.  You may want to enrich this documentation by adding @Doc annotations to your controller (or href-variables): ```java @Controller @RequestMapping(value = "/helloworld") @Doc(value = {"A link to a hello-world resource",                "Multiple lines of documentation are also supported"},      link = "http://example.org/some-external-documentation.html"      rel = "/rel/hello-world" ) public class HelloWorldController { ... ``` The rel attribute of the @Doc is referring to the link-relation type. The value and/or the link to the documentation will be rendered into the HTML documentation, rendered by the RelController using some Freemarker templates. Feel free to modify the templates to your needs.  There are some more possibilities like overriding href or href-template (using @Href or @HrefTemplate), specify required preconditions (etag, last-modified) or set the status (deprecated, gone) of a resource using @Hints.  Please have a look at the example again, or download the sources and check the unit-tests for more examples.  ### 2.2 Consuming json-home documents  There is also a very small client-library and a json-home parser available. If you want to consume a json-home document (access the linked resources without building "known" URIs using string-magic), you may try the following: ```java final URI rel = URI.create("http://example.org/rel/foo");  final JsonHomeClient client = new HttpJsonHomeClient(); final JsonHome jsonHome = client.get(URI.create("http://example.org/json-home"); if (jsonHome.hasResourceFor(rel)) {    final DirektLink link = jsonHome.getResourceFor(rel).asDirectLink();    final URI uri = link.getHref();    // GET the uri using your favorite HTTP client...    ... } ```  Accessing a templated link like http://example.org/{foo} is nearly as easy. Remember that in json-home, the variable foo is not identified by name, but by an URI (=>URI varType). ```java final URI rel = URI.create("http://example.org/rel/foo"); final URI varType = URI.create("http://example.org/var-type/foo");  final JsonHomeClient client = new HttpJsonHomeClient(); final JsonHome jsonHome = client.get(URI.create("http://example.org/json-home"); if (jsonHome.hasResourceFor(rel)) {    final TemplatedLink templatedLink = jsonHome.getResourceFor(rel).asTemplatedLink();    final URI uri = templatedLink.expandToUri(varType, "42");    // GET the uri using your favorite HTTP client...    ... } ```  The HttpJsonHomeClient is supporting HTTP caching so the client is not hitting the server all the time. You only should reuse the client instance, otherwise the caching (at least in the default in-memory implementation) will not work. You may want to force an update of a cached resource (for example, if a resource is not accessible  anymore): in this case you should call client.updateAndGet() instead of get().  Internally, the client is based on Apache's CachingHttpClient. You may want to use the same client to access the resource itself - but this is up to you. Providing a full "REST client" is out of scope of this project.   ## 3. More Features  There are some more features like: * jsonhome-jersey: a Jersey based implementation of jsonhome. * jsonhome-registry: a standalone server used to serve json-home documents for different environments (develop, test, live). The registry is also able to aggregate multiple json-home documents into one single document. * DocController: a (currently only Spring-based) controller used to serve Markdown documents. * HtmlController: a (currently only Spring-based) controller used serve a HTML representation of your json-home, enriched with documentation.  ## 4. Project Status  As of draft-nottingham-json-home-02, only the accept-patch hint is not fully supported. Spring is supporting HTTP PATCH with release 3.2. Because we are currently using Spring 3.1.*, accept-patch is only available with jsonhome-jersey.  Draft-nottingham-json-home-03 should be published in a few days or weeks. This will open more 'to be implemented' features.  A full support of the json-home spec is already planned, please have a look at the GitHub Issues. Release 1.0 should support the final specification. Until then, minor releases of the jsonhome project will be published.   The library is actively used (and developed) at otto (http://www.otto.de). You may consider it as "Beta" software.  ## 5. Licensing  The project is released under version 2.0 of the Apache License. See LICENSE.txt for details.  ## 6. Maven, Gradle  You can find all releases in Maven Central and in the public Sonatype repository:  https://oss.sonatype.org/content/repositories/releases  The current release is 0.3.1:  * de.otto:jsonhome-core:0.3.1 * de.otto:jsonhome-generator:0.3.1 * de.otto:jsonhome-spring:0.3.1 * de.otto:jsonhome-jersey:0.3.1 * de.otto:jsonhome-registry:0.3.1  Snapshot releases will be published here:  https://oss.sonatype.org/content/repositories/snapshots  The current snapshot-release is 0.3.2-SNAPSHOT:  * de.otto:jsonhome-core:0.3.2-SNAPSHOT * de.otto:jsonhome-generator:0.3.2-SNAPSHOT * de.otto:jsonhome-spring:0.3.2-SNAPSHOT * de.otto:jsonhome-jersey:0.3.2-SNAPSHOT * de.otto:jsonhome-registry:0.3.2-SNAPSHOT  There is no de.otto:jsonhome-example:* because this is only an example, you should not depend on it.  ## 7. Contributing  Every kind of feedback - also negative - is appreciated. Even more appreciated are contributions to the code base.  To contact us, please send an email to guido.steinacker@gmail.com  In order to contribute source code:  * Fork it * Create your feature branch (git checkout -b my-new-feature) * Commit your changes (git commit -am 'Added some feature') * Push to the branch (git push origin my-new-feature) * Create new Pull Request  ## 8. Links and Documentation  * **Json-Home draft specification**: [http://tools.ietf.org/html/draft-nottingham-json-home-02](http://tools.ietf.org/html/draft-nottingham-json-home-02)  * **URI Templates**: [http://tools.ietf.org/html/rfc6570](http://tools.ietf.org/html/rfc6570)  * **Link-Relation Types**: [http://tools.ietf.org/html/rfc5988](http://tools.ietf.org/html/rfc5988)  **Please also have a look at the project's [wiki pages](https://github.com/otto-de/jsonhome/wiki).**
zml2008/configurate	# Configurate Configurate is a simple configuration library released under the [Apache 2.0](LICENSE) that provides a node-tree representation of configurations in a variety of formats.  *Build Status*: [![Travis CI](https://travis-ci.org/zml2008/configurate.svg)](https://travis-ci.org/zml2008/configurate)  *Javadocs*: http://configurate.aoeu.xyz/apidocs   ## Building We use Maven, so this part is pretty easy.   Configurate requires JDK 8 to build and run.  Make sure Maven is installed and from the project's directory (the root of this repository), run `mvn clean install` to build Configuate and install its artifacts to the local Maven repository.  ## Usage  **Maven**: ```xml <dependency>     <groupId>ninja.leaping.configurate</groupId>     <artifactId>configurate-hocon</artifactId>     <version>3.3</version> <!-- Update this with the most recent version --> </dependency> ```   **Gradle**: ```groovy repositories {     mavenCentral() }  dependencies {     compile 'ninja.leaping.configurate:configurate-hocon:3.3' } ```  For other build systems, take a look at the [full list on the Configurate site](http://configurate.aoeu.xyz/configurate-hocon/dependency-info.html)  This dependency statement is for the hocon format implementation. Other formats managed in this repository use the same group id and versioning.  Now, to load: ```java ConfigurationLoader<CommentedConfigurationNode> loader = HoconConfigurationLoader.builder().setPath(file).build(); // Create the loader CommentedConfigurationNode node = loader.load(); // Load the configuration into memory  node.getNode("some", "value").getValue(); // Get the value ``` More detailed explanations of all the methods available in ConfigurationNode are available in the javadocs.  ## Contributing We love PRs! However, when contributing, here are some things to keep in mind:  - In general, we follow the Oracle style guidelines for code style - Please, please, please test PRs. It makes the process a lot easier for everybody
cowtowncoder/java-json-performance-benchmarks	# Overview  This project contains a set of performance micro-benchmarks, based on excellent [JMH](http://openjdk.java.net/projects/code-tools/jmh/) package. Benchmarks exercise JSON reading and/or writing performance, using widely-used, popular Java JSON libraries like:  * [GSON](https://github.com/google/gson) * [Jackson](https://github.com/FasterXML/jackson)     * Also: lighter-weight [Jackson-jr](https://github.com/FasterXML/jackson-jr)  as well as some of newer Java JSON library (or multi-format with JSON support)  options:  * [Boon](https://github.com/boonproject/boon/wiki/Boon-JSON-in-five-minutes) * [DSL-JSON](https://github.com/ngs-doo/dsl-json) (compatible with [DSL-Platform](https://dsl-platform.com/)) * [Johnzon](http://johnzon.incubator.apache.org/)     * pre-1.0, incubation release * [json-io](https://github.com/jdereg/json-io) * [json-parse](https://github.com/mitchhentges/json-parse) * [Juneau](http://juneau.apache.org/) * [Moshi](https://github.com/square/moshi): Android-optimized lib (by some of GSON authors)  The criteria for inclusion here is that for a library to be included it should  1. be published to the central Maven repository (so we can include official builds) 2. be able to read and write POJOs, not just "Lists and Maps" or "custom tree nodes library defines" (although some tests may also exercise these styles as well)  and for this reason some commonly used libraries (like old "org.json" library and "simple-json") are not included.  ## Usage  To run the tests, you will first need to build the test jar which contains all test code as well as libraries being tested. This can be done by:      mvn clean install  after this, tests are run the way `jmh` tests are, by just running "executable" jar with:      java [jvm-options] -jar target/microbenchmarks.jar [options] [test-regexp]  for example:      java -Xmx256m -jar target/microbenchmarks.jar -wi 4 -i 5 -f 9 ".*DZoneReadPojo.*read10FromStream"      java -Xmx256m -jar target/microbenchmarks.jar -wi 4 -i 5 -f 9 ".*DZoneWrite.*write10UsingStream"   both of which would run the "DZone" write test with 10 items, using 9 iterations of 5 seconds, with warmup time of 4 seconds; first test for read (JSON into POJO) and second write (POJO to JSON) performance.  All options are explained by jmh documentation; an easy way to see options available is to enter:      java -jar target/microbenchmarks.jar -h  ## Tests inspired by Dzone article  Following tests were written inspired by a [DZone Java Performance](https://t.co/10lR0tQJjV] article. The original tests had many unfortunate problems; starting with the fact that it does not perform proper warmup, nor run long enough to give statistically meaning results. One can also argue whether serialization as String is a meanginful test (since it is rarely used for production), but that test case is included as-is along with other options.  Test as implemented here relies on `jmh` to provide proper performance test setup, measurements, warmup, and to avoid common gotchas that plague naive Java performance tests. If not done so yet, you may want to read the longer explanation of [reasons to use JMH](http://psy-lob-saw.blogspot.com/2013/04/writing-java-micro-benchmarks-with-jmh.html). And for common problems with Java/JSON performance testing, you may want to read [On proper performance testing of Java JSON processing](http://www.cowtowncoder.com/blog/archives/2011/05/entry_455.html).  ### Writing a List of POJOs  There are 3 flavors of the test for writing out a List of 10, 1000 and 100,000 items of type `MeasurementRecord`:  * `write10AsString` (and `write1kAsString`, `write100kAsString`): serialization as `java.lang.String` * `write10UsingWriter` (`write1kUsingWriter`, `write100kUsingWriter`): serialization using a "do-nothing" `java.io.Writer` * `write10UsingStream` (`write1kUsingStream`, `write100kUsingStream`): serialization using a "do-nothing" `java.io.OutputStream`  the idea being that different types of output incur different kinds and amounts of overhead. For example, aggregating output as a `java.lang.String` requires much more memory allocation and (indirectly) more Garbage Collection for discarded JSON Strings. Conversely libraries are optimized differently for different types of output targets; some implement native output for all targets and others only have one basic target.  Size of each POJO is quite small; field names are long and values mostly numbers. This may not be the most commonly found kind of content, but was used by the original set up and is used here unmodified.  To run the test with a List of 10 items, serializing results as a Java String, you could use:      java -Xmx256m -jar target/microbenchmarks.jar ".*DZoneWrite.*write10AsString" -wi 4 -i 5 -f 9  or 1000 items into `OutputStream`      java -Xmx256m -jar target/microbenchmarks.jar ".*DZoneWrite.*write1kUsingStream" -wi 4 -i 5 -f 9  ### Reading a List of POJOs  Similar to writing, there are similar variations for reading JSON as POJOs. These tests are named like `DZoneReadPojoJackson`:  * `read10FromString` (and `read1kFromString`, `read100kFromString`): deserialization from `java.lang.String` * `read10FromBytes` (`read1kFromBytes`, `read100kFromBytes`): deserialization from given `byte[]`  But in addition, there are also alternatives for reading same JSON as "untyped" values; that is, as `java.util.Map`s, `java.util.List`s, `String`s, `Number`s and `Boolean`s:  * `read10FromString` (and `read1kFromString`, `read100kFromString`): deserialization from `java.lang.String` * `read10FromBytes` (`read1kFromBytes`, `read100kFromBytes`): deserialization from given `byte[]`  and the difference is from naming tests classes like `DZoneReadMapJackson` (replacing `POJO` with `Map`); test names are the same.  ## Sample results  See [Wiki](../../wiki) for sample results.
crnk-project/crnk-framework	# crnk.io - Crank up the development of RESTful applications!   [![Maven Central](https://img.shields.io/maven-central/v/io.crnk/crnk-core.svg)](http://mvnrepository.com/artifact/io.crnk/crnk-core) [![Build Status](https://travis-ci.org/crnk-project/crnk-framework.svg?branch=master)](https://travis-ci.org/crnk-project/crnk-framework) [![Gitter](https://img.shields.io/gitter/room/crkn-io/lobby.svg)](https://gitter.im/crnk-io/Lobby) [![License](https://img.shields.io/badge/License-Apache%202.0-yellowgreen.svg)](https://github.com/crnk-project/crnk-framework/blob/master/LICENSE.txt) [![Coverage Status](https://coveralls.io/repos/github/crnk-project/crnk-framework/badge.svg?branch=master)](https://coveralls.io/github/crnk-project/crnk-framework?branch=master)   ## What is Crnk?  Crnk is an implementation of the [JSON API](https://http://jsonapi.org/) specification and recommendations in Java to  facilitate building RESTful applications. It provides many conventions and building blocks that application can benefit from.  This includes features such as  sorting, filtering, pagination, requesting complex object graphs, sparse  field sets, attaching links to data or atomically execute multiple operations. Further integration  with frameworks and libraries such as Spring, CDI, JPA, Bean Validation, Dropwizard, Servlet API, Zipkin and and more ensure that JSON API plays well together with the Java ecosystem. Have a look at  [www.crnk.io](http://www.crnk.io) and the [documentation](http://www.crnk.io/documentation/) for more detailed  information.  ## Requirements  Crnk requires Java 1.7 or later.   ## Licensing  Crnk is licensed under the Apache License, Version 2.0. You can grab a copy of the license at http://www.apache.org/licenses/LICENSE-2.0.   ## Building from Source  Crnk make use of Gradle for its build. To build the complete project run      gradlew clean build      Note as part of the build a local Node installation is downloaded to build the frontend parts (crnk-ui) of the project.       ## Links  * [Homepage](http://www.crnk.io) * [Documentation](http://www.crnk.io/documentation) * [Source code](https://github.com/crnk-project/crnk-framework/) * [Issue tracker](https://github.com/crnk-project/crnk-framework/issues) * [Forum](https://gitter.im/crnk-io/Lobby) * [Build](https://travis-ci.org/crnk-project/crnk-framework/)
zalando/problem	# Problem  [![Bubble Gum on Shoe](docs/bubble-gum.jpg)](https://pixabay.com/en/bubble-gum-shoes-glue-dirt-438404/)  [![Build Status](https://img.shields.io/travis/zalando/problem/master.svg)](https://travis-ci.org/zalando/problem) [![Coverage Status](https://img.shields.io/coveralls/zalando/problem/master.svg)](https://coveralls.io/r/zalando/problem) [![Javadoc](https://javadoc-emblem.rhcloud.com/doc/org.zalando/problem/badge.svg)](http://www.javadoc.io/doc/org.zalando/problem) [![Release](https://img.shields.io/github/release/zalando/problem.svg)](https://github.com/zalando/problem/releases) [![Maven Central](https://img.shields.io/maven-central/v/org.zalando/problem.svg)](https://maven-badges.herokuapp.com/maven-central/org.zalando/problem) [![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/zalando/problem/master/LICENSE)  > **Problem** noun, /ˈpɹɒbləm/: A difficulty that has to be resolved or dealt with.  *Problem* is a library that implements  [`application/problem+json`](https://tools.ietf.org/html/rfc7807). It comes with an extensible set of interfaces/implementations as well as convenient functions for every day use. It's decoupled from any JSON library, but contains a separate module for Jackson.  ## Features  - proposes a common approach for expressing errors in REST API implementations - compatible with `application/problem+json`  ## Dependencies  - Java 8 - Any build tool using Maven Central, or direct download - Jackson (optional)  ## Installation  Add the following dependency to your project:  ```xml <dependency>     <groupId>org.zalando</groupId>     <artifactId>problem</artifactId>     <version>${problem.version}</version> </dependency> <dependency>     <groupId>org.zalando</groupId>     <artifactId>jackson-datatype-problem</artifactId>     <version>${problem.version}</version> </dependency> ```  ## Configuration  In case you're using Jackson, make sure you register the module with your `ObjectMapper`:  ```java ObjectMapper mapper = new ObjectMapper()     .registerModule(new Jdk8Module())     .registerModule(new ProblemModule()); ```  Alternatively, you can use the SPI capabilities:  ```java ObjectMapper mapper = new ObjectMapper()     .findAndRegisterModules(); ```  ## Usage  ### Creating problems  There are different ways to express problems. Ranging from limited, but easy-to-use to highly flexible and extensible,  yet with slightly more effort:  #### Generic  There are cases in which an [HTTP status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) is basically  enough to convey the necessary information. Everything you need is the status you want to respond with and we will  create a problem from it:  ```java Problem.valueOf(Status.NOT_FOUND); ```  Will produce this:  ```json {   "title": "Not Found",   "status": 404 } ```  As specified by [Predefined Problem Types](https://tools.ietf.org/html/rfc7807#section-4.2):  > The "about:blank" URI, when used as a problem type, > indicates that the problem has no additional semantics beyond that of > the HTTP status code.    > When "about:blank" is used, the title SHOULD be the same as the > recommended HTTP status phrase for that code (e.g., "Not Found" for > 404, and so on), although it MAY be localized to suit client > preferences (expressed with the Accept-Language request header).  But you may also have the need to add some little hint, e.g. as a custom detail of the problem:  ```java Problem.valueOf(Status.SERVICE_UNAVAILABLE, "Database not reachable"); ```  Will produce this:  ```json {   "title": "Service Unavailable",   "status": 503,   "detail": "Database not reachable" } ```  #### Builder  Most of the time you'll need to define specific problem types, that are unique to your application. And you want to  construct problems in a more flexible way. This is where the *Problem Builder* comes into play. It offers a fluent API  and allows to construct problem instances without the need to create custom classes:  ```java Problem.builder()     .withType(URI.create("https://example.org/out-of-stock"))     .withTitle("Out of Stock")     .withStatus(BAD_REQUEST)     .withDetail("Item B00027Y5QG is no longer available")     .build(); ```  Will produce this:  ```json {   "type": "https://example.org/out-of-stock",   "title": "Out of Stock",   "status": 400,   "detail": "Item B00027Y5QG is no longer available" } ```  Alternatively you can add custom properties, i.e. others than `type`, `title`, `status`, `detail` and `instance`:  ```java Problem.builder()     .withType(URI.create("https://example.org/out-of-stock"))     .withTitle("Out of Stock")     .withStatus(BAD_REQUEST)     .withDetail("Item B00027Y5QG is no longer available")     .with("product", "B00027Y5QG")     .build(); ```  Will produce this:  ```json {   "type": "https://example.org/out-of-stock",   "title": "Out of Stock",   "status": 400,   "detail": "Item B00027Y5QG is no longer available",   "product": "B00027Y5QG" } ```  #### Custom Problems  The highest degree of flexibility and customizability is achieved by implementing `Problem` directly. This is  especially convenient if you refer to it in a lot of places, i.e. it makes it easier to share. Alternatively you can extend `AbstractThrowableProblem`:  ```java @Immutable public final class OutOfStockProblem extends AbstractThrowableProblem {      static final URI TYPE = URI.create("https://example.org/out-of-stock");      private final String product;      public OutOfStockProblem(final String product) {         super(TYPE, "Out of Stock", BAD_REQUEST, format("Item %s is no longer available", product));         this.product = product;     }      public String getProduct() {         return product;     }  } ```  ```java new OutOfStockProblem("B00027Y5QG"); ```  Will produce this:  ```json {   "type": "https://example.org/out-of-stock",   "title": "Out of Stock",   "status": 400,   "detail": "Item B00027Y5QG is no longer available",   "product": "B00027Y5QG" } ```  ### Throwing problems  *Problems* have a loose, yet direct connection to *Exceptions*. Most of the time you'll find yourself transforming one  into the other. To make this a little bit easier there is an abstract `Problem` implementation that subclasses  `RuntimeException`: the `ThrowableProblem`. It allows to throw problems and is already in use by all default  implementations. Instead of implementing the `Problem` interface, just inherit from `ThrowableProblem`:  ```java public final class OutOfStockProblem extends ThrowableProblem ```  If you already have an exception class that you want to extend, you should implement the "marker" interface `Exceptional`:  ```java public final class OutOfStockProblem extends BusinessException implements Exceptional ```  The Jackson support module will recognize this interface and deal with the inherited properties from `Throwable`  accordingly. Note: This interface only exists, because `Throwable` is a concrete class, rather than an interface.  ### Handling problems  Reading problems is very specific to the JSON parser in use. This section assumes you're using Jackson, in which case  reading/parsing problems usually boils down to this:  ```java Problem problem = mapper.readValue(.., Problem.class); ```  If you're using Jackson, please make sure you understand its  [Polymorphic Deserialization](http://wiki.fasterxml.com/JacksonPolymorphicDeserialization) feature. The supplied  Jackson module makes heavy use of it. Considering you have a custom problem type `OutOfStockProblem`, you'll need to  register it as a subtype:  ```java mapper.registerSubtypes(OutOfStockProblem.class); ``` You also need to make sure you assign a `@JsonTypeName` to it and declare a `@JsonCreator`:  ```java @JsonTypeName(OutOfStockProblem.TYPE_VALUE) public final class OutOfStockProblem implements Problem {      @JsonCreator     public OutOfStockProblem(final String product) { ```  Jackson is now able to deserialize specific problems into their respective types. By default, e.g. if a type is not  associated with a class, it will fallback to a `DefaultProblem`.   ### Catching problems  If you read about [Throwing problems](#throwing-problems) already, you should be familiar with `ThrowableProblem`.  This can be helpful if you read a problem, as a response from a server, and what to find out what it actually is.  Multiple `if` statements with `instanceof` checks could be an option, but usually nicer is this:  ```java try {     throw mapper.readValue(.., ThrowableProblem.class); } catch (OutOfStockProblem e) {     tellTheCustomerTheProductIsNoLongerAvailable(e.getProduct()); } catch (InsufficientFundsProblem e) {     askCustomerToUseDifferentPaymentMethod(e.getBalance(), e.getDebit()); } catch (InvalidCouponProblem e) {     askCustomerToUseDifferentCoupon(e.getCouponCode()); } catch (ThrowableProblem e) {     tellTheCustomerSomethingWentWrong(); } ```  If you used the `Exceptional` interface rather than `ThrowableProblem` you have to adjust your code a little bit:  ```java try {     throw mapper.readValue(.., Exceptional.class).propagate(); } catch (OutOfStockProblem e) {     ... ```  ### Stack traces and causal chains  Exceptions in Java can be chained/nested using *causes*. `ThrowableProblem` adapts the pattern seamlessly to problems:  ```java ThrowableProblem problem = Problem.builder()     .withType(URI.create("https://example.org/order-failed"))     .withTitle("Order failed")     .withStatus(BAD_REQUEST)     .withCause(Problem.builder()       .withType(URI.create("about:blank"))       .withTitle("Out of Stock")       .withStatus(BAD_REQUEST)       .build())     .build();      problem.getCause(); // standard API of java.lang.Throwable ```  Will produce this:  ```json {   "type": "https://example.org/order-failed",   "title": "Order failed",   "status": 400,   "cause": {     "type": "https://example.org/out-of-stock",     "title": "Out of Stock",     "status": 400,     "detail": "Item B00027Y5QG is no longer available"   } } ```  Another important aspect of exceptions are stack traces, but since they leak implementation details to the outside world,  [**we strongly advise against exposing them**](http://zalando.github.io/restful-api-guidelines/common-data-objects/CommonDataObjects.html#must-an-error-message-must-not-contain-the-stack-trace) in problems. That being said, there is a legitimate use case when you're debugging an issue on an integration environment and you don't have direct access to the log files. Serialization of stack traces can be enabled on the problem module:  ```java ObjectMapper mapper = new ObjectMapper()     .registerModule(new Jdk8Module())     .registerModule(new ProblemModule().withStackTraces()); ```  After enabling stack traces all problems will contain a `stacktrace` property:  ```json {   "type": "about:blank",   "title": "Unprocessable Entity",   "status": 400,   "stacktrace": [     "org.example.Example.execute(Example.java:17)",     "org.example.Example.main(Example.java:11)"   ] } ```  Since we discourage the  serialization of them, there is currently, by design, no way deserialize them from JSON. Nevertheless the runtime will fill in the stack trace when the problem instance is created. That stack trace is usually not 100% correct, since it looks like the exception originated inside your deserialization framework. *Problem* comes with a special service provider interface `StackTraceProcessor` that can be registered using the  [`ServiceLoader` capabilities](http://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html). It can be used to modify the stack trace, e.g. remove all lines before your own client code, e.g. Jackson/HTTP client/etc.  ```java public interface StackTraceProcessor {      Collection<StackTraceElement> process(final Collection<StackTraceElement> elements);  } ```  By default no processing takes place.  ## Getting help  If you have questions, concerns, bug reports, etc, please file an issue in this repository's Issue Tracker.  ## Getting involved  To contribute, simply make a pull request and add a brief description (1-2 sentences) of your addition or change.  For more details check the [contribution guidelines](CONTRIBUTING.md).  ## Credits and references  Users of the [Spring Framework] are highly encouraged to check out [Problems for Spring Web MVC] (https://github.com/zalando/problem-spring-web), a library that seemlessly integrates problems into Spring.
xcesco/kripton	[![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Kripton%20Persistence%20Library-orange.svg?style=flat)](https://android-arsenal.com/details/1/5149) [![download](https://api.bintray.com/packages/xcesco/kripton/kripton-android-library/images/download.svg)](https://bintray.com/xcesco/kripton/kripton-android-library/_latestVersion) [![maven central](https://maven-badges.herokuapp.com/maven-central/com.abubusoft/kripton/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.abubusoft/kripton) [![test coverage](https://img.shields.io/codecov/c/github/xcesco/kripton/master.svg?style=flat-square)](https://codecov.io/gh/xcesco/kripton?branch=master) [![Project Stats](https://www.openhub.net/p/kripton-persistence-library/widgets/project_thin_badge.gif)](https://www.openhub.net/p/kripton-persistence-library)  # Kripton Persistence Library Kripton is a java library, for Android and Java platform, that provides a simple and uniform way to manage persistence of Java classes in different flavours through annotations and interface. Supported persistence format are:  <img src="https://github.com/xcesco/wikis/blob/master/kripton/overview2.0.png">  To get max performance and avoid boilerplate-code, Kripton use annotation processor. With the power of annotation processor is possible to create code to persist a java class, simply with an annotation. There are many other libraries that do this, but Kripton allows to persists java object without using reflection and with just few lines of code.  See [wiki](https://github.com/xcesco/kripton/wiki) for more informations.  See [benchmarks](https://github.com/xcesco/kripton/wiki/Performance) for more informations about JSON persistence perfomance.  If you are interested in Kripton Persistence Library, visit [abubusoft.com](http://www.abubusoft.com/wp/)  # Setup You can use Kritpon Annotation Processor and Kripton Library via maven  ```xml <dependencies>   ...       <dependency>     <groupId>com.abubusoft</groupId>     <artifactId>kripton</artifactId>     <version>2.0.2</version>   </dependency>   ... </dependencies> ...		 <build>    <plugins>     ...     <plugin>       <groupId>org.apache.maven.plugins</groupId> 	  <artifactId>maven-compiler-plugin</artifactId>       <version>3.6.0</version> 	  <configuration> 	    <source>1.7</source> 		<target>1.7</target> 		<annotationProcessorPaths> 		  <path> 		    <groupId>com.abubusoft</groupId> 		    <artifactId>kripton-processor</artifactId> 		    <version>2.0.2</version> 		</path> 	    </annotationProcessorPaths> 	  </configuration>     </plugin>     ...   </plugins> </build> ```  or via gradle  ``` // annotation processor annotationProcessor "com.abubusoft:kripton-processor:2.0.2"  // https://mvnrepository.com/artifact/com.abubusoft/kripton compile "com.abubusoft:kripton-android-library:2.0.2" ```  Snapshots of the development version are available in [Sonatype's snapshots repository](https://oss.sonatype.org/content/repositories/snapshots/com/abubusoft/).  Kritpon requires at minimum Java 7 or Android 2.3.  ![logo](https://github.com/xcesco/wikis/blob/master/kripton/logo320_120.png)   # Build To build entire library collections just download repository and launch from base directory   ``` mvn clean install -Prelease ```  # Supported platforms There are two platform: the android environment and generic Java environment. For each platform there is a version of library. Android platform already include a json library and xml parser library. Java JDK does not include a json library and have different xml parser libraries.  # Donate If you like Kripton and you want to support its development, please donate!  <a href='https://pledgie.com/campaigns/33279'><img alt='Click here to lend your support to: Support Kripton Persistence Library and make a donation at pledgie.com !' src='https://pledgie.com/campaigns/33279.png?skin_name=chrome' border='0' ></a>  # License  ``` Copyright 2015 Francesco Benincasa.  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```
harshalbenake/hbworkspace1-100	# hbworkspace1-100  (1) Name :- accelormeterSensor Description :- Using acceloremeter sensor to print Sensor event values.  (2)  Name :- ActionBarDropdownNavigation Description :- Action bar with dropdown navigation type.  (3)  Name :- android-actionbar-master Description :- Action bar buttons like add/delete/show.  (4)  Name :- Android Contact ListView Description :- Fake Contact listview.  (5)  Name :- AndroidListViewActivity Description :- Listview by extending ListActivity.  (6)  Name :- android-pulltorefresh-master Description :- Pulltorefresh demo.  (7)  Name :- Android-PullToRefresh-master Description :- Pulltorefresh handmark demo.  (8)  Name :- Android-Universal-Image-Loader-master Description :- Universal Image loader.  (9)  Name :- AnimationAllInOne Description :- Animation like fade/zoom/rotate.  (10)  Name :- arrayloop Description :- Different types of loops for arraylist items.  (11)  Name :- autocompletetextimagedemo Description :- Autocomplete with image and text.  (12)  Name :- BarcodeScanner Description :- Intent for Barcord scanner from google play.  (13)  Name :- bluetoothtoggle Description :- Toggle on/off bluetooth.  (14)  Name :- buttonpressed Description :-Status of button is pressed or not.  (15)  Name :- call Description :- Using TelephonyManager to get device phone number.  (16)  Name :- calling Description :- Intent to make call to specific number.  (17)  Name :- cellid Description :- Get cellid to get location.  (18)  Name :- Compass Description :- Google Glass - compass.  (19)  Name :- countrycode Description :- Get country code using Locale.  (20)  Name :- CustomLinkyfy Description :- Using custom linkyfy for various intents.  (21)  Name :- customlistviewBaseAdapter Description :- Listview with custom base adapter.  (22)  Name :- custompopup Description :- Dialog custom popup.  (23)  Name :- CustomSpinner Description :- Custom spinner with default value.  (24)  Name :- custom-ui Description :- Social Auth – custom UI.  (25)  Name :- databaseFromAsset Description :- Access database from asset folder.  (26)  Name :- dragndrop Description :- Drag and drop image demo.  (27)  Name :- expandablelistview Description :- Expandable listview demo.  (28)  Name :- flightmode Description :- Toggle on/off flight mode.  (29)  Name :- FragmentsTest Description :- Simple Fragment demo.  (30)  Name :- gallerydemo Description :- Image Gallery demo.  (31)  Name :- GestureDetection Description :- Detect gestures from user.  (32)  Name :- google image loader api complete Description :- Google image loader.  (33)  Name :- gpsonoff Description :- Toggle on/off GPS.  (34)  Name :- gpsonofstatus Description :- Get status of GPS on/off.  (35)  Name :- Gridlayout Description :- Grid layout demo.  (36)  Name :- gridviewsimple Description :- Simple grid view.  (37)  Name :- gsondemo Description :- Gson demo.  (38)  Name :- hbcustomlibaray Description :- Custom library demo.   (39)  Name :- HBfragment Description :- Fragment demo with detail and list view.  (40)  Name :- hideappfromlauncher Description :- Hide app icon from launcher.  (41)  Name :- highlightedittext Description :- Highlight the selected text.  (42)  Name :- home pressed Description :- Detect home button press.  (43)  Name :- HorizontalScrollViewActivity Description :- Horizontal scroll view demo.  (44)  Name :- ImageGridActivity Description :- Image grid using lru cache.  (45)  Name :- imageloadergoogle Description :- Google Image loader for auto complete.  (46)  Name :- imageloaderListViewWithJSONFromURL Description :- Image loader in listview pasring.  (47)  Name :- InstalledAppNames Description :- Get list of installed apps.  (48)  Name :- itemcount Description :- Item count calculation.  (49)  Name :- jasondemo Description :- Jason parsing demo.  (50)  Name :- jasonparsedemo Description :- Various kind of object json parsing using pojo.  (51)  Name :- JSONExampleActivity Description :- Json parsing post.  (52)  Name :- Jsonparsefromtxtfile Description :- Json parsing from txt file.  (53)  Name :- layoutadddynamically Description :- Adding infinity layout dynamically on button press.  (54)  Name :- layoutweightdemo Description :- Using layout weight for UI.  (55)  Name :- Linkedin Description :- Linkedin integration.  (56)  Name :- linkedinbest Description :- Linkedin integration.  (57)  Name :- Listview_baseadapter_getItemViewType Description :- Listview with getitemview type for different ui view per listitem.  (58)  Name :- LiveWallpaper Description :- Live Wallpaper demo.  (59)  Name :- MyAndroidAppActivity Description :- Simple string buffer.  (60)  Name :- mypopup Description :- Custom dailog.  (61)  Name :- notification Description :- Simple notification demo.  (62)  Name :- Notification_count Description :- Notification Badge count.  (63)  Name :- Paginated ListView Demo Description :- Pagination for listview.  (64)  Name :- PayPalSDKExample Description :- Paypal integration.  (65)  Name :- PinItDemo Description :- Pint it integration.  (66)  Name :- progressbardemo Description :- Progressbar demo.  (67)  Name :- ProximatySensorDemo Description :- Using Proximaty sensor for printing values.  (68)  Name :- pulltorefresh and dragndrop to gridview Description :- Pulltorefresh and drag n drop to  gridview.  (69)  Name :- readtextfile Description :- Read simple text file.  (70)  Name :- recentRunningBackgroundAppList Description :- Get list of apps that were  running recently.  (71)  Name :- rfile Description :- Get id value from view using r file.  (72)  Name :- ribbonsample Description :- Ribbon sample demo.  (73)  Name :- roatation Description :- Get status of rotation on/off.  (74)  Name :- rotatecenter Description :- Animate image rotation at center point.  (75)  Name :- screenorientation Description :- Get status of screen orientation landscape/portrait.  (76)  Name :- SdcardFormat Description :- Format sd card.  (77)  Name :- selectspeed Description :- Select speed ui txtsheild.  (78)  Name :- sendemail Description :- Send an email using intent.  (79)  Name :- share-bar Description :- Social Auth – custom UI.  (80)  Name :- share-button Description :- Social Auth – custom UI.  (81)  Name :- sharemyapp Description :- Share app apk from device.  (82)  Name :- signature Description :- Signature using image bitmap paint.  (83)  Name :- SimpleListView Description :- Simple listview demo.  (84)  Name :- smserrors Description :- Send sms and get various exceptions.  (85)  Name :- socialauth-android Description :- Social Auth demo.  (86)  Name :- Stopwatch Description :- Google Glass - Stopwatch.  (87)  Name :- SwitchButton Description :- SwitchButton toggle on/off demo.  (88)  Name :- textlink Description :- Text as a link url.  (89)  Name :- Timer Description :- Google Glass - Timer.  (90)  Name :- toggleButton Description :- Get status Toggle button on/off.  (91)  Name :- triangledrawable Description :- Draw triangle using drawable xml.  (92)  Name :- uninstallapp Description :- Uninstall app.  (93)  Name :- unknownsource Description :- Toggle on/off unknown source flag.  (94)  Name :- videodemo Description :- Simple video view to play rstp files.  (95)  Name :- videoviewdemo Description :- Video view to play Youtube files.  (96)  Name :- ViewPagerDemo Description :- Simple viewpager demo.  (97)  Name :- ViewpagerInDialogPopup Description :- View pager inside Dialog pop.  (98)  Name :- webviewnonxss Description :- You tube video play in webview using video id.  (99)  Name :- webviewyoutubeapi Description :- Simple video play in webview.  (100)  Name :- zoomtry Description :- Zoom in and zoom out animation.
ngs-doo/dsl-json	DSL-JSON library ================  DSL Platform compatible JSON library for Java and Android.  Java JSON library designed for performance. Built for invasive software composition with DSL Platform compiler.  ![JVM serializers benchmark results](https://cloud.githubusercontent.com/assets/1181401/13662269/8c49a02c-e699-11e5-9e46-f98f07fd68ef.png)  ## Distinguishing features   * supports external schema - Domain Specification Language (DSL)  * works on existing POJO classes via annotation processor - it converts POJO to DSL schema and constructs specialized converters at compile time  * performance - faster than any other Java JSON library. On pair with fastest binary JVM codecs  * works on byte level - deserialization can work on byte[] or InputStream. It doesn't need intermediate char representation  * extensibility - custom types can be registered for serialization/deserialization  * streaming support - large JSON lists support streaming with minimal memory usage  * zero-copy operations - converters avoid producing garbage  * minimal size - runtime dependency weights around 100KB  * no runtime overhead - both schema and annotation based POJOs are prepared at compile time  * no unsafe code - library doesn't rely on Java UNSAFE/internal methods  * legacy name mapping - multiple versions of JSON property names can be mapped into a single POJO using alternativeNames annotation  * binding to an existing instance - during deserialization an existing instance can be provided to reduce GC  ## Schema based serialization  DSL can be used for defining schema from which POJO classes with embedded JSON conversion are constructed. This is useful in large, multi-language projects where model is defined outside of Java classes. More information about DSL can be found on [DSL Platform](https://dsl-platform.com) website.  ## @CompiledJson annotation  Annotation processor works by translating Java classes into DSL and running DSL Platform compiler on it. DSL compiler will generate optimized converters and register them into `META-INF/services`. This will be loaded during `DslJson` initialization with `ServiceLoader`. Converters will be created even for dependent objects which don't have `@CompiledJson` annotation. This can be used to create serializers for pre-existing classes without annotating them. Both bean properties and public non-final fields are supported.  Annotation processor can be added as Maven dependency with:      <dependency>       <groupId>com.dslplatform</groupId>       <artifactId>dsl-json-processor</artifactId>       <version>1.5.0</version>       <scope>provided</scope>     </dependency>  For use in Android, Gradle can be configured with:      apply plugin: 'android-apt'     dependencies {       compile 'com.dslplatform:dsl-json:1.5.1'       apt 'com.dslplatform:dsl-json-processor:1.5.0'     }  Project examples can be found in [examples folder](examples)  ### Java/DSL property mapping  | Java type                 | DSL type     | Java type                          | DSL type     | | ------------------------- | ------------ | ---------------------------------- | ------------ | | int                       |  int         | byte[]                             |  binary      | | long                      |  long        | java.util.Map&lt;String,String&gt; |  properties? | | float                     |  float       | java.net.InetAddress               |  ip?         | | double                    |  double      | java.awt.Color                     |  color?      | | boolean                   |  bool        | java.awt.geom.Rectangle2D          |  rectangle?  | | java.lang.String          |  string?     | java.awt.geom.Point2D              |  location?   | | java.lang.Integer         |  int?        | java.awt.geom.Point                |  point?      | | java.lang.Long            |  long?       | java.awt.image.BufferedImage       |  image?      | | java.lang.Float           |  float?      | android.graphics.Rect              |  rectangle?  | | java.lang.Double          |  double?     | android.graphics.PointF            |  location?   | | java.lang.Boolean         |  bool?       | android.graphics.Point             |  point?      | | java.math.BigDecimal      |  decimal?    | android.graphics.Bitmap            |  image?      | | java.time.LocalDate       |  date?       | org.w3c.dom.Element                |  xml?        | | java.time.OffsetDateTime  |  timestamp?  | org.joda.time.LocalDate            |  date?       | | java.util.UUID            |  uuid?       | org.joda.time.DateTime             |  timestamp?  |   ### Java/DSL collection mapping  | Java type             | DSL type      | | --------------------- | ------------- | | array                 |  Array        | | java.util.List        |  List         | | java.util.Set         |  Set          | | java.util.LinkedList  |  Linked List  | | java.util.Queue       |  Queue        | | java.util.Stack       |  Stack        | | java.util.Vector      |  Vector       | | java.util.Collection  |  Bag          |  Collections can be used on supported Java types, other POJOs and enums.  ### Custom types  Types without builtin mapping can be supported in three ways:   * by implementing `JsonObject` and appropriate `JSON_READER`  * by defining custom conversion class and annotating it with `@JsonConverter`  * by defining custom conversion class and referencing it from property with converter through `@JsonAttribute`  Custom converter for `java.util.Date` can be found in [example project](examples/Maven/src/main/java/com/dslplatform/maven/Example.java#L116) Annotation processor will check if custom type implementations have appropriate signatures. Converter for `java.util.ArrayList` can be found in [same example project](examples/Maven/src/main/java/com/dslplatform/maven/Example.java#L38)  `@JsonConverter` which implements `Configuration` will also be registered in `META-INF/services` which makes it convenient to [setup initialization](examples/Maven/src/main/java/com/dslplatform/maven/ImmutablePerson.java#L48).  ### @JsonAttribute features  DSL-JSON property annotation supports several customizations/features:   * name - define custom serialization name  * alternativeNames - different incoming JSON attributes can be mapped into appropriate property. This can be used for simple features such as casing or for complex features such as model evolution  * ignore - don't serialize specific property into JSON  * nullable - tell compiler that this property can't be null. Compiler can remove some checks in that case for minuscule performance boost  * mandatory - mandatory properties must exists in JSON. Even in omit-defaults mode. If property is not found, `IOException` will be thrown  * hashMatch - DSL-JSON matches properties by hash values. If this option is turned off exact comparison will be performed which will add minor deserialization overhead, but invalid properties with same hash names will not be deserialized into "wrong" property. In case when model contains multiple properties with same hash values, compiler will inject exact comparison by default, regardless of this option value.  * converter - custom conversion per property. Can be used for formatting or any other custom handling of JSON processing for specific property  * typeSignature - disable inclusion of $type during abstract type serialization. By default abstract type will include additional information which is required for correct deserialization. Abstract types can be deserialized into a concreted type by defining `deserializeAs` on `@CompiledJson` which allows the removal of $type during both serialization and deserialization  ### External annotations  For existing classes which can't be modified with `@JsonAttribute` alternative external annotations are supported:  #### Nullability annotations  During translation from Java objects into DSL schema, existing type system nullability rules are followed. With the help of non-null annotations, hints can be introduced to work around some Java nullability type system limitations. List of supported non-null annotations can be found in [processor source code](https://github.com/ngs-doo/dsl-json/blob/master/processor/src/main/java/com/dslplatform/json/CompiledJsonProcessor.java#L85)  #### Property aliases  Annotation processor supports external annotations for customizing property name in JSON:   * com.fasterxml.jackson.annotation.JsonProperty  * com.google.gson.annotations.SerializedName  Those annotations will be translated into specialized DSL for specifying serialization name.  #### Ignored properties  Existing bean properties and fields can be ignored using one of the supported annotations:   * com.fasterxml.jackson.annotation.JsonIgnore  * org.codehaus.jackson.annotate.JsonIgnore  Ignored properties will not be translated into DSL schema.  #### Required properties  Jackson `required = true` can be used to fail if property is missing in JSON:  ## Serialization modes  Library has two serialization modes:   * minimal serialization - omits default properties which can be reconstructed from schema definition  * all properties serialization - will serialize all properties from schema definition  Best serialization performance can be obtained with combination of minimal serialization and minified property names/aliases.  ## Benchmarks  Independent benchmarks can validate the performance of DSL-JSON library:   * [JVM serializers](https://github.com/eishay/jvm-serializers/wiki) - benchmark for all kind of JVM codecs. Shows DSL-JSON as fast as top binary codecs  * [Kostya JSON](https://github.com/kostya/benchmarks) - fastest performing Java JSON library  * [JMH JSON benchmark](https://github.com/fabienrenaud/java-json-benchmark) - benchmarks for Java JSON libraries  Reference benchmark (built by library authors):   * [.NET vs JVM JSON](https://github.com/ngs-doo/json-benchmark) - comparison of various JSON libraries  ## Dependencies  To create compile time databinding, annotation processor will invoke DSL compiler, which requires Mono/.NET. There is no runtime Mono/.NET dependency, only JVM. Java8 Java-Time API is supported as a separate jar, since core library targets Java6.  Library can be added as Maven dependency with:      <dependency>       <groupId>com.dslplatform</groupId>       <artifactId>dsl-json</artifactId>       <version>1.5.1</version>     </dependency>  ## Best practices  ### Reusing reader/writer.  `JsonWriter` It has two modes of operations:    * populating the entire output into `byte[]`  * targeting output stream and flushing local `byte[]` to target output stream   `JsonWriter` can be reused via `reset` methods which binds it to specified target. When used directly it should be always created via `newWriter` method on `DslJson` instance.  Several `DslJson` serialize methods will reuse the writer via thread local variable. When using `JsonWriter` via the first mode, result can be copied to stream via `.toStream(OutputStream)` method.      DslJson<Object> json = ... // always reuse     OutputStream stream = ... // stream with JSON in UTF-8     json.serialize(pojo, stream); //will use thread local writer      `JsonReader` can process `byte[]` or `InputStream` inputs. It can be reused via the `process` methods.  When calling `DslJson` deserialize methods often exists in two flavors:   * with `byte[]` argument, in which case a new `JsonReader` will be created, but for best performance `byte[]` should be reused  * without `byte[]` argument in which case thread local reader will be reused     For small messages it's better to use `byte[]` API. When reader is used directly it should be always created via `newReader` method on `DslJson` instance.      DslJson<Object> json = ... // always reuse     InputStream stream = ... // stream with JSON in UTF-8     POJO instance = json.deserialize(POJO.class, stream); //will use thread local reader  ### Binding  `JsonReader` has `iterateOver` method for exposing input collection as consumable iterator. Also, since v1.5 binding API is available which can reuse instances for deserialization.      DslJson<Object> json = new DslJson<Object>(); //always reuse     byte[] bytes = "{\"number\":123}".getBytes("UTF-8");     JsonReader<Object> reader = json.newReader().process(bytes, bytes.length);     POJO instance = new POJO(); //can be reused     POJO bound = reader.next(POJO.class, instance); //bound is the same as instance above  ## FAQ   ***Q***: What is `TContext` in `DslJson` and what should I use for it?    ***A***: Generic `TContext` is used for library specialization. Use `DslJson<Object>` when you don't need it and just provide `null` for it.    ***Q***: Why is DSL-JSON faster than others?    ***A***: Almost zero allocations. Works on byte level. Better algorithms for conversion from `byte[]` -> type and vice-versa. Minimized unexpected branching.    ***Q***: DslJson is failing with unable to resolve reader/writer. What does it mean?    ***A***: During startup DslJson loads services through `ServiceLoader`. For this to work `META-INF/services/com.dslplatform.json.Configuration` must exist with the content of `dsl_json.json.ExternalSerialization` which is the class crated during compilation step. Make sure you've referenced processor library (which is responsible for setting up readers/writers during compilation) and double check if annotation processor is running. Refer to [example projects](examples) for how to set up environment.    ***Q***: Maven/Gradle are failing during compilation with `@CompiledJson`. What can I do about it?    ***A***: If Mono/.NET is available it *should* work out-of-the-box. But if some strange issue occurs, detailed log can be enabled to see what is causing the issue. Log is disabled by default, since some Gradle setups fail if something is logged during compilation. Log can be enabled with `dsljson.loglevel` [processor option](examples/Maven/pom.xml#L35)   ***Q***: Annotation processor checks for new DSL compiler version on every compilation. How can I disable that?    ***A***: If you specify custom `dsljson.compiler` processor option or put `dsl-compiler.exe` in project root it will use that one and will not check online for updates   ***Q***: What is this DSL Platform?    ***A***: DSL Platform is a proprietary compiler written in C#. It's free to use, but access to source code is licensed. If you need access to compiler or need performance consulting [let us know](https://dsl-platform.com)
owlike/genson	[![Build Status](https://travis-ci.org/owlike/genson.svg?branch=master)](https://travis-ci.org/owlike/genson)  # Genson  [![Join the chat at https://gitter.im/owlike/genson](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/owlike/genson?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  Genson is a complete json <-> java conversion library, providing full databinding, streaming and much more.  Gensons main strengths?   - Easy to use and just works!  - Its modular and configurable architecture.  - Speed and controlled small memory foot print making it scale.  ## Online Documentation  Checkout our new website - <http://owlike.github.io/genson/>.   The old website at <http://code.google.com/p/genson/>, hosts the documentation and javadoc until release 0.99 inclusive. But starting with 1.0 everything has been moved to github and the new website.  ## Motivation  You might wonder, why create another Json databinding lib for Java? Well...most libraries, miss of important features or have a lot of features but you can hardly add new features by yourself. Gensons initial motivation is to solve those problems by trying to come with useful features out of the box and stay as much as possible open to extension.   ## Features you will like    - Easy to use, fast, highly configurable, lightweight and all that into a single small jar!   - Full databinding and streaming support for efficient read/write   - Support for polymorphic types (able to deserialize to an unknown type)   - Does not require a default no arg constructor and really passes the values not just null, encouraging immutability. It can even be used with factory methods instead of constructors!   - Full support for generic types   - Easy to filter/include properties without requiring the use of annotations or mixins   - Genson provides a complete implementation of JSR 353   - Starting with Genson 0.95 JAXB annotations and types are supported!   - Automatic support for JSON in JAX-RS implementations   - Serialization and Deserialization of maps with complex keys  ## Goals   - Be as much extensible as possible by allowing users to add new features in a clean and easy way. Genson applies the philosophy that *"We can not think of every use case, so give to users the ability to do it by them self in a easy way*".  - Provide an easy to use API.  - Try to be as fast and scalable or even faster than the most performant librairies.  - Full support of Java generics.  - Provide the ability to work with classes of which you don't have the source code.  - Provide an efficient streaming API.  ## Download  Genson is provided as a all in one solution containing all the features. It provides also a couple of extensions and integrations with different other libraries such JAX-RS implementations, Spring, Joda time, Scala available out of the box. These libraries are of course not included in Genson and if you are using maven won't be pulled transitively (they are marked as optional).  To get you running you can download it manually from [maven central](http://repo1.maven.org/maven2/com/owlike/genson/) or add the dependency to your pom if you use Maven.  ```xml <dependency>   <groupId>com.owlike</groupId>   <artifactId>genson</artifactId>   <version>{{latest_version}}</version> </dependency> ```  You can also build it from the sources using Maven.  ## POJO databinding  The main entry point in Genson library is the Genson class. It provides methods to serialize Java objects to JSON  and deserialize JSON streams to Java objects. Instances of Genson are immutable and thread safe, you should reuse them. In general the recommended way is to have a single instance per configuration type.  The common way to use Genson is to read JSON and map it to some POJO and vice versa, read the POJO and write JSON.  ```java Genson genson = new Genson();  // read from a String, byte array, input stream or reader Person person = genson.deserialize("{\"age\":28,\"name\":\"Foo\"}", Person.class);  String json = genson.serialize(person); // or produce a byte array byte[] jsonBytes = genson.serializeBytes(person); // or serialize to a output stream or writer genson.serialize(person, outputStream);  public class Person {   public String name;   public int age; } ```   ## Java collections  But you can also work with standard Java collections such as Map and Lists. If you don't tell Genson what type to use, it will deserialize JSON Arrays to Java List and JSON Objects to Map, numbers to Long and Double.  Using Java standard types instead of POJO can be a easy way to start learning JSON. In that case you will deal only with List, Map, Long, Double, String, Boolean and null.  ```java // will be deserialized to a list of maps List<Object> persons = genson.deserialize("[{\"age\":28,\"name\":\"Foo\"}]", List.class); // will produce same result as Object persons = genson.deserialize("[{\"age\":28,\"name\":\"Foo\"}]", Object.class); ```   Instead of using the previous Person class we can use a Map. By default if you don't specify the type of the keys, Genson will deserialize to String and serialize using toString method of the key.  ```java Map<String, Object> person = new HashMap<String, Object>() {{   put("name", "Foo");   put("age", 28); }};  // {"age":28,"name":"Foo"} String singlePersonJson = genson.serialize(person); // will contain a long for the age and a String for the name Map<String, Object> map = genson.deserialize(singlePersonJson, Map.class); ```  ## Deserialize generic types  You can also deserialize to generic types such as a list of Pojos.  ```java String json = "[{\"age\":28,\"name\":\"Foo\"}]";  List<Person> persons = genson.deserialize(json, new GenericType<List<Person>>(){});  // or lets say we want to use something else than String as the keys of our Map. Map<Integer, Object> map = genson.deserialize(   "{\"1\":28, \"2\":\"Foo\"}",   new GenericType<Map<Integer, Object>>(){} ); ```  Note in the previous example we defined the keys (1, 2) as json strings. JSON specification allows only strings as object property names, but Genson allows to map this keys to some - limited - other types.  ## Customizing Genson  If the default configuration of Genson does not fit your needs you can customize it via the GensonBuilder. For example to enable indentation of the output, serialize all objects using their runtime type and deserialize to classes that don't provide a default no argument constructor can be achieved with following configuration.  ```java Genson genson = new GensonBuilder()   .useIndentation(true)   .useRuntimeType(true)   .useConstructorWithArguments(true)   .create(); ```   You are ready to rock the JSON! :)   ## Copyright and license  Copyright 2011-2014 Genson - Cepoi Eugen  Licensed under the **[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)** (the "License"); you may not use this file except in compliance with the License.  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
google/openrtb	Google OpenRTB Libraries ----------------------------------------------------------------------  This library supports the OpenRTB specification, providing bindings for all protobuf-supported languages, and additional support for Java such as JSON serialization and validation.  See our [wiki](https://github.com/google/openrtb/wiki) to get started! Use the Github issue tracker for bugs, RFEs or any support. Check the [changelog](CHANGELOG.md) for detailed release notes.   BUILDING NOTES ----------------------------------------------------------------------  You need: JDK 7, Maven 3.2, Protocol buffers (protoc) 2.6.1. Building is supported from the command line with Maven and from any IDE that can load Maven projects.  On Eclipse, the latest m2e is recommended but it can't run the code generation step, so you need to run a "mvn install" from the command line after checkout or after any mvn clean.
yahoo/elide	[![Codacy Badge](https://api.codacy.com/project/badge/Grade/986e1e05fee64702a2377272d664ec6d)](https://www.codacy.com/app/Elide/elide?utm_source=github.com&utm_medium=referral&utm_content=yahoo/elide&utm_campaign=badger) [![Gitter](https://badges.gitter.im/yahoo/elide.svg)](https://gitter.im/yahoo/elide?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![Build Status](https://travis-ci.org/yahoo/elide.svg?branch=master)](https://travis-ci.org/yahoo/elide) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.yahoo.elide/elide-core/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.yahoo.elide/elide-core)  ![Elide Logo](http://elide.io/assets/images/elide.svg)  *Read this in other languages: [中文](./README-zh.md).*  ## What Is Elide?  Elide is a Java library that lets you stand up a [JSON API](http://jsonapi.org) web service with minimal effort starting from a [JPA annotated data model](https://en.wikipedia.org/wiki/Java_Persistence_API). Elide is designed to quickly build and deploy **production quality** web services that expose data models as services.  Elide provides:   1. **Access** to JPA entities via JSON API CRUD operations.  Entities can be explicitly included or excluded via annotations.   2. **Patch Extension** Elide supports the [JSON API Patch extension](http://jsonapi.org/extensions/jsonpatch/) allowing multiple create, edit, and delete operations in a single request.   3. **Atomic Requests** All requests to the library (including the patch extension) can be embedded in transactions to ensure operational integrity.   4. **Authorization** All operations on entities and their fields can be assigned custom permission checks limiting who has access to your data.   5. **Audit** Logging can be customized for any operation on any entity.   6. **Extension** Elide allows the ability to customize business logic for any CRUD operation on the model.  Any persistence backend can be skinned with JSON-API by wiring in a JPA provider or by implementing a custom `DataStore`.   7. **Test** Elide includes a test framework that explores the full surface of the API looking for security vulnerabilities.   8. **Client API** Elide is developed in conjunction with a Javascript client library that insulates developers from changes to the specification.  Alternatively, Elide can be used with any [JSON API client library](http://jsonapi.org/implementations/).  ## Documentation  More information about Elide can be found at [elide.io](http://elide.io/).  ## Elide on Maven  To integrate Elide into your project, simply include elide-core into your project's pom.xml:  ```xml <!-- Elide --> <dependency>     <groupId>com.yahoo.elide</groupId>     <artifactId>elide-core</artifactId> </dependency> ```  Additionally, if you do not plan to write your own data store, select the appropriate data store for your setup and include it as well. For instance, if you plan on using the "in-memory database" (not recommended for production use) then you would add the following:  ```xml <dependency>     <groupId>com.yahoo.elide</groupId>     <artifactId>elide-datastore-inmemorydb</artifactId> </dependency> ```  ## Development  If you are contributing to Elide using an IDE, such as IntelliJ, make sure to install the [Lombok](https://projectlombok.org/) plugin.  ## Tutorials [Create a JSON API REST Service With Spring Boot and Elide](https://dzone.com/articles/create-a-json-api-rest-service-with-spring-boot-an)  [Custom Security With a Spring Boot/Elide Json API Server](https://dzone.com/articles/custom-security-with-a-spring-bootelide-json-api-s)  [Logging Into a Spring Boot/Elide JSON API Server](https://dzone.com/articles/logging-into-a-spring-bootelide-json-api-server)  [Securing a JSON API REST Service With Spring Boot and Elide](https://dzone.com/articles/securing-a-json-api-rest-service-with-spring-boot)  [Creating Entities in a Spring Boot/Elide JSON API Server](https://dzone.com/articles/creating-entities-in-a-spring-bootelide-json-api-s)  [Updating and Deleting with a Spring Boot/Elide JSON API Server](https://dzone.com/articles/updating-and-deleting-with-a-spring-bootelide-json)  ## License  The use and distribution terms for this software are covered by the Apache License, Version 2.0 (http://www.apache.org/licenses/LICENSE-2.0.html).
bazaarvoice/jolt	Jolt ========  JSON to JSON transformation library written in Java where the "specification" for the transform is itself a JSON document.  ### Useful For  1. Transforming JSON data from ElasticSearch, MongoDb, Cassandra, etc before sending it off to the world 1. Extracting data from a large JSON documents for your own consumption  ## Table of Contents     1. [Overview](#Overview)    2. [Documentation](#Documentation)    3. [Shiftr Transform DSL](#Shiftr_Transform_DSL)    4. [Demo](#Demo)    5. [Getting Started](#Getting_Started)    6. [Getting Transform Help](#Getting_Transform_Help)    7. [Why Jolt Exists](#Why_Jolt_Exists)    8. [Alternatives](#Alternatives)    9. [Performance](#Performance)    10. [CLI](#CLI)    11. [Code Coverage](#Code_Coverage)    12. [Release Notes](#Release_Notes)  ## <a name="Overview"></a> Overview  Jolt :  * provides a set of transforms, that can be "chained" together to form the overall JSON to JSON transform. * focuses on transforming the *structure* of your JSON data, not manipulating specific values     * The idea being: use Jolt to get most of the structure right, then write code to fix values * consumes and produces "hydrated" JSON : in-memory tree of Maps, Lists, Strings, etc.     * use Jackson (or whatever) to serialize and deserialize the JSON text  ### Stock Transforms  The Stock transforms are:      shift       : copy data from the input tree and put it the output tree     default     : apply default values to the tree     remove      : remove data from the tree     sort        : sort the Map key values alphabetically ( for debugging and human readability )     cardinality : "fix" the cardinality of input data.  Eg, the "urls" element is usually a List, but if there is only one, then it is a String  Each transform has it's own DSL (Domain Specific Language) in order to facilitate it's narrow job.  Currently, all the Stock transforms just effect the "structure" of the data. To do data manipulation, you will need to write Java code.   If you write your Java "data manipulation" code to implement the Transform interface, then you can insert your code in the transform chain.  The out-of-the-box Jolt transforms should be able to do most of your structural transformation, with custom Java Transforms implementing your data manipulation.  ## <a name="Documentation"></a> Documentation  Jolt [Slide Deck](https://docs.google.com/presentation/d/1sAiuiFC4Lzz4-064sg1p8EQt2ev0o442MfEbvrpD1ls/edit?usp=sharing) : covers motivation, development, and transforms.  Javadoc explaining each transform DSL :  * [shift](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/Shiftr.java) * [default](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/Defaultr.java) * [remove](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/Removr.java) * [cardinality](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/CardinalityTransform.java) * [sort](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/Sortr.java) * full qualified Java ClassName : Class implements the Transform or ContextualTransform interfaces, and can optionally be SpecDriven (marker interface)     * [Transform](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/Transform.java) interface     * [SpecDriven](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/SpecDriven.java)         * where the "input" is "hydrated" Java version of your JSON Data  Running a Jolt transform means creating an instance of [Chainr](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/Chainr.java)  with a list of transforms.  The JSON spec for Chainr looks like : [unit test](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/test/resources/json/chainr/integration/firstSample.json).  The Java side looks like :  ``` java Chainr chainr = JsonUtils.classpathToList( "/path/to/chainr/spec.json" );  Object input = elasticSearchHit.getSource(); // ElasticSearch already returns hydrated JSon  Object output = chainr.transform( input );  return output; ```  ### <a name="Shiftr_Transform_DSL"></a> Shiftr Transform DSL  The Shiftr transform generally does most of the "heavy lifting" in the transform chain. To see the Shiftr DSL in action, please look at our unit tests ([shiftr tests](https://github.com/bazaarvoice/jolt/tree/master/jolt-core/src/test/resources/json/shiftr)) for nice bite sized transform examples, and read the extensive Shiftr [javadoc](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/main/java/com/bazaarvoice/jolt/Shiftr.java).  Our unit tests follow the pattern :  ``` json {     "input": {         // sample input     },      "spec": {         // transform spec     },      "expected": {         // what the output of the transform looks like     } } ```  We read in "input", apply the "spec", and [Diffy](https://github.com/bazaarvoice/jolt/blob/master/json-utils/src/main/java/com/bazaarvoice/jolt/Diffy.java) it against the "expected".  To learn the Shiftr DSL, examine "input" and "output" json, get an understanding of how data is moving, and *then* look at the transform spec to see how it facilitates the transform.  For reference, [this](https://github.com/bazaarvoice/jolt/blob/master/jolt-core/src/test/resources/json/shiftr/firstSample.json) was the very first test we wrote.   ## <a name="Demo"></a> Demo  There is a demo available at [jolt-demo.appspot.com](http://jolt-demo.appspot.com/). You can paste in JSON input data and a Spec, and it will post the data to server and run the transform.  Note  * it is hosted on a free Google App Engine instance, so it may take a minute to spin up. * it validates in input JSON and spec client side.  ## <a name="Getting_Started"></a> Getting Started  Getting started code wise has it's [own doc](gettingStarted.md).  ## <a name="Getting_Transform_Help"></a> Getting Transform Help  If you can't get a transform working and you need help, create and Issue in Jolt (for now).  Make sure you include what your "input" is, and what you want your "output" to be.  ## <a name="Why_Jolt_Exists"></a> Why Jolt Exists  Aside from writing your own custom code to do a transform, there are two general approaches to doing a JSON to JSON transforms in Java.  1) JSON -> XML -> XSLT or STX -> XML -> JSON  Aside from being a Rube Goldberg approach, XSLT is more complicated than Jolt because it is trying to do the whole transform with a single DSL.  2) Write a Template (Velocity, FreeMarker, etc) that take hydrated JSON input and write textual JSON output  With this approach you are working from the output format backwards to the input, which is complex for any non-trivial transform. Eg, the structure of your template will be dictated by the output JSON format, and you will end up coding a parallel tree walk of the input data and the output format in your template. Jolt works forward from the input data to the output format which is simpler, and it does the parallel tree walk for you.  ## <a name="Alternatives"></a> Alternatives  Being in the Java JSON processing "space", here are some other interesting JSON manipulation tools to look at / consider :  * [jq](https://stedolan.github.io/jq) - Awesome command line tool to extract data from JSON files (use it all the time, available via brew) * [JsonPath](https://github.com/jayway/JsonPath) - Java : Extract data from JSON using XPATH like syntax. * [JsonSurfer](https://github.com/jsurfer/JsonSurfer) - Java : Streaming JsonPath processor dedicated to processing big and complicated JSON data.  ## <a name="Performance"></a> Performance  The primary goal of Jolt was to improve "developer speed" by providing the ability to have a declarative rather than imperative transforms. That said, Jolt should have a better runtime than the alternatives listed above.  Work has been done to make the stock Jolt transforms fast:  1. Transforms can be initialized once with their spec, and re-used many times in a multi-threaded environment.     * We reuse initialized Jolt transforms to service multiple web requests from a DropWizard service. 2. "*" wildcard logic was redone to reduce the use of Regex in the common case, which was a dramatic speed improvement. 3. The parallel tree walk performed by Shiftr was optimized.  Two things to be aware of :  1. Jolt is not "stream" based, so if you have a very large Json document to transform you need to have enough memory to hold it. 2. The transform process will create and discard a lot of objects, so the garbage collector will have work to do.  ## <a name="CLI"></a> Jolt CLI  Jolt Transforms and tools can be run from the command line. Command line interface doc [here](cli/README.md).  ## <a name="Code_Coverage"></a> Code Coverage  [![Build Status](https://secure.travis-ci.org/bazaarvoice/jolt.png)](http://travis-ci.org/bazaarvoice/jolt)  For the moment we have Cobertura configured in our poms.  ``` sh mvn cobertura:cobertura open jolt-core/target/site/cobertura/index.html ```  Currently, for the jolt-core artifact, code coverage is at 89% line, and 83% branch.  ## <a name="Release_Notes"></a> Release Notes  [Versions and Release Notes available here](https://github.com/bazaarvoice/jolt/releases).
TommyLemon/APIJSON	# APIJSON  [![GitHub release](https://img.shields.io/github/release/TommyLemon/APIJSON.svg)](https://github.com/TommyLemon/APIJSON/releases) [![Java API](https://img.shields.io/badge/Java-1.6%2B-brightgreen.svg?style=flat)](http://www.oracle.com/technetwork/java/api-141528.html)  [![Android API](https://img.shields.io/badge/Android-15%2B-brightgreen.svg?style=flat)](https://developer.android.com/guide/topics/manifest/uses-sdk-element.html#ApiLevels)  [![JavaScrpit API](https://img.shields.io/badge/JavaScript-ES5%2B-brightgreen.svg?style=flat)](http://www.ecma-international.org/publications/standards/Standard.htm) [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)  <br />  [Java-Server](https://github.com/TommyLemon/APIJSON/tree/master/APIJSON-Java-Server)    [Android](https://github.com/TommyLemon/APIJSON/tree/master/APIJSON-Android)    [JavaScript](https://github.com/TommyLemon/APIJSON/tree/master/APIJSON-JavaScript)    [Vue.js](https://github.com/TommyLemon/APIJSON-JS-Vue)  [English Document](https://github.com/TommyLemon/APIJSON/blob/master/README(English).md)  [在线测试](http://139.196.140.118)  <br />  * ### [1.简介](#1) * ### [2.对比传统方式](#2) * [2.1 开发流程](#2.1) * [2.2 客户端请求](#2.2) * [2.3 服务端操作](#2.3) * [2.4 客户端解析](#2.4) * [2.5 对应不同需求的请求](#2.5) * [2.6 对应不同请求的结果](#2.6) * ### [3.对应关系总览](#3) * [3.1 操作方法](#3.1) * [3.2 功能符](#3.2) * ### [4.快速上手](#4) * [4.1 下载解压](#4.1) * [4.2 导入数据库表](#4.2) * [4.3 运行服务端工程](#4.3) * [4.4 运行客户端工程](#4.4) * [4.4 操作客户端App](#4.5) * ### [5.其它](#5) * [5.1 相关推荐](#5.1) * [5.2 关于作者](#5.2) * [5.3 下载试用](#5.3) * [5.4 更新日志](#5.4) * [5.5 Star&Fork](#5.5)  ## <h2 id="1">1.简介<h2/>  APIJSON是一种JSON传输结构协议。<br />  客户端可以定义任何JSON结构去向服务端发起请求，服务端就会返回对应结构的JSON，所求即所得。<br /> 一次请求任意结构任意数据，方便灵活，不需要专门接口或多次请求。<br /> 支持增删改查、模糊搜索、远程函数、权限管理等。还能去除重复数据，节省流量提高速度！<br />  从此HTTP传输JSON数据没有接口，更不需要文档！<br /> 客户端再也不用和服务端沟通接口或文档问题了！再也不会被文档各种错误坑了！<br /> 服务端再也不用为了兼容旧版客户端写新版接口和文档了！再也不会被客户端随时随地没完没了地烦了！ <br /> [为什么要用APIJSON？](https://github.com/TommyLemon/APIJSON/wiki)  ![](https://raw.githubusercontent.com/TommyLemon/APIJSON/master/picture/apijson_all_pages_0.jpg)  ![](https://raw.githubusercontent.com/TommyLemon/APIJSON/master/picture/apijson_all_pages_1.jpg)  ![](https://raw.githubusercontent.com/TommyLemon/APIJSON/master/picture/apijson_all_pages_2.jpg)  ![](https://raw.githubusercontent.com/TommyLemon/APIJSON/master/picture/apijson_all_pages_3.jpg)   ![](https://raw.githubusercontent.com/TommyLemon/APIJSON/master/picture/server_idea_log_complex.jpg)   ![](https://raw.githubusercontent.com/TommyLemon/APIJSON/master/picture/mysql_workbench_request.jpg)  ![](https://raw.githubusercontent.com/TommyLemon/APIJSON/master/picture/mysql_workbench_user.jpg)  ![](https://raw.githubusercontent.com/TommyLemon/APIJSON/master/picture/mysql_workbench_moment.jpg)   <br /> <br />  ### 举几个例子:  #### 查询用户 请求： <pre><code class="language-json"> {   "User":{   } } </code></pre>  [点击这里测试](http://139.196.140.118:8080/get/{"User":{}})  返回： <pre><code class="language-json"> {   "User":{     "id":38710,     "sex":0,     "name":"TommyLemon",     "certified":true,     "tag":"Android&Java",     "phone":13000038710,     "head":"http://static.oschina.net/uploads/user/1218/2437072_100.jpg?t=1461076033000",     "date":1485948110000,     "pictureList":[       "http://static.oschina.net/uploads/user/1218/2437072_100.jpg?t=1461076033000",       "http://common.cnblogs.com/images/icon_weibo_24.png"     ]   },   "code":200,   "msg":"success" } </code></pre>  <br />  #### 查询用户列表 请求： <pre><code class="language-json"> {   "[]":{     "count":3,             //只要3个     "User":{       "@column":"id,name"  //只要id,name这两个字段     }   } } </code></pre>  [点击这里测试](http://139.196.140.118:8080/get/{"[]":{"count":3,"User":{"@column":"id,name"}}})  返回： <pre><code class="language-json"> {   "[]":[     {       "User":{         "id":38710,         "name":"TommyLemon"       }     },     {       "User":{         "id":70793,         "name":"Strong"       }     },     {       "User":{         "id":82001,         "name":"Android"       }     }   ],   "code":200,   "msg":"success" } </code></pre>  <br />  #### 查询动态及发布者用户 请求： <pre><code class="language-json"> {   "Moment":{   },   "User":{     "id@":"Moment/userId"  //User.id = Moment.userId   } } </code></pre>  [点击这里测试](http://139.196.140.118:8080/get/{"Moment":{},"User":{"id@":"Moment%252FuserId"}})  返回： <pre><code class="language-json"> {   "Moment":{     "id":12,     "userId":70793,     "date":"2017-02-08 16:06:11.0",     "content":"1111534034"   },   "User":{     "id":70793,     "sex":0,     "name":"Strong",     "tag":"djdj",     "head":"http://static.oschina.net/uploads/user/585/1170143_50.jpg?t=1390226446000",     "contactIdList":[       38710,       82002     ],     "date":"2017-02-01 19:21:50.0"   },   "code":200,   "msg":"success" } </code></pre>  <br />  #### 查询类似微信朋友圈的动态列表 请求： <pre><code class="language-json"> {   "[]":{                             //请求一个数组     "page":0,                        //数组条件     "count":2,     "Moment":{                       //请求一个名为Moment的对象       "content$":"%a%"               //对象条件，搜索content中包含a的动态     },     "User":{       "id@":"/Moment/userId",        //User.id = Moment.userId  缺省引用赋值路径，从所处容器的父容器路径开始       "@column":"id,name,head"       //指定返回字段     },     "Comment[]":{                    //请求一个名为Comment的数组，并去除Comment包装       "count":2,       "Comment":{         "momentId@":"[]/Moment/id"   //Comment.momentId = Moment.id  完整引用赋值路径       }     }   } } </code></pre>  [点击这里测试](http://139.196.140.118:8080/get/{"[]":{"page":0,"count":2,"Moment":{"content$":"%2525a%2525"},"User":{"id@":"%252FMoment%252FuserId","@column":"id,name,head"},"Comment[]":{"count":2,"Comment":{"momentId@":"[]%252FMoment%252Fid"}}}})  返回： <pre><code class="language-json"> {   "[]":[     {       "Moment":{         "id":15,         "userId":70793,         "date":1486541171000,         "content":"APIJSON is a JSON Transmission Structure Protocol…",         "praiseUserIdList":[           82055,           82002,           82001         ],         "pictureList":[           "http://static.oschina.net/uploads/user/1218/2437072_100.jpg?t=1461076033000",           "http://common.cnblogs.com/images/icon_weibo_24.png"         ]       },       "User":{         "id":70793,         "name":"Strong",         "head":"http://static.oschina.net/uploads/user/585/1170143_50.jpg?t=1390226446000"       },       "Comment[]":[         {           "id":176,           "toId":166,           "userId":38710,           "momentId":15,           "date":1490444883000,           "content":"thank you"         },         {           "id":1490863469638,           "toId":0,           "userId":82002,           "momentId":15,           "date":1490863469000,           "content":"Just do it"         }       ]     },     {       "Moment":{         "id":58,         "userId":90814,         "date":1485947671000,         "content":"This is a Content...-435",         "praiseUserIdList":[           38710,           82003,           82005,           93793,           82006,           82044,           82001         ],         "pictureList":[           "http://static.oschina.net/uploads/img/201604/22172507_aMmH.jpg"         ]       },       "User":{         "id":90814,         "name":7,         "head":"http://static.oschina.net/uploads/user/51/102723_50.jpg?t=1449212504000"       },       "Comment[]":[         {           "id":13,           "toId":0,           "userId":82005,           "momentId":58,           "date":1485948050000,           "content":"This is a Content...-13"         },         {           "id":77,           "toId":13,           "userId":93793,           "momentId":58,           "date":1485948050000,           "content":"This is a Content...-77"         }       ]     }   ],   "code":200,   "msg":"success" } </code></pre>   <br />  [在线测试](http://139.196.140.118)  <br /> <br />   ## <h2 id="2">2.对比传统RESTful方式<h2/>  ### <h3 id="2.1">2.1 开发流程<h3/>  开发流程 | 传统方式 | APIJSON -------- | ------------ | ------------  接口传输 | 等服务端编辑接口，然后更新文档，客户端再按照文档编辑请求和解析代码 | 客户端按照自己的需求编辑请求和解析代码。<br />没有接口，更不需要文档！客户端再也不用和服务端沟通接口或文档问题了！  兼容旧版 | 服务端增加新接口，用v2表示第2版接口，然后更新文档 | 什么都不用做！    <br />   ### <h3 id="2.2">2.2 客户端请求<h3/>  客户端请求 | 传统方式 | APIJSON -------- | ------------ | ------------  要求 | 客户端按照文档在对应URL后面拼接键值对 | 客户端按照自己的需求在固定URL后拼接JSON  结构 | 同一个URL内table_name只能有一个 <br /><br /> base_url/get/table_name?<br />key0=value0&key1=value1... | 同一个URL后TableName可传任意数量个 <br /><br /> base_url/get/<br />{<br > &nbsp;&nbsp; TableName0:{<br > &nbsp;&nbsp;&nbsp;&nbsp; key0:value0,<br > &nbsp;&nbsp;&nbsp;&nbsp; key1:value1,<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; TableName1:{<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; }<br > &nbsp;&nbsp; ...<br > }  URL | 不同的请求对应不同的URL，基本上有多少个不同的请求就得有多少个接口URL | 相同的操作方法(增删改查)都用同一个URL，<br />大部分请求都用7个通用接口URL的其中一个  键值对 | key=value | key:value    <br />   ### <h3 id="2.3">2.3 服务端操作<h3/>  服务端操作 | 传统方式 | APIJSON -------- | ------------ | ------------  解析和返回 | 取出键值对，把键值对作为条件用预设的的方式去查询数据库，最后封装JSON并返回给客户端 | 把Parser#parse方法的返回值返回给客户端就行  返回JSON结构的设定方式 | 由服务端设定，客户端不能修改 | 由客户端设定，服务端不能修改    <br />   ### <h3 id="2.4">2.4 客户端解析<h3/>  客户端解析 | 传统方式 | APIJSON -------- | ------------ | ------------  查看方式 | 查文档或问后端，或等请求成功后看日志 | 看请求就行，所求即所得，不用查、不用问、不用等。也可以等请求成功后看日志  解析方法 | 用JSON解析器来解析JSONObject | 可以用JSONResponse解析JSONObject，或使用传统方式    <br />   ### <h3 id="2.5">2.5 客户端对应不同需求的请求<h3/>  客户端的请求 | 传统方式 | APIJSON -------- | ------------ | ------------  User | base_url/get/user?id=38710 | [base_url/get/<br >{<br > &nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710<br > &nbsp;&nbsp; }<br >}](http://139.196.140.118:8080/get/{"User":{"id":38710}})  Moment和对应的User | 分两次请求<br />Moment: <br /> base_url/get/moment?userId=38710<br /><br />User: <br /> base_url/get/user?id=38710 | [base_url/get/<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "userId":38710<br > &nbsp;&nbsp; }, <br > &nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710<br > &nbsp;&nbsp; }<br >}](http://139.196.140.118:8080/get/{"Moment":{"userId":38710},"User":{"id":38710}})  User列表 | base_url/get/user/list?<br />page=0&count=3&sex=0 | [base_url/get/<br >{<br > &nbsp;&nbsp; "User[]":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "page":0,<br > &nbsp;&nbsp;&nbsp;&nbsp;  "count":3, <br > &nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "sex":0<br > &nbsp;&nbsp;&nbsp;&nbsp; }<br > &nbsp;&nbsp; }<br >}](http://139.196.140.118:8080/get/{"User[]":{"page":0,"count":3,"User":{"sex":0}}})  Moment列表，<br />每个Moment包括<br />1.发布者User<br />2.前3条Comment | Moment里必须有<br />1.User对象<br >2.Comment数组<br /><br /> base_url/get/moment/list?<br />page=0&count=3&commentCount=3 | [base_url/get/<br >{<br > &nbsp;&nbsp; "[]":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "page":0, <br > &nbsp;&nbsp;&nbsp;&nbsp; "count":3, <br > &nbsp;&nbsp;&nbsp;&nbsp; "Moment":{}, <br > &nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id@":"/Moment/userId"<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; "Comment[]":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "count":3,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "momentId@":"[]/Moment/id"<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br > &nbsp;&nbsp;&nbsp;&nbsp; }<br > &nbsp;&nbsp; }<br >}](http://139.196.140.118:8080/get/{"[]":{"page":0,"count":3,"Moment":{},"User":{"id@":"%252FMoment%252FuserId"},"Comment[]":{"count":3,"Comment":{"momentId@":"[]%252FMoment%252Fid"}}}})  User发布的Moment列表，<br /> 每个Moment包括<br /> 1.发布者User<br /> 2.前3条Comment | 1.Moment里必须有User对象和Comment数组<br > 2.字段名必须查接口文档，例如评论数量字段名可能是<br /> commentCount,comment_count或者简写cmt_count等各种奇葩写法... <br /><br /> base_url/get/moment/list?<br />page=0&count=3<br />&commentCount=3&userId=38710 | 有以下几种方式:<br /><br /> ① 把以上请求里的<br >"Moment":{}, "User":{"id@":"/Moment/userId"}<br >改为<br >["Moment":{"userId":38710}, "User":{"id":38710}](http://139.196.140.118:8080/get/{"[]":{"page":0,"count":3,"Moment":{"userId":38710},"User":{"id":38710},"Comment[]":{"count":3,"Comment":{"momentId@":"[]%252FMoment%252Fid"}}}}) <br /><br /> ② 或把User放在上面的最外层省去重复的User<br />[base_url/get/<br >{<br > &nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "[]":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "page":0,<br > &nbsp;&nbsp;&nbsp;&nbsp; "count":3, <br > &nbsp;&nbsp;&nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "userId":38710<br > &nbsp;&nbsp;&nbsp;&nbsp; }, <br > &nbsp;&nbsp;&nbsp;&nbsp; "Comment[]":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "count":3,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "momentId@":"[]/Moment/id"<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br > &nbsp;&nbsp;&nbsp;&nbsp; }<br > &nbsp;&nbsp; }<br >}](http://139.196.140.118:8080/get/{"User":{"id":38710},"[]":{"page":0,"count":3,"Moment":{"userId":38710},"Comment[]":{"count":3,"Comment":{"momentId@":"[]%252FMoment%252Fid"}}}})<br /><br /> ③ 如果User之前已经获取到了，还可以不传User来节省请求和返回数据的流量并提升速度<br />[base_url/get/<br >{<br > &nbsp;&nbsp; "[]":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "page":0,<br > &nbsp;&nbsp;&nbsp;&nbsp; "count":3, <br > &nbsp;&nbsp;&nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "userId":38710<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; "Comment[]":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "count":3,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "momentId@":"[]/Moment/id"<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br > &nbsp;&nbsp;&nbsp;&nbsp; }<br > &nbsp;&nbsp; }<br >}](http://139.196.140.118:8080/get/{"[]":{"page":0,"count":3,"Moment":{"userId":38710},"Comment[]":{"count":3,"Comment":{"momentId@":"[]%252FMoment%252Fid"}}}})    <br />   ### <h3 id="2.6">2.6 服务端对应不同请求的返回结果<h3/>  服务端的返回结果 | 传统方式 | APIJSON -------- | ------------ | ------------  User | {<br > &nbsp;&nbsp; "data":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp; "name":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >} | {<br > &nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp; "name":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}  Moment和对应的User | 分别返回两次请求的结果，获取到Moment后取出userId作为User的id条件去查询User <br /><br /> Moment: <br > {<br > &nbsp;&nbsp; "data":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >} <br /><br /> User: <br > {<br > &nbsp;&nbsp; "data":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp; "name":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >} | 一次性返回，没有传统方式导致的 长时间等待结果、两次结果间关联、线程多次切换 等问题 <br /><br /> {<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp; "name":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}  User列表 | {<br > &nbsp;&nbsp; "data":[<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":82001,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; ],<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >} | {<br > &nbsp;&nbsp; "User[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":82001,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; ],<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}  Moment列表，每个Moment包括发布者User和前3条Comment | Moment里必须有<br />1.User对象<br />2.Comment数组 <br /><br /> {<br > &nbsp;&nbsp; "data":[<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":301,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; ],<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >} | 1.高灵活，可任意组合<br />2.低耦合，逻辑很清晰<br /><br />{<br > &nbsp;&nbsp; "[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":301,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; ],<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}  User发布的Moment列表，每个Moment包括发布者User和前3条Comment | 1.大量重复User，浪费流量和服务器性能<br />2.优化很繁琐，需要后端扩展接口、写好文档，前端/客户端再配合优化<br /><br />{<br > &nbsp;&nbsp; "data":[<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name":"Tommy"<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":470,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name":"Tommy"<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":511,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name":"Tommy"<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":595,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name":"Tommy"<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; ],<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >} | 以上不同请求方式的结果:<br /><br /> ① 常规请求 <br > {<br > &nbsp;&nbsp; "[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "name":"Tommy"<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; ],<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}<br /><br /> ② 省去重复的User <br > {<br > &nbsp;&nbsp; "User":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp; "name":"Tommy",<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; ],<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}<br /><br /> ③ 不查询已获取到的User <br > {<br > &nbsp;&nbsp; "[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp; {<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "content":"xxx",<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Comment[]":[<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]<br > &nbsp;&nbsp;&nbsp;&nbsp; },<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; ],<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}   1.base_url指基地址，一般是顶级域名，其它分支url都是在base_url后扩展。如base_url:http://www.google.com/ ，对应的GET分支url:http://www.google.com/get/ 。下同。<br > 2.请求中的key或value任意一个为null值时，这个 key:value键值对 被视为无效。下同。<br > 3.请求中的 / 需要转义。JSONRequest.java已经用URLEncoder.encode转义，不需要再写；但如果是浏览器或Postman等直接输入url/request，需要把request中的所有 / 都改成 %252F 。下同。<br > 4.code，指返回结果中的状态码，200表示成功，其它都是错误码，值全部都是HTTP标准状态码。下同。<br > 5.msg，指返回结果中的状态信息，对成功结果或错误原因的详细说明。下同。<br > 6.code和msg总是在返回结果的同一层级成对出现。对所有请求的返回结果都会在最外层有一对总结式code和msg。对非GET类型的请求，返回结果里面的每个JSONObject里都会有一对code和msg说明这个JSONObject的状态。下同。<br > 7.id等字段对应的值仅供说明，不一定是数据库里存在的，请求里用的是真实存在的值。下同。  <br /> <br />  ## <h2 id="3">3.对应关系总览<h2/>  ### <h3 id="3.1">3.1 操作方法<h3/>    方法及说明 | URL | Request | Response ------------ | ------------ | ------------ | ------------ GET: <br > 普通获取数据，<br > 明文，<br > 可用浏览器调试 | base_url/get/ | {<br > &nbsp;&nbsp; TableName:{<br > &nbsp;&nbsp;&nbsp;&nbsp; … <br > &nbsp;&nbsp; }<br >} <br > {…}内为限制条件<br ><br > 例如获取一个id为235的Moment：<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":235<br > &nbsp;&nbsp; }<br >} | {<br > &nbsp;&nbsp; TableName:{<br > &nbsp;&nbsp;&nbsp;&nbsp; ...<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}<br >例如<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp; "userId":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp; "content":"APIJSON,let interfaces and documents go to hell !"<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br > } HEAD: <br > 普通获取数量，<br > 明文，<br > 可用浏览器调试 | base_url/head/ | {<br > &nbsp;&nbsp; TableName:{<br > &nbsp;&nbsp;&nbsp;&nbsp; …<br > &nbsp;&nbsp; }<br > } <br > {…}内为限制条件 <br ><br > 例如获取一个id为38710的User所发布的Moment总数：<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "userId":38710<br > &nbsp;&nbsp; }<br >} | {<br > &nbsp;&nbsp; TableName:{<br > &nbsp;&nbsp;&nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp;&nbsp;&nbsp; "msg":"success",<br > &nbsp;&nbsp;&nbsp;&nbsp; "count":10<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >} <br > 例如<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp;&nbsp;&nbsp; "msg":"success",<br > &nbsp;&nbsp;&nbsp;&nbsp; "count":10<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp;  "msg":"success"<br >} POST_GET: <br > 安全/私密获取数据，<br >非明文，<br > 用于获取钱包等<br >对安全性要求高的数据 | base_url/post_get/ | 最外层加一个"tag":tag，其它同GET | 同GET POST_HEAD: <br > 安全/私密获取数量，<br >非明文，<br > 用于获取银行卡数量等<br >对安全性要求高的数据总数 | base_url/post_head/ | 最外层加一个"tag":tag，其它同HEAD | 同HEAD POST: <br > 新增数据，<br > 非明文 | base_url/post/ | {<br > &nbsp;&nbsp; TableName:{<br > &nbsp;&nbsp;&nbsp;&nbsp; …<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "tag":tag<br >} <br > {…}中id由服务端生成，不能传 <br ><br >例如一个id为38710的User发布一个新Moment：<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "userId":38710,<br > &nbsp;&nbsp;&nbsp;&nbsp; "content":"APIJSON,let interfaces and documents go to hell !"<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "tag":"Moment"<br >} | {<br > &nbsp;&nbsp; TableName:{<br > &nbsp;&nbsp;&nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp;&nbsp;&nbsp; "msg":"success",<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":38710<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >}<br >例如<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp;&nbsp;&nbsp; "msg":"success",<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":120<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "code":200,<br > &nbsp;&nbsp; "msg":"success"<br >} PUT: <br > 修改数据，<br > 非明文，<br > 只修改所传的字段 | base_url/put/ | {<br > &nbsp;&nbsp; TableName:{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":id,<br > &nbsp;&nbsp;&nbsp;&nbsp; …<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "tag":tag<br >} <br > {…}中id必传 <br ><br >例如修改id为235的Moment的content：<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":235,<br > &nbsp;&nbsp;&nbsp;&nbsp; "content":"APIJSON,let interfaces and documents go to hell !"<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "tag":"Moment"<br >} | 同POST DELETE: <br > 删除数据，<br > 非明文 | base_url/delete/ | {<br > &nbsp;&nbsp; TableName:{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":id<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "tag":tag<br >} <br > {…}中id必传，一般只传id <br ><br >例如删除id为120的Moment：<br >{<br > &nbsp;&nbsp; "Moment":{<br > &nbsp;&nbsp;&nbsp;&nbsp; "id":120<br > &nbsp;&nbsp; },<br > &nbsp;&nbsp; "tag":"Moment"<br >} | 同POST   1.TableName指要查询的数据库表Table的名称字符串。第一个字符为大写字母，剩下的字符要符合英语字母、数字、下划线中的任何一种。对应的值的类型为JSONObject，结构是 {...}，里面放的是Table的字段(列名)。下同。<br > 2."tag":tag 后面的tag是非GET、HEAD请求中匹配请求的JSON结构的key，一般是要查询的table的名称，由服务端Request表中指定。下同。<br > 3.GET、HEAD请求是开放请求，可任意组合任意嵌套。其它请求为受限制的安全/私密请求，对应的 方法、tag、结构 都必须和 服务端Request表中所指定的 一一对应，否则请求将不被通过。下同。<br > 4.POST_GET与GET、POST_HEAD与HEAD分别为同一类型的操作方法，请求稍有不同但返回结果相同。下同。<br > 5.在HTTP通信中，GET、HEAD方法一般用HTTP GET请求，其它一般用HTTP POST请求。下同。<br > 6.所有JSONObject都视为容器(或者文件夹)，结构为 {...} ，里面可以放普通对象或子容器。下同。<br > 7.每个对象都有一个唯一的路径(或者叫地址)，假设对象名为refKey，则用 key0/key1/.../refKey 表示。下同。  <br >  ### <h3 id="3.2">3.2 功能符<h3/>    功能 | 键值对格式 | 使用示例 ------------ | ------------ | ------------  查询数组 | "key[]":{}，后面是JSONObject，key可省略。当key和里面的Table名相同时，Table会被提取出来，即 {Table:{Content}} 会被转化为 {Content} | [{"User[]":{"User":{}}}](http://139.196.140.118:8080/get/{"User[]":{"count":3,"User":{}}})，查询一个User数组。这里key和Table名都是User，User会被提取出来，即 {"User":{"id", ...}} 会被转化为 {"id", ...}   匹配选项范围 | "key{}":[]，后面是JSONArray，作为key可取的值的选项 | ["id{}":[38710,82001,70793]](http://139.196.140.118:8080/get/{"User[]":{"count":3,"User":{"id{}":[38710,82001,70793]}}})，查询id符合38710,82001,70793中任意一个的一个User数组  匹配条件范围 | "key{}":"条件0,条件1..."，条件为任意SQL比较表达式字符串，非Number类型必须用''包含条件的值，如'a' | ["id{}":"<=80000,\>90000"](http://139.196.140.118:8080/get/{"User[]":{"count":3,"User":{"id{}":"<=80000,\>90000"}}})，查询id符合id\<=80000 \| id>90000的一个User数组  包含选项范围 | "key<\>":Object  =>  "key<\>":[Object]，key对应值的类型必须为JSONArray，Object类型不能为JSON |  ["contactIdList<\>":38710](http://139.196.140.118:8080/get/{"User[]":{"count":3,"User":{"contactIdList<\>":38710}}})，查询contactIdList包含38710的一个User数组  远程调用函数 | "key()":"函数表达式"，函数表达式为 function(Type0:value0,Type1:value1...)。函数参数类型为Object或泛型时可省略类型，即 Object:value 改写为 value | ["isPraised()":"isContain(Collection:praiseUserIdList,userId)"](http://139.196.140.118:8080/get/{"Moment":{"id":301,"isPraised()":"isContain(Collection:praiseUserIdList,userId)"}})，请求完成后会调用 boolean isContain(Collection collection, Object object) 函数，然后变为 "isPraised":true 这种（假设点赞用户id列表包含了userId，即这个User点了赞）  引用赋值 | "key@":"引用路径"，引用路径为用/分隔的字符串。以/开头的是缺省引用路径，从声明key所处容器的父容器路径开始；其它是完整引用路径，从最外层开始。<br /> 被引用的refKey必须在声明key的上面。如果对refKey的容器指定了返回字段，则被引用的refKey必须写在@column对应的值内，例如 "@column":"refKey,key1,..." | ["Moment":{<br /> &nbsp;&nbsp; "userId":38710<br />},<br />"User":{<br /> &nbsp;&nbsp; "id@":"/Moment/userId"<br />}](http://139.196.140.118:8080/get/{"Moment":{"userId":38710},"User":{"id@":"%252FMoment%252FuserId"}})<br /> User内的id引用了与User同级的Moment内的userId，<br />即User.id = Moment.userId，请求完成后<br > "id@":"/Moment/userId" 会变成 "id":38710  模糊搜索 | "key$":"SQL搜索表达式"  =>  "key$":["SQL搜索表达式"]，任意SQL搜索表达式字符串，如 %key%(包含key), key%(以key开始), %k%e%y%(包含字母k,e,y) 等，%表示任意字符 | ["name$":"%m%"](http://139.196.140.118:8080/get/{"User[]":{"count":3,"User":{"name$":"%2525m%2525"}}})，查询name包含"m"的一个User数组  正则匹配 | "key?":"正则表达式"  =>  "key?":["正则表达式"]，任意正则表达式字符串，如 ^[0-9]+$ ，可用于高级搜索 | ["name?":"^[0-9]+$"](http://139.196.140.118:8080/get/{"User[]":{"count":3,"User":{"name%253F":"^[0-9]%252B$"}}})，查询name中字符全为数字的一个User数组  新建别名 | "name:alias"，name映射为alias，用alias替代name。可用于 column,Table,SQL函数 等。只用于GET类型、HEAD类型的请求 | ["@column":"toId:parentId"](http://139.196.140.118:8080/get/{"Comment":{"@column":"id,toId:parentId","id":51}})，将查询的字段toId变为parentId返回  增加 或 扩展 | "key+":Object，Object的类型由key指定，且类型为Number,String,JSONArray中的一种。如 82001,"apijson",["url0","url1"] 等。只用于PUT请求 | "praiseUserIdList+":[82001]，添加一个点赞用户id，即这个用户点了赞  减少 或 去除 | "key-":Object，与"key+"相反 | "balance-":100.00，余额减少100.00，即花费了100元  逻辑运算 | &, \|, ! 逻辑运算符。<br />① & 可用于"key&{}":"条件"等<br />② \| 可用于"key\|{}":"条件", "key\|{}":[]等，一般可省略<br />③ ! 可单独使用，如"key!":Object，也可像&,\|一样配合其他功能符使用 |  ① ["id&{}":">80000,<=90000"](http://139.196.140.118:8080/head/{"User":{"id&{}":">80000,<=90000"}})，即id满足id>80000 & id<=90000<br /> ② ["id\|{}":">90000,<=80000"](http://139.196.140.118:8080/head/{"User":{"id\|{}":">90000,<=80000"}})，同"id{}":">90000,<=80000"，即id满足id>90000 \| id<=80000<br /> ③ ["id!{}":[82001,38710]](http://139.196.140.118:8080/head/{"User":{"id!{}":[82001,38710]}})，即id满足 ! (id=82001 \| id=38710)，可过滤黑名单的消息  数组关键词 | "key":Object，key为 "[]":{} 中{}内的关键词，Object的类型由key指定<br />① "count":Integer，指定查询数量，假设允许查询数组的最大数量为max，则当count在1~max范围内时，查询count个；否则查询max个 <br />② "page":Integer，指定查询页码，从0开始，一般和count一起用<br />③ "query":Integer，指定查询内容<br />0-对象，1-总数，2-以上全部<br />总数关键词为total，和query同级，通过引用赋值得到，如 "total@":"/[]/total" <br />这里query及total仅为GET类型的请求提供方便，一般可直接用HEAD类型的请求获取总数 | ① 查询User数组，最多5个：<br />["count":5](http://139.196.140.118:8080/get/{"[]":{"count":5,"User":{}}})<br /> ② 查询第3页的User数组，每页5个：<br />["count":5,<br />"page":3](http://139.196.140.118:8080/get/{"[]":{"count":5,"page":3,"User":{}}})<br /> ③ 查询User数组和对应的User总数：<br />["[]":{<br /> &nbsp;&nbsp; "query":2,<br /> &nbsp;&nbsp; "User":{}<br />},<br />"total@":"/[]/total"](http://139.196.140.118:8080/get/{"[]":{"query":2,"count":5,"User":{}},"total@":"%252F[]%252Ftotal"})  对象关键词，可自定义 | "@key":Object，@key为 Table:{} 中{}内的关键词，Object的类型由@key指定<br />① "@about":true, 查询字段属性<br />② "@column":"key0,key1...", 指定返回字段<br />③ "@order":"key0,key1+,key2-..."，指定排序方式<br />④ "@group":"key0,key1,key2..."，指定分组方式。如果@column里声明了Table的id，则id也必须在@group中声明；其它情况下必须满足至少一个条件:<br />1.分组的key在@column里声明<br />2.Table主键在@group中声明 <br />⑤ "@having":"function0(...)?valu0,function1(...)?valu1,function2(...)?value2..."，指定SQL函数条件，一般和@group一起用，函数一般在@column里声明 | ① 查询User表中字段的属性：<br />["@about":true](http://139.196.140.118:8080/get/{"User[]":{"User":{"@about":true}}})<br /> ② 只查询id,sex,name这几列并且请求结果也按照这个顺序：<br />["@column":"id,sex,name"](http://139.196.140.118:8080/get/{"User":{"@column":"id,sex,name","id":38710}})<br /> ③ 查询按 name降序、id默认顺序 排序的User数组：<br />["@order":"name-,id"](http://139.196.140.118:8080/get/{"[]":{"count":10,"User":{"@column":"name,id","@order":"name-,id"}}})<br /> ④ 查询按userId分组的Moment数组：<br />["@group":"userId,id"](http://139.196.140.118:8080/get/{"[]":{"count":10,"Moment":%7B"@column":"userId,id","@group":"userId,id"}}})<br /> ⑤ 查询 按userId分组、id最大值>=100 的Moment数组：<br />["@column":"userId,max(id)",<br />"@group":"userId",<br />"@having":"max(id)>=100"](http://139.196.140.118:8080/get/{"[]":{"count":10,"Moment":{"@column":"userId,max(id)","@group":"userId","@having":"max(id)>=100"}}})<br />还可以指定函数返回名：<br />["@column":"userId,max(id):maxId",<br />"@group":"userId",<br />"@having":"maxId>=100"](http://139.196.140.118:8080/get/{"[]":{"count":10,"Moment":{"@column":"userId,max(id):maxId","@group":"userId","@having":"maxId>=100"}}})<br /> ⑥ 从pictureList获取第0张图片：<br />["@position":0, //这里@position为自定义关键词<br />"firstPicture()":"get(Collection:pictureList,int:@position)"](http://139.196.140.118:8080/get/{"User":{"id":38710,"@position":0,"firstPicture()":"get(Collection:pictureList,int:@position)"}})<br /> ...  <br > <br >  ## <h2 id="4">4.快速上手<h2/>  ### <h3 id="4.1">4.1 下载后解压APIJSON工程<h3/>  Clone or download &gt; Download ZIP &gt; 解压到一个路径并记住这个路径。  #### 你可以跳过步骤4.2和步骤4.3，用我的服务器IP地址 139.196.140.118:8080 来测试服务端对客户端请求的返回结果。  ### <h3 id="4.2">4.2 导入表文件到数据库<h3/>  服务端需要MySQL Server和MySQLWorkbench，没有安装的都先下载安装一个。<br /> 我的配置是Windows 7 + MySQL Community Server 5.7.16 + MySQLWorkbench 6.3.7 和 OSX EI Capitan + MySQL Community Server 5.7.16 + MySQLWorkbench 6.3.8，其中系统和软件都是64位的。  启动MySQLWorkbench &gt; 进入一个Connection &gt; 点击Server菜单 &gt; Data Import &gt; 选择刚才解压路径下的APIJSON-Master/table &gt; Start Import &gt; 刷新SCHEMAS, 左下方sys/tables会出现添加的table。  ### <h3 id="4.3">4.3 用Eclipse for JavaEE或IntellIJ IDEA Ultimate运行服务端工程<h3/>  如果以上编辑器一个都没安装，运行前先下载安装一个。<br /> 我的配置是Windows 7 + JDK 1.7.0_71 + Eclipse 4.6.1 + IntellIJ 2016.3 和 OSX EI Capitan + JDK 1.8.0_91 + Eclipse 4.6.1 + IntellIJ 2016.2.5  #### Eclipse for JavaEE  1.导入<br /> File > Import > Maven > Existing Maven Projects > Next > Browse > 选择刚才解压路径下的APIJSON-Master/APIJSON(Server)/APIJSON(Eclipse_JEE) > Finish  2.运行<br /> Run > Run As > Java Application > 选择APIJSONApplication > OK  #### IntellIJ IDEA Ultimate  1.导入<br /> Open > 选择刚才解压路径下的APIJSON-Master/APIJSON(Server)/APIJSON(Idea) > OK  2.运行<br /> Run > Run APIJSONApplication  ### <h3 id="4.4">4.4 用ADT Bundle或Android Studio运行客户端工程<h3/>  可以跳过这个步骤，直接下载下方提供的客户端App。  如果以上编辑器一个都没安装，运行前先下载安装一个。<br /> 我的配置是Windows 7 + JDK 1.7.0_71 + ADT Bundle 20140702 + Android Studio 2.2 和 OSX EI Capitan +（JDK 1.7.0_71 + ADT Bundle 20140702）+（JDK 1.8.0_91 + Android Studio 2.1.2），其中系统和软件都是64位的。  #### ADT Bundle  1.导入<br /> File > Import > Android > Existing Android Code Into Workspace > Next > Browse > 选择刚才解压路径下的APIJSON-Master/APIJSON(Android)/APIJSON(ADT) > Finish  2.运行<br /> Run > Run As > Android Application  #### Android Studio  1.导入<br /> Open an existing Android Studio project > 选择刚才解压路径下的APIJSON-Master/APIJSON(Android)/APIJSON(AndroidStudio)/APIJSONApp （或APIJSONTest） > OK  2.运行<br /> Run > Run app  ### <h3 id="4.5">4.5 操作客户端App<h3/>  选择发送APIJSON请求并等待显示结果。<br /> 如果默认url不可用，修改为一个可用的，比如正在运行APIJSON服务端工程的电脑的IPV4地址，然后点击查询按钮重新请求。  <br /> <br />  ## <h2 id="5">5.其它<h2/>  ### <h3 id="5.1">5.1 相关推荐<h3/> [APIJSON, 让接口和文档见鬼去吧！](https://my.oschina.net/tommylemon/blog/805459)  [仿QQ空间和微信朋友圈，高解耦高复用高灵活](https://my.oschina.net/tommylemon/blog/885787)  [3步创建APIJSON服务端新表及配置](https://my.oschina.net/tommylemon/blog/889074)  ### <h3 id="5.2">5.2 关于作者<h3/> TommyLemon：[https://github.com/TommyLemon](https://github.com/TommyLemon)<br /> QQ群：607020115  如果有什么问题或建议可以[提ISSUE](https://github.com/TommyLemon/APIJSON/issues)、加群或者[发我邮件](https://github.com/TommyLemon)，交流技术，分享经验。<br > 如果你解决了某些bug，或者新增了一些通用性强的功能，欢迎[贡献代码](https://github.com/TommyLemon/APIJSON/pulls)，感激不尽^_^  ### <h3 id="5.3">5.3 下载试用客户端App<h3/>  仿微信朋友圈动态实战项目<br /> [APIJSONApp.apk](http://files.cnblogs.com/files/tommylemon/APIJSONApp.apk)  测试及自动生成代码工具<br /> [APIJSONTest.apk](http://files.cnblogs.com/files/tommylemon/APIJSONTest.apk)  ### <h3 id="5.4">5.4 更新日志<h3/> [https://github.com/TommyLemon/APIJSON/commits/master](https://github.com/TommyLemon/APIJSON/commits/master)  ### <h3 id="5.5">5.5 点Star支持我，点Fork研究它<h3/>  [https://github.com/TommyLemon/APIJSON](https://github.com/TommyLemon/APIJSON)
bluelinelabs/LoganSquare	[![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-LoganSquare-brightgreen.svg?style=flat)](https://android-arsenal.com/details/1/1550) [![Android Weekly](https://img.shields.io/badge/Android%20Weekly-141-blue.svg?style=flat)](http://androidweekly.net/issues/issue-141) [![Travis Build](https://travis-ci.org/bluelinelabs/LoganSquare.svg)](https://travis-ci.org/bluelinelabs/LoganSquare)  #LoganSquare  The fastest JSON parsing and serializing library available for Android. Based on Jackson's streaming API, LoganSquare is able to consistently outperform GSON and Jackson's Databind library by 400% or more<sup>[1](#1)</sup>. By relying on compile-time annotation processing to generate code, you know that your JSON will parse and serialize faster than any other method available.  By using this library, you'll be able to utilize the power of Jackson's streaming API without having to code tedius, low-level code involving `JsonParser`s or `JsonGenerator`s. Instead, just annotate your model objects as a `@JsonObject` and your fields as `@JsonField`s and we'll do the heavy lifting for you.  Don't believe it could improve upon Jackson Databind's or GSON's performance that much? Well, then check out the nifty graphs below for yourself. Not convinced? Feel free to build and run the BenchmarkDemo app included in this repository.  <a name="1"></a> *<sup>1</sup> <sub>Note: Our "400% or more" performance improvement metric was determined using ART. While LoganSquare still comes out on top with Dalvik, it seems as though the comparison is much closer. The benchmarks shown are actual screenshots taken from a 2nd gen Moto X.<sub>*  ![Benchmarks](docs/benchmarks.jpg)  ##Download  Note that Gradle is the only supported build configuration for LoganSquare. To add the library to your app's build.gradle file.  ```groovy buildscript {     repositories {         jcenter()     }     dependencies {         classpath 'com.neenbedankt.gradle.plugins:android-apt:1.8'     } } apply plugin: 'com.neenbedankt.android-apt'  dependencies {     apt 'com.bluelinelabs:logansquare-compiler:1.3.6'     compile 'com.bluelinelabs:logansquare:1.3.6' }  ``` For the curious, the buildscript and apply plugin lines add the [apt plugin](https://bitbucket.org/hvisser/android-apt), which is what allows us to do compile-time annotation processing. The first dependency is what tells Gradle to process your JSON annotations, and the second dependency is our tiny 19kb runtime library that interfaces with the generated code for you.  ##Usage  Using LoganSquare is about as easy as it gets. Here are a few docs to get you started:   * [Creating your models](docs/Models.md)  * [Parsing from JSON](docs/Parsing.md)  * [Serializing to JSON](docs/Serializing.md)  * [Supporting custom types](docs/TypeConverters.md)  ##Proguard Like all libraries that generate dynamic code, Proguard might think some classes are unused and remove them. To prevent this, the following lines can be added to your proguard config file.  ``` -keep class com.bluelinelabs.logansquare.** { *; } -keep @com.bluelinelabs.logansquare.annotation.JsonObject class * -keep class **$$JsonObjectMapper { *; } ```  ##Why LoganSquare?  We're BlueLine Labs, a mobile app development company based in Chicago. We love this city so much that we named our company after the blue line of the iconic 'L.' And what's one of the most popular stops on the blue line? Well, that would be Logan Square of course. Does it have anything to do with JSON? Nope, but we're okay with that.  ##Props   * [Jackson's streaming API](https://github.com/FasterXML/jackson-core) for being a super-fast, awesome base for this project.  * [Instagram's ig-json-parser](https://github.com/Instagram/ig-json-parser) for giving us the idea for this project.  * [Jake Wharton's Butterknife](https://github.com/JakeWharton/butterknife) for being a great reference for annotation processing and code generation.  ##License      Copyright 2015 BlueLine Labs, Inc.      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
square/moshi	Moshi =====  Moshi is a modern JSON library for Android and Java. It makes it easy to parse JSON into Java objects:  ```java String json = ...;  Moshi moshi = new Moshi.Builder().build(); JsonAdapter<BlackjackHand> jsonAdapter = moshi.adapter(BlackjackHand.class);  BlackjackHand blackjackHand = jsonAdapter.fromJson(json); System.out.println(blackjackHand); ```  And it can just as easily serialize Java objects as JSON:  ```java BlackjackHand blackjackHand = new BlackjackHand(     new Card('6', SPADES),     Arrays.asList(new Card('4', CLUBS), new Card('A', HEARTS)));  Moshi moshi = new Moshi.Builder().build(); JsonAdapter<BlackjackHand> jsonAdapter = moshi.adapter(BlackjackHand.class);  String json = jsonAdapter.toJson(blackjackHand); System.out.println(json); ```  ### Built-in Type Adapters  Moshi has built-in support for reading and writing Java’s core data types:   * Primitives (int, float, char...) and their boxed counterparts (Integer, Float, Character...).  * Arrays, Collections, Lists, Sets, and Maps  * Strings  * Enums  It supports your model classes by writing them out field-by-field. In the example above Moshi uses these classes:  ```java class BlackjackHand {   public final Card hidden_card;   public final List<Card> visible_cards;   ... }  class Card {   public final char rank;   public final Suit suit;   ... }  enum Suit {   CLUBS, DIAMONDS, HEARTS, SPADES; } ```  to read and write this JSON:  ```json {   "hidden_card": {     "rank": "6",     "suit": "SPADES"   },   "visible_cards": [     {       "rank": "4",       "suit": "CLUBS"     },     {       "rank": "A",       "suit": "HEARTS"     }   ] } ```  The [Javadoc][javadoc] catalogs the complete Moshi API, which we explore below.  ### Custom Type Adapters  With Moshi, it’s particularly easy to customize how values are converted to and from JSON. A type adapter is any class that has methods annotated `@ToJson` and `@FromJson`.  For example, Moshi’s default encoding of a playing card is verbose: the JSON defines the rank and suit in separate fields: `{"rank":"A","suit":"HEARTS"}`. With a type adapter, we can change the encoding to something more compact: `"4H"` for the four of hearts or `"JD"` for the jack of diamonds:  ```java class CardAdapter {   @ToJson String toJson(Card card) {     return card.rank + card.suit.name().substring(0, 1);   }    @FromJson Card fromJson(String card) {     if (card.length() != 2) throw new JsonDataException("Unknown card: " + card);      char rank = card.charAt(0);     switch (card.charAt(1)) {       case 'C': return new Card(rank, Suit.CLUBS);       case 'D': return new Card(rank, Suit.DIAMONDS);       case 'H': return new Card(rank, Suit.HEARTS);       case 'S': return new Card(rank, Suit.SPADES);       default: throw new JsonDataException("unknown suit: " + card);     }   } } ```  Register the type adapter with the `Moshi.Builder` and we’re good to go.  ```java Moshi moshi = new Moshi.Builder()     .add(new CardAdapter())     .build(); ```  Voilà:  ```json {   "hidden_card": "6S",   "visible_cards": [     "4C",     "AH"   ] } ```  #### Another example  Note that the method annotated with `@FromJson` does not need to take a String as an argument. Rather it can take input of any type and Moshi will first parse the JSON to an object of that type and then use the `@FromJson` method to produce the desired final value. Conversely, the method annotated with `@ToJson` does not have to produce a String.  Assume, for example, that we have to parse a JSON in which the date and time of an event are represented as two separate strings.  ```json {   "title": "Blackjack tournament",   "begin_date": "20151010",   "begin_time": "17:04" } ```  We would like to combine these two fields into one string to facilitate the date parsing at a later point. Also, we would like to have all variable names in CamelCase. Therefore, the `Event` class we want Moshi to produce like this:  ```java class Event {   String title;   String beginDateAndTime; } ```  Instead of manually parsing the JSON line per line (which we could also do) we can have Moshi do the transformation automatically. We simply define another class `EventJson` that directly corresponds to the JSON structure:  ```java class EventJson {   String title;   String begin_date;   String begin_time; } ```  And another class with the appropriate `@FromJson` and `@ToJson` methods that are telling Moshi how to convert an `EventJson` to an `Event` and back. Now, whenever we are asking Moshi to parse a JSON to an `Event` it will first parse it to an `EventJson` as an intermediate step. Conversely, to serialize an `Event` Moshi will first create an `EventJson` object and then serialize that object as usual.  ```java class EventJsonAdapter {   @FromJson Event eventFromJson(EventJson eventJson) {     Event event = new Event();     event.title = eventJson.title;     event.beginDateAndTime = eventJson.begin_date + " " + eventJson.begin_time;     return event;   }    @ToJson EventJson eventToJson(Event event) {     EventJson json = new EventJson();     json.title = event.title;     json.begin_date = event.beginDateAndTime.substring(0, 8);     json.begin_time = event.beginDateAndTime.substring(9, 14);     return json;   } } ```  Again we register the adapter with Moshi.  ```java Moshi moshi = new Moshi.Builder()     .add(new EventJsonAdapter())     .build(); ```  We can now use Moshi to parse the JSON directly to an `Event`.  ```java JsonAdapter<Event> jsonAdapter = moshi.adapter(Event.class); Event event = jsonAdapter.fromJson(json); ```  ### Parse JSON Arrays  Say we have a JSON string of this structure:  ```java [ 	 { 	   "rank": "4", 	   "suit": "CLUBS" 	 }, 	 { 	   "rank": "A", 	   "suit": "HEARTS" 	 } ] ```  We can now use Moshi to parse the JSON string into a `List<Card>`.  ```java String cardsJsonResponse = ...; Type type = Types.newParameterizedType(List.class, Card.class); JsonAdapter<List<Card>> adapter = moshi.adapter(type); List<Card> cards = adapter.fromJson(cardsJsonResponse); ```  ### Fails Gracefully  Automatic databinding almost feels like magic. But unlike the black magic that typically accompanies reflection, Moshi is designed to help you out when things go wrong.  ``` JsonDataException: Expected one of [CLUBS, DIAMONDS, HEARTS, SPADES] but was ANCHOR at path $.visible_cards[2].suit   at com.squareup.moshi.JsonAdapters$11.fromJson(JsonAdapters.java:188)   at com.squareup.moshi.JsonAdapters$11.fromJson(JsonAdapters.java:180) 	... ```  Moshi always throws a standard `java.io.IOException` if there is an error reading the JSON document, or if it is malformed. It throws a `JsonDataException` if the JSON document is well-formed, but doesn’t match the expected format.  ### Built on Okio  Moshi uses [Okio][okio] for simple and powerful I/O. It’s a fine complement to [OkHttp][okhttp], which can share buffer segments for maximum efficiency.  ### Borrows from Gson  Moshi uses the same streaming and binding mechanisms as [Gson][gson]. If you’re a Gson user you’ll find Moshi works similarly. If you try Moshi and don’t love it, you can even migrate to Gson without much violence!  But the two libraries have a few important differences:   * **Moshi has fewer built-in type adapters.** For example, you need to configure your own date    adapter. Most binding libraries will encode whatever you throw at them. Moshi refuses to    serialize platform types (`java.*`, `javax.*`, and `android.*`) without a user-provided type    adapter. This is intended to prevent you from accidentally locking yourself to a specific JDK or    Android release.  * **Moshi is less configurable.** There’s no field naming strategy, versioning, instance creators,    or long serialization policy. Instead of naming a field `visibleCards` and using a policy class    to convert that to `visible_cards`, Moshi wants you to just name the field `visible_cards` as it    appears in the JSON.  * **Moshi doesn’t have a `JsonElement` model.** Instead it just uses built-in types like `List` and    `Map`.  * **No HTML-safe escaping.** Gson encodes `=` as `\u003d` by default so that it can be safely    encoded in HTML without additional escaping. Moshi encodes it naturally (as `=`) and assumes that    the HTML encoder – if there is one – will do its job.  ### Custom field names with @Json  Moshi works best when your JSON objects and Java objects have the same structure. But when they don't, Moshi has annotations to customize data binding.  Use `@Json` to specify how Java fields map to JSON names. This is necessary when the JSON name contains spaces or other characters that aren’t permitted in Java field names. For example, this JSON has a field name containing a space:  ```json {   "username": "jesse",   "lucky number": 32 } ```  With `@Json` its corresponding Java class is easy:  ```java class Player {   String username;   @Json(name = "lucky number") int luckyNumber;    ... } ```  Because JSON field names are always defined with their Java fields, Moshi makes it easy to find fields when navigating between Java and JSON.  ### Alternate type adapters with @JsonQualifier  Use `@JsonQualifier` to customize how a type is encoded for some fields without changing its encoding everywhere. This works similarly to the qualifier annotations in dependency injection tools like Dagger and Guice.  Here’s a JSON message with two integers and a color:  ```json {   "width": 1024,   "height": 768,   "color": "#ff0000" } ```  By convention, Android programs also use `int` for colors:  ```java class Rectangle {   int width;   int height;   int color; } ```  But if we encoded the above Java class as JSON, the color isn't encoded properly!  ```json {   "width": 1024,   "height": 768,   "color": 16711680 } ```  The fix is to define a qualifier annotation, itself annotated `@JsonQualifier`:  ```java @Retention(RUNTIME) @JsonQualifier public @interface HexColor { } ```  Next apply this `@HexColor` annotation to the appropriate field:  ```java class Rectangle {   int width;   int height;   @HexColor int color; } ```  And finally define a type adapter to handle it:  ```java /** Converts strings like #ff0000 to the corresponding color ints. */ class ColorAdapter {   @ToJson String toJson(@HexColor int rgb) {     return String.format("#%06x", rgb);   }    @FromJson @HexColor int fromJson(String rgb) {     return Integer.parseInt(rgb.substring(1), 16);   } } ```  Use `@JsonQualifier` when you need different JSON encodings for the same type. Most programs shouldn’t need this `@JsonQualifier`, but it’s very handy for those that do.  ### Omit fields with `transient`  Some models declare fields that shouldn’t be included in JSON. For example, suppose our blackjack hand has a `total` field with the sum of the cards:  ```java public final class BlackjackHand {   private int total;    ... } ```  By default, all fields are emitted when encoding JSON, and all fields are accepted when decoding JSON. Prevent a field from being included by adding Java’s `transient` keyword:  ```java public final class BlackjackHand {   private transient int total;    ... } ```  Transient fields are omitted when writing JSON. When reading JSON, the field is skipped even if the JSON contains a value for the field. Instead it will get a default value.   ### Default Values & Constructors  When reading JSON that is missing a field, Moshi relies on the the Java or Android runtime to assign the field’s value. Which value it uses depends on whether the class has a no-arguments constructor.  If the class has a no-arguments constructor, Moshi will call that constructor and whatever value it assigns will be used. For example, because this class has a no-arguments constructor the `total` field is initialized to `-1`.  ```java public final class BlackjackHand {   private int total = -1;   ...    private BlackjackHand() {   }    public BlackjackHand(Card hidden_card, List<Card> visible_cards) {     ...   } } ```  If the class doesn’t have a no-arguments constructor, Moshi can’t assign the field’s default value, **even if it’s specified in the field declaration**. Instead, the field’s default is always `0` for numbers, `false` for booleans, and `null` for references. In this example, the default value of `total` is `0`!  ```java public final class BlackjackHand {   private int total = -1;   ...    public BlackjackHand(Card hidden_card, List<Card> visible_cards) {     ...   } } ```  This is surprising and is a potential source of bugs! For this reason consider defining a no-arguments constructor in classes that you use with Moshi, using `@SuppressWarnings("unused")` to prevent it from being inadvertently deleted later:  ```java public final class BlackjackHand {   private int total = -1;   ...    @SuppressWarnings("unused") // Moshi uses this!   private BlackjackHand() {   }    public BlackjackHand(Card hidden_card, List<Card> visible_cards) {     ...   } } ```  ### Kotlin Support  Kotlin classes work with Moshi out of the box, with the exception of annotations. If you need to annotate your Kotlin classes with an `@Json` annotation or otherwise, you will need to use the `moshi-kotlin` artifact, and set up Moshi to use its converter factory: ```kotlin val moshi = Moshi.Builder()     .add(KotlinJsonAdapterFactory())     .build() ```  Download --------  Download [the latest JAR][dl] or depend via Maven: ```xml <dependency>   <groupId>com.squareup.moshi</groupId>   <artifactId>moshi</artifactId>   <version>1.5.0</version> </dependency> ``` or Gradle: ```groovy compile 'com.squareup.moshi:moshi:1.5.0' ``` and for additional Kotlin support: ```xml <dependency>   <groupId>com.squareup.moshi</groupId>   <artifactId>moshi-kotlin</artifactId>   <version>1.5.0</version> </dependency> ``` or Gradle: ```groovy compile 'com.squareup.moshi:moshi-kotlin:1.5.0' ```  Snapshots of the development version are available in [Sonatype's `snapshots` repository][snap].   ProGuard --------  If you are using ProGuard you might need to add the following options: ``` -dontwarn okio.** -dontwarn javax.annotation.** -keepclasseswithmembers class * {     @com.squareup.moshi.* <methods>; } -keep @com.squareup.moshi.JsonQualifier interface * ``` Additional rules are needed if you are using the Kotlin artifact: ``` -keepclassmembers class kotlin.Metadata {     public <methods>; } ```  License --------      Copyright 2015 Square, Inc.      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.    [dl]: https://search.maven.org/remote_content?g=com.squareup.moshi&a=moshi&v=LATEST  [snap]: https://oss.sonatype.org/content/repositories/snapshots/com/squareup/moshi/  [okio]: https://github.com/square/okio/  [okhttp]: https://github.com/square/okhttp/  [gson]: https://github.com/google/gson/  [javadoc]: http://square.github.io/moshi/1.x/moshi/
sonian/elasticsearch-jetty	h1. Jetty Plugin  h2. Compatibility  The following table shows the versions of elasticsearch and jetty that Jetty Plugin was built with.  |_. Jetty Plugin |_.  Elasticsearch    |_.  Jetty  | | 1.2.1 | 1.2.1    | 8.1.14.v20131031 | | 1.1.1-beta | 1.1.1    | 8.1.14.v20131031 | | 1.1.0-beta | 1.1.0    | 8.1.14.v20131031 | | 1.0.1    | 1.0.1    | 8.1.14.v20131031 | | 0.90.12   | 0.90.12   | 8.1.14.v20131031   | | 0.90.0   | 0.90.0   | 8.1.4.v20120524   | | 0.20.1   | 0.20.2   | 8.1.4.v20120524   | | 0.19.9-master | 0.19.9  | 8.1.4.v20120524   | | 0.19.6-0.19.8 | 0.19.8  | 8.1.4.v20120524   | | 0.19.0-0.19.5   | 0.19.2  | 7.4.5.v20110725   | | 0.18.1-0.18.6   | 0.18.5  | 7.4.5.v20110725   | | 0.18.0   | 0.18.4   | 7.4.5.v20110725   |  h2. Overview  The elasticsearch-jetty plugin brings full power of Jetty and adds several new features to elasticsearch. With this plugin elasticsearch can now handle SSL connections, support basic authentication, and log all or some incoming requests in plain text or json formats.  h2. Installation and Configuration  h3. Installation  The elasticsearch-jetty plugin can be installed as any other ES plugin using bin/plugin utility:  (specifying the URL is required since github downloads are going away)  <pre> $ bin/plugin -url https://oss-es-plugins.s3.amazonaws.com/elasticsearch-jetty/elasticsearch-jetty-1.2.1.zip -install elasticsearch-jetty-1.2.1 </pre>  The core of the plugin is JettyHttpServerTransport module that works as a replacement for NettyHttpServerTransport. To enable the elasticsearch-jetty plugin, the default netty http transport should be replaced with jetty http transport by adding the following line to @elasticsearch.yml@.  <pre> http.type: com.sonian.elasticsearch.http.jetty.JettyHttpServerTransportModule </pre>  The elasticsearch-jetty plugin adds @Server: Jetty(8.1.4.v20120524)@ header to all responses. So, it's possible to verify that jetty plugin is running by checking the response headers using the following curl command:  <pre> $ curl -I "http://localhost:9200/" HTTP/1.1 200 OK Content-Type: text/plain;charset=UTF-8 Access-Control-Allow-Origin: * Content-Length: 0 Server: Jetty(8.1.4.v20120524) </pre>   h3. Configuration Files  The embedded jetty is configured using standard @jetty-*.xml@ files. The list of config files can be specified using the @sonian.elasticsearch.http.jetty.config@ setting. This setting should contain a comma-separated list of jetty configuration files. The files are loaded one by one in the order specified in the setting and used to configure the Jetty server. Elasticsearch tries to find each file by checking the following locations:  * absolute path * elasticsearch config directory * elasticsearch classpath * config directory in elasticsearch classpath  If the @sonian.elasticsearch.http.jetty.config@ setting is not set, the elasticsearch-jetty plugin tries to locate and load a single file called @jetty.xml@.  The elasticsearch-jetty plugin comes with several generic @jetty-*.xml@ configuration files that can be used to simplify the plugin setup. These files can be found in the @plugins/jetty/config@ directory.  * <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty.xml">jetty.xml</a> - basic elasticsearch configuration file that should be always the first file in the list of config files. * <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-hash-auth.xml">jetty-hash-auth.xml</a> - adds login service for basic file-based authentication. * <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-restrict-all.xml">jetty-restrict-all.xml</a> and <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-restrict-writes.xml">jetty-restrict-writes.xml</a> - set of security constraint that requires password for *all* or *write* access to elasticsearch. Only one of these files should be used at a time. * <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-ssl.xml">jetty-ssl.xml</a> and <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-strong-ssl.xml">jetty-strong-ssl.xml</a> - both files add an SSL connector, the second file limits ciphers that SSL connector can use to only known strong ciphers. Only one of these files should be used at a time. * <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-gzip.xml">jetty-gzip.xml</a> - enables GZip support. If used, this file should be the last file in the list.  h3. Adding SSL Support  First step in enabling SSL support is generation of keys and certificates. The process is described on the <a href="http://wiki.eclipse.org/Jetty/Howto/Configure_SSL">How to Configure SSL page</a>. For the test purposes, the <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/keystore">keystore</a> file from the elasticsearch-jetty plugin can be also used. The generated or downloaded keystore file should be places in the @config@ directory of elasticsearch. The SSL connector can be enabled by adding the following settings in the @elasticsearch.yml@ file:  <pre> http.type: com.sonian.elasticsearch.http.jetty.JettyHttpServerTransportModule sonian.elasticsearch.http.jetty:     config: jetty.xml,jetty-ssl.xml     ssl_port: 9443     keystore_password: "OBF:1nc01vuz1w8f1w1c1rbu1rac1w261w9b1vub1ndq" </pre>  The @keystore_password@ should contain the password used for keystore generation. The password @"OBF:1nc01vuz1w8f1w1c1rbu1rac1w261w9b1vub1ndq"@ can be used with the test keystore downloaded from the elasticsearch-jetty plugin page. The @jetty-strong-ssl.xml@ config file can be used instead of @jetty-ssl.xml@ if it's required to disable known weak ciphers and protocols. The password can be obfuscated using Jetty <a href="http://wiki.eclipse.org/Jetty/Howto/Secure_Passwords">Password Utility</a>.  h3. Adding Basic Authentication  Setting up authentication for elasticsearch starts with configuring login service. The elasticsearch-jetty plugin comes with sample script <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-hash-auth.xml">jetty-hash-auth.xml</a> that shows how to setup <a href="http://wiki.eclipse.org/Jetty/Tutorial/Realms#HashLoginService">HashLoginService</a>. This service obtains usersnames, passwords and roles from the file @realm.properties@ in the @config@ directory of elasticsearch. The file should contain a list of users, one line per user in the following format:  <pre> username: password[,rolename ...] </pre>  For example:  @config/realm.properties@  <pre> superuser: Adm1n,admin,readwrite user: Passw0rd,readwrite </pre>  The passwords can be obfuscated or MD5 hashed using Jetty <a href="http://wiki.eclipse.org/Jetty/Howto/Secure_Passwords">Password Utility</a>  After the login service is configured, the next step is to set security constraints. The elasticsearch-jetty plugin comes with two sample constraints configuration files: <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-restrict-all.xml">jetty-restrict-all.xml</a> and <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/jetty-restrict-writes.xml">jetty-restrict-writes.xml</a> that demonstrate two different approaches to controlling access to elasticsearch. The first file requires password for any access to elasticsearch, and the second one uses more granular restrictions depending on the type of the access.  The following settings in @elasticsearch.yml@ will enable granular restrictions:  <pre> http.type: com.sonian.elasticsearch.http.jetty.JettyHttpServerTransportModule sonian.elasticsearch.http.jetty:     config: jetty.xml,jetty-hash-auth.xml,jetty-restrict-writes.xml </pre>  Authentication can be used with SSL connector. The following settings will restrict access to all pages and enable SSL connector:  <pre> http.type: com.sonian.elasticsearch.http.jetty.JettyHttpServerTransportModule sonian.elasticsearch.http.jetty:     config: jetty.xml,jetty-ssl.xml,jetty-hash-auth.xml,jetty-restrict-all.xml     ssl_port: 9443     keystore_password: "OBF:1nc01vuz1w8f1w1c1rbu1rac1w261w9b1vub1ndq" </pre>  Security constraints in the elasticsearch-jetty plugin is very similar to security constraints in Jetty. The only difference is the custom security handler @com.sonian.elasticsearch.http.jetty.security.RestConstraintSecurityHandler@, which is a modified version of standard Jetty security handler @org.eclipse.jetty.security.ConstraintSecurityHandler@. The standard Jetty security handler is using servlet spec for path mapping, which makes it very difficult to express elasticsearch paths. The @RestConstraintSecurityHandler@ behaves identically to @ConstraintSecurityHandler@ but it treats path wildcards in a slightly different way. @RestConstraintSecurityHandler@ supports both named @{name}@ and anonymous @*@ wildcards. Both types of wildcards match exactly one element of the path and can appear at any position in the path. For example: @/{index}/_search@ will match @/myindex/_search@ but will not match @/_search@ or @/myindex/mytype/_search@.  h3. Request Logging  The elasticsearch-jetty plugin contains two versions of the HTTP Server Transport: @JettyHttpServerTransport@ and @FilterHttpTransportModule@. While @JettyHttpServerTransport@ uses Jetty to handle all incoming requests, @FilterHttpTransportModule@ simply wraps another HTTP Server Transport (@JettyHttpServerTransport@ by default) to filter requests and responses. @FilterHttpTransportModule@ can be configured to pass all incoming requests through a chain of filters. The elasticsearch-jetty plugin contains one such filter that can be used for request logging.  In order to setup request logging, elasticsearch should be switched to use @FilterHttpTransportModule@. It can be done by the following setting:  <pre> http.type: com.sonian.elasticsearch.http.filter.FilterHttpServerTransportModule </pre>  Then @FilterHttpTransportModule@ has to be configured with an appropriate filter:  <pre> sonian.elasticsearch.http.filter:     http_filter_chain: ["logging"]     http_filter:         # Request logging filter         logging:             type: com.sonian.elasticsearch.http.filter.logging.LoggingFilterHttpServerAdapter             logger: request             format: text             level: INFO             log_body: false </pre>  This configuration will create one logger with the name @request@ that will log all incoming requests in plain text format on the INFO level and request bodies will not be logged. By default, this new logger will log all messages into the same log file as a standard elasticsearch logger. It's possible to redirect requests logging into a separate log file. The following @logging.yml@ configuration specifies that logger @request@ logs on the @INFO@ level and above using appender @request_log_file@, which is the same @dailyRollingFile@ appender that is used by elasticsearch, but it logs into @logs/elasticsearch-request.log@ instead of @logs/elasticsearch.log@ file. Dots "........." in the example below indicate existing lines in the @logger@ and @appender@ sections, that should be left intact.  <pre> logger:   ........   request: INFO, request_log_file  additivity:   request: false  appender:   .........   request_log_file:       type: dailyRollingFile       file: ${path.logs}/${cluster.name}_requests.log       datePattern: "'.'yyyy-MM-dd"       layout:         type: pattern         conversionPattern: "[%d{ABSOLUTE}] %m%n" </pre>  Not all requests are equally important. For example, @/_cluster/health@ requests can be ignored, and body of a @/_search@ request is an important part of the request. So, the @LoggingFilterHttpServerAdapter@ can be configured to treat different request in different ways. The following @elasticsearch.yml@ fragment will cause cluster health, state, info and stats requests to be logged on the @TRACE@ level, while all @_search@ and @_count@ requests will be logged with request bodies.  <pre> sonian.elasticsearch.http.filter:     http_filter_chain: ["logging"]     http_filter:         # Request logging filter         logging:             logger: request             format: text             type: com.sonian.elasticsearch.http.filter.logging.LoggingFilterHttpServerAdapter             level: INFO             log_body: false             loggers:                 stats:                     path: ["/_cluster/health", "/_cluster/nodes", "/_cluster/state", "/_cluster/nodes/{node}/stats"]                     method: GET                     level: TRACE                 searches:                     path: ["/_search", "/_search/scroll", "/_search/scroll/{scroll_id}", "/{index}/_search",                             "/{index}/{type}/_search", "/{index}/{type}/{id}/_mlt"]                     method: GET, POST                     log_body: true                 count:                     path: ["/_count", "/{index}/_count", "/{index}/{type}/_count"]                     method: GET, POST                     log_body: true </pre>  The @LoggingFilterHttpServerAdapter@ can also support @json@ format, which is more difficult to read, but much easier to parse and index. @FilterHttpTransportModule@ supports chaining of multiple filters, see <a href="https://github.com/sonian/elasticsearch-jetty/blob/master/config/elasticsearch.xml">elasticsearch.xml</a> for an example.  h3. GZip Compression of Responses  Compression of responses can be enabled by adding @jetty-gzip.xml@ to the *end* of the configuration file list:  <pre> sonian.elasticsearch.http.jetty:     port: 9200     ssl_port: 9443     keystore_password: "OBF:1nc01vuz1w8f1w1c1rbu1rac1w261w9b1vub1ndq"     config: jetty.xml,jetty-strong-ssl.xml,jetty-hash-auth.xml,jetty-restrict-writes.xml,jetty-gzip.xml </pre>  h3. Jetty Plugin Settings  The following settings under @sonian.elasticsearch.http.jetty@ can be used to configure elasticsearch-jetty plugin  |_. Setting  |_.  Description    |_. Default |_. Used by | | @config@  | List of jetty config files  | @jetty.xml@ | plugin | | @server_id@   | The id of the Jetty Server that will handle elasticsearch requests | @ESServer@ | plugin | | @port@   | The port Jetty should listen on | @http.port@ or @9200-9300@  | @jetty.xml@ | | @bind_host@   | The port Jetty should listen on | @http.bind_host@ or @http.host@ | @jetty.xml@ | | @publish_host@   | The port Jetty should listen on | @http.publish_host@ or @http.host@ | @jetty.xml@ | | @ssl_port@  | The port SSL connector should listen on | | @jetty-ssl.xml@ and @jetty-strong-ssl.xml@ | | @ssl_bind_host@  | The bind host SSL connector should use | | @jetty-ssl.xml@ and @jetty-strong-ssl.xml@ | | @keystore_password@  | The keystore password for SSL connector. Plain text of obfuscated passwords can be used. Hashed passwords are not supported. |  | @jetty-ssl.xml@ and @jetty-strong-ssl.xml@ |  h2. Advanced Configuration  h3. Custom configuration  The elasticsearch-jetty plugin comes with several sample @jetty*.xml@ files, but it can be used with entirely custom configuration. For the plugin to function correctly the custom configuration has to only satisfy the following two requirements:  * The custom configuration has to create at least one Jetty server. * The first connector of the created Jetty server has to be Internet Socket Connector. If the custom configuration creates more than one Jetty Server, the plugin tries to find server with ID specified in the @sonian.elasticsearch.http.jetty.server_id@, which is @ESServer@ by default. If such server doesn't exists, elasticsearch picks the Jetty Server that was created first. * All elasticsearch requests have to be handled by @JettyHttpServerTransportHandler@ that has to be configured the following way:  <pre>     <New class="com.sonian.elasticsearch.http.jetty.handler.JettyHttpServerTransportHandler"          id="HttpServerAdapterHandler">         <Set name="transport"><Ref id="ESServerTransport"/></Set>     </New> </pre>  To simplify custom configuration, the elasticsearch-jetty plugin exposes several ES settings as Jetty properties that can be used in Jetty configuration files using @<Property name="property.name"/>@  * @es.home@ - ElasticSearch home directory * @es.config@ - ElasticSearch configuration directory * @es.data@ - ElasticSearch data directory * @es.cluster.data@ - ElasticSearch data directory for the cluster * @es.cluster@ - Cluster name * @jetty.bind_host@ - Http bind host, if the setting @sonian.elasticsearch.http.jetty.bind_host@ is not specified, ElasticSearch will try to use values specified in @http.bind_host@ and @http.host@ settings. * @jetty.port@ - Http port, if the setting @sonian.elasticsearch.http.jetty.port@ is not specified, ElasticSearch will try to use http.port or default to 9200-9300. If port is specified in form of the range, ElasticSearch will try to start Jetty with each individual port number iterating through the range. * @jetty.*@ - all other settings specified in @sonian.elasticsearch.http.jetty.@ are exposed as Jetty properties. For example, the setting @sonian.elasticsearch.http.jetty.foo@ can be referred to in Jetty configuration as *<Property name="jetty.foo"/>*.  It also exposes two elasticsearch objects using the following ids:  * @ESServerTransport@ - The @JettyHttpServerTransport@ object that is needed to configure @JettyHttpServerTransportHandler@ properly. * @ESClient@ - elasticsearch client that can be used by custom component to communicate to local instance of elasticsearch.  h2. User group  Join <a href="https://groups.google.com/forum/#!forum/elasticsearch-jetty">elasticsearch-jetty group</a> for questions and discussions related to this plugin.
childe/hangout	模仿[logstash](https://www.elastic.co/products/logstash)做的一个应用.  最近在将部分内容进行代码上的优化。  This product includes GeoLite2 data created by MaxMind, available from [http://www.maxmind.com](http://www.maxmind.com)  我们一直用logstash从Kafka消费数据进ES, 随着数据量的越来越大, 需要的logstash实例和机器也越来越多.   于是就拿java实现了一下目前我们用到的Logstash的插件.   做为java初学者, 肯定有很多地方写的不好.  - input 	 * stdin 	 * kafka 	 * newKafka - output 	* elastickseach 	* kafka 	* stdout   - filter 	* Grok 	* Date  	* Json 	* Gsub 	* Drop 	* Trim 	* Translate 	* Rename 	* Lowercase 	* Uppercase 	* Remove 	* Add 	* KV 	* URLDecode 	* GeoIP2 	* Filters 	 用一个典型配置做测试, 包括Grok Date AddField If条件判断等, 吞吐量是Logstash的5倍左右 .   不压测吞吐量, 正常消费的情况下, CPU使用率大概是Logstash的50%到25%.  # 运行 	bin/hangout -f app.yml  日志库使用的[log4j2](https://logging.apache.org/log4j/2.x/). 默认的配置文件就是Hangout目录下的log4j2.xml   默认日志记录是info级别, 环境变量HO_LOG_LEVEL可以更改日志级别. `export HO_LOG_LEVEL=error`更改到error   # 配置 配置在一定程度上也是模仿logstash, 但用的是通用的yaml格式. 因为能力有限, 不能实现logstash的那套语法, if是用了另外的格式. [可以参考一个配置的示例](https://github.com/childe/hangout/blob/master/conf/example.yml)  ## Input ### Stdin      - Stdin:         codec: plain         hostname: true # if add hostname to event; default false  ### Kafka 从Kafka读数据. 下面示例是说, 开2个线程取app这个topic的数据, 编码格式为plain纯文本.  consumer_settings中的参数, 参考[kafka]官方文档](http://kafka.apache.org/documentation.html#consumerconfigs), 所有参数都可以在这里配置. 比较重要的是group.id, zookeeper.connect这两个, 一定要有.   zookeeper.connect的格式为 hostname1:port1,hostname2:port2,hostname3:port3.      - Kafka:         topic:            app: 2         consumer_settings:           group.id: hangout           zookeeper.connect: 192.168.1.200:2181           auto.commit.interval.ms: "1000"           socket.receive.buffer.bytes: "1048576"           fetch.message.max.bytes: "1048576"         codec: plain  支持白名单匹配，使用topic_pattern指定正则表达式来匹配topic名称，如果指定此参数，topic参数会被忽略。      - topic_pattern: #pattern has high priority,if specified, topic will be ignored         test.*: 3          ### newKafka 新的kafka api.[http://kafka.apache.org/documentation.html#newconsumerapi](http://kafka.apache.org/documentation.html#newconsumerapi)      - NewKafka:         topic:             nginx: 1         codec: json         consumer_settings:             bootstrap.servers: 192.168.0.1:9092             value.deserializer: org.apache.kafka.common.serialization.StringDeserializer             key.deserializer: org.apache.kafka.common.serialization.StringDeserializer             group.id: hangout  所有可以配置的参数[http://kafka.apache.org/documentation.html#newconsumerconfigs](http://kafka.apache.org/documentation.html#newconsumerconfigs)  ## Filter ### Grok Grok是为了把一个字符串切割成多个field, 用的是[Joni库](https://github.com/jruby/joni), 不完全支持logstash里面的patterns语法,%{INT:bytes:int}这种语法不支持, 只支持%{INT:bytes},字段类型需要在ES中定义.  会依次匹配match中的正则, 直到有一个成功的.  **注意, 如果正则中的groupname和已有的字段一样, 原来的字段被覆盖**  	src: message #default message 	match: 		- '(?<logtime>\S+) (?<user>\S+) (-|(?<level>\w+))' 		- '(?<logtime>\S+\s+\S+) (?<user>\S+) (-|(?<level>\w+))' 	remove_fields: ['message'] 	tag_on_failure: grokfail #default grokfail  可以添加自己的Grok Pattern:      pattern_paths:         - '/opt/hangout/grokpatternpaths'     match:         - '(%{NGINXACCESSLOG})'  ### Date Date是用的[jona-time](http://www.joda.org/joda-time/)做解析和格式化.  会依次匹配formats里面的格式, 直到成功.  	src: logtime # default logtime 	formats: 		- 'ISO8601' 		- 'UNIX' 		- 'UNIX_MS' 		- 'YYYY-MM-dd HH:mm:ss.SSS' 	remove_fields: ['logtime'] 	tag_on_failure: datefail # default datefail  ### Json 解析json字符串, **如果json里面的字段和原有的字段重复, 原有字段会被覆盖!**      - Json:         field: message # required  ### GeoIP2 geoip2用的是maxmind公司的开源数据和算法. This product includes GeoLite2 data created by MaxMind, available from [http://www.maxmind.com](http://www.maxmind.com)  geoip2里面可以获取的数据也比较多, 目前我只是用到了country_code country_name city_name latitude longitude location 6个字段.  据[ELKstack-guide](https://github.com/garyelephant/ELKstack-guide-cn/blob/99550ba5cc4be177db1b6b62037fb77ce55c304f/logstash/develop_logstash_filter_geoip2.md), 速度比GeoIP有速倍的提升.  maxmind也提供了数据的下载 [http://dev.maxmind.com/geoip/geoip2/geolite2/](http://dev.maxmind.com/geoip/geoip2/geolite2/)       - GeoIP2:         source: message # required         target: geoip # default geoip         database: '/tmp/GeoLite2-City.mmdb'         country_code: false # default true         country_name: false # default true         country_isocode: false # default true         subdivision_name: false # default true         city_name: false # default true         latitude: false # default true         longitude: false # default true         location: false # default true  ### Drop 没有额外参数, 配合if使用.  	if: 		- '<#if user?matches("huhan3")>true</#if>'  ### IF 应用于filter. 是一个列表, 需要满足列表中的每一个条件, 使用[freemarker](http://freemarker.org/) 模板引擎.   下面这个例子是添加一个target字段, target的值为url字段的第5个部分(下标从0开始).   只有在满足以下2个条件的时候才会触发添加字段这个行为.  1. 日志含有url字段 2.  url中包含 "http://images4.c-ctrip.com/target/" 或者 ".c-ctrip.com/images/" 字符串.  ``` - Add:    fields:      target: '<#assign a=url?split("/")>${a[4]}'    if:      - '<#if url??>true</#if>'      - '<#if url?contains("http://images4.c-ctrip.com/target/")>true<#elseif url?contains(".c-ctrip.com/images/")>true</#if>' ```  ### Translate 每隔一段时间, 会重新加载字典. 字典是yaml格式. 用的[snakeyaml](https://bitbucket.org/asomov/snakeyaml)加载yaml文件的, 如果key是整数类型,会首先尝试转换成Integer, 超出Integer范围才会尝试Long.   但是Json格式的日志会被直接转成Long类型的, 导致不能匹配.  所以yaml里面需要这么写:  	!!java.lang.Long 123: 信用卡 	!!java.lang.Long 345: 借记卡  Tranlate 配置：  	source: user 	target: nick 	dictionary_path: /user-nick.yml 	refresh_interval: 300 # default 300 seconds  ### KV 将 a=1&b=2, 或者name=app id=123 type=nginx 这样的字符串拆分成{a:1,b:2}  {name:app, id:123, type:nginx} 等多个字段, 放到日志里面去.  配置如下.   如果targete有定义, 会把拆分出来字段放在这个字段中, 如果没有定义,放到在顶层.   trim 是把拆分出来的字段内容做前后修整. 将不需要的字符去掉. 下面的示例就是说把双引号和tag都去掉.    trimkey和trim类似, 处理的是字段名称.      - KV:         source: msg # default message         target: kv   # default null         field_split: ' '  # default " "         value_split: '='  # default "="         trim: '\t\"'  #default null         trimkey: '\"'  # default null         tag_on_failure: "KVfail" # default "KVfail"         remove_fields: ['msg']  ### Filters     可以在一个If条件下,顺序的执行多个filters, 而不用每次都执行If判断.  ### 其它filter 配置都比较简单, 可以参考[配置文件示例](https://github.com/childe/hangout/blob/master/conf/example.yml)  ### freemarker If 条件和addfileds等插件使用[freemarker](http://freemarker.org/)模板引擎, 速度快, 功能强大.  [在线测试工具](http://freemarker-online.kenshoo.com/)   ## Output ### Elasticsearch 使用[bulk api](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html) 批量将数据写入ES.  插入失败的时候, 只对 TOO_MANY_REQUESTS, SERVICE_UNAVAILABLE 这两种错误重试.  参数含义参考[java client bulk api](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/bulk.html)  concurrent_requests设置成大于0的数, 意思着多线程处理, 以我应用的经验,还有是一定OOM风险的. 因为在执行bulk的时候, bulkProcessor还在继续接收新的文档, 如果bulk失败了, 会把失败的actionrequest继续放到bulkProcessor里面. 一段时间内, 由于各种原因一直失败的话, 内存就会越用越多.   sniff设置为true的话, 会把hosts当做一个入口, 然后去寻找真正的data nodes写数据. 如果设置成false,数据就直接写在配置的hosts列表机器上,可以缓解data node上面的流量压力和小部分CPU压力.  	cluster: prod # cluster name, required 	hosts: # required 		- 192.168.1.100 		- 192.168.1.200:9301 	index: 'hangout-%{+YYYY.MM.dd}' 	index_type: logs # default logs 	bulk_actions: 20000 # default 20000 	bulk_size: 15	#default 15 	flush_interval: 10	#default 10 	concurrent_requests: 0	#default 0     sniff: false #default true  ### ElasticsearchHTTP 使用[rest api](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html) 批量将数据写入ES. 目前在es2.4.0 和es 5.2.0 上进行过初步测试         - ElasticsearchHTTP:             cluster: elasticsearch             hosts:               - 192.168.145.128             index: 'myindex'             index_type: "mytype" # default logs             bulk_actions: 5 #default 20000             sniff: false #default true  ### Kafka 	#bootstrap_servers format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a subset of brokers. 	bootstrap_servers: 192.168.1.100:9092 # required 	topic: test # required  ### Stdout 主要测试用吧. 因为配置是yml格式, 所以没有其它条件的话, 需要写成       - Stdout: {}  ## Metrics 为了实现对各个agent的统一性能和运行情况监控，在input/filter/output之外又提供了种叫Metrics的plugin, 用来监控应用的指标, 目前(0.3.0)提供了两种插件, 一种写一些性能数据到Graphit, 一种是restful接口提供性能监控、探活.  ``` metrics:     - Graphit:         port: 2004  # graphit port         host: 10.0.0.100  # graphit host         prefix: hangout         metrics:             com.codahale.metrics.JvmAttributeGaugeSet: [] # empty list will register all metrics in it             com.codahale.metrics.jvm.MemoryUsageGaugeSet: []             com.codahale.metrics.jvm.ThreadStatesGaugeSet: []             com.codahale.metrics.jvm.GarbageCollectorMetricSet: []      - Watcher:         host: 0.0.0.0         port: 8080 ```  metric count, 可以记录流经每个plugin的数据多少, 开发新的plugin可以参考stdin-input和stdout-output里面的计数方法.   在使用的时候需要注意, yml配置文件中需要指明meter_name才可以记录.  ``` - Stdin:   meter_name: stdin1 ```  # 开发: 当然可以在hangout的结构上继续开发, 可以参考项目结构中baseplugin和input,output,filters等插件.   也可以开一个独立的项目. 我写了几个简单的例子做为教程, 供大家参考.  1. 一个最简单的filter [https://github.com/childe/hangout-filter-reverse](https://github.com/childe/hangout-filter-reverse) 2. 操作json结构中的深层数据. [https://github.com/childe/hangout-filter-reverse/tree/multilevel](https://github.com/childe/hangout-filter-reverse/tree/multilevel) 3. 对消息做聚合, 额外输出聚合后的新消息. 聚合访问日志,统计一定时间段内每个Url的响应时间(平均值,最小最大值,条数等信息), 然后输出给下游. [https://github.com/childe/hangout-filter-statmetric](https://github.com/childe/hangout-filter-statmetric) 4. output.  示例3中的聚合数据如果需要告警, 比如Url的响应时间过高, 可以通过邮件发送出去. [https://github.com/childe/hangout-output-mail](https://github.com/childe/hangout-output-mail) 5. input. TODO  # LICENSE The MIT License (MIT)  Copyright (c) 2015 Childe, https://github.com/childe  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
hibernate/hibernate-search	# Hibernate Search  *Version: 5.8.0.Final - 13-09-2017*  ## Description  Full text search for Java objects  This project provides synchronization between entities managed by Hibernate ORM and full-text indexing services like Apache Lucene and Elasticsearch.  It will automatically apply changes to indexes, which is tedious and error prone coding work, while leaving you full control on the query aspects. The development community constantly researches and refines the index writing techniques to improve performance.  Mapping your objects to the indexes is declarative, using a combination of Hibernate Search specific annotations and the knowledge it can gather from your existing Hibernate/JPA mapping.  Queries can be defined by any combination of:  - "native" Apache Lucene queries  - writing "native" Elasticsearch queries in Json format (if using Elasticsearch, which is optional)  - using our DSL which abstracts the previous two generating optimal backend specific queries  Query results can include projections to be loaded directly from the index, or can materialize fully managed Hibernate entities loaded from the database within the current transactional scope.  Hibernate Search is using [Apache Lucene](http://lucene.apache.org/) under the cover; this can be used directly (running embedded in the same JVM) or remotely provided by Elasticsearch over its REST API.  ## Requirements  This version of Hibernate Search requires:  * Java SE 8 * Hibernate ORM 5.2.3.Final or a later 5.2.x * Apache Lucene 5.5.x  Hibernate ORM versions 5.2.0.Final, 5.2.1.Final and 5.2.2.Final are NOT fully compatible.  ## Instructions  ### Maven  Include the following to your dependency list:      <dependency>        <groupId>org.hibernate</groupId>        <artifactId>hibernate-search-orm</artifactId>        <version>5.8.0.Final</version>     </dependency>  ### Sourceforge Bundle  Download the distribution bundle from [SourceForge](http://sourceforge.net/projects/hibernate/files/hibernate-search) and unzip to installation directory. Then read the documentation available in *docs/reference*.  ### Building from source      > git clone git@github.com:hibernate/hibernate-search.git     > cd hibernate-search     > mvn clean install -s settings-example.xml  #### Build options (profiles and properties)  ##### Documentation  The documentation is based on [Asciidoctor](http://asciidoctor.org/) and is automatically generated from the standard maven build.      > mvn clean install -s settings-example.xml  This will produce both documentation in both `HTML` and `PDF` formats.  You can then find the freshly built documentation in the following location:      > ./documentation/target/asciidoctor/en-US  ##### Elasticsearch  The Elasticsearch module tests against one single version of Elasticsearch at a time, launching an Elasticsearch server automatically on port 9200. You may redefine the version to use by specifying the right profile and using the `test.elasticsearch.host.version` property, while disabling the default profile:      > mvn clean install -P!elasticsearch-5.2,elasticsearch-2.0 -Dtest.elasticsearch.host.version=2.1.0  The following profiles are available:   * `elasticsearch-2.0` for 2.0.x and 2.1.x  * `elasticsearch-2.2` for 2.2.x and later 2.x  * `elasticsearch-5.0` for 5.0.x and 5.1.x  * `elasticsearch-5.2` for 5.2.x and later 5.x (the default)  A list of available versions for `test.elasticsearch.host.version` can be found on [Maven Central](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.elasticsearch%22%20AND%20a%3A%22elasticsearch%22).  Alternatively, you can prevent the build from launching an Elasticsearch server automatically and run Elasticsearch-related tests against your own server using the `test.elasticsearch.host.provided` and `test.elasticsearch.host.url` properties:      > mvn clean install -Dtest.elasticsearch.host.provided=true -Dtest.elasticsearch.host.url=http://localhost:9200  If you want to run tests against an older Elasticsearch version  (2.x for instance), you will still have to select a profile among those listed above, and disable the default profile:      > mvn clean install -P!elasticsearch-5.2,elasticsearch-2.2 -Dtest.elasticsearch.host.provided=true -Dtest.elasticsearch.host.url=http://localhost:9200  You may also use authentication:      > mvn clean install -Dtest.elasticsearch.host.provided=true -Dtest.elasticsearch.host.url=https://localhost:9200 -Dtest.elasticsearch.host.username=ironman -Dtest.elasticsearch.host.password=j@rV1s  Also, the elasticsearch module (and only this one) can execute its integration tests against an Elasticsearch service on AWS. You will need to execute something along the lines of:      > mvn integration-test -pl elasticsearch -Dtest.elasticsearch.host.provided=true -Dtest.elasticsearch.host.url=<The full URL of your Elasticsearch endpoint> -Dtest.elasticsearch.host.aws.access_key=<Your access key> -Dtest.elasticsearch.host.aws.secret_key=<Your secret key> -Dtest.elasticsearch.host.aws.region=<Your AWS region ID>  ### Contributing  New contributors are always welcome. We collected some helpful hints on how to get started on our website at [Contribute to Hibernate Search](http://hibernate.org/search/contribute/)  ### Source code structure  The project is split in several Maven modules:  * _backends_: Remote backends receiving an indexing job and executing it via different protocols.  * _build-config_: Code related artefacts like checkstyle rules.  * _distribution_: Builds the distribution package.  * _documentation_: The project documentation.  * _elasticsearch_: All code relating to the Elasticsearch integration.  * _engine_: The engine of the project. Most of the beef is here.  * _integrationtest_: Integration tests with various technologies like WildFly, Spring and Karaf. Also includes performance tests.  * _modules_: Integration with [WildFly](http://www.wildfly.org/) using JBoss Modules.  * _orm_: Native integration for [Hibernate ORM](http://hibernate.org/orm/), and also home of most public API code.  * _serialization_: Serialization code used by remote backends.  * _testing_: Various helper classes to write tests using Hibernate Search. This module is semi private.  ## Contact  ### Latest Documentation:  * [http://search.hibernate.org](http://hibernate.org/search/documentation/)  ### Bug Reports:  * Hibernate JIRA [HSEARCH](https://hibernate.atlassian.net/browse/HSEARCH) (preferred) * hibernate-dev@lists.jboss.org  ### Free Technical Support:  * [Hibernate Forum](http://forum.hibernate.org/viewforum.php?f=9) * [Stackoverflow](http://stackoverflow.com/questions/tagged/hibernate-search); please use tag `hibernate-search`.  ## License  This software and its documentation are distributed under the terms of the FSF Lesser GNU Public License (see lgpl.txt).
dadoonet/spring-elasticsearch	# Spring factories for Elasticsearch    Welcome to the Spring factories for [Elasticsearch](http://www.elasticsearch.org/) project.    Actually, since version 1.4.1, this project has been split in two parts:    * [Elasticsearch Beyonder](https://github.com/dadoonet/elasticsearch-beyonder/) which find resources in  project classpath to automatically create indices, types and templates.  * This project which is building Client beans using [Spring framework](http://projects.spring.io/spring-framework/).    From 5.0, this project provides 2 implementations of an elasticsearch Client:    * The REST client  * The Transport client (deprecated)    ## Documentation    * For 5.x elasticsearch versions, you are reading the latest documentation.  * For 2.x elasticsearch versions, look at [es-2.x branch](https://github.com/dadoonet/spring-elasticsearch/tree/es-2.x).  * For 1.x elasticsearch versions, look at [es-1.4 branch](https://github.com/dadoonet/spring-elasticsearch/tree/es-1.4).  * For 0.x elasticsearch versions, look at [0.x branch](https://github.com/dadoonet/spring-elasticsearch/tree/0.x).    |   spring-elasticsearch  | elasticsearch |   Spring     | Release date |  |:-----------------------:|:-------------:|:------------:|:------------:|  |     5.0-SNAPSHOT        |  5.0 - 5.x    |    4.3.10    |  2017-07-??  |  |           2.2.0         |  2.0 - 2.4    |    4.2.3     |  2017-03-09  |  |           2.1.0         |  2.0, 2.1     |    4.2.3     |  2015-11-25  |  |           2.0.0         |      2.0      |    4.1.4     |  2015-10-25  |  |           1.4.2         |     < 2.0     |    4.1.4     |  2015-03-03  |  |           1.4.1         |      1.4      |    4.1.4     |  2015-02-28  |  |           1.4.0         |      1.4      |    4.1.4     |  2015-01-03  |  |           1.3.0         |      1.3      |    4.0.6     |  2014-09-01  |  |           1.0.0         |      1.0      |    3.2.2     |  2014-02-14  |    ## Build Status    Thanks to Travis for the [build status](https://travis-ci.org/dadoonet/spring-elasticsearch):   [![Build Status](https://travis-ci.org/dadoonet/spring-elasticsearch.svg)](https://travis-ci.org/dadoonet/spring-elasticsearch)      ## Getting Started    ### Maven dependency    Import spring-elasticsearch in you project `pom.xml` file:    ```xml  <dependency>    <groupId>fr.pilato.spring</groupId>    <artifactId>spring-elasticsearch</artifactId>    <version>2.2.0</version>  </dependency>  ```    If you want to set a specific version of the Rest client, add it to your `pom.xml` file:    ```xml  <dependency>      <groupId>org.elasticsearch.client</groupId>      <artifactId>rest</artifactId>      <version>5.5.0</version>  </dependency>  ```    If you want to use a transport client (deprecated), you must add it to your `pom.xml` file:    ```xml  <dependency>      <groupId>org.elasticsearch.client</groupId>      <artifactId>transport</artifactId>      <version>5.5.0</version>  </dependency>  ```    If you want to try out the most recent SNAPSHOT version [deployed on Sonatype](https://oss.sonatype.org/content/repositories/snapshots/fr/pilato/spring/spring-elasticsearch/):    ```xml  <dependency>    <groupId>fr.pilato.spring</groupId>    <artifactId>spring-elasticsearch</artifactId>    <version>5.0-SNAPSHOT</version>  </dependency>  ```    Don't forget to add if needed the following repository in your `pom.xml`:    ```xml  <repositories>      <repository>          <id>oss-snapshots</id>          <name>Sonatype OSS Snapshots</name>          <url>https://oss.sonatype.org/content/repositories/snapshots/</url>          <releases>              <enabled>false</enabled>          </releases>          <snapshots>              <enabled>true</enabled>          </snapshots>      </repository>  </repositories>  ```    ### Logger    We are using [slf4j](http://www.slf4j.org/) for logging but you have to provide the logging implementation  you want to use and bind it.    For example for this project we are using for tests [log4j2](http://logging.apache.org/log4j/).   If you want to do so, add to your `pom.xml`:    ```xml  <dependency>      <groupId>org.apache.logging.log4j</groupId>      <artifactId>log4j-1.2-api</artifactId>      <version>2.7</version>  </dependency>  <dependency>      <groupId>org.apache.logging.log4j</groupId>      <artifactId>log4j-slf4j-impl</artifactId>      <version>2.7</version>  </dependency>  <dependency>      <groupId>org.apache.logging.log4j</groupId>      <artifactId>log4j-core</artifactId>      <version>2.7</version>  </dependency>  ```    ### Using elasticsearch spring namespace for XML files    In your spring context file, just add namespaces like this:    ```xml  <?xml version="1.0" encoding="UTF-8"?>  <beans xmlns="http://www.springframework.org/schema/beans"         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"         xmlns:elasticsearch="http://www.pilato.fr/schema/elasticsearch"         xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd  		http://www.pilato.fr/schema/elasticsearch http://www.pilato.fr/schema/elasticsearch/elasticsearch-5.0.xsd">  </beans>  ```    ### Getting a rest client bean    From 5.0, you can now get a REST Client implementation.    #### Define a rest client bean    In your spring context file, just define a client like this:    ```xml  <elasticsearch:rest-client id="esClient" />  ```    By default, you will get an [Elasticsearch Low Level Rest Client](https://www.elastic.co/guide/en/elasticsearch/client/java-rest/master/java-rest-low.html)  connected to an Elasticsearch node already running at `localhost:9200`.    You can set the nodes you want to connect to:    ```xml  <elasticsearch:rest-client id="esClient" esNodes="localhost:9200,localhost:9201" />  ```    #### Injecting the rest client in your java project    You can use the rest client in your java classes.    ```java  import org.elasticsearch.client.RestClient;    RestClient client = ctx.getBean("esClient", RestClient.class);  ```    Better, you should use `@Autowired` annotation.    ```java  // Inject your client...  @Autowired RestClient client;  ```    ### Getting a transport client bean (deprecated)    From 5.0, the Transport Client implementation is deprecated. It is now marked as `optional` so  you need to explicitly add it to your project as described in [Maven dependency chapter](#maven-dependency).    #### Define a client Transport bean    In your spring context file, just define a client like this:    ```xml  <elasticsearch:client id="esClient" />  ```        By default, you will get an [Elasticsearch Transport Client](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html)  connected to an Elasticsearch node already running at `localhost:9300` using `elasticsearch` as cluster name.    You can set the nodes you want to connect to:    ```xml  <elasticsearch:client id="esClient" esNodes="localhost:9300,localhost:9301" />  ```    **Important notes**    > Note that you should define the same clustername as the one you defined on your running nodes.  > Otherwise, your Transport Client won't connect to the node. See [Elasticsearch properties](#transport-client-properties-deprecated).    > Note also that you must define the transport client port (9300-9399) and not the REST port (9200-9299).  > Transport client does not use REST API.    #### Injecting transport client in your java project    Now, you can use the transport client in your java classes.    ```java  import org.elasticsearch.client.Client;    Client client = ctx.getBean("esClient", Client.class);  ```    Better, you should use `@Autowired` annotation.    ```java  // Inject your client...  @Autowired Client client;  ```    #### Transport Client properties (deprecated)    You can define your transport client properties using a property file such as:    ```properties  cluster.name=myclustername  ```    And load it in Spring context:    ```xml  <util:properties id="esproperties"      location="classpath:fr/pilato/spring/elasticsearch/xml/esclient-transport.properties"/>  ```    Note that you can also define properties as follow:    ```xml  <util:map id="esProperties">      <entry key="cluster.name" value="newclustername"/>  </util:map>  ```    <elasticsearch:client id="esClient" esNodes="localhost:9300,localhost:9301"      properties="esproperties"/>    Injecting properties in client is now easy:    ```xml  <elasticsearch:client id="esClient" properties="esproperties" />  ```    You can also add plugins to the transport client in case it needs it:    ```xml  <elasticsearch:client id="esClient" plugins="org.elasticsearch.plugin.deletebyquery.DeleteByQueryPlugin" />  ```    ## Automatically create indices    The following examples are documented using the Rest Client implementation `elasticsearch:rest-client` but you can  replace them with the Transport client by using `elasticsearch:client` instead.    ### Managing indexes and types    If you want to manage indexes and types at startup (creating missing indexes/types and applying mappings):    ```xml  <elasticsearch:rest-client id="esClient"      mappings="twitter/tweet" />  ```    This will create an [Elasticsearch Low Level Rest Client](https://www.elastic.co/guide/en/elasticsearch/client/java-rest/master/java-rest-low.html)  that will check when starting that index `twitter` exists and `tweet` type is defined.    If you need to manage more than one type or index, just use a comma separated list:    ```xml  <elasticsearch:rest-client id="esClient"      mappings="twitter/tweet,twitter/user,facebook/user" />  ```    If you add in your classpath a file named `es/twitter/_settings.json`, it will be automatically applied to define  settings for your `twitter` index.    For example, create the following file `src/main/resources/es/twitter/_settings.json` in your project:    ```javascript  {    "index" : {      "number_of_shards" : 3,      "number_of_replicas" : 2    }  }  ```    Also, if you define a file named `es/twitter/tweet.json`, it will be automatically applied as the mapping for  the `tweet` type in the `twitter` index.    For example, create the following file `src/main/resources/es/twitter/tweet.json` in your project:    ```javascript  {    "tweet" : {      "properties" : {        "message" : {"type" : "text", "store" : "yes"}      }    }  }  ```    ### Using convention over configuration    By default, the factory will find every mapping file located under `es` directory.  So, if you have a mapping file named `es/twitter/tweet.json` in your classpath, it will be automatically used by  the factory without defining anything:    ```xml  <elasticsearch:rest-client id="esClient" />  ```    You can disable this automatic lookup by setting the `autoscan` property to `false`:    ```xml  <elasticsearch:rest-client id="esClient" autoscan="false" mappings="twitter/tweet" />  ```    ### Creating aliases to indexes    When creating an index, it could be useful to add an alias on it.  For example, if you planned to have indexes per year for twitter feeds (twitter2012, twitter2013, twitter2014) and you want  to define an alias named twitter, you can use the `aliases` property:    ```xml  <elasticsearch:rest-client id="esClient"      aliases="twitter:twitter2012,twitter:twitter2013,twitter:twitter2014" />  ```    ### Creating templates    Sometimes it's useful to define a template mapping that will automatically be applied to new indices created.     For example, if you planned to have indexes per year for twitter feeds (twitter2012, twitter2013, twitter2014) and you want  to define a template named `twitter_template`, you can use the `templates` property:    ```xml  <!--      We add also a facebook_template template just for showing how to      define more than one template...  -->  <elasticsearch:rest-client id="esClient"      templates="twitter_template,facebook_template" />  ```    To configure your template you have to define a file named `es/_template/twitter_template.json` in your project:    ```javascript  {      "template" : "twitter*",      "settings" : {          "number_of_shards" : 1      },      "mappings" : {          "tweet" : {              "properties" : {                  "message" : {                      "type" : "text",                      "store" : "yes"                  }              }          }      }  }  ```    ### Changing classpath search path for mapping and settings files    By default, the factory look in `es` classpath folder to find if there is index settings or mappings definitions.  If you need to change it, you can use the `classpathRoot` property:    ```xml  <elasticsearch:rest-client id="esClient" classpathRoot="myownfolder" />  ```    So, if a `myownfolder/twitter/_settings.xml` file exists in your classpath, it will be used by the factory.    ### Merge mappings    If you need to merge mapping for an existing `type`, set  `mergeMapping` property to `true`.    ```xml  <elasticsearch:rest-client id="esClient" mergeMapping="true" />  ```    If merging fails, the factory will not start ([BeanCreationException](https://github.com/SpringSource/spring-framework/blob/master/spring-beans/src/main/java/org/springframework/beans/factory/BeanCreationException.java)   will be raised).    ### Merge settings    If you need to merge settings for an existing `index`, add a file named  `es/twitter/_update_settings.json` in your  classpath. The factory will detect it and will try to merge settings unless you explicitly set `mergeSettings` to `false`.    ```xml  <elasticsearch:rest-client id="esClient" mergeSettings="false" />  ```    If merging fails, the factory will not start.      ### Force rebuild indices (use with caution)    For test purpose or for continuous integration, you could force the factory to clean the previous `indices` when starting the client.  It will *remove all your datas* for every index which has been defined. Just set  `forceMapping` property to `true`.    ```xml  <elasticsearch:rest-client id="esClient" forceMapping="true" />  ```    ### Force rebuild templates (use with caution)    For test purpose or for continuous integration, you could force the factory to clean the previous `template` when starting the client.  Just set  `forceTemplate` property to `true`.    ```xml  <elasticsearch:rest-client id="esClient" forceTemplate="true" />  ```    ### Asynchronous initialization    Client bean initialization is by default synchronously. It can be initialized asynchronously with the attributes `async` and `taskExecutor`.    ```xml  <task:executor pool-size="4" id="taskExecutor"/>  <elasticsearch:rest-client id="esClient" async="true" taskExecutor="taskExecutor"/>  ```  Asynchronous initialization does not block Spring startup but it continues on background on another thread.  Any methods call to these beans before elasticsearch is initialized will be blocked. `taskExecutor` references a standard Spring's task executor.    ## Using Java Annotations    Let's say you want to use Spring Java Annotations, here is a typical application you can build.    `pom.xml`:    ```xml  <?xml version="1.0" encoding="UTF-8"?>  <project xmlns="http://maven.apache.org/POM/4.0.0"           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"           xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">      <modelVersion>4.0.0</modelVersion>        <groupId>fr.pilato.tests</groupId>      <artifactId>spring-elasticsearch-test</artifactId>      <version>1.0-SNAPSHOT</version>        <dependencies>          <dependency>              <groupId>fr.pilato.spring</groupId>              <artifactId>spring-elasticsearch</artifactId>              <version>5.0</version>          </dependency>      </dependencies>  </project>  ```    `App.java`:    ```java  package fr.pilato.tests;    import fr.pilato.spring.elasticsearch.ElasticsearchRestClientFactoryBean;  import org.elasticsearch.client.RestClient;  import org.springframework.beans.factory.annotation.Autowired;  import org.springframework.context.annotation.AnnotationConfigApplicationContext;  import org.springframework.context.annotation.Bean;  import org.springframework.context.annotation.Configuration;  import org.springframework.stereotype.Component;    import java.io.IOException;    @Component  public class RestApp {        @Configuration      public class AppConfig {          @Bean          public RestClient esClient() throws Exception {              ElasticsearchRestClientFactoryBean factory = new ElasticsearchRestClientFactoryBean();              factory.setEsNodes(new String[]{"127.0.0.1:9200"});              factory.afterPropertiesSet();              return factory.getObject();          }      }        @Autowired      private RestClient client;        public static void main(String[] args) throws IOException {          AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();          context.scan("fr.pilato.tests");          context.refresh();            RestApp p = context.getBean(RestApp.class);          p.run();            context.close();      }        private void run() throws IOException {          client.performRequest("GET", "/");      }  }  ```    ## Old fashion bean definition    Note that you can use the old fashion method to define your beans instead of using `<elasticsearch:...>` namespace:    ```xml  <?xml version="1.0" encoding="UTF-8"?>  <beans xmlns="http://www.springframework.org/schema/beans"  	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"      xmlns:util="http://www.springframework.org/schema/util"  	xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd  		http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-3.0.xsd">        <util:map id="esproperties">          <entry key="cluster.name" value="newclustername"/>      </util:map>        <!-- The Rest Client -->      <bean id="esRestClient" class="fr.pilato.spring.elasticsearch.ElasticsearchRestClientFactoryBean" >          <property name="esNodes">              <list>                  <value>localhost:9200</value>                  <value>localhost:9201</value>              </list>          </property>            <property name="autoscan" value="false" />          <property name="mappings">              <list>                  <value>twitter/tweet</value>              </list>          </property>          <property name="classpathRoot" value="myownfolder" />          <property name="forceMapping" value="true" />          <property name="mergeSettings" value="true" />          <property name="templates">              <list>                  <value>twitter_template</value>              </list>          </property>          <property name="forceTemplate" value="true" />          <property name="aliases">              <list>                  <value>twitter:twitter2012</value>                  <value>twitter:twitter2013</value>                  <value>twitter:twitter2014</value>              </list>          </property>      </bean>        <!-- The deprecated Transport Client -->      <bean id="esTransportClient" class="fr.pilato.spring.elasticsearch.ElasticsearchTransportClientFactoryBean" >          <property name="esNodes">              <list>                  <value>localhost:9300</value>                  <value>localhost:9301</value>              </list>          </property>            <property name="properties" ref="esproperties" />            <property name="autoscan" value="false" />          <property name="mappings">              <list>                  <value>twitter/tweet</value>              </list>          </property>          <property name="classpathRoot" value="myownfolder" />          <property name="forceMapping" value="true" />          <property name="mergeSettings" value="true" />          <property name="templates">              <list>                  <value>twitter_template</value>              </list>          </property>          <property name="forceTemplate" value="true" />          <property name="aliases">              <list>                  <value>twitter:twitter2012</value>                  <value>twitter:twitter2013</value>                  <value>twitter:twitter2014</value>              </list>          </property>      </bean>    </beans>  ```    # Thanks    Special thanks to    - [Nicolas Huray](https://github.com/nhuray) for his contribution about  [templates](https://github.com/dadoonet/spring-elasticsearch/pull/4)  - [Nicolas Labrot](https://github.com/nithril‎) for his contribution about  [async](https://github.com/dadoonet/spring-elasticsearch/pull/30)      # License    This software is licensed under the Apache 2 license, quoted below.    	Copyright 2011-2017 David Pilato  	  	Licensed under the Apache License, Version 2.0 (the "License"); you may not  	use this file except in compliance with the License. You may obtain a copy of  	the License at  	  	    http://www.apache.org/licenses/LICENSE-2.0  	  	Unless required by applicable law or agreed to in writing, software  	distributed under the License is distributed on an "AS IS" BASIS, WITHOUT  	WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the  	License for the specific language governing permissions and limitations under  	the License.
elastic/elasticsearch-metrics-reporter-java	# Metrics Elasticsearch Reporter  ![Project unmaintained](https://img.shields.io/badge/project-unmaintained-red.svg)  **This project is no longer maintained. If you want to maintain it, please fork and we will link to your new repository.**  ## Introduction  This is a reporter for the excellent [Metrics library](http://metrics.dropwizard.io/), similar to the [Graphite](http://metrics.dropwizard.io/3.1.0/manual/graphite/) or [Ganglia](http://metrics.dropwizard.io/3.1.0/manual/ganglia/) reporters, except that it reports to an Elasticsearch server.  In case, you are worried, that you need to include the 20MB elasticsearch dependency in your project, you do not need to be. As this reporter is using HTTP for putting data into elasticsearch, the only library needed is the awesome [Jackson JSON library](http://wiki.fasterxml.com/JacksonHome), more exactly the Jackson Databind library to easily serialize the metrics objects.  If you want to see this in action, go to the `samples/` directory and read the readme over there, to get up and running with a sample application using the Metrics library as well as a dashboard application to graph.  ## Compatibility  |   Metrics-elasticsearch-reporter  |    elasticsearch    | Release date | |-----------------------------------|---------------------|:------------:| | 2.3.0-SNAPSHOT                    | 2.3.0  -> master    |  NONE        | | 2.2.0                             | 2.2.0  -> 2.2.x     |  2016-02-10  | | 2.0                               | 1.0.0  -> 1.7.x     |  2014-02-16  | | 1.0                               | 0.90.7 -> 0.90.x    |  2014-02-05  |  ## Travis CI build status  [![Build status](https://api.travis-ci.org/elastic/elasticsearch-metrics-reporter-java.svg?branch=master)](https://travis-ci.org/elastic/elasticsearch-metrics-reporter-java)  ## Installation  You can simply add a dependency in your `pom.xml` (or whatever dependency resolution system you might have)  ``` <dependency>   <groupId>org.elasticsearch</groupId>   <artifactId>metrics-elasticsearch-reporter</artifactId>   <version>2.2.0</version> </dependency> ```  ## Configuration  ``` final MetricRegistry registry = new MetricRegistry(); ElasticsearchReporter reporter = ElasticsearchReporter.forRegistry(registry)     .hosts("localhost:9200", "localhost:9201")     .build(); reporter.start(60, TimeUnit.SECONDS); ```  Define your metrics and registries as usual  ``` private final Meter incomingRequestsMeter = registry.meter("incoming-http-requests");  // in your app code incomingRequestsMeter.mark(1); ```   ### Options  * `hosts()`: A list of hosts used to connect to, must be in the format `hostname:port`, default is `localhost:9200` * `timeout()`: Milliseconds to wait for an established connections, before the next host in the list is tried. Defaults to `1000` * `bulkSize()`: Defines how many metrics are sent per bulk requests, defaults to `2500` * `filter()`: A `MetricFilter` to define which metrics written to the elasticsearch * `percolationFilter()`: A `MetricFilter` to define which metrics should be percolated against. See below for an example * `percolationNotifier()`: An implementation of the `Notifier` interface, which is executed upon a matching percolator. See below for an example. * `index()`: The name of the index to write to, defaults to `metrics` * `indexDateFormat()`: The date format to make sure to rotate to a new index, defaults to `yyyy-MM` * `timestampFieldname()`: The field name of the timestamp, defaults to `@timestamp`, which makes it easy to use with kibana  ### Mapping  **Note**: The reporter automatically checks for the existence of an index template called `metrics_template`. If this template does not exist, it is created. This template ensures that all strings used in metrics are set to `not_analyzed` and disables the `_all` field.   ## Notifications with percolations  ``` ElasticsearchReporter reporter = ElasticsearchReporter.forRegistry(registry)     .percolationNotifier(new PagerNotifier())     .percolationFilter(MetricFilter.ALL)     .build(); reporter.start(60, TimeUnit.SECONDS); ```  Write a custom notifier  ``` public class PagerNotifier implements Notifier {    @Override   public void notify(JsonMetrics.JsonMetric jsonMetric, String percolateMatcher) {     // send pager duty here   } } ```  Add a percolation  ``` curl http://localhost:9200/metrics/.percolator/http-monitor -X PUT -d '{   "query" : {      "bool" : {        "must": [         { "term": { "name" : "incoming-http-requests" } },         { "range": { "m1_rate": { "to" : "10" } } }       ]     }   } }' ```  ## JSON Format of metrics  This is how the serialized metrics looks like in elasticsearch  ### Counter  ``` {   "name": "usa-gov-heartbearts",   "@timestamp": "2013-07-20T09:29:58.000+0000",   "count": 18 } ```  ### Timer  ``` {   "name" : "bulk-request-timer",   "@timestamp" : "2013-07-20T09:43:58.000+0000",   "count" : 114,   "max" : 109.681,   "mean" : 5.439666666666667,   "min" : 2.457,   "p50" : 4.3389999999999995,   "p75" : 5.0169999999999995,   "p95" : 8.37175,   "p98" : 9.6832,   "p99" : 94.68429999999942,   "p999" : 109.681,   "stddev" : 9.956913151098842,   "m15_rate" : 0.10779994503690074,   "m1_rate" : 0.07283351433589833,   "m5_rate" : 0.10101298115113727,   "mean_rate" : 0.08251056571678642,   "duration_units" : "milliseconds",   "rate_units" : "calls/second" } ```  ### Meter  ``` {   "name" : "usagov-incoming-requests",   "@timestamp" : "2013-07-20T09:29:58.000+0000",   "count" : 224,   "m1_rate" : 0.3236309568191993,   "m5_rate" : 0.45207208204948995,   "m15_rate" : 0.5014348927301423,   "mean_rate" : 0.4135529888278531,   "units" : "events/second" } ```  ### Histogram  ``` {   "name" : "my-histgram",   "@timestamp" : "2013-07-20T09:29:58.000+0000",   "count" : 114,   "max" : 109.681,   "mean" : 5.439666666666667,   "min" : 2.457,   "p50" : 4.3389999999999995,   "p75" : 5.0169999999999995,   "p95" : 8.37175,   "p98" : 9.6832,   "p99" : 94.68429999999942,   "p999" : 109.681,   "stddev" : 9.956913151098842,} } ```  ### Gauge  ``` {   "name" : "usagov-incoming-requests",   "@timestamp" : "2013-07-20T09:29:58.000+0000",   "value" : 123 } ```   ## Next steps  * Integration with Kibana would be awesome
codelibs/elasticsearch-river-web	Elasticsearch River Web =======================  ## Overview  Elasticsearch River Web is a web crawler application for Elasticsearch. This application provides a feature to crawl web sites and extract the content by CSS Query. (As of version 1.5, River Web is not Elasticsearch plugin)  If you want to use Full Text Search Server, please see [Fess](https://github.com/codelibs/fess "Fess").  ## Version  | River Web | Tested on ES  | Download | |:---------:|:-------------:|:--------:| | master    | 2.4.X         | [Snapshot](http://maven.codelibs.org/org/codelibs/river-web/ "Snapshot") | | 2.4.0     | 2.4.0         | [Download](https://github.com/codelibs/elasticsearch-river-web/releases/tag/river-web-2.4.0 "2.4.0") | | 2.0.2     | 2.3.1         | [Download](https://github.com/codelibs/elasticsearch-river-web/releases/tag/river-web-2.0.2 "2.0.2") | | 2.0.1     | 2.2.0         | [Download](https://github.com/codelibs/elasticsearch-river-web/releases/tag/river-web-2.0.1 "2.0.1") | | 2.0.0     | 2.1.2         | [Download](https://github.com/codelibs/elasticsearch-river-web/releases/tag/river-web-2.0.0 "2.0.0") |  For old version, see [README\_ver1.md](https://github.com/codelibs/elasticsearch-river-web/blob/master/README_ver1.md "README_ver1.md") or [README\_ver1.5.md](https://github.com/codelibs/elasticsearch-river-web/blob/master/README_ver1.5.md "README_ver1.5.md").  ### Issues/Questions  Please file an [issue](https://github.com/codelibs/elasticsearch-river-web/issues "issue"). (Japanese forum is [here](https://github.com/codelibs/codelibs-ja-forum "here").)  ## Installation  ### Install River Web   #### Zip File      $ unzip elasticsearch-river-web-[VERSION].zip  #### Tar.GZ File      $ tar zxvf elasticsearch-river-web-[VERSION].tar.gz  ## Usage  ### Create Index To Store Crawl Data  An index for storing crawl data is needed before starting River Web. For example, to store data to "webindex/my_web", create it as below:      $ curl -XPUT 'localhost:9200/webindex' -d '     {         "settings":{           "index":{             "refresh_interval":"1s",           "number_of_shards":"10",           "number_of_replicas" : "0"         }       },       "mappings":{           "my_web":{             "properties":{               "url":{                 "type":"string",               "index":"not_analyzed"             },             "method":{                 "type":"string",               "index":"not_analyzed"             },             "charSet":{                 "type":"string",               "index":"not_analyzed"             },             "mimeType":{                 "type":"string",               "index":"not_analyzed"             }           }         }       }     }'  Feel free to add any properties other than the above if you need them.  ### Register Crawl Config Data  A crawling configuration is created by registering a document to .river\_web index as below. This example crawls sites of http://www.codelibs.org/ and http://fess.codelibs.org/.      $ curl -XPUT 'localhost:9200/.river_web/config/my_web' -d '{         "index" : "webindex",         "type" : "my_web",         "urls" : ["http://www.codelibs.org/", "http://fess.codelibs.org/"],         "include_urls" : ["http://www.codelibs.org/.*", "http://fess.codelibs.org/.*"],         "max_depth" : 3,         "max_access_count" : 100,         "num_of_thread" : 5,         "interval" : 1000,         "target" : [           {             "pattern" : {               "url" : "http://www.codelibs.org/.*",               "mimeType" : "text/html"             },             "properties" : {               "title" : {                 "text" : "title"               },               "body" : {                 "text" : "body"               },               "bodyAsHtml" : {                 "html" : "body"               },               "projects" : {                 "text" : "ul.nav-list li a",                 "isArray" : true               }             }           },           {             "pattern" : {               "url" : "http://fess.codelibs.org/.*",               "mimeType" : "text/html"             },             "properties" : {               "title" : {                 "text" : "title"               },               "body" : {                 "text" : "body",                 "trimSpaces" : true               },               "menus" : {                 "text" : "ul.nav-list li a",                 "isArray" : true               }             }           }         ]     }'  The configuration is:  | Property                      | Type    | Description                                     | |:------------------------------|:-------:|:------------------------------------------------| | index                         | string  | Stored index name.                              | | type                          | string  | Stored type name.                               | | urls                          | array   | Start point of URL for crawling.                | | include\_urls                 | array   | White list of URL for crawling.                 | | exclude\_urls                 | array   | Black list of URL for crawling.                 | | max\_depth                    | int     | Depth of crawling documents.                    | | max\_access\_count            | int     | The number of crawling documents.               | | num\_of\_thread               | int     | The number of crawler threads.                  | | interval                      | int     | Interval time (ms) to crawl documents.          | | incremental                   | boolean | Incremental crawling.                           | | overwrite                     | boolean | Delete documents of old duplicated url.         | | user\_agent                   | string  | User-agent name when crawling.                  | | robots\_txt                   | boolean | If you want to ignore robots.txt, false.        | | authentications               | object  | Specify BASIC/DIGEST/NTLM authentication info.  | | target.urlPattern             | string  | URL pattern to extract contents by CSS Query.   | | target.properties.name        | string  | "name" is used as a property name in the index. | | target.properties.name.text   | string  | CSS Query for the property value.               | | target.properties.name.html   | string  | CSS Query for the property value.               | | target.properties.name.script | string  | Rewrite the property value by Script(Groovy).   |  ### Start Crawler      ./bin/riverweb --config-id [config doc id] --cluster-name [Elasticsearch Cluster Name] --cleanup  For example,      ./bin/riverweb --config-id my_web --cluster-name elasticsearch --cleanup  ### Unregister Crawl Config Data  If you want to stop the crawler, kill the crawler process and then delete the config document as below:      $ curl -XDELETE 'localhost:9200/.river_web/config/my_web'  ## Examples  ### Full Text Search for Your site (ex. http://fess.codelibs.org/)      $ curl -XPUT 'localhost:9200/.river_web/fess/fess_site' -d '{         "index" : "webindex",         "type" : "fess_site",         "urls" : ["http://fess.codelibs.org/"],         "include_urls" : ["http://fess.codelibs.org/.*"],         "max_depth" : 3,         "max_access_count" : 1000,         "num_of_thread" : 5,         "interval" : 1000,         "target" : [           {             "pattern" : {                 "url" : "http://fess.codelibs.org/.*",                 "mimeType" : "text/html"             },             "properties" : {                 "title" : {                     "text" : "title"                 },                 "body" : {                     "text" : "body",                     "trimSpaces" : true                 }             }           }         ]     }'   ### Aggregate a title/content from news.yahoo.com      $ curl -XPUT 'localhost:9200/.river_web/config/yahoo_site' -d '{         "index" : "webindex",         "type" : "my_web",         "urls" : ["http://news.yahoo.com/"],         "include_urls" : ["http://news.yahoo.com/.*"],         "max_depth" : 1,         "max_access_count" : 10,         "num_of_thread" : 3,         "interval" : 3000,         "user_agent" : "Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko",         "target" : [           {             "pattern" : {               "url" : "http://news.yahoo.com/video/.*html",               "mimeType" : "text/html"             },             "properties" : {               "title" : {                 "text" : "title"               }             }           },           {             "pattern" : {               "url" : "http://news.yahoo.com/.*html",               "mimeType" : "text/html"             },             "properties" : {               "title" : {                 "text" : "h1.headline"               },               "content" : {                 "text" : "section#mediacontentstory p"               }             }           }         ]     }'  (if news.yahoo.com is updated, the above example needs to be updated.)  ## Others  ### BASIC/DIGEST/NTLM authentication  River Web supports BASIC/DIGEST/NTLM authentication. Set authentications object.      ...     "num_of_thread" : 5,     "interval" : 1000,     "authentications":[       {         "scope": {           "scheme":"BASIC"         },         "credentials": {           "username":"testuser",           "password":"secret"         }       }],     "target" : [     ...  The configuration is:  | Property                                | Type    | Description                                     | |:----------------------------------------|:-------:|:------------------------------------------------| | authentications.scope.scheme            | string  | BASIC, DIGEST or NTLM                           | | authentications.scope.host              | string  | (Optional)Target hostname.                      | | authentications.scope.port              | int     | (Optional)Port number.                          | | authentications.scope.realm             | string  | (Optional)Realm name.                           | | authentications.credentials.username    | string  | Username.                                       | | authentications.credentials.password    | string  | Password.                                       | | authentications.credentials.workstation | string  | (Optional)Workstation for NTLM.                 | | authentications.credentials.domain      | string  | (Optional)Domain for NTLM.                      |  For example, if you want to use an user in ActiveDirectory, the configuration is below:      "authentications":[       {         "scope": {           "scheme":"NTLM"         },         "credentials": {           "domain":"your.ad.domain",           "username":"taro",           "password":"himitsu"         }       }],   ### Use attachment type  River Web supports [attachment type](https://github.com/elasticsearch/elasticsearch-mapper-attachments). For example, create a mapping with attachment type:      curl -XPUT "localhost:9200/web/test/_mapping?pretty" -d '{       "test" : {         "properties" : {     ...           "my_attachment" : {               "type" : "attachment",               "fields" : {                 "file" : { "index" : "no" },                 "title" : { "store" : "yes" },                 "date" : { "store" : "yes" },                 "author" : { "store" : "yes" },                 "keywords" : { "store" : "yes" },                 "content_type" : { "store" : "yes" },                 "content_length" : { "store" : "yes" }               }           }     ...  and then start your river. In "properties" object, when a value of "type" is "attachment", the crawled url is stored as base64-encoded data.      curl -XPUT localhost:9200/.river_web/config/2 -d '{           "index" : "web",           "type" : "data",           "urls" : "http://...",     ...           "target" : [     ...             {               "settings" : {                 "html" : false               },               "pattern" : {                 "url" : "http://.../.*"               },               "properties" : {                 "my_attachment" : {                   "type" : "attachment"                 }               }             }           ]     ...  ### Use Multibyte Characters  An example in Japanese environment is below. First, put some configuration file into conf directory of Elasticsearch.      $ cd $ES_HOME/conf    # ex. /etc/elasticsearch if using rpm package     $ sudo wget https://raw.github.com/codelibs/fess-server/master/src/tomcat/solr/core1/conf/mapping_ja.txt     $ sudo wget http://svn.apache.org/repos/asf/lucene/dev/trunk/solr/example/solr/collection1/conf/lang/stopwords_ja.txt   and then create "webindex" index with analyzers for Japanese. (If you want to use uni-gram, remove cjk\_bigram in filter)      $ curl -XPUT "localhost:9200/webindex" -d '     {       "settings" : {         "analysis" : {           "analyzer" : {             "default" : {               "type" : "custom",               "char_filter" : ["mappingJa"],               "tokenizer" : "standard",               "filter" : ["word_delimiter", "lowercase", "cjk_width", "cjk_bigram"]             }           },           "char_filter" : {             "mappingJa": {               "type" : "mapping",               "mappings_path" : "mapping_ja.txt"             }           },           "filter" : {             "stopJa" : {               "type" : "stop",               "stopwords_path" : "stopwords_ja.txt"             }           }         }       }     }'  ### Rewrite a property value by Script  River Web allows you to rewrite crawled data by Java's ScriptEngine. "javascript" is available. In "properties" object, put "script" value to a property you want to rewrite.      ...             "properties" : {     ...               "flag" : {                 "text" : "body",                 "script" : "value.indexOf('Elasticsearch') > 0 ? 'yes' : 'no';"               },  The above is, if a string value of body element in HTML contains "Elasticsearch", set "yes" to "flag" property.  ### Use HTTP proxy  Put "proxy" property in "crawl" property.      curl -XPUT 'localhost:9200/.river_web/config/my_web' -d '{         "index" : "webindex",         "type" : "my_web",     ...             "proxy" : {               "host" : "proxy.server.com",               "port" : 8080             },  ### Specify next crawled urls when crawling  To set "isChildUrl" property to true, the property values is used as next crawled urls.      ...         "target" : [           {     ...             "properties" : {               "childUrl" : {                 "value" : ["http://fess.codelibs.org/","http://fess.codelibs.org/ja/"],                 "isArray" : true,                 "isChildUrl" : true               },  ### Intercept start/execute/finish/close actions  You can insert your script to Executing Crawler(execute)/Finished Crawler(finish). To insert scripts, put "script" property as below:      curl -XPUT 'localhost:9200/.river_web/config/my_web' -d '{         "script":{           "execute":"your script...",           "finish":"your script...",         },         ...  ## FAQ  ### What does "No scraping rule." mean?  In a river setting, "url" is starting urls to crawl a site, "include_urls" filters urls whether are crawled or not, and "target.pattern.url" is a rule to store extracted web data. If a crawling url does not match "target.pattern.url", you would see the message. Therefore, it means the crawled url does not have an extraction rule.  ### How to extract an attribute of meta tag  For example, if you want to grab a content of description's meta tag, the configuration is below:      ...     "target" : [     ...       "properties" : {     ...         "meta" : {           "attr" : "meta[name=description]",           "args" : [ "content" ]         },  ### Incremental crawling dose not work?  "url" field needs to be "not\_analyzed" in a mapping of your stored index. See [Create Index To Store Crawl Data](https://github.com/codelibs/elasticsearch-river-web#create-index-to-store-crawl-data "Create Index To Store Crawl Data").   ### Where is crawled data stored?  crawled data are stored to ".s2robot" index during cralwing, data extracted from them are stored to your index specified by a river setting, and then data in "robot" index are removed when the crawler is finished.  ## Powered By  * [Lasta Di](https://github.com/lastaflute/lasta-di "Lasta Di"): DI Container * [Fess Crawler](https://github.com/codelibs/fess-crawler "Fess Crawler"): Web Crawler
codelibs/fess	Enterprise Search Server: Fess [![Build Status](https://travis-ci.org/codelibs/fess.svg?branch=master)](https://travis-ci.org/codelibs/fess) ====  ## Overview  Fess is very powerful and easily deployable Enterprise Search Server. You can install and run Fess quickly on any platforms, which have Java runtime environment. Fess is provided under Apache license.  Fess is Elasticsearch based search server, but knowledge/experience about Elasticsearch is NOT needed because of All-in-One Enterprise Search Server. Fess provides Administration GUI to configure the system on your browser. Fess also contains a crawler, which can crawl documents on Web/FileSystem/DB and supports many file formats, such as MS Office, pdf and zip.  ## Web Sites  [fess.codelibs.org](http://fess.codelibs.org/)  ## Issues/Questions  Please check filed [questions](https://github.com/codelibs/fess/issues?q=label%3Aquestion), and then file an [issue](https://github.com/codelibs/fess/issues "issue") if not filed.  ## Getting Started  ### Download  Fess 11.4 is available. The release page is [HERE](https://github.com/codelibs/fess/releases "download").  ### Install/Run Fess      $ unzip fess-11.4.x.zip     $ cd fess-11.4.x     $ ./bin/fess  For the details, see [Installation Guide](http://fess.codelibs.org/11.3/install/index.html).  ### Access Fess  - Search UI: http://localhost:8080/  ![Search UI](http://fess.codelibs.org/_images/fess_search_result1.png)  - Admin UI: http://localhost:8080/admin/ (username/password is admin/admin)  ![Admin UI](http://fess.codelibs.org/_images/fess_admin_dashboard.png)  You can register crawling targets on Web/File System/Data Store of admin pages, and then start Crawler on Scheduler page manually.  ## Fess on Docker Hub  We provide Docker image on Docker Hub. For more details, see [Public Repository](https://hub.docker.com/r/codelibs/fess/).  ## Migration from Other Systems  Please see [MIGRATION.md](https://github.com/codelibs/fess/blob/master/MIGRATION.md).  ## Localization  ### Japanese   - [Web Site](http://fess.codelibs.org/ja/)  ### Korean   - [PDF Document](https://github.com/nocode2k/fess/releases/download/11.0.1-ko/Fess-ko_11.0.1_manual.pdf)  - [Forum](https://github.com/nocode2k/fess-kr-forum)  ## Development Information  ### Get Source Code  First of all, clone Fess's repository:      $ cd ~/workspace     $ git clone https://github.com/codelibs/fess.git  and then import it as Maven project on Eclipse or other IDE.  ### Setup for Elasticsearch Plugins  Run antrun:run to download plugins into plugins directory:      $ mvn antrun:run  ### Run Fess  Run or debug org.codelibs.fess.FessBoot on IDE, and then access http://localhost:8080/  ### Build Package  Run package goal and then the release file is created in target/releases.      $ mvn package     $ mvn rpm:rpm   # .rpm package     $ mvn jdeb:jdeb # .deb package  ### Generate Source Code      $ mvn dbflute:download # (one time command)     $ mvn dbflute:freegen     $ mvn license:format  ### Integration Tests  Launch Fess Server and run the following command:      $ mvn test -P integrationTests -Dtest.fess.url="http://localhost:8080" -Dtest.es.url="http://localhost:9201"  ### Translate In Your Language  Fess is internationalized software.  If you want to add labels/messages for your language, please translate properties file and then rename to fess\_\*\_[lang].properties.  * [fess_label_en.properties](https://github.com/codelibs/fess/blob/master/src/main/resources/fess_label_en.properties) * [fess_message_en.properties](https://github.com/codelibs/fess/blob/master/src/main/resources/fess_message_en.properties)  For search/index analyzer, if [doc.json](https://github.com/codelibs/fess/blob/master/src/main/resources/fess_indices/fess/doc.json) contains lang\_[lang] for your language, please modify analyzer for your language. For more details about elasticsearch's analyzer, see [Analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html).  We are waiting for pull requests of your language.  ## Powered By  * [Lasta Di](https://github.com/lastaflute/lasta-di "Lasta Di"): DI Container * [LastaFlute](https://github.com/lastaflute/lastaflute "LastaFlute"): Web Framework * [Lasta Job](https://github.com/lastaflute/lasta-job "Lasta Job"): Job Scheduler * [Fess Crawler](https://github.com/codelibs/fess-crawler "Fess Crawler"): Web Crawler * [Elasticsearch](https://github.com/elastic/elasticsearch "Elasticsearch"): Search Engine
spinscale/elasticsearch-suggest-plugin	# DO NOT USE THIS PLUGIN ANYMORE  This plugin has been superceded by the [completion suggester](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html) in Elasticsearch and is not developed further. There is an excellent [introductory blog post](https://www.elastic.co/blog/you-complete-me) available as well.  **This plugin is not developed further than for Elasticsearch 1.3, which you should not use anymore!**  # Suggester Plugin for Elasticsearch  **Note**: If you only need prefix suggestions, please use the new `completion suggest` feature available since elasticsearch 0.90.3, which features blazing fast real time suggestions, uses the `AnalyzingSuggester` under the hood and will also support fuzzy mode in 0.90.4.  This plugin uses the FSTSuggester, the AnalyzingSuggester or the FuzzySuggester from Lucene to create suggestions from a certain field for a specified term instead of returning the whole document data.  Feel free to comment, improve and help - I am thankful for any insights, no matter whether you want to help with elasticsearch, lucene or my other flaws I will have done for sure.  Oh and in case you have not read it above:  In case you want to contact me, drop me a mail at alexander@reelsen.net  ## Breaking changes for elasticsearch 0.90   Because elasticsearch now comes with its own suggest API (not based on in-memory automatons per shard), big parts of this plugin needs to be changed.  ### REST endpoints have been moved  Both REST endpoints have been moved. The `/_suggest` endpoint now resides at `__suggest`. Refreshing has changed from `_suggestRefresh` to `__suggestRefresh`. I do not like this renaming either, but I have not yet got the ieda of a better name. I am totally open for better names. This is a WIP until elasticsearch 1.0 is released.  ### Package names have been moved  Everything is now in the `de.spinscale` package name space in order to avoid clashes. This means, if you are using the request builder classes, you will have to change your application.  ## Installation  If you do not want to work on the repository, just use the standard elasticsearch plugin command (inside your elasticsearch/bin directory)  ``` bin/plugin -install de.spinscale/elasticsearch-plugin-suggest/0.90.5-0.9 ```  ### Compatibility   **Note**: Please make sure the plugin version matches with your elasticsearch version. Follow this compatibility matrix      ----------------------------------------     | suggest plugin   | Elasticsearch     |     ----------------------------------------     | 1.3.2-2.0.1      | 1.3.2 -> master   |     ----------------------------------------     | 1.0.1-2.0.0      | 1.0.1             |     ----------------------------------------     | 0.90.12-1.1      | 0.90.12           |     ----------------------------------------     | 0.90.7-1.0       | 0.90.7            |     ----------------------------------------     | 0.90.5-0.9       | 0.90.5            |     ----------------------------------------     | 0.90.3-0.8.*     | 0.90.3            |     ----------------------------------------     | 0.90.1-0.7       | 0.90.1            |     ----------------------------------------     | 0.90.0-0.6.*     | 0.90.0            |     ----------------------------------------     | 0.20.5-0.5       | 0.20.5 -> 0.20.6  |     ----------------------------------------     | 0.20.2-0.4       | 0.20.2 -> 0.20.4  |     ----------------------------------------     | 0.19.12-0.2      | 0.19.12           |     ----------------------------------------     | 0.19.11-0.1      | 0.19.11           |     ----------------------------------------   ### Development  If you want to work on the repository   * Clone this repo with `git clone git://github.com/spinscale/elasticsearch-suggest-plugin.git`  * Checkout the tag (find out via `git tag`) you want to build with (possibly master is not for your elasticsearch version)  * Run: `mvn clean package -DskipTests=true` - this does not run any unit tests, as they take some time. If you want to run them, better run `mvn clean package`  * Install the plugin: `/path/to/elasticsearch/bin/plugin -install elasticsearch-suggest -url file:///$PWD/target/releases/elasticsearch-suggest-$version.zip`  Alternatively you can now use this plugin via maven and include it via the sonatype repo likes this in your pom.xml (or any other dependency manager)  ``` <repositories>   <repository>     <id>Sonatype</id>     <name>Sonatype</name>     <url>http://oss.sonatype.org/content/repositories/releases/</url>   </repository> </repositories>  <dependencies>   <dependency>     <groupId>de.spinscale</groupId>     <artifactId>elasticsearch-suggest-plugin</artifactId>     <version>1.3.2-2.0.1</version>   </dependency>   ... <dependencies> ```  The maven repo can be visited at https://oss.sonatype.org/content/repositories/releases/de/spinscale/elasticsearch-plugin-suggest/  ## Usage  ### FST based suggestions  Fire up curl like this, in case you have a products index and the right fields - if not, read below how to setup a clean elasticsearch in order to support suggestions.  ``` # curl -X POST 'localhost:9200/products1/product/__suggest?pretty=1' -d '{ "field": "ProductName.suggest", "term": "tischwäsche", "size": "10"  }' {   "suggest" : [ "tischwäsche", "tischwäsche 100",      "tischwäsche aberdeen", "tischwäsche acryl", "tischwäsche ambiente",      "tischwäsche aquarius", "tischwäsche atlanta", "tischwäsche atlas",      "tischwäsche augsburg", "tischwäsche aus", "tischwäsche austria" ] } ```  As you can see, this queries the products index for the field `ProductName.suggest` with the specified term and size.  You can also use HTTP GET for getting suggestions - even with the `callback` and the `source` parameters like in any normal elasticsearch search.  You might want to check out the included unit test as well. I use a shingle filter in my examples, take a look at the files in `src/test/resources` directory.  ### Full suggestions  With Lucene 4 (and the upgrade to elasticsearch 0.90.0) two new suggesters were added, one of them the [AnalyzingSuggester](http://lucene.apache.org/core/4_3_0/suggest/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.html) and the [FuzzySuggester](http://lucene.apache.org/core/4_3_0/suggest/org/apache/lucene/search/suggest/analyzing/FuzzySuggester.html) based on the first one. Both have the great capability of returning the original form, but search on an analyzed one. Take this example (notice the search for a lowercase `b`, but getting back the original field name):  ``` » curl -X POST localhost:9200/cars/car/__suggest -d '{ "field" : "name", "type": "full", "term" : "b", "analyzer" : "standard" }'  {"suggestions":["BMW 320","BMW 525d"],"_shards":{"total":5,"successful":5,"failed":0}} ```  *Note*: If you use type `full` or type `fuzzy`, the `similarity` parameter will not have any effect. In addition, these parameters are supported only for `full` and `fuzzy`:  * `analyzer`: * `index_analyzer`: * `search_analyzer`:  This suggester can even ignore stopwords if configured appropriately - but only if you disable position increments for stopwords. Use this mapping and index settings when creating an index:  ``` curl -X DELETE localhost:9200/cars curl -X PUT localhost:9200/cars -d '{   "mappings" : {     "car" : {       "properties" : {         "name" : {           "type" : "multi_field",           "fields" : {             "name":    { "type": "string", "index": "not_analyzed" }           }         }       }     }   },   "settings" : {     "analysis" : {       "analyzer" : {         "suggest_analyzer_stopwords" : {           "type" : "custom",           "tokenizer" : "standard",           "filter" : [ "standard", "lowercase", "stopword_no_position_increment" ]         },         "suggest_analyzer_synonyms" : {           "type" : "custom",           "tokenizer" : "standard",           "filter" : [ "standard", "lowercase", "my_synonyms" ]         }       },       "filter" : {         "stopword_no_position_increment" : {           "type" : "stop",           "enable_position_increments" : false         },         "my_synonyms" : {           "type" : "synonym",           "synonyms" : [ "jetta, bora" ]         }       }     }   } }'   curl -X POST localhost:9200/cars/car -d '{ "name" : "The BMW ever" }' curl -X POST localhost:9200/cars/car -d '{ "name" : "BMW 320" }' curl -X POST localhost:9200/cars/car -d '{ "name" : "BMW 525d" }' curl -X POST localhost:9200/cars/car -d '{ "name" : "VW Jetta" }' curl -X POST localhost:9200/cars/car -d '{ "name" : "VW Bora" }' ```  Now when querying with a stopwords analyzer, you can even get back `The BMW ever`  ``` » curl -X POST localhost:9200/cars/car/__suggest -d '{ "field" : "name", "type": "full", "term" : "b", "analyzer" : "suggest_analyzer_stopwords" }' {"suggestions":["BMW 320","BMW 525d","The BMW ever"],"_shards":{"total":5,"successful":5,"failed":0}} ```  Or you could use synonyms (FYI: jetta and bora were the same cars, but named different in USA and Europe, so a search should return both)  ``` » curl -X POST localhost:9200/cars/car/__suggest -d '{ "field" : "name", "type": "full", "term" : "vw je", "analyzer" : "suggest_analyzer_synonyms" }'  {"suggestions":["VW Bora","VW Jetta"],"_shards":{"total":5,"successful":5,"failed":0}} ```  ### Full fuzzy suggestions  The FuzzySuggester uses LevenShtein distance to cater for typos.  ``` » curl -X POST localhost:9200/cars/car/__suggest -d '{ "field" : "name", "type": "fuzzy", "term" : "bwm", "analyzer" : "standard" }'  {"suggestions":["BMW 320","BMW 525d"],"_shards":{"total":5,"successful":5,"failed":0}} ```  ### Statistics  The `FuzzySuggester` and the `AnalyzingSuggester` suggesters contain a method to find out their size, which is also exposed as an own endpoint, in case you want to monitor memory consumption of the in-memory structures.  ``` » curl localhost:9200/__suggestStatistics {"_shards":{"total":2,"successful":2,"failed":0},"fstStats":{"cars-0":[{"analyzingsuggester-name-queryAnalyzer:suggest_analyzer_synonyms-indexAnalyzer:suggest_analyzer_synonyms":147},{"analyzingsuggester-name-queryAnalyzer:suggest_analyzer_stopwords-indexAnalyzer:suggest_analyzer_stopwords":126}]}} ```  ### Configuration  Furthermore the suggest data is not updated, whenever you index a new product but every few minutes. The default is to update the index every 10 minutes, but you can change that in your elasticsearch.yml configuration:  ``` suggest:   refresh_interval: 600s ```  In this case the suggest indexes are refreshed every 10 minutes. This is also the default. You can use values like "10s", "10ms" or "10m" as with most other time based configuration settings in elasticsearch.  If you want to deactivate automatic refresh completely, put this in your elasticsearch configuration  ``` suggest:   refresh_disabled: true ```  If you want to refresh your FST suggesters manually instead of waiting for 10 minutes just issue a POST request to the `/__suggestRefresh` URL.  ``` # curl -X POST 'localhost:9200/__suggestRefresh'  # curl -X POST 'localhost:9200/products/product/__suggestRefresh'  # curl -X POST 'localhost:9200/products/product/__suggestRefresh' -d '{ "field" : "ProductName.suggest" }' ```  ## Usage from Java  ``` SuggestRequest request = new SuggestRequest(index); request.term(term); request.field(field); request.size(size); request.similarity(similarity);  SuggestResponse response = node.client().execute(SuggestAction.INSTANCE, request).actionGet(); ```  Refresh works like this - you can add an index and a field in the suggest refresh request as well, if you want to trigger it externally:  ``` SuggestRefreshRequest refreshRequest = new SuggestRefreshRequest(); SuggestRefreshResponse response = node.client().execute(SuggestRefreshAction.INSTANCE, refreshRequest).actionGet(); ```  You can also use the included builders  ``` List<String> suggestions = new SuggestRequestBuilder(client)             .field(field)             .term(term)             .size(size)             .similarity(similarity)             .execute().actionGet().suggestions(); ```  ``` SuggestRefreshRequestBuilder builder = new SuggestRefreshRequestBuilder(client); builder.execute().actionGet(); ```  ## Thanks  * Shay ([@kimchy](http://twitter.com/kimchy)) for giving feedback * David ([@dadoonet](http://twitter.com/dadoonet)) for pushing me to get it into the maven repo * Adrien ([@jpountz](http://twitter.com/jpountz)) for helping me to understand the the AnalyzingSuggester details and having the idea for only creating the FST on the primary shard  ## TODO  * Find and verify the absence of the current resource leak (open deleted files after lots of merging) with the new architecture * Create the FST structure only on the primary shard and send it to the replica over the wire as byte array * Allow deletion of of fields in cache instead of refresh * Reenable the field refresh tests by checking statistics * Also expose the guava cache statistics in the endpoint * Create a testing rule that does start a node/cluster only once per test run, not per test. This costs so much time.  ## Changelog  * 2014-09-03: Version bump to 1.3.2, also created a 1.0, 1.1 and 1.2 branch * 2014-03-01: Version bump to 1.0.1, created a 0.90 branch * 2014-03-01: Version bump to 0.90.12, switched to randomized elasticsearch testing resulting in testing code cleanups and waaaaaaaay faster tests * 2013-12-10: Version bump to 0.90.7 * 2013-08-13: Version bump to 0.90.3. Due to changes in Lucene 4.4, please check the tests to see that stopwords need to be handled on the request side if you use the fuzzy or full mode. * 2013-05-31: Removing usage of jdk7 only methods, version bump to 0.90.1 * 2013-05-25: Changing suggest statistics format, fixing cache loading bug for analyzing/fuzzysuggester * 2013-05-12: Fix for trying to access a closed index reader in AnalyzingSuggesster (i.e. after refresh)  * 2013-05-12: Documentation update * 2013-05-01: Added support for the fuzzy suggester * 2013-04-28: Added support for the analyzing suggester and stopwords * 2013-03-20: Moved to own package namespaces, changed REST endpoints in order to be compatible with elasticsearch 0.90 * 2013-01-18: Support for HTTP GET, together with JSONP and the source parameter. * 2012-10-21: The REST urls can now be used without specifiying a type (which is unused at the moment anyway). You can use now the `$index/_suggest` and `$index/_suggestRefresh` urls * 2012-10-21: Allowing to set `suggest.refresh_disabled = true` in order to deactivate automatic refreshing of the suggest index * 2012-10-06: Shutting down the shard suggest service clean in case the instance is stopped or a shard is moved * 2012-10-03: Starting cluster nodes in parallel in tests where several nodes are created (big speedup) * 2012-10-03: Added tests for refreshing suggest in memory structures for one index or one field in an index only * 2012-10-03: Replaced gradle with maven * 2012-10-03: Updated to elasticsearch 0.19.10 * 2012-10-03: You can use the plugin now with a TransportClient for the first time. Yay! * 2012-10-03: Using the FSTCompletionLookup now instead of the deprecated FSTLookup * 2012-10-03: Pretty much a core rewrite today (having tests is great, even if they run 10 minutes). The suggest service is now implemented as service on shard level - no more central Suggester structures. The whole implementation is much cleaner and adheres way better to the whole elasticsearch architecture instead of being cowboy coded together - at least that is what I think. * 2012-09-30: Updated to elasticsearch 0.19.9. Making TransportClients work again not spitting an exception on startup, when the module is in classpath. Updated this docs. * 2012-06-25: Trying to fix another resource leak, which did not eat up diskspace but still did not close all files * 2012-06-11: Fixing bad resoure leak due to not closing index reader properly - this lead to lots of deleted files, which still had open handles, thus taking up space * 2012-05-13: Updated to work with elasticsearch 0.19.3 * 2012-03-07: Updated to work with elasticsearch 0.19.0 * 2012-02-10: Created `SuggestRequestBuilder` and `SuggestRefreshRequestBuilder` classes - results in easy to use request classes (check the examples and tests) * 2011-12-29: The refresh interval can now be chosen as time based value like any other elasticsearch configuration * 2011-12-29: Instead of having all nodes sleeping the same time and updating the suggester asynchronously, the master node now triggers the update for all slaves * 2011-12-20: Added transport action (and REST action) to trigger reloading of all FST suggesters * 2011-12-11: Fixed the biggest issue: Searchers are released now and do not leak * 2011-12-11: Indexing is now done periodically * 2011-12-11: Found a way to get the injector from the node, so I can build my tests without using HTTP requests  # HOWTO - the long version  This HOWTO will help you to setup a clean elasticsearch installation with the correct index settings and mappings, so you can use the plugin as easy as possible. We will setup elasticsearch, index some products and query those for suggestions.  Get elasticsearch, install it, get this plugin, install it.  Add a suggest and a lowercase analyzer to your `elasticsearch/config/elasticsearch.yml` config file (or do it on index creation whatever you like)  ``` index:   analysis:     analyzer:       lowercase_analyzer:         type: custom         tokenizer: standard         filter: [standard, lowercase]        suggest_analyzer:         type: custom         tokenizer: standard         filter: [standard, lowercase, shingle] ```   Start elasticsearch and create a mapping. You can either create it via configuration in a file or during index creation. We will create an index with a mapping now  ``` curl -X PUT localhost:9200/products -d '{     "mappings" : {         "product" : {             "properties" : { 	        "ProductId":	{ "type": "string", "index": "not_analyzed" }, 	        "ProductName" : { 	            "type" : "multi_field", 	            "fields" : { 	                "ProductName":  { "type": "string", "index": "not_analyzed" }, 	                "lowercase":    { "type": "string", "analyzer": "lowercase_analyzer" }, 	                "suggest" :     { "type": "string", "analyzer": "suggest_analyzer" } 	            } 	        }             }         }     } }' ```  Lets add some products  ``` for i in 1 2 3 4 5 6 7 8 9 10 100 101 1000; do     json=$(printf '{"ProductId": "%s", "ProductName": "%s" }', $i, "My Product $i")     curl -X PUT localhost:9200/products/product/$i -d "$json" done ```  ## Queries  Time to query and understand the different analyzers  Queries the not analyzed field, returns 10 matches (default), always the full product name:  ``` curl -X POST localhost:9200/products/product/_suggest -d '{ "field": "ProductName", "term": "My" }' ```  Queries the not analyzed field, returns nothing (because lowercase):  ``` curl -X POST localhost:9200/products/product/_suggest -d '{ "field": "ProductName", "term": "my" }' ```  Queries the lowercase field, returns only the occuring word (which is pretty bad for suggests):  ``` curl -X POST localhost:9200/products/product/_suggest -d '{ "field":  "ProductName.lowercase", "term": "m" }' ```  Queries the suggest field, returns two words (this is the default length of the shingle filter), in this case "my" and "my product"  ``` curl -X POST localhost:9200/products/product/_suggest -d '{ "field": "ProductName.suggest", "term": "my" }' ```  Queries the suggest field, returns ten product names as we started with the second word + another one due to the shingle  ``` curl -X POST localhost:9200/products/product/_suggest -d '{ "field": "ProductName.suggest", "term": "product" }' ```  Queries the suggest field, returns all products with "product 1" in the shingle  ``` curl -X POST localhost:9200/products/product/_suggest -d '{ "field": "ProductName.suggest", "term": "product 1" }' ```  The same query as above, but limits the result set to two   ``` curl -X POST localhost:9200/products/product/_suggest -d '{ "field": "ProductName.suggest", "term": "product 1", "size": 2 }' ```  And last but not least, typo finding, the query without similarity parameter set returns nothing:  ``` curl -X POST localhost:9200/products/product/_suggest -d '{ "field": "ProductName.suggest", "term": "proudct", similarity: 0.7 }' ```  The similarity is a float between 0.0 and 1.0 - if it is not specified 1.0 is used, which means it must match exactly. I've found 0.7 ok for cases, when two letters were exchanged, but mileage may very as I tested merely on german product names.  With the tests I did, a shingle filter held the best results. Please check http://www.elasticsearch.org/guide/reference/index-modules/analysis/shingle-tokenfilter.html for more information about setup, like the default tokenization of two terms.  Now test with your data, come up and improve this configuration. I am happy to hear about your specific configuration for successful suggestion queries.
Aconex/scrutineer	Analyses a secondary stream of information against a known point-of-truth and reports inconsistencies.  The Why =======  When you have a Lucene-based index of substantial size, say many hundreds of millions of records, what you want is confidence that your index is correct. In many cases, people use Solr/ElasticSearch/Compass to index their central database, mongodb, hbase etc so the index is a secondary storage of data.  How do you know if your index is accurate? Can you just reindex 500 million documents anytime you like? (That's the Aliens: "Nuke the site from Orbit... It's the only way to be sure" approach). No, if there _ARE_ inconsistencies in your index, then you want to:  * find the items that are incorrect (and only them) * do it fast  Scrutineer has been designed with this in mind, it can find any inconsistencies in your index fast.   How does this work? ===================  Scrutineer relies on your data having 2 core properties:  * an ID - a unique identifier for your object * a Version - something stored in your primary datastore for that object that represents the temporal state of that object  The Version property is commonly used in an Optimistic Locking pattern. If you store the ID & Version information in your secondary store (say, Solr/ElasticSearch) then you can always compare for any given item whether the version in secondary store is up to date.  Scrutineer takes a stream from your primary, and a stream from your secondary store, presumes they are sorted identically (more on that later) and walks the streams doing a merge comparison. It detects 4 states:  1. Both items are identical (yay!) 2. An ID is missing from the secondary stream (A missed add?  maybe that index message you sent to Solr/ElasticSearch never made it, anyway, it's not there) 3. An ID was detected in the secondary, but wasn't in the primary stream (A missed delete?  something was deleted on the primary, but the secondary never got the memo) 4. The ID exists in both streams, but the Version values are inconsistent (A missed update?  similar to the missed add, this time perhaps an update to a row in your DB never made it to Solr/ElasticSearch)  Example ======= Here's an example, 2 streams in sorted order, one from the Database (your point-of-truth),  and one from ElasticSearch (the one you're checking) with the <ID>:<VERSION> for each side:  <table border="1">   <tr><th>Database</th><th>ElasticSearch</th></tr>   <tr><td>1:12345</td><td>1:12345</td></tr>   <tr><td>2:23455</td><td>3:84757</td></tr>   <tr><td>3:84757</td><td>4:98765</td></tr>   <tr><td>4:98765</td><td>5:38475</td></tr>   <tr><td>6:34666</td><td>6:34556</td></tr> </table>  Scrutineer picks up that:  * ID '2' is missing from ElasticSearch * ID '5' was deleted from the database at some point, but ElasticSearch still has it in there * ID '6' is visible in ElasticSearch but appears to have the wrong version  Running Scrutineer ==================  The very first thing you'll need to do is get your JDBC Driver jar and place it in the 'lib' directory of the unpacked package.  We already have a JTDS driver in there if you're using SQL Server (that's just what we use).      bin/scrutineer \                 --jdbcURL=jdbc:jtds:sqlserver://mydbhost/mydb  \                 --jdbcDriverClass=net.sourceforge.jtds.jdbc.Driver \                 --jdbcUser=itasecret \                 --jdbcPassword=itsasecret   \                 --sql="select id,version from myobjecttype order by cast(id as varchar(100))" \                 --clusterName=mycluster \                 --indexName=myindex \                 --query="_type:myobjecttype" \                 --numeric  *Note:* if you're weirded out about that '...cast(...)' then don't worry, we'll explain that shortly.  * **jdbcURL** – Standard JDBC URL you would use for your app to connect to your database * **jdbcDriverClass** - Fully qualified class name of your JDBC Driver (don't forget to put your JDBC Driver jar in the lib directory as said above!) * **jdbcUser** - user account to access your JDBC Database * **jdbcPassword** -- password required for the user credentials * **sql** - The SQL used to generate a lexicographical stream of ID & Version values (in that column order) * **clusterName** - this is your ElasticSearch cluster name used to autodetect and connect to a node in your cluster * **indexName** - the name of the index on your ElasticSearch cluster * **query** - A query_parser compatible search query that returns all documents in your ElasticSearch index relating to the SQL query you're using   Since it is common for an index to contain a type-per-db-table you can use the "_type:<type>" search query to filter for all values for that type. * **numeric** - use this if your query returns results numerically ordered    Output ====== Scrutineer writes any inconsistencies direct to Standard Error, in a well-defined, tab-separated format for easy parsing to feed into a system to reindex/cleanup.  If we use the Example scenario above, this is what Scrutineer would print out:       NOTINSECONDARY    2    23455     MISMATCH    6    34666    secondaryVersion=34556     NOTINPRIMARY    5    38475  The general format is:     **FailureType**\t**ID**\t**VERSION**\t**Optional:Additional Info**  ### NOTINSECONDARY This means you are missing this item in your secondary and you should reindex/re-add to your secondary stream  ### MISMATCH This means the version of the object stored in the secondary is not the same information as the primary, and you should reindex  ### NOTINPRIMARY The object was removed from the Primary store, but the secondary still has it.  You should remove this item from your secondary.  Scrutineer does _not_ report when items match, we'll presume you're just fine with that...  Versions as Timestamps ====================== If you use timestamps as your version property, it is sometimes useful to know the underlying time value of the timestamp to triage why the differences are occurcing.  For example, if a MISMATCH error occurs, you can look at the timestamp version of the Primary to work out when the last update was done to see why the Secondary never received it, perhaps by digging through your own application logs.  By default, Scrutineer doesn't presume this, so you can use the      --versions-as-timestamps  Command-line option to tell Scrutineer that your versions are timestamps, and a slightly different formatting of the result is chosen.  Each time the Version property is reported (in both Primary or Secondary values, if printed) also include a human-readable, ISO8601 timestamp using the servers local Timezone for convenience.  For example, using the above output sample:      NOTINSECONDARY    2    23455(1970-1-1T16:30:55.000+10:00)     MISMATCH    6    34666(1970-1-1T19:37:46.000+10:00)    secondaryVersion=34556(1970-1-1T19:35:56.000+10:00)     NOTINPRIMARY    5    38475(1970-1-1T20:41:15.000+10:00)  The TAB value is still the field delimiter here.  Memory ======  By default, Scrutineer allocates 256m to the Java Heap, which is used for sort, and ElasticSearch result buffers.  This should be more than enough for the majority of cases but if you find you get an OutOfMemoryError, you can override the JAVA_OPTS environment variable to provide more heap.  e.g.      export JAVA_OPTS=-Xmx1048m   Sorting =======  __*VERY IMPORTANT*__: Scrutineer relies on both streams to be sorted using an identical mechanism. It requires input streams to be in lexicographical (default) or numerical (indicate using `--numeric`) sort order.  ElasticSearch =============  Since Aconex uses ElasticSearch, Scrutineer supports ES out of the box, but it would not be difficult for others to integrate a Solr stream and wire something up. Happy to take Pull Requests!  What are the 'best practices' for using Scrutineer? ===================================================  The authors of Scrutineer, Aconex, index content from a JDBC data source and index using ElasticSearch.  We do the following:  * In the database table of the object being indexed we add an Insert/Update trigger to populate a 'lastupdated' timestamp column as our Version property * When we index into ElasticSearch, we set the Version property of the item using the VersionType.EXTERNAL setting.   * We create an SQL Index on this tuple so these 2 fields can be retrieved from the database very fast   Assumptions ===========  * Your Version property is Long compatible.  You can use java.sqlTimestamps column types too as a Version (that's what we do) * Aconex is DB->ElasticSearch centric at the moment.  We've tried to keep things loosely coupled, so it should be  simple to add further integration points for other Primary & Secondary sources (HBase, MongoDB, Solr).  JDBC Drivers ============ Scrutineer ships with the SQL Server JTDS driver by default (it's what we use).  All you _should_ need to do is drop your own JDBC driver in the 'repo' sub-directory of the Scrutineer distribution (where all the other jars are).  We use the Maven AppAssembler plugin which is configured to automatically load all JARs in this path onto the classpath.  Building ======== Scrutineer is a Maven project, which really _should_ just build right out of the box if you have Maven installed.  Just type:      mvn package  And you should have a Tarball in the 'target' sub-directory.  Submitting Pull Requests ========================  First, Please add unit tests!  Second, Please add integration tests!  Third, We have tightened up the quality rule set for CheckStyle, PMD etc pretty hard.  Before you issue a pull request, please run:      mvn verify  which will run all quality checks.  Sorry to be super-anal, but we just like Clean Code.  Roadmap =======  * Scrutineer currently only runs in a single thread based on a single stream.   It would be good to provide a 'manifest' to Scrutineer to outline a set of stream verifications to perform, perhaps one for each type you have so that your multi-core system can perform multiple stream comparisons in parallel.  * Incremental checking – Right now Scrutineer checks the whole thing, BUT if you are using timestamp-based versions, there's no reason  it couldn't only check objects that were changed after the last known full verification.  This would require one to keep track of deletes on the primary stream (perhaps an OnDelete Trigger in your SQL database) so that IDs that were deleted in the primary stream after the last full check could be detected correctly.  * Obviously we'd love to have a Solr implementation here, we hope the community can help here.
Anchormen/sql4es	Elastic announced the great news that they are workong on SQL support at Elastic{ON} 2017! You can find the talk on this topic on the [Elastic Website](https://www.elastic.co/elasticon/conf/2017/sf/elasticsearch-sql)  ### sql4es: JDBC driver for Elasticsearch  Sql-for-Elasticsearch (sql4es) is a jdbc 4.1 driver for **Elasticsearch 2.0 - 2.4** implementing the majority of the JDBC interfaces: Connection, Statement, PreparedStatment, ResultSet, Batch and DataBase- /  ResultSetMetadata. The screenshot below shows SQLWorkbenchJ with a selection of SQL statements that can be executed using the driver. As of version 0.8.2.3 the driver supports Shield allowing the use of credentials and SSL.  ![SQLWorkbenchJ screenshot with examples](release/workbench_examples.png)  #### Usage  The sql4es driver can be used by adding the jar file, found within the releases directory of the project, to the tool/application used and load the driver with name '***nl.anchormen.sql4es.jdbc.ESDriver***'. The driver expects an URL with the following format: ***jdbc:sql4es://host:port/index?params***.   - host: the hostname or ip of one of the es hosts (required) - port: an optional the port number to use for the transport client (default is 9300) - index: the optional index to set active within the driver. Most statements like SELECT, DELETE and INSERT require an active index (also see USE [index/alias] statement below). It is however possible to create new indices, types and aliases without an active index. - params: an optional set of parameters used to influence the internals of the driver (specify additional hosts, maximum number of documents to fetch in a single request etc). If your clustername is not 'elasticsearch' you should specify the clustername witin the url (see example below). Please see the Configuration section of this readme for a description of all driver specific parameters.  ``` java // register the driver and get a connection for index 'myidx' Class.forName("nl.anchormen.sql4es.jdbc.ESDriver"); Connection con = DriverManager.getConnection("jdbc:sql4es://localhost:9300/myidx?cluster.name=your-cluster-name"); Statement st = con.createStatement(); // execute a query on mytype within myidx ResultSet rs = st.executeQuery("SELECT * FROM mytype WHERE something >= 42"); ResultSetMetaData rsmd = rs.getMetaData(); int nrCols = rsmd.getColumnCount(); // get other column information like type while(rs.next()){ 	for(int i=1; i<=nrCols; i++){   		System.out.println(rs.getObject(i)); 	} } rs.close(); con.close(); ```  The driver can also be used from applications able to load the jdbc driver. It has been tested with [sqlWorkbench/J](http://www.sql-workbench.net/) and [Squirrel](http://squirrel-sql.sourceforge.net/) on an Elasticsearch 2.3 cluster. A description on how to use sql4es with sqlWorkbenchJ along with a number of example queries can be found at the bottom of this readme.  As of version 0.8.2.3 the driver supports **Shield**. The following URL prameters must be set in order to use shield:  * shield.user='username:password' to set global credentials * ssl (enables SSL) * cluster.name='elastic cloud cluster id' (only applies when connecting with a cluster in the Elastic Cloud)  The example below is used to connect with a Shield protected cluster in the Elastic Cloud using SSL.  ```scala Connection con = DriverManager.getConnection("jdbc:sql4es://f03c93be1efeb9be9b2f46b660d10d90.eu-west-1.aws.found.io:9343/indexname?shield.user=username:password&cluster.name=f03c93be1efeb9be9b2f46b660d10d90&ssl"); ```    ### Supported SQL  Simply said the sql4es driver translates SQL statements to their Elasticsearch counterparts and parses results into ResultSet implementations. The following sql statements are supported:  - SELECT: fetches documents (with or without scoring) or aggregations from elasticsearch   * COUNT (DISTINCT ...), MIN, MAX, SUM, AVG   * DISTINCT   * WHERE (=, >, >=, <, <=, <>, IN, LIKE, AND, OR, IS NULL, IS NOT NULL, NOT [condition])   * GROUP BY   * HAVING   * ORDER BY   * LIMIT (without offset, offsets are not supported by sql4es) - CREATE TABLE (AS) creates an index/type and optionally indexes the result of a query into it - CREATE VIEW (AS): creates an alias, optionally with a filter - DROP TABLE/VIEW removes an index or alias - INSERT INTO (VALUES | SELECT): inserts documents into an index/type; either provided values or results of a query. Possible to UPDATE documents using INSERT by specifying existing document _id's - UPDATE: executed as an elasticsearch Upsert - DELETE FROM (WHERE): removes documents - USE: selects an index as the driver's active one (used to interpret queries) - EXPLAIN SELECT: returns the Elasticsearch query performed for a SELECT statement - Table aliases like SELECT … FROM table1 as T1, table2 t2...   - Table aliases are parsed but not used during query execution  **Remarks**  Elasticsearch does not support transactions. Hence executing batches cannot be rolled back upon failure (nor can statements be committed). It also takes some time for documents to be indexed fully so executing an INSERT directly followed by a SELECT might not include the inserted documents.  Some SQL statements or Elasticsearch features that are ***not (yet) supported***:  - ~~UPDATE is not supported, although it is possible to update documents by inserting values for an existing _id~~   - added in 0.7.2.1: it is now possible to executes updates like UPDATE index.type SET myInt=100 WHERE myString = 'hundred' - ~~Not possible to INSERT nestested objects~~   - added in 0.7.2.1 using double quotes: INSERT INTO mytype ("myObject.nestedDoc.myInt") VALUES (1) - Not possible to specify offsets (OFFSET offset or LIMIT offset, number) - ~~Fields with type 'nested' are not supported because this type requires different methods to query and retrieve data.~~    - added in 0.6.2.1: Nested types are detected by the driver and queries on those fields are executed accordingly - Parent child relationships are not supported. It is currently not possible to index or retrieve fields of this type. - Elasticsearch features like ~~full text search, highlighting,~~ suggestions and  templates are not supported.   - added in 0.6.2.1: full text search can be done using *_search = '…'* and highlighting trough *SELECT highlight(some-field) FROM …* - ~~Count (Distinct …)~~    - Added in 0.9.2.4. It is possible to set the Elasticsearch precision threshold (see [cardinality aggregations](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html)) by using the precision.threshold parameter in the connection url.  ### Concepts  Since elasticsearch is a NO-SQL database it does not contain the exact relational objects most people are familiar with (like databases, tables and records). Elasticsearch does however have a similar hierarchy of objects (index, type and document). The conceptual mapping used by sql4es is the following:  - Database = Index - Table = Type - Record = document - Column = Field - View = Alias (this does not fit from a hierarchical perspective but does match given the purpose of views / aliases)  Elasticsearch responses, both search results and aggregations, are put into a ResultSet implementation. Any nested objects are 'exploded' into a lateral view by default; this means that nested objects are treated as joined tables which are put inside the he same row (see [this page](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView) for explanation). It is possible to represent nested objects as a nested ResultSet, see the Configuration section. Note, that although objects are exploded, arrays with primitives are not! They are put in a java.sql.Array implementation supported by JDBC.  Sql4es works from an active index/alias which means that it resolves references to types from this index. If for example *myIndex* is currently active the query *SELECT * FROM sometype* will only return any results if sometype is part of myindex. Executing a SELECT on a type that does not exist within an index will return an empty result. It is possible to change the active index by executing *USE [otherIndex]* as described below.   ### QUERIES  This section describes how SQL is interpreted and converted into SE statements. The presto parser is used to parse SQL statements, please see the syntax definition on the [presto website](https://prestodb.io/docs/current/sql.html).  #### SELECT  ``` sql /* basic syntax */ SELECT [field (AS alias)] FROM [types] WHERE [condition] GROUP BY [fields] HAVING [condition] ORDER BY [field (ASC|DESC)] LIMIT [number] ```  - fields (AS alias): defines the fields to be retrieved from elasticsearch and put in the ResultSet. It is possible to use * to indicate all fields should be retrieved (including _id and _type). Fields can be addressed by their name, nested fields can be addressed  using their hierarchical names in dotted notation like: *nesteddoc.otherdoc.field*. Using a star will simply fetch all fields, also nested ones, present in a document. It is possible to specify the root of an object in order to fetch all its fields. A query like *SELECT nesteddoc FROM type* will fetch all fields present in nesteddoc. As a result it might return hundreds of columns if nesteddoc has hundreds of fields. - types: the types to execute the query against. This can only be types present in the index or alias that is currently active (also see 'use' statement). - condition: standard SQL condition using =, >, >=, <, <=, <>, IN and LIKE operators. Sql4es does not support the NOT operator but '<>' can be used instead. Use AND and OR to combine conditions.  - limit: only works on non aggregating queries. The use of offset is not supported!  ``` sql /* the following wil explode any nested objects into a lateral view */ SELECT * from mytype  SELECT _id as id, myInt, myString FROM mytype WHERE myInt >= 3 OR (myString IN ('hello','hi','bye') AND myInt <= 3)  /* If nestedDoc contains 2 fields the result will be exploded to [myInt,nestedDoc.field1, nestedDoc.field2] */ SELECT myInt, nestedDoc FROM mytype WHERE myInt > 3 AND myString <> 'bye'  /* If the array contains 3 objects the resultset will contain 3 rows, despite the LIMIT used! */ SELECT array_of_nested_objects FROM mytype LIMIT 1 ```  **Tables/Types**  Only types part of the active index or alias can be addressed in the FROM clause. An alias must be created if types from different indices must be accessed in a single query (see CREATE VIEW for alias creation). The *query cache* is the only exception to this rule. When the query cache identifier (default 'query_cache') is used within FROM it indicates the use of the query cache. Whenever present a query is fetched from the cache rather than executed which minimizes query time in case of time consuming queries.  ``` sql /* fetch some data from type */ SELECT DISTINCT field, count(1) FROM type, query_cache /* exactly the same as above but now also hitting the query cache */ SELECT DISTINCT field, count(1) FROM type ```  **Text matching, search and scoring**  By default queries are executed as a filter which means elasticsearch does not scores the results and they are returned in an arbitrary order. Add '_score' as one of the selected columns in order to change this behaviour and request scoring. By default results are returned sorted on _score DESC (can be changed to ORDER BY _score ASC). Ordering on another field will disable scoring! In addition it is possible to get the id and type of a document by specifying _id and _type respectively.  Sql4es does not make a difference between searching and matching on textual fields. Behaviour totally depends on the analysis (if any) performed on the textual field being queried/searched. Under the hood a couple of simple rules are used to determine what type of query should be use:  - a single word will be put in a TermQuery (example: myString = 'hello') - multiple words will be put in a MatchPhraseQuery (example: myString = 'hello there') - presence of wildcards (% and \_) will trigger the use of a WildcardQuery (% is replaced with * and _ with ?). Examples: mystring = '%something' is the same as mystring LIKE '%something') - the use of IN (…) will be put in a TermsQuery  In addition it is possible to execute a regular search with all features supported by ES. Searching is done by executing a match on the fictional field '_search' (see examples below). It is possible to request highlights for any text field using the highlight function like: SELECT highlight(field), … Fragment size and number can be set through the global configuration.  ``` sql /* term query */ SELECT _score, myString FROM mytype WHERE myString = 'hello' OR myString = 'there' /* Same as above */ SELECT _score, myString FROM mytype WHERE myString IN ('hello', 'there') /* use of NOT; find all documents which do not contain 'hello' or 'there' */ SELECT _score, myString FROM mytype WHERE NOT myString IN ('hello', 'there')  /* check for NULL values (missing fields) */ SELECT myInt FROM mytype WHERE myString NOT NULL SELECT myInt FROM mytype WHERE myString IS NULL  /* phrase query */ SELECT _score, highlight(myString), myString FROM mytype WHERE myString = 'hello there' /* wildcard query */ SELECT _score, myString FROM mytype WHERE myString = 'hel%' /* a search for exactly the same as the first two */ SELECT _score, highlight(mystirng) FROM mytype WHERE _search = 'myString:(hello OR there)' ```  **Get document by _id**  It is possible to execute searches for document id's by specifying '=' or IN predicates on the _id field. It is possible to combine the match on an _id with other fields but matching multiple _id should always be done using IN.  ``` sql SELECT * FROM mytype WHERE _id = 'whatever_id' SELECT * FROM mytype WHERE _id = 'whatever_id' AND myInt > 3 SELECT * FROM mytype WHERE _id = 'whatever_id' OR _id = 'another_ID' /* WRONG */ SELECT * FROM mytype WHERE _id IN ('whatever_id', 'another_ID') /* CORRECT */ ```  **Aggregation**  Sql4es will request an aggregation whenever it finds a DISTINCT, GROUP BY or aggregation functions (MIN, MAX, SUM, AVG or COUNT) are requested without any normal fields. No search results are returned whenever an aggregation is requested.   Sql4es supports some basic arithmetic functions: \*, /, +, - and % (modulo).  It is also possible to combine different fields from the resultset within a calculation like AVG(field)/100 and SUM(field)/count(1). Note that within the current implementation these calculations are performed within the driver once data has been fetched from Elasticsearch. It is possible to refer to values in other rows within functions using brackets *[offset]*. For example SUM(volume)/SUM(volume)[-1] will devide the sum of volume column for row X with the value in row X-1. If a value cannot be calculated, for example row number 0 in the example above, it will get value Float.NaN.    ``` sql /* Aggregates on a boolean and returns the sum of an int field in desc order */ SELECT myBool, sum(myInt) as summy FROM mytype GROUP BY myBool ORDER BY summy DESC  /* This is the same as above */ SELECT DISTINCT myBool, sum(myInt) as summy ROM mytype ORDER BY summy DESC  /* Aggregates on a boolean and returns the sum of an int field only if it is larger than 1000 */ SELECT myBool, sum(myInt) as summy ROM mytype GROUP BY myBool HAVING sum(myInt) > 1000  /* Gets the average of myInt in two different ways... */ SELECT myBool, sum(myInt)/count(1) as average, avg(myInt) FROM mytype GROUP BY myBool  /* Calculates the percentage of growth of the myInt value acros increasing dates */ SELECT myDate, sum(myInt)/sum(myInt)[-1]*100 FROM mytype GROUP BY myDate ORDER BY myDate ASC  /* aggregation on all documents without a DISTINCT or GROUP BY */ SELECT count(*), SUM(myInt) from mytype  /* the following will NOT WORK, a DISTINCT or GROUP BY on mytext is required */ SELECT mytext, count(*), SUM(myInt) from mytype ```  Some notes on SELECT:  - limit only works on non aggregating queries. Any 'limits' on an aggregation will be omitted  - calculations on fields are currently performed within the driver - having (filtering on aggregated results) is currently performed within the driver - sorting of aggregated results are currently performed within the driver  #### EXPLAIN  Explain can be used to view the elasticsearch query executed for a SELECT statement by executing:  ***EXPLAIN [SELECT statement]***  #### USE  Sql4es uses an active index/alias. By default this is the index/alias specified within the URL used to get the connection (if any). It is possible to change the active index/alias by executing:  ***USE [index / alias]***  All subsequent statements will be executed from the specified index/alias. This action only influences the driver and has no effect on Elasticsearch  #### CREATE & DROP  Sql4es supports creation of indices, types (create table) and aliases (create view). These statements require knowledge of ES mechanics like mappings, type definitions and aliases.   ***CREATE TABLE (index.)type ([field] "[field definition]" (, [field2])...) WITH (property="value" (, property2=...) )***  This creates a mapping for [type] within the currently active index or in the index specified using dot notation. Whenever dotnotation is used it is assumed the part before the first dot refers to the index. If the index specified already exists it just adds the type to this index.  The field definition is the json definition put in the mapping without quoting the json elements! A string type can be defined as follows: *CREATE TABLE mytype (stringField "type:string, index:analyzed, analyzer:dutch")*. Any mapping elements, like templates, can be set using the WITH clause (see example below). All these json parts will be quoted properly and mashed together into a mapping request.  ``` sql /* creates a mapping for mytype within newindex with a template to store any strings without analysis */ CREATE TABLE index.mytype ( 	myInt "type:integer",   	myDate "type:date, format:yyyy-MM-dd"   	myString "type:string, index:analyzed, analyzer:dutch" ) WITH (   dynamic_templates="[{     default_mapping: {      	match: *,     	match_mapping_type: string,      	mapping: {type: string, index: not_analyzed }     }   }]" ) ```  An empty index can be created using CREATE TABLE index.type (_id "type:string"). The _id field is omitted because it is a standard ES field.  ***CREATE TABLE (index.)type AS SELECT ...***  Creates a new index/type based on the results of a SELECT statement. The new fieldnames are taken from the SELECT, it is possible to use column-aliases to influence the fieldnames. For example CREATE TABLE myaverage AS SELECT avg(somefield) AS average will result in a new type myaverage within the currently active index with a single Double field called 'average'. Note that this is a two step process taking place at the driver. First the query is executed and secondly the index is created and results are written (in bulk) to the new type.  ``` sql /*Create another index with a type mapping based on the mapping created before*/ CREATE TABLE index.mytype AS SELECT myDate as date, myString as text FROM anyType  /* create a type with a (possibly expensive to calculate) aggregation result */ CREATE TABLE index.myagg AS SELECT myField, count(1) AS count, sum(myInt) AS sum from anyType GROUP BY myField ORDER BY count DESC ```  ***CREATE VIEW [alias] AS SELECT * FROM index1 (, [index2])... (WHERE [condition])***  Creates a new ES alias containing the specified indexes or adds the indexes to an existing alias. The optional WHERE clause adds a filter on the index-alias pairs specified. See the [elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html) for information on aliases  ***DROP TABEL [index] / DROP VIEW [alias]***  Removes the specified index or alias  ``` sql /*Create an elasticsearch alias which includes two indexes with their types */ CREATE VIEW newalias AS SELECT * FROM newindex, newindex2  /* Same as above but with a filter*/ CREATE VIEW newalias AS SELECT * FROM newindex, newindex2 WHERE myInt > 99  /*Use the alias so it can be queried*/ USE newalias  /* removes myindex and remove newalias */ DROP TABLE myindexindex DROP VIEW newalias ```  #### INSERT & DELETE  Describes inserting and deleting data through sql  ***INSERT INTO (index.)type ([field1], [field2]...) VALUES ([value1], [value2], ...), ([value1], ...), …***  Adds one or more documents to the specified type within the index. Fields must be defined and the number of values must match the number of fields defined. It is possible to add multiple documents within a single INSERT statement. It is possible to specify the _id field within the insert statement. In this case it will force elasticsearch to insert the specified document id. The insert acts as an UPDATE if the _id already exists! It is not possible to insert nested objects as they cannot be specified in the SQL language.  ***INSERT INTO  (index.)type SELECT …***  Adds all of the results from the SELECT statement to the specified type within the index. Fieldnames to insert are taken from the result (i.e. column aliases can be used). Note that, similar to the 'CREATE TABLE .. AS SELECT' the results are pulled into the driver and then indexed (using Bulk).  ``` sql /* Insert two documents into the mytype mapping */ INSERT INTO mytype (myInt, myDouble, myString) VALUES (1, 1.0, 'hi there'), (2, 2.0, 'hello!')  /* insert a single document, using quotes around nested object fields */ INSERT INTO mytype (myInt, myDouble, "nestedObject.myString") VALUES (3, 3.0, 'bye, bye')  /* update or insert a document with specified _id */ INSERT INTO mytype (_id, myInt, myDouble) VALUES ('some_document_id', 4, 4.0)  /* copy records from anotherindex.mytype to myindex.mytype that meet a certain condition */ USE anotherindex INSERT INTO myindex.mytype SELECT * from newtype WHERE myInt < 3  ```  ***DELETE FROM type (WHERE [condition])***  Deletes all documents from the specified type that meet the condition. If no WHERE clause is specified all documents will be removed. As Elasticsearch can only delete documents based on their \_id which means that this statement is executed in two steps. First collect all _id's from documents that meet the condition, secondly delete those documents using the bulk API  ``` sql /* delete documents that meet a certain condition*/ DELETE FROM mytype WHERE myInt == 3  /*delete all documents from mytype*/ DELETE FROM mytype  ```  ### UPDATE  It is possible to update documents within an index/type using standard SQL syntax. Note that nested object names must be surrounded by double quotes:  ***UPDATE index.type SET field1=value, fiedl2='value', "doc.field"=value WHERE condition***  The update is executed in two steps. First the _id's of all documents matching the condition are fetched after which the specified fields for these documents are updated in batch using the Upsert API.   ### Configuration  It is possible to set parameters through the provided url. All parameters are exposed to elastic search as well which means that is is possible to set Client parameters, see [elasticsearch docs](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/configuration.html). The following driver specific parameters can be set:  - es.hosts: a comma separated list with additional hosts with optional ports in the format host1(:port1), host2(:port2) … The default port 9300 is taken when no port is specified.  - fetch.size (int default 10000): maximum number of results to fetch in a single request (10000 is elasticsearch's maximum). Can be lowered to avoid memory issues when documents fetched are very large. - results.split (default: false): setting this will split the entire result into multiple ResultSet objects, each with maximal *fetch.size* number of records. The next ResultSet can be fetched using Statement.getMoreResults(). The default is *false* and the driver will put all results within a single ResultSet. This setting should be used when the client has insufficient memory to hold all the results within a single ResultSet. - scroll.timeout.sec (int, default 10): the time a scroll id remains valid and 'getMoreResults()' can be called. Should be increased or decreased depending on the scenario at hand. - query.timeout.ms (int, default 10000): the timeout set on a query. Can be altered depending on the use case. - default.row.length (int, default 250): the initial number of columns created for results. Increase this property only when results do not fit (typically indicated by an array index out of bounds exception) triggered when search results are parsed. - query.cache.table (string, default 'query_cache'): the fictional table name used to indicate using elasticsearch query cache. Can be changed to make it shorter or more convenient. - result.nested.lateral (boolean, default true): specifies weather nested results must be exploded (the default) or not. Can be set to false when working with the driver from your own code. In this case a column containing a nested object (wrapped in a ResultSet) will have java.sql.Types =  Types.JAVA_OBJECT and can be used as (ResultSet)rs.getObject(colNr). - fragment.size (int, default 100): specifies the preferred fragment length in characters. - fragment.count (int, default 1): specifies the maximum number of fragments to return when requesting highlighting. - precision.threshold (int, default 3000): specifies the precision used for [cardinality aggregations](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html)  ### Example using SQLWorkbenchJ  SQLWorkbenchJ is a SQL GUI that can be used to access an Elasticsearch cluster using the sql4es driver. Follow the steps below to set it up and execute the example statements.  1. download SQLWorkbenchJ for your platform from the [website](http://www.sql-workbench.net/downloads.html) 2. Install SQLWorkbencJ and open it 3. Add the SQL4ES driver:    1. click 'Manage Drivers' in the bottom left corner    2. click the blank document icon on the left to add a new driver    3. Give the driver a descriptive name (sql4es for example)    4. point the Library to the sql4es jar file on your system (found within the release directory of the project)    5. set 'nl.anchormen.sql4es.jdbc.ESDriver' as the Classname    6. set 'jdbc:sql4es://localhost:9300/index' as the sample URL 4. Click 'Ok' to save the configuration 5. Add a new database connection using the sql4es driver    1. click the 'create new connection profile' button in the top    2. give the profile a descriptive name    3. specify the sql4es driver added before    4. specify the url of your Elasticsearch 2.X cluster using the index 'myindex' ('jdbc:sql4es://your.es.host:port/myindex'    5. click the save button on the top 6. Select the created profile and click 'Ok' to create the connection    1. An empty workbench will open when everything is ok. 7. Copy the statements below into the workbench 8. Execute the statements one by one and view the results. A brief description of each statement can be found below the SQL statements    1. Running all the statements as a sequence works as well but will not provide any results for the SELECT statements because the cluster is still indexing the inserts when they are being executed (remember, elasticsearch is not a relational database!)  ``` sql INSERT INTO mytype (myLong, myDouble, myDate, myText) VALUES (1, 1.25, '2016-02-01', 'Hi there!'),(10, 103.234, '2016-03-01', 'How are you?');  SELECT * FROM mytype; SELECT _score, myLong, myText FROM mytype WHERE myText = 'hi' AND myDate > '2016-01-01';  EXPLAIN SELECT _score, myLong, myText FROM mytype WHERE myText = 'hi' AND myDate > '2016-01-01';  CREATE TABLE myindex2.mytype2 AS SELECT myText, myLong*10 as myLong10, myDouble FROM mytype;  DELETE FROM mytype WHERE myDouble = 1.25; USE myindex2; SELECT * FROM mytype2; CREATE VIEW myview AS SELECT * FROM myindex, myindex2 WHERE myDouble > 2; USE myview; SELECT * FROM mytype, mytype2; drop view myview; drop table myindex; drop table myindex2; ```  1. insert two documents into the new type called mytype (the type will be created and mapping will be done dynamically by elasticsearch). Check the DatabaseExplorer option to view the mapping created.  2. show all documents present in mytype  3. execute a search and show the score  4. show the Elasticsearch query performed for the search  5. create a new index and a new type based on the documents in mytype. Note that mytype2 has some different fieldnames than the type it was created of.  6. delete some documents  7. make the newly created index 'myindex2' the active one  8. show all documents within mytype2  9. create a view (alias) for myindex and myindex2 with a filter  10. make the newly created view the active one  11. select all documents present within the view, note that:   12. not all documents are shown due to the filter on the alias  13. some of the fields are empty because the two types queried the empty fields because the two types have a couple of different fields (myDate, myLong and myLong10)       ​
salyh/elasticsearch-security-plugin	# News/Status This plugin is no longer maintained, if you're looking for security for elasticsearch i recommend  * [Search Guard](https://github.com/floragunncom/search-guard) - Free (and open source) plugin from [floragunn GmbH](https://floragunn.com/searchguard/), supports Elasticsearch 2 as well as Elasticsearch 5 * [Shield/X-Pack Security](https://www.elastic.co/products/shield) - Commercial plugin from elastic  ## elasticsearch-security-plugin (Unmaintained) ### This plugin is to be considered as insecure, do not use it because its unmaintained ![Unmaintained](http://upload.wikimedia.org/wikipedia/en/thumb/5/57/Circle-style-warning.svg/200px-Circle-style-warning.svg.png)  Documentation removed.  <h3>License</h3> Copyright 2013-2014 Hendrik Saly  Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions)  on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or  conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the  appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.  Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless  required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to  You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of  this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage,  computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the  possibility of such damages.  Licensed under the "No License" license (github default):  http://choosealicense.com/licenses/no-license/
sirensolutions/siren-join	:warning: This project is unsupported, Siren technology for real time relational joins in Elasticsearch (also for 5.x)  is now available as part of the Siren Platform (link to http://siren.io).  # SIREn Join Plugin for Elasticsearch  This plugin extends Elasticsearch with new search actions and a filter query parser that enables to perform a "Filter Join" between two set of documents (in the same index or in different indexes).  The Filter Join is basically a (left) semi-join between two set of documents based on a common attribute, where the result only contains the attributes of one of the joined set of documents. This join is used to filter one document set based on a second document set, hence its name. It is equivalent to the `EXISTS()` operator in SQL.  ## Compatibility  The following table shows the compatibility between releases of Elasticsearch and the SIREn Join plugin:  Elasticsearch|SIREn Join ---|--- 2.4.5|2.4.5 2.4.4|2.4.4 2.4.3|2.4.3 2.4.2|2.4.2-1 2.4.1|2.4.1-1 2.3.5|2.3.5-1 2.3.4|2.3.4-1 2.3.3|2.3.3-1 2.2.0|2.2.0-1 2.1.2|2.1.2 2.1.1|2.1.1 1.7.x|1.0  ## Installing the Plugin  ### Online Download  You can use the following command to download the plugin from the online repository:      $ bin/plugin install solutions.siren/siren-join/2.4.4  ### Offline Download  - Get the ZIPball from [maven.org](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22solutions.siren%22%20AND%20a%3A%22siren-join%22) - Install with the downloaded file      $ bin/plugin install file:/path/to/folder/with/siren-join-2.4.4.zip  ### Manual  Alternatively, you can assemble it via Maven (you must build it as a *non-root* user):  ``` $ git clone git@github.com:sirensolutions/siren-join.git $ cd siren-join $ mvn package ```  This creates a single Zip file that can be installed using the Elasticsearch plugin command:      $ bin/plugin install file:/PATH-TO-SIRENJOIN-PROJECT/target/releases/siren-join-2.4.4.zip  ### Interacting with the Plugin  You can now start Elasticsearch and see that our plugin gets loaded:      $ bin/elasticsearch     ...     [2013-09-04 17:33:27,443][INFO ][node    ] [Andrew Chord] initializing ...     [2013-09-04 17:33:27,455][INFO ][plugins ] [Andrew Chord] loaded [siren-join], sites []     ...  To uninstall the plugin:      $ bin/plugin remove siren-join  ## Usage  ### Coordinate Search API  This plugin introduces two new search actions, `_coordinate_search` that replaces the `_search` action,  and `_coordinate_msearch` that replaces the `_msearch` action. Both actions are wrappers around the original elasticsearch actions and therefore supports the same API. One must use these actions with the `filterjoin` filter, as the `filterjoin` filter is not supported by the original elaticsearch actions.   ### Parameters  * `filterjoin`: the filter name * `indices`:  the index names to lookup the terms from (optional, default to all indices). * `types`: the index types to lookup the terms from (optional, default to all types). * `path`: the path within the document to lookup the terms from. * `query`: the query used to lookup terms with. * `orderBy`: the ordering to use to lookup the maximum number of terms: default, doc_score (optional, default to default ordering). * `maxTermsPerShard`: the maximum number of terms per shard to lookup (optional, default to all terms). * `termsEncoding`: the encoding to use when transferring terms across the network: long, integer, bloom, bytes (optional, default to long).  ### Example  In this example, we will join all the documents from `index1` with the documents of `index2`.  The query first filters documents from `index2` and of type `type` with the query  `{ "terms" : { "tag" : [ "aaa" ] } }`. It then retrieves the ids of the documents from the field `id`  specified by the parameter `path`. The list of ids is then used as filter and applied on the field   `foreign_key` of the documents from `index1`.  ```json     {       "bool" : {         "filter" : {           "filterjoin" : {             "foreign_key" : {               "indices" : ["index2"],               "types" : ["type"],               "path" : "id",               "query" : {                 "terms" : {                   "tag" : [ "aaa" ]                 }               }             }           }         }       }     } ```  ### Response Format  The response returned by the coordinate search API is identical to the response returned by Elasticsearch's search API, but augmented with additional information about the execution of the relational query planning. This additional information is stored within the field named `coordinate_search` at the root of the response, see example below. The object contains the following parameters:  * `actions`: a list of actions that has been executed - an action represents the execution of one single join. * `relations`: the definition of the relations of the join - it contains two nested objects, `from` and `to`, one for each relation. * `size`: the size of the filter used to compute the join, i.e., the number of terms across all shards used by the filterjoin. * `size_in_bytes`: the size in bytes of the filter used to compute the join. * `is_pruned`: a flag to indicate if the join computation has been pruned based on the `maxTermsPerShard` limit. * `cache_hit`: a flag to indicate if the join was already computed and cached. * `terms_encoding`: the terms encoding used to transfer terms across the network. * `took`: the time it took to construct the filter.  ```json     {       "coordinate_search": {         "actions": [           {             "relations": {               "from": {                 "indices": ["index2"],                 "types": ["type"],                 "field": "id"               },               "to": {                 "indices": null,                 "types": null,                 "field": "foreign_key"               }             },             "size": 2,             "size_in_bytes": 20,             "is_pruned": false,             "cache_hit": false,             "terms_encoding" : "long",             "took": 313           }         ]       },     ...     } ```  ## Performance Considerations  * We recommend to activate caching for all queries via the setting `index.queries.cache.everything: true`. The new caching policy of Elasticsearch will not cache a `filterjoin` query on small segments which can lead to a significant drop of performance. See issue [16529](https://github.com/elastic/elasticsearch/issues/16259) for more information. * Joining numeric attributes is more efficient than joining string attributes. * The bloom filter is the most efficient and the default encoding method for terms. It can encode 40M unique values in ~30MB. However, this trades precision for space, i.e., the bloom filter can lead to false-positive results. If precision is critical, then it is recommended to switch to the terms encoding to long. * If the joined attributes of your documents contain incremental integers, switch the terms encoding to integer. * The `filterjoin` includes a circuit breaker to prevent OOME when joining a field with a large number of unique values. As a rule of thumb, the maximum amount of unique values transferred across the shards should be around 50 to 100M when using bloom encoding, 5 to 10M when using long or integer encoding. It is recommended to configure a `maxTermsPerShard` limit if the attribute defined by the `path` parameter contains a larger number of values. * The `bytes` terms encoding will likely provide better performance for highly selective queries over large indices, as it will perform the filtering based on a dictionary lookup instead of a doc value scan.  ## Acknowledgement  Part of this plugin is inspired and based on the pull request [3278](https://github.com/elastic/elasticsearch/pull/3278) submitted by [Matt Weber](https://github.com/mattweber) to the [Elasticsearch](https://github.com/elastic/elasticsearch) project.  - - -  Copyright (c) 2016, SIREn Solutions. All Rights Reserved.
reachkrishnaraj/kafka-elasticsearch-standalone-consumer	# Welcome to the kafka-elasticsearch-standalone-consumer wiki!  ## Architecture of the kafka-elasticsearch-standalone-consumer [indexer]  ![](https://raw.githubusercontent.com/ppine7/kafka-elasticsearch-standalone-consumer/master/img/IndexerV2Design.jpg)   # This project has moved to below repository ### Please see https://github.com/BigDataDevs/kafka-elasticsearch-consumer  # Introduction  ### **Kafka Standalone Consumer [Indexer] will read messages from Kafka, in batches, process and bulk-index them into ElasticSearch.**  ### _As described in the illustration above, here is how the indexer works:_  * Kafka has a topic named, say `Topic1`  * Lets say, `Topic1` has 5 partitions.  * In the configuration file, kafka-es-indexer.properties, set firstPartition=0 and lastPartition=4 properties   * start the indexer application as described below   * there will be 5 threads started, one for each consumer from each of the partitions  * when a new partition is added to the kafka topic - configuration has to be updated and the indexer application has to be restarted   # How to use ?   ### Running as a standard Jar   **1. Download the code into a `$INDEXER_HOME` dir.  **2. cp `$INDEXER_HOME`/src/main/resources/kafka-es-indexer.properties.template /your/absolute/path/kafka-es-indexer.properties file - update all relevant properties as explained in the comments  **3. cp `$INDEXER_HOME`/src/main/resources/logback.xml.template /your/absolute/path/logback.xml   specify directory you want to store logs in: 	<property name="LOG_DIR" value="/tmp"/> 	  adjust values of max sizes and number of log files as needed  **4. build/create the app jar (make sure you have MAven installed):  		cd $INDEXER_HOME      	mvn clean package      	  The kafka-es-indexer-2.0.jar will be created in the $INDEXER_HOME/bin.  All dependencies will be placed into $INDEXER_HOME/bin/lib.  All JAR dependencies are linked via kafka-es-indexer-2.0.jar manifest.  **5. edit your $INDEXER_HOME/run_indexer.sh script: 		-- make it executable if needed (chmod a+x $INDEXER_HOME/run_indexer.sh) 		-- update properties marked with "CHANGE FOR YOUR ENV" comments - according to your environment  **6. run the app [use JDK1.8] :    		./run_indexer.sh   # Versions  ### Kafka Version: 0.8.2.1  ### ElasticSearch: > 1.5.1  ### Scala Version for Kafka Build: 2.10.0  # Configuration  Indexer app configuration is specified in the kafka_es_indexer.properties file, which should be created from a provided template, kafka-es-indexer.properties.template. All properties are described in the template:  [kafka-es-indexer.properties.template](https://github.com/ppine7/kafka-elasticsearch-standalone-consumer/blob/master/src/main/resources/kafka-es-indexer.properties.template)  Logging properties are specified in the logback.xml file, which should be created from a provided template, logback.xml.template:   [logback.xml.template](https://github.com/ppine7/kafka-elasticsearch-standalone-consumer/blob/master/src/main/resources/logback.xml.template)   # Message Handler Class  *  `org.elasticsearch.kafka.consumer.MessageHandler` is an Abstract class that has most of the functionality of reading data from Kafka and batch-indexing into ElasticSearch already implemented. It has one abstract method, `transformMessage()`, that can be overwritten in the concrete sub-classes to customize message transformation before posting into ES  * `org.elasticsearch.kafka.consumer.messageHandlers.RawMessageStringHandler` is a simple concrete sub-class of the MessageHAndler that sends messages into ES with no additional transformation, as is, in the 'UTF-8' format  * Usually, its effective to Index the message in JSON format in ElasticSearch. This can be done using a Mapper Class and transforming the message from Kafka by overriding/implementing the `transformMessage()` method. An example can be found here: `org.elasticsearch.kafka.consumer.messageHandlers.AccessLogMessageHandler`  * _**Do remember to set the newly created message handler class in the `messageHandlerClass` property in the kafka-es-indexer.properties file.**_  # IndexHandler Interface and basic implementation  *  `org.elasticsearch.kafka.consumer.IndexHandler` is an interface that defines two methods: getIndexName(params) and getIndexType(params).   * `org.elasticsearch.kafka.consumer.BasicIndexHandler` is a simple imlementation of this interface that returnes indexName and indexType values as configured in the kafkaESConsumer.properties file.   * one might want to create a custom implementation of IndexHandler if, for example, index name and type are not static for all incoming messages but depend on the event data - for example customerId, orderId, etc. In that case, pass all info that is required to perform that custom index determination logic as a Map of parameters into the getIndexName(params) and getIndexType(params) methods (or pass NULL if no such data is required)  * _**Do remember to set the index handler class in the `indexHandlerClass` property in the kafka-es-indexer.properties file. By default, BasicIndexHandler is used**_  # License  kafka-elasticsearch-standalone-consumer  	Licensed under the Apache License, Version 2.0 (the "License"); you may 	not use this file except in compliance with the License. You may obtain 	a copy of the License at  	     http://www.apache.org/licenses/LICENSE-2.0  	Unless required by applicable law or agreed to in writing, 	software distributed under the License is distributed on an 	"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY 	KIND, either express or implied.  See the License for the 	specific language governing permissions and limitations 	under the License.  # Contributors  - [Krishna Raj](https://github.com/reachkrishnaraj)  - [Marina Popova](https://github.com/ppine7)  - [Dhyan ](https://github.com/dhyan-yottaa)
confluentinc/kafka-connect-elasticsearch	# Kafka Connect Elasticsearch Connector  kafka-connect-elasticsearch is a [Kafka Connector](http://kafka.apache.org/documentation.html#connect) for copying data between Kafka and Elasticsearch.  # Development  To build a development version you'll need a recent version of Kafka. You can build kafka-connect-elasticsearch with Maven using the standard lifecycle phases.   # Contribute  - Source Code: https://github.com/confluentinc/kafka-connect-elasticsearch - Issue Tracker: https://github.com/confluentinc/kafka-connect-elasticsearch/issues   # License  The project is licensed under the Apache 2 license.
dashbuilder/dashbuilder	Dashbuilder ===========  Dashbuilder is a general purpose dashboard and reporting web app which allows for:  * Visual configuration and personalization of dashboards * Support for different types of visualizations using several charting libraries * Full featured editor for the definition of chart visualizations * Definition of interactive report tables * Data extraction from external systems, through different protocols * Support for both analytics and real-time dashboards  Licensed under the Apache License, Version 2.0  For further information, please visit the project web site <a href="http://dashbuilder.org" target="_blank">dashbuilder.org</a>  Upcoming features =================  * New renderers based on D3 JS, Lienzo GWT & Chart JS * Hierarchical (nested group) displayer types: Tree & Pie * Support for multiple dynamic data series * Rich mobility support * Alerts and SLA configuration * RESTful API  Architecture =================  * Not tied to any chart rendering technology. Pluggable renderers. * No tied to any data storage. * Ability to read data from: CSV files, Databases, Elastic Search orJava generators. * Decoupled client & server layers. Ability to build pure lightweight client dashboards. * Ability to push & handle data sets on client for better performance. * Based on <a href="http://www.uberfireframework.org" target="_blank">Uberfire</a>, a framework for building rich workbench styled apps on the web.  Change log ==========  0.6.0.Final  * New perspective for the authoring of new application pages and configuring the navigation menus.  (Content Management Perspective => [https://issues.jboss.org/browse/DASHBUILDE-166](https://issues.jboss.org/browse/DASHBUILDE-166))  0.4.0.Final  * User interface upgraded to Bootstrap3/PatternFly  * MonetDB support added to the SQL provider.   (the provider has been tested under the MonetDB 11.21.5 release)  * MariaDB support added to the SQL provider.   (the provider has been tested under the MariaDB 10.1.10 release)  * New data set filtering functions "IN" and "NOT IN"   * Data Set Core API available as an embeddable java library   (further details [here](https://github.com/dgutierr/datasets-sample-project))   0.3.0.Final  * New provider for the definition of data sets stored into SQL databases.   The following is the list of tested and supported DBs:    - MySQL 5.5   - Postgres 9.2   - H2 1.3.168+   - Oracle 12c and 11gR2   - IBM DB2 9.7 and 10.5   - Sybase ASE 15.7  * New provider for the retrieval of data stored into Elastic Search nodes.   The provider has been tested under Elastic Search 1.7.1 release.  * New data set editor UI module:     - Creation of SQL, Bean, CSV and Elastic Search data set definitions     - Data set retrieval testing and preview     - Filter, sort and export the data previews  * New displayer for showing single value metrics.  * Added new displayer subtypes: bar (stacked), pie (3d, donut), line (smooth)  * Support for real-time dashboards. Displayer refresh settings.  * Displayer editor data set lookup enhancements:     - Filter editor for retrieving only a data subset.     - Time frame function for the retrieval of time series data in real-time.     - Different strategies for grouping time series data.     - Ability to add/remove the columns/series to display.  0.2.0.Final  * Data set definition files: Support for CSV & Bean generated data sets * Displayer Editor widget for the creation of displayer definitions * Perspective editor integration which allows the creation of dashboards by drag&drop  0.1.1.Final  * Notify clients about data set registration/removal events * Assign an HTML identifier to every Displayer instance (useful for testing purposes)  0.1.0.Final  Main goal of this very first release is to make it possible the creation of composite dashboards using an straightforward API. Feature set:  * Shared API for defining and registering data sets * Shared operation engine for executing filter, group & sort operations over a data set * Client API & widgets for defining Displayer instances * Uberfire wrapper screen for the Displayer widget * Showcase App. providing a built-in displayer gallery plus some dashboard samples * Default renderer based on the Google Visualization library * Additional table renderer based on the Uberfire PagedTable widget * Tomcat 7 and JBoss AS 7 distributions provided  Build & run ===========  Prerequisites ------------- * Git client * Maven 3.2.5+ * Java 1.6+  First steps -----------  Clone the project      git clone git@github.com:dashbuilder/dashbuilder.git  Now you can build & run the project in development or production mode.  Development mode ----------------  Development mode allows a user to develop with the framework by compiling classes and client assets on runtime, which decreases the development time. There are more implications such as browser compatibilities, language support, etc. It's useful for developing and testing the application.  Dashbuilder is currently built using GWT 2.7, so you are forced to use [SuperDevMode](http://www.gwtproject.org/articles/superdevmode.html) to run the application.  Super development mode is the new way to work in GWT since version <code>2.5</code> (Native support & the default mode in GWT <code>2.7</code>). It works in most new browsers and it's based on [Source Map](https://docs.google.com/document/d/1U1RGAehQwRypUTovF1KRlpiOFze0b-_2gc6fAH0KY0k/edit?hl=en_US&pli=1&pli=1) spec. It's faster and more efficient than the old hosted mode. There are lots of benefits and other important reasons to use it, you can find more information [here](http://www.gwtproject.org/articles/superdevmode.html).  Dashbuilder supports and it's configured by default to use SuperDevMode.  Using it means running two servers, one for the web application and one for the Code Server that compiles classes for SDM when the compile button is pushed on the web page or in the bookmark.  To build the application:      cd dashbuilder     mvn clean install -DskipTests  To run it:      cd dashbuilder-webapp     mvn gwt:run  Login:      admin / admin   (If you are an IntelliJ fan, we also provide a setup for running the application under this fantastic IDE. Details [here](https://groups.google.com/forum/#!topic/dashbuilder-development/tRa6AAMb8fM))  Production mode ---------------  Production mode is used to build & package the application for a production environment. The application is compiled and the javascript assets are build using all permutations (browser support), all languages, etc.  In order to build the production mode:      cd dashbuilder     mvn clean install -DskipTests -Dfull  Once build is finished, you'll find the WAR distributions for Wildfly and Tomcat into <code>dashbuilder/dashbuilder-distros/target/</code>.  Just deploy the WAR file into your application server!
javanna/elasticshell	elasticshell - a shell for elasticsearch ==============================  The elasticshell is a javascript shell written in Java. It allows to interact with a running [elasticsearch](http://www.elasticsearch.org) cluster using the [Java API](http://www.elasticsearch.org/guide/reference/java-api/).  The elasticshell was a very nice experiment in my early days playing around with elasticsearch. It turned out to be hard to maintain and keep in sync with the elasticsearch codebase though. Also, I got to the conclusion that a shell is not that beneficial to a system that already has extremely user-friendly REST api. Furthermore, the addition of the [cat api](http://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html) to elasticsearch filled the gap when it comes to providing text output rather than json. This is why I haven't been maintaining the project for a long time.  Getting Started ==============================  Versions ------------------------------  The elasticshell version is tightly coupled with the elasticsearch version since it uses its Java API to connect to it. There currently are three active development branches: one for 0.19.x, one for 0.20.x and one for 0.90.x.  <table> 	<thead> 		<tr> 			<td>elasticshell</td> 			<td>elasticsearch</td> 		</tr> 	</thead> 	<tbody> 	    <tr>             <td>master</td>             <td>0.90.0.RC2</td>         </tr>         <tr>             <td><a href="http://dl.bintray.com/content/javanna/elasticsearch-tools/release/org/elasticsearch/elasticshell/0.90.0.RC2/elasticshell-0.90.0.RC2.zip?direct">0.90.0.RC2</a></td>             <td>0.90.0.RC2</td>         </tr> 		<tr>             <td><a href="http://dl.bintray.com/content/javanna/elasticsearch-tools/release/org/elasticsearch/elasticshell/0.20.6-RC/elasticshell-0.20.6-RC.zip?direct">0.20.6-RC</a></td>             <td>0.20.x</td>         </tr> 		<tr>             <td><a href="http://dl.bintray.com/content/javanna/elasticsearch-tools/release/org/elasticsearch/elasticshell/0.19.12-RC/elasticshell-0.19.12-RC.zip?direct">0.19.12-RC</a></td>         	<td>0.19.x</td>         </tr> 	</tbody> </table>  Installation ------------------------------  * Download and unzip the elasticshell distribution * Run `bin/elasticshell` on unix, or `bin/elasticshell.bat` on windows  Help ------------------------------  Use the help() command to have a look at the elasticshell help. Every command is exposed as a javascript function. If you want to get help for a specific command, just type its name without the curly brackets. The help output is currently available only for a few available commands, some more documentation will be added soon.  Auto-suggestions ------------------------------  Have a look at the auto-suggestions through the tab key to see the available commands and variables. JSON is native within the elasticshell, thus auto-suggestions are available within JSON objects too.  Connecting to a cluster ------------------------------  The elasticshell will automatically try to create a new transport client connected to a node running on localhost:9300. That default transport client will be registered with the es variable name, same result as the following command:  `var es = transportClient('localhost:9300');`   You can manually connect to a running elasticsearch cluster using the following commands: `var es = transportClient('hostname:port')` creates a new [transport client](http://www.elasticsearch.org/guide/reference/java-api/client.html). You can provide a list of addresses too.  `var es = nodeClient('clusterName')` creates a new [node client](http://www.elasticsearch.org/guide/reference/java-api/client.html).  Let's index a document ------------------------------  ```javascript var jsonDoc = {    "user": "kimchy",    "postDate": "2009-11-15T13:12:00",    "message": "Trying out Elastic Search, so far so good?" } ```  `es.index('twitter','tweet','1', jsonDoc);`  We can also use the available index builder, which allows to use all the options available when [indexing a document](http://www.elasticsearch.org/guide/reference/api/index_.html):  `es.indexBuilder().index('twitter').type('tweet').id('1').source(jsonDoc).execute();`  Interact with a specific index or type ------------------------------  You can easily execute operations on a specific index or type like this:  `es.<index>.<type>.search();`  If the elasticshell does not accept the name of the index or type, for instance if the name contains a space or starts with a number, you can use the following alternate syntax:  `es['index name'].search();`   Let's retrieve a document ------------------------------  `es.twitter.tweet.get('1');`  The above command retrieves the previously indexed document using the [get API](http://www.elasticsearch.org/guide/reference/api/get.html).  Let's [search](http://www.elasticsearch.org/guide/reference/api/search/) ------------------------------  ```javascript var termQuery = {     "query" : {         "term" : { "user": "kimchy" }     } } es.search(termQuery); ```  We can also use the search builder: `es.searchBuilder().query(termQuery.query).execute();`  We can also make use of the elasticsearch [query builders](http://www.elasticsearch.org/guide/reference/java-api/query-dsl-queries.html) like this:  `es.searchBuilder().queryBuilder(QueryBuilders.termQuery('user','kimchy')).execute();`  Let's add a facet to the previous query ------------------------------  ```javascript es.searchBuilder()     .query(QueryBuilders.termQuery('user','kimchy'))     .facet(FacetBuilders.termsFacet('user').field('user')).execute(); ```  All the elasticsearch API are exposed through the elasticshell. Remember that the elasticshell is a javascript shell, thus you can have fun with javascript code. On the other hand, the elasticshell has been built on top of the Rhino engine, which means that you can execute Java code too.  Contribute =======  You can easily fork the project in order to contribute to it and send your pull requests. Due to limitations on all IDEs console, it's recommended to test your changes from a real command line. The project uses in fact the great [JLine](https://github.com/jline/jline2) which needs to execute a bit of native code to provide nice auto-suggestions and so on. You can easily run the elasticshell from the command line through maven using the following command which compiles the project and run its main class:  ```mvn compile exec:java```  The above command has the same result as executing the elasticshell from the normal distribution using the executable provided within the bin folder.  License =======  ``` This software is licensed under the Apache 2 license, quoted below.  Copyright 2015 Luca Cavanna  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```
avthart/spring-boot-axon-sample	Introduction ============ This is a sample application to demonstrate Spring Boot, ElasticSearch and Axon Framework.  The Todo application makes use of the following design patterns: - Domain Driven Design - CQRS - Event Sourcing - Task based User Interface  Building ======== > mvn package  Running ======= > mvn spring-boot:run  Browse to http://localhost:8080/index.html  Implementation ============== Implementation notes: - The event store is backed by a filesystem implementation which comes with Axon - The query model is backed by a local ElasticSearch node (running in the same JVM) using Spring Data ElasticSearch - The user interface is updated asynchronously via stompjs over websockets using Spring Websockets support  Documentation ============= * Axon Framework - http://www.axonframework.org/ * Spring Boot - http://projects.spring.io/spring-boot/ * Spring Framework - http://projects.spring.io/spring-framework/ * Spring Data ElasticSearch - https://github.com/spring-projects/spring-data-elasticsearch
salyh/elasticsearch-imap	elasticsearch-importer-imap  Elasticsearch 2.x ==============================================  Support Elasticsearch 5.0 readyness and keep elasticsearch imap importer free. Currently IMAP importer is only working with Elasticsearch 2 and it costs a lot of time and effort to update and maintain it for Elasticsearch 5. Donations welcome!  Pledgie:<br /> <a href='https://pledgie.com/campaigns/32955'><img alt='Click here to lend your support to: Elasticsearch IMAP Importer and make a donation at pledgie.com !' src='https://pledgie.com/campaigns/32955.png?skin_name=chrome' border='0' ></a>  Paypal:<br /> [![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=UKSFLRDLYGYZG)  Patreon:<br /> https://patreon.com/salyh  <hr/>  Import e-mails from IMAP (and POP3) into Elasticsearch 2.x  <a href="mailto:hendrikdev22@gmail.com">E-Mail hendrikdev22@gmail.com</a><p> <a href="https://twitter.com/hendrikdev22">Twitter @hendrikdev22</a>  This importer connects to IMAP4 or POP3 servers, poll your mail and index it. The emails on the server will be never modified or removed from the server. The importer tracks (after the first initial full load) which mails are new or deleted and then only update the index for this mails.  Features:  * Incremental indexing of e-mails from a IMAP or POP3 server * Support indexing of attachments * Support for UTF-7 encoded e-mails (through jutf7) * SSL, STARTTLS and SASL are supported (through JavaMail API) * IMAP only: Folders which should be indexed can be specified with a regex pattern * IMAP only: Subfolders can also be indexed (whole traversal of all folders) * No special server capabilities needed * Bulk indexing * Works also with Gmail, iCloud, Yahoo, etc.   The importer acts as a disconnected client. This means that the importer is polling and for every indexing run a new server connection is opened and, after work is done, closed.  <h3>Installation</h3>  Prerequisites:  * Java 7 or 8 * Elasticsearch 2.x * At least one IMAP4 or POP3 server to connect to  Download .zip or .tar.gz from https://github.com/salyh/elasticsearch-river-imap/releases/latest (only Version 0.8.6 or higher) and unpack them somewhere.  Then run  * ``bin/importer.sh [-e] <config-file>``    * -e: Start embedded elasticsearch node (only for testing !!)    * config-file: path to the json configuration file (see next chapter)  <h3>Configuration</h3> Put the following configuration in a file and store them somewhere with a extension of .json <pre> {    "mail.store.protocol":"imap",    "mail.imap.host":"imap.server.com",    "mail.imap.port":993,    "mail.imap.ssl.enable":true,    "mail.imap.connectionpoolsize":"3",    "mail.debug":"false",    "mail.imap.timeout":10000,    "users":["user@domain.com"],    "passwords":["secret"],    "schedule":null,    "interval":"60s",    "threads":5,    "folderpattern":null,    "bulk_size":100,    "max_bulk_requests":"30",    "bulk_flush_interval":"5s",    "mail_index_name":"imapriverdata",    "mail_type_name":"mail",    "with_striptags_from_textcontent":true,    "with_attachments":false,    "with_text_content":true,    "with_flag_sync":true,    "keep_expunged_messages":false,    "index_settings" : null,    "type_mapping" : null,    "user_source" : null,    "ldap_url" : null,    "ldap_user" : null,    "ldap_password" : null,    "ldap_base" : null,    "ldap_name_field" : "uid",    "ldap_password_field" : null,    "ldap_refresh_interval" : null,    "master_user" : null,    "master_password" : null,        "client.transport.ignore_cluster_name": false,    "client.transport.ping_timeout": "5s",    "client.transport.nodes_sampler_interval": "5s",    "client.transport.sniff": true,    "cluster.name": "elasticsearch",    "elasticsearch.hosts": "localhost:9300,127.0.0.1:9300"     }</pre>  * ``mail.*`` - see JAVAMail documentation https://javamail.java.net/nonav/docs/api/  (default: none) * ``user`` - user name for server login (default: ``null``) - deprecated, use ``users`` * ``password`` - password for server login (default: ``null``) - deprecated, use ``passwords`` * ``users`` - array of users name for server login (default: ``null``) * ``passwords`` - array of passwords for server login (default: ``null``) * ``schedule`` - a cron expression like ``0/3 0-59 0-23 ? * *`` (default: ``null``) * ``interval`` - if no ``schedule`` is set then this is will be the indexing interval (default: ``60s``) * ``threads`` - How many thready for parallel indexing (must be 1 or higher) (default: ``5``) * ``folderpattern`` - IMAP only: regular expressions which folders should be indexed (default: ``null``) * ``bulk_size`` - the length of each bulk index request submitted (default: ``100``) * ``max_bulk_requests`` - the maximum number of concurrent bulk requests (default: ``30``) * ``bulk_flush_interval`` - the time period the bulk processor is flushing outstanding documents (default: ``5s``) * ``mail_index_name`` - name of the index which holds the mail (default: ``imapriverdata``) * ``mail_index_name_strategy`` - how the indexname should be composed for each user (default: ``all_in_one``)    * ``all_in_one`` - Put all mails from all users, the index name is ``mail_index_name``    * ``username`` - Put mail from each user in a index with the users username    * ``username_crop`` - Put mail from each user in a index with the users username but crop at the @ sign    * ``prefixed_username`` - Put mail from each user in a index with the users username prefixed by ``mail_index_name``    * ``prefixed_username_crop`` - Put mail from each user in a index with the users username prefixed by ``mail_index_name`` but crop at the @ sign * ``mail_type_name`` - name of the type (default: ``mail``) * ``with_striptags_from_textcontent`` - if ``true`` then html/xml tags are stripped from text content (default: ``true``) * ``with_attachments`` - if ``true`` then attachments will be indexed (default: ``false``) * ``with_text_content`` - if ``true`` then the text content of the mail is indexed (default: ``true``) * ``with_flag_sync`` - IMAP only: if ``true`` then message flag changes will be detected and indexed. Maybe slow for very huge mailboxes. (default: ``true``) * ``keep_expunged_messages`` - if ``true`` then message which are expunged/deleted on the server will be kept in elasticsearch. (default: ``false``) * ``index_settings`` - optional settings for the Elasticsearch index * ``type_mapping`` - optional mapping for the Elasticsearch index type * ``headers_to_fields`` - array with e-mail header names to include as proper fields. To create a legal field name, the header name is prefixed with ``header_``, lowercased and has all non-alphanumeric characters replaced with ``_``. For example, an input of ``["Message-ID"]`` will copy that header into a field with name ``header_message_id``. * ``user_source`` - If "ldap" then query user and passwords from an ldap directory, if null or missing use users/passwords from the config file  (default: null)    * ``ldap_url`` - LDAP host and port (default: null), example: "ldap://ldaphostname:389"    * ``ldap_user`` - Ldap user which is used to query IMAP users (default: null), example: "cn=Directory Manager"    * ``ldap_password`` - Ldap password for ``ldap_user`` (default: null)    * ``ldap_base`` - Base Dn where to search for users, (default: null), example: "ou=users,ou=accounts,dc=company,dc=org"    * ``ldap_name_field`` - The fieldname (attribute) where the IMAP username is stored (default "dn"), example: "uid"    * ``ldap_password_field`` - The fieldname (attribute) of the IMAP password (default: "userPassword")    * ``ldap_refresh_interval`` - Refresh interval in minutes (default:"60"), set to "0" to disable automatic refreshing. If enabled this will automatically refresh the users/passwords from ldap every n minutes.    * ``master_user`` - For Dovecot, a master user account can be supplied who can access all users' mailboxes, even if their passwords are encrypted (default: null)    * ``master_password`` -  master user password (default: null) * ``client.transport.*`` see https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html * ``cluster.name`` see https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html * ``elasticsearch.hosts`` Comma separated list of elasticsearch nodes/servers (mandatory)   Note: For POP3 only the "INBOX" folder is supported. This is a limitation of the POP3 protocol.  <h3>Default Mapping Example</h3> ```json "mail" : {         "properties" : {           "attachmentCount" : {             "type" : "long"           },           "bcc" : {             "properties" : {               "email" : {                 "type" : "string"               },               "personal" : {                 "type" : "string"               }             }           },           "cc" : {             "properties" : {               "email" : {                 "type" : "string"               },               "personal" : {                 "type" : "string"               }             }           },           "contentType" : {             "type" : "string"           },           "flaghashcode" : {             "type" : "integer"           },           "flags" : {             "type" : "string"           },           "folderFullName" : {             "type" : "string",             "index" : "not_analyzed"           },           "folderUri" : {             "type" : "string"           },           "from" : {             "properties" : {               "email" : {                 "type" : "string"               },               "personal" : {                 "type" : "string"               }             }           },           "headers" : {             "properties" : {               "name" : {                 "type" : "string"               },               "value" : {                 "type" : "string"               }             }           },           "mailboxType" : {             "type" : "string"           },           "receivedDate" : {             "type" : "date",             "format" : "basic_date_time"           },           "sentDate" : {             "type" : "date",             "format" : "basic_date_time"           },           "size" : {             "type" : "long"           },           "subject" : {             "type" : "string"           },           "textContent" : {             "type" : "string"           },           "to" : {             "properties" : {               "email" : {                 "type" : "string"               },               "personal" : {                 "type" : "string"               }             }           },           "uid" : {             "type" : "long"           }         }       }     } ```  For advanced mapping ideas look here: * https://github.com/salyh/elasticsearch-river-imap/issues/4 * https://github.com/salyh/elasticsearch-river-imap/issues/13  <h3>Advanced Mapping Example (to be set manually using "type_mapping")</h3> ```json {    "mail":{       "properties":{          "textContent":{             "type":"langdetect"          },          "email":{             "type":"string",             "index":"not_analyzed"          },          "subject":{             "type":"multi_field",             "fields":{                "text":{                   "type":"string"                },                "raw":{                   "type":"string",                   "index":"not_analyzed"                }             }          },          "personal":{             "type":"multi_field",             "fields":{                "title":{                   "type":"string"                },                "raw":{                   "type":"string",                   "index":"not_analyzed"                }             }          }       }    } }  ```  <h3>Content Example</h3> ```json {       "_index" : "imapriverdata",       "_type" : "mail",       "_id" : "50220::imap://test%40xxx.com@imap.strato.de/import",       "_score" : 1.0, "_source" : {   "attachmentCount" : 0,   "attachments" : null,   "bcc" : null,   "cc" : null,   "contentType" : "text/plain; charset=ISO-8859-15",   "flaghashcode" : 16,   "flags" : [ "Recent" ],   "folderFullName" : "test",   "folderUri" : "imap://test%40xxx.com@imap.strato.de/import",   "from" : {     "email" : "suchagent@isrch.de",     "personal" : null   },   "headers" : [ {     "name" : "Subject",     "value" : "Suchagent Wohnung mieten in Berlin -  1 neues Objekt gefunden!"   }, {     "name" : "Return-Path",     "value" : "<suchagent@isrch.de>"   }, {     "name" : "Content-Transfer-Encoding",     "value" : "quoted-printable"   }, {     "name" : "To",     "value" : "sss@ddd.org"   }, {     "name" : "X-OfflineIMAP-1722382714-52656d6f7465-6165727a7465",     "value" : "1248516496-0146849121575-v5.99.4"   }, {     "name" : "Message-ID",     "value" : "<8277550.1132283844462.JavaMail.noreply@isrch.de>"   }, {     "name" : "Mime-Version",     "value" : "1.0"   }, {     "name" : "X-Gmail-Labels",     "value" : "ablage,hendrik.yyy@gmx.de"   }, {     "name" : "X-GM-THRID",     "value" : "1309162987234255956"   }, {     "name" : "Delivered-To",     "value" : "GMX delivery to sss@ddd.org"   }, {     "name" : "Reply-To",     "value" : "suchagent@isrch.de"   }, {     "name" : "Date",     "value" : "Fri, 18 Nov 2005 04:17:24 +0100 (MET)"   }, {     "name" : "Auto-Submitted",     "value" : "auto-generated"   }, {     "name" : "Received",     "value" : "(qmail invoked by alias); 18 Nov 2005 03:17:25 -0000"   }, {     "name" : "Content-Type",     "value" : "text/plain; charset=\"ISO-8859-15\""   }, {     "name" : "From",     "value" : "suchagent@isrch.de"   } ],   "mailboxType" : "IMAP",   "popId" : null,   "receivedDate" : 1132283845000,   "sentDate" : 1132283844000,   "size" : 3645,   "subject" : "Suchagent Wohnung mieten in Berlin -  1 neues Objekt gefunden!",   "textContent" : "Sehr geehrter Nutzer, ... JETZT AUCH IM FERNSEHEN: IMMOBILIENANGEBOTE FÜR HAMBURG UND UMGEBUNG!\r\n\tFinden Sie Ihre Wunschwohnung oder  ..."   "to" : [ {     "email" : "sss@ddd.org",     "personal" : null   } ],   "uid" : 50220 }     }  ```  <h3>Indexing attachments</h3>  If you want also indexing your mail attachments look here: * https://github.com/salyh/elasticsearch-river-imap/issues/10#issuecomment-50125929 * https://github.com/salyh/elasticsearch-river-imap/issues/13 * http://tinyurl.com/nbujv7h * https://github.com/salyh/elasticsearch-river-imap/blob/master/src/test/java/de/saly/elasticsearch/imap/AttachmentMapperTest.java  <h3>Contributers/Credits</h3>  * Hans Jørgen Hoel (https://github.com/hansjorg) * Stefan Thies (https://github.com/megastef) * René Peinl (University Hof)   <h3>License</h3>  Copyright (C) 2014-2015 by Hendrik Saly (http://saly.de) and others.  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
ncolomer/elasticsearch-osmosis-plugin	| Take a look at the brand new <a href="https://github.com/ncolomer/discovery">discovery</a> site-plugin to visualize and explore your freshly indexed OpenStreetMap data! | | :-: |  # elasticsearch-osmosis-plugin  elasticsearch-osmosis-plugin is an [Osmosis](http://wiki.openstreetmap.org/wiki/Osmosis) plugin that index [OpenStreetMap](http://www.openstreetmap.org) data into an [elasticsearch](http://www.elasticsearch.org) cluster.   It aims to help indexing the world, no more, no less :)  ## Documentation  Following documentation is available in the project's [wiki](https://github.com/ncolomer/elasticsearch-osmosis-plugin/wiki):  * [Motivations](https://github.com/ncolomer/elasticsearch-osmosis-plugin/wiki/Motivations) * **[Quick start](https://github.com/ncolomer/elasticsearch-osmosis-plugin/wiki/Quick-start)** * [Data mapping](https://github.com/ncolomer/elasticsearch-osmosis-plugin/wiki/Data-mapping) * [Usage](https://github.com/ncolomer/elasticsearch-osmosis-plugin/wiki/Usage) * [Releases](https://github.com/ncolomer/elasticsearch-osmosis-plugin/wiki/Releases) * [How to contribute](https://github.com/ncolomer/elasticsearch-osmosis-plugin/wiki/How-to-contribute) * [Links](https://github.com/ncolomer/elasticsearch-osmosis-plugin/wiki/Links)  ## Download  | elasticsearch version | plugin branch | build status | release | |:-:|:-:|:-:|:-:| | 2.1.x | 2.1.x | [![Circle CI](https://circleci.com/gh/ncolomer/elasticsearch-osmosis-plugin/tree/2.1.x.svg?style=shield)](https://circleci.com/gh/ncolomer/elasticsearch-osmosis-plugin/tree/2.1.x) | [![2.1.0](http://img.shields.io/badge/download-2.1.0-blue.svg)](http://sourceforge.net/projects/es-osmosis/files/releases/elasticsearch-osmosis-plugin-2.1.0.jar) | | 1.4.x | 1.4.x-tmp | [![Circle CI](https://circleci.com/gh/ncolomer/elasticsearch-osmosis-plugin/tree/1.4.x-tmp.svg?style=shield)](https://circleci.com/gh/ncolomer/elasticsearch-osmosis-plugin/tree/1.4.x-tmp) | [![1.4.0](http://img.shields.io/badge/download-1.4.0-blue.svg)](http://sourceforge.net/projects/es-osmosis/files/releases/elasticsearch-osmosis-plugin-1.4.0.jar) | | 0.90.x | 0.90.x | [![Circle CI](https://circleci.com/gh/ncolomer/elasticsearch-osmosis-plugin/tree/0.90.x.svg?style=shield)](https://circleci.com/gh/ncolomer/elasticsearch-osmosis-plugin/tree/0.90.x) | [![1.3.0](http://img.shields.io/badge/download-1.3.0-blue.svg)](http://sourceforge.net/projects/es-osmosis/files/releases/elasticsearch-osmosis-plugin-1.3.0.jar) |  ## License  This plugin is licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)   OpenStreetMap data is licensed under the [Open Data Commons Open Database License](http://opendatacommons.org/licenses/odbl/1.0/) (ODbL)  ## Powered by  [![OpenStreetMap](https://raw.github.com/ncolomer/elasticsearch-osmosis-plugin/master/assets/openstreetmap.png)](http://www.openstreetmap.org) [![elasticsearch](https://raw.github.com/ncolomer/elasticsearch-osmosis-plugin/master/assets/elasticsearch.png)](http://www.elasticsearch.org) [![SourceForge](https://raw.github.com/ncolomer/elasticsearch-osmosis-plugin/master/assets/sourceforge.png)](http://www.sourceforge.net) [![CircleCI](https://raw.github.com/ncolomer/elasticsearch-osmosis-plugin/master/assets/circleci.png)](http://www.circleci.com)
graphaware/neo4j-to-elasticsearch	GraphAware Neo4j Elasticsearch Integration (Neo4j Module) =========================================================  [![Build Status](https://travis-ci.org/graphaware/neo4j-to-elasticsearch.png)](https://travis-ci.org/graphaware/neo4j-to-elasticsearch) | <a href="http://graphaware.com/products/" target="_blank">Downloads</a> | <a href="http://graphaware.com/site/neo4j-to-elasticsearch/latest/apidocs/" target="_blank">Javadoc</a> | Latest Release: 3.2.5.51.7  GraphAware Elasticsearch Integration is an enterprise-grade bi-directional integration between Neo4j and Elasticsearch. It consists of two independent modules plus a test suite. Both modules can be used independently or together to achieve full integration.  The first module (this project) is a plugin to Neo4j (more precisely, a [GraphAware Transaction-Driven Runtime Module](https://github.com/graphaware/neo4j-framework/tree/master/runtime#graphaware-runtime)), which can be configured to transparently and asynchronously replicate data from Neo4j to Elasticsearch. This module is now production-ready and officially supported by GraphAware for  <a href="http://graphaware.com/enterprise/" target="_blank">GraphAware Enterprise</a> subscribers.  The <a href="https://github.com/graphaware/graph-aided-search" target="_blank">second module (a.k.a. Graph-Aided Search)</a> is a plugin to Elasticsearch that can consult the Neo4j database during an Elasticsearch query to enrich the result (boost the score) by results that are more efficiently calculated in a graph database, e.g. recommendations.  # Neo4j -> Elasticsearch  ## Getting the Software  ### Server Mode  When using Neo4j in the <a href="http://docs.neo4j.org/chunked/stable/server-installation.html" target="_blank">standalone server</a> mode, you will need three (3) .jar files (all of which you can <a href="http://graphaware.com/downloads/" target="_blank">download here</a>) dropped into the `plugins` directory of your Neo4j installation:  *  <a href="https://github.com/graphaware/neo4j-framework" target="_blank">GraphAware Neo4j Framework</a> *  <a href="https://github.com/graphaware/neo4j-uuid" target="_blank">GraphAware Neo4j UUID</a> (only required of you are using UUIDs) *  <a href="https://github.com/graphaware/neo4j-to-elasticsearch" target="_blank">GraphAware Neo4j Elasticsearch Integration</a> (this project)  After changing a few lines of config (read on) and restarting Neo4j, the module will do its magic.  ### Embedded Mode / Java Development  Java developers that use Neo4j in <a href="http://docs.neo4j.org/chunked/stable/tutorials-java-embedded.html" target="_blank">embedded mode</a> and those developing Neo4j <a href="http://docs.neo4j.org/chunked/stable/server-plugins.html" target="_blank">server plugins</a>, <a href="http://docs.neo4j.org/chunked/stable/server-unmanaged-extensions.html" target="_blank">unmanaged extensions</a>, GraphAware Runtime Modules, or Spring MVC Controllers can include use the module as a dependency for their Java project.  ### Releases  Releases are synced to <a href="http://search.maven.org/#search%7Cga%7C1%7Ca%3A%22uuid%22" target="_blank">Maven Central repository</a>. When using Maven for dependency management, include the following dependency in your pom.xml.      <dependencies>         ...         <dependency>             <groupId>com.graphaware.integration.es</groupId>             <!-- this will be com.graphaware.neo4j in the next release -->             <artifactId>neo4j-to-elasticsearch</artifactId>             <version>3.2.5.51.7</version>         </dependency>         ...     </dependencies>  #### Snapshots  To use the latest development version, just clone this repository, run `mvn clean install` and change the version in the dependency above to 3.2.5.51.8-SNAPSHOT.  #### Note on Versioning Scheme  The version number has two parts. The first four numbers indicate compatibility with Neo4j GraphAware Framework.  The last number is the version of the Elasticsearch Integration library. For example, version 2.3.2.37.1 is version 1 of the Elasticsearch Integration library  compatible with GraphAware Neo4j Framework 2.3.2.37 (and thus Neo4j 2.3.2).  ### Note on UUID  It is a very bad practice to expose internal Neo4j node IDs to external systems. The reason for that is that these IDs are not guaranteed to be stable and are re-used when nodes are deleted. For this reason, unless you have your own unique identifier for your nodes already, we highly recommend using <a href="https://github.com/graphaware/neo4j-uuid" target="_blank">GraphAware Neo4j UUID Module</a> in conjunction with the Elasticsearch Integration Library. The rest of this manual will show you how to do that.  Configuring things as described below means all (or a selected subset of) your nodes will automatically be assigned an immutable uuid property, which will be indexed in Neo4j and used in Elasticsearch as the key for your indexed nodes (a.k.a. documents). When Elasticsearch returns a result, it will be the UUID that you will use to retrieve the Node from Neo4j.  ## Setup and Configuration  ### Server Mode  Edit `neo4j.conf` to register the required modules:  ```  # This setting should only be set once for registering the framework and all the used submodules dbms.unmanaged_extension_classes=com.graphaware.server=/graphaware  com.graphaware.runtime.enabled=true  #UIDM becomes the module ID: com.graphaware.module.UIDM.1=com.graphaware.module.uuid.UuidBootstrapper  #optional, default is "uuid". (only if using the UUID module) com.graphaware.module.UIDM.uuidProperty=uuid  #optional, default is all nodes: com.graphaware.module.UIDM.node=hasLabel('Label1') || hasLabel('Label2')  #optional, default is uuidIndex com.graphaware.module.UIDM.uuidIndex=uuidIndex  #prevent the whole db to be assigned a new uuid if the uuid module is settle up together with neo4j2es com.graphaware.module.UIDM.initializeUntil=0  #ES becomes the module ID: com.graphaware.module.ES.2=com.graphaware.module.es.ElasticSearchModuleBootstrapper  #URI of Elasticsearch com.graphaware.module.ES.uri=localhost  #Port of Elasticsearch com.graphaware.module.ES.port=9201  #optional, Elasticsearch index name, default is neo4j-index com.graphaware.module.ES.index=neo4j-index  #optional, node property key of a propery that is used as unique identifier of the node. Must be the same as com.graphaware.module.UIDM.uuidProperty (only if using UUID module), defaults to uuid #use "ID()" to use native Neo4j IDs as Elasticsearch IDs (not recommended) com.graphaware.module.ES.keyProperty=uuid  #optional, whether to retry if a replication fails, defaults to false com.graphaware.module.ES.retryOnError=false  #optional, size of the in-memory queue that queues up operations to be synchronised to Elasticsearch, defaults to 10000 com.graphaware.module.ES.queueSize=10000  #optional, size of the batch size to use during re-initialization, defaults to 1000 com.graphaware.module.ES.reindexBatchSize=2000  #optional, specify which nodes to index in Elasticsearch, defaults to all nodes com.graphaware.module.ES.node=hasLabel('Person')  #optional, specify which node properties to index in Elasticsearch, defaults to all properties com.graphaware.module.ES.node.property=key != 'age'  #optional, specify whether to send updates to Elasticsearch in bulk, defaults to true (highly recommended) com.graphaware.module.ES.bulk=true  #optional, read explanation below, defaults to 0 com.graphaware.module.ES.initializeUntil=0  ```  For explanation of the UUID configurations, please see the [UUID Module docs](https://github.com/graphaware/neo4j-uuid).  For explanation of the syntax used in the configuration, refer to the [Inclusion Policies](https://github.com/graphaware/neo4j-framework/tree/master/common#inclusion-policies).  The Elasticsearch Integration configuration is described in the inline comments above. The only property that needs a little more explanation is `com.graphaware.module.ES.initializeUntil`:  Every GraphAware Framework Module has methods (`initialize()` and `reinitialize()`) that provide a mechanism to get the world into a state equivalent to a situation in which the module has been running since the database was empty. These methods kick in in one of the following scenarios:  * The database is not empty when the module has been registered for the first time (GraphAware Framework used on an existing database) * The configuration of the module has changed since the last time it was run * Some failure occurred that causes the Framework to think it should fix things.  We've decided that we should not shoot the whole database at Elasticsearch in one of these scenarios automatically, because it could well be quite large. Therefore, in order to trigger (re-)indexing, i.e. sending every node that should be indexed to Elasticsearch upon Neo4j restart, you have to manually intervene.  The way you intervene is set the com.graphaware.module.ES.initializeUntil to a number slightly higher than a Java call to `System.currentTimeInMillis()` would return when the module is starting. This way, the database will be (re-)indexed once, not with every following restart. In other words, re-indexing will happen iff `System.currentTimeInMillis() < com.graphaware.module.ES.initializeUntil`. If you're not sure what all of this means or don't know how to find the right number to set this value to, you're probably best off leaving it alone or getting in touch for some (paid) support.   #### ElasticSearch Shield Support  If Shield plugin is installed and enabled on Elasticsearch node, it is possible to add authentication parameters in the configuration. Here an example:  ``` #optional, specify the Shield user com.graphaware.module.ES.authUser=neo4j_user  #optional, specify the Shield password com.graphaware.module.ES.authPassword=123456 ```  Both of them MUST be specified to enabling Authentication. The user must be able to perform writes on the elasticsearch instance.  ### Embedded Mode / Java Development  To use the ElasticSearch Integration Module programmatically, register the module like this  ```java GraphAwareRuntime runtime = GraphAwareRuntimeFactory.createRuntime(database); //where database is an instance of GraphDatabaseService runtime.registerModule(new UuidModule("UUID", UuidConfiguration.defaultConfiguration(), database));  configuration = ElasticSearchConfiguration.defaultConfiguration(HOST, PORT); runtime.registerModule(new ElasticSearchModule("ES", new ElasticSearchWriter(configuration), configuration));  runtime.start(); ```  Alternatively: ```java  GraphDatabaseService database = new GraphDatabaseFactory().newEmbeddedDatabaseBuilder(pathToDb)     .loadPropertiesFromFile(this.getClass().getClassLoader().getResource("neo4j.properties").getPath())     .newGraphDatabase();   //make sure neo4j.properties contain the lines mentioned in previous section ```  ## Usage  Apart from the configuration described above, the GraphAware ElasticSearch Integration Module requires nothing else to function. It will replicate transactions asynchronously to ElasticSearch.  ### Cypher Procedures  This module provides a set of Cypher procedures that allows communicate with Elasticsearch using the Cypher query language. These are the available procedures:  #### Searching for nodes or relationships  This procedures allows to perform search queries on indexed nodes or relationships and return them for further use in the cypher query. Example of usage:  ``` CALL ga.es.queryNode('{\"query\":{\"match\":{\"name\":\"alessandro\"}}}') YIELD node, score RETURN node, score" ```  Together with the nodes also the related score is returned.  Any search query can be submitted through the procedure, it will be performed on the index configured for replication on Elasticsearch.  Similar procedures are `queryNodeRaw` and `queryRelationshipRaw` procedures. These procedures are similar to the `queryNode` and `queryRelationship` (they accept the same parameters) but they return a JSON-encoded value of the node or relationship as returned by Ealsticsearch. Example: ``` CALL ga.es.queryRelationshipRaw('{\"query\":{\"match\":{\"city\":\"paris\"}}}') YIELD json, score RETURN json, score" ```  #### Monitoring the status of the reindexing process  Depending on your configuration, the module can be in `initialization` mode when starting, processing a complete reindexing of the Neo4j graph database content (in accordance with your configuration settings)  You can monitor the status of the `init` mode: ``` CALL ga.es.initialized() YIELD status RETURN status ```  Returns `true` or `false`  #### Getting the current node or relationship mapping  You can retrieve the current node or relationship mapping from Elasticsearch using the following procedure:  ``` CALL ga.es.nodeMapping() YIELD json as mapping RETURN mapping ``` or ``` CALL ga.es.relationshipMapping() YIELD json as mapping RETURN mapping ```  This will return a JSON string containing the mapping returned by Elasticsearch's [Get Mapping API](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html). The returned JSON string needs to be decoded using JSON parsing library.  #### Getting Elasticsearch information  ``` CALL ga.es.info() YIELD json as info return info ```  This will return a JSON string containing Elasticsearch server information as returned bas the Basic Status API. The returned JSON string needs to be decoded using JSON parsing library. An example of parsed result: ``` {   "name" : "Sharon Friedlander",   "cluster_name" : "elasticsearch",   "version" : {     "number" : "2.4.0",     "build_hash" : "ce9f0c7394dee074091dd1bc4e9469251181fc55",     "build_timestamp" : "2016-08-29T09:14:17Z",     "build_snapshot" : false,     "lucene_version" : "5.5.2"   },   "tagline" : "You Know, for Search" } ```  ### Version of ElasticSearch  This module has been tested with ElasticSearch 2.3.0+.  License -------  Copyright (c) 2015-2017 GraphAware  GraphAware is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>.
endgameinc/elasticsearch-river-kafka	Kafka River Plugin for ElasticSearch ==================================  The Kafka River plugin allows index bulk format messages into elasticsearch.  1. Download & Build Kafka 	 	See [Apacke Kafka Quick Start Guide](http://kafka.apache.org/07/quickstart.html)  for instructions on how to Download and Build.  	If you are installing on an encyrpted Ubuntu drive you may get "File name too long" error during the build.  	This can be solved by building on an unencrypted file system and moving the files to your desired install point.   2. install kafka in your maven repo:          mvn install:install-file -Dfile=./core/target/scala_2.8.0/kafka-0.7.2.jar -DgroupId=org.apache.kafka \             -DartifactId=kafka -Dversion=0.7.2 -Dpackaging=jar  3. Build this plugin:          mvn compile test package          # this will create a file here: target/releases/elasticsearch-river-kafka-1.0.1-SNAPSHOT.zip         PLUGIN_PATH=`pwd`/target/releases/elasticsearch-river-kafka-1.0.1-SNAPSHOT.zip  4. Install the PLUGIN          cd $ELASTICSEARCH_HOME         ./bin/plugin -url file:/$PLUGIN_PATH -install elasticsearch-river-kafka  5. Updating the plugin          cd $ELASTICSEARCH_HOME         ./bin/plugin -remove elasticsearch-river-kafka         ./bin/plugin -url file:/$PLUGIN_PATH -install elasticsearch-river-kafka  ##### Version Support  ElasticSearch version 0.90.0   Deployment ==========  Creating the Kafka river is as simple as (all configuration parameters are provided, with default values):      bulk_size_bytes - max size of messages to pull from Kafka each request     bulk_timeout - socket timeout for Kafka  	curl -XPUT 'localhost:9200/_river/my_kafka_river_0/_meta' -d '{ 	    "type" : "kafka", 	    "kafka" : { 	        "broker_host" : "localhost",  			"message_handler_factory_class" : "org.elasticsearch.river.kafka.JsonMessageHandlerFactory", 	        "zookeeper" : "localhost", 	        "topic" : "my_topic", 	        "partition" : "0", 	        "broker_port" : 9092 	    }, 	    "index" : { 	        "bulk_size_bytes" : 10000000, 	        "bulk_timeout" : "1000ms"         },         "statsd":{             "prefix": "es-kafka-river",             "host": "ambassador",             "port": "8125"         }  	}'  Kafka offsets are stored in zookeeper.  NOTE: in its current form, this River only reads from a single broker and a single partition.  This will likely change in the future.  In  order to consume from multiple partitions and multiple brokers, multiple rivers need to be configured.  	curl -XPUT 'localhost:9200/_river/my_kafka_river_0/_meta' -d '{ 	    "type" : "kafka", 	    "kafka" : { 	        "broker_host" : "localhost",  			"message_handler_factory_class" : "org.elasticsearch.river.kafka.JsonMessageHandlerFactory", 	        "zookeeper" : "localhost", 	        "topic" : "my_topic", 	        "partition" : "0", 	        "broker_port" : 9092 	    }, 	    "index" : { 	        "bulk_size_bytes" : 10000000, 	        "bulk_timeout" : "1000ms"         },         "statsd":{             "prefix": "es-kafka-river",             "host": "ambassador",             "port": "8125"         }  	}' 	curl -XPUT 'localhost:9200/_river/my_kafka_river_1/_meta' -d '{ 	    "type" : "kafka", 	    "kafka" : { 	        "broker_host" : "localhost",  			"message_handler_factory_class" : "org.elasticsearch.river.kafka.JsonMessageHandlerFactory", 	        "zookeeper" : "localhost", 	        "topic" : "my_topic", 	        "partition" : "1", 	        "broker_port" : 9092 	    }, 	    "index" : { 	        "bulk_size_bytes" : 10000000, 	        "bulk_timeout" : "1000ms"         },         "statsd":{             "prefix": "es-kafka-river",             "host": "ambassador",             "port": "8125"         }  	}' 	curl -XPUT 'localhost:9200/_river/my_kafka_river_2/_meta' -d '{ 	    "type" : "kafka", 	    "kafka" : { 	        "broker_host" : "localhost",  			"message_handler_factory_class" : "org.elasticsearch.river.kafka.JsonMessageHandlerFactory", 	        "zookeeper" : "localhost", 	        "topic" : "my_topic", 	        "partition" : "2", 	        "broker_port" : 9092 	    }, 	    "index" : { 	        "bulk_size_bytes" : 10000000, 	        "bulk_timeout" : "1000ms"         },         "statsd":{             "prefix": "es-kafka-river",             "host": "ambassador",             "port": "8125"         }  	}' 	curl -XPUT 'localhost:9200/_river/my_kafka_river_3/_meta' -d '{ 	    "type" : "kafka", 	    "kafka" : { 	        "broker_host" : "localhost",  			"message_handler_factory_class" : "org.elasticsearch.river.kafka.JsonMessageHandlerFactory", 	        "zookeeper" : "localhost", 	        "topic" : "my_topic", 	        "partition" : "3", 	        "broker_port" : 9092 	    }, 	    "index" : { 	        "bulk_size_bytes" : 10000000, 	        "bulk_timeout" : "1000ms"         },         "statsd":{             "prefix": "es-kafka-river",             "host": "ambassador",             "port": "8125"         }  	}'      The river is automatically bulking queue messages if the queue is overloaded, allowing for faster catchup with the  messages streamed into the queue. The `ordered` flag allows to make sure that the messages will be indexed in the  same order as they arrive in the query by blocking on the bulk request before picking up the next data to be indexed.  It can also be used as a simple way to throttle indexing.  If `message_handler_factory_class` is not set it will use the `JsonMessageHandlerFactory` and will expect json messages from Kafka with this format:  	{	 		"index" : "example_index",  		"type" : "example_type",  		"id" : "asdkljflkasjdfasdfasdf",  		"source" : { ..... }  	}     License -------  elasticsearch-river-kafka 	 Copyright 2013 [Endgame, Inc.](http://www.endgame.com/)  ![Endgame, Inc.](http://www.endgame.com/images/navlogo.png) 	 This product includes software plugin developed for ElasticSearch and Shay Banon – [Elasticsearch](http://www.elasticsearch.org/) 	 Inspiration was taken from David Pilato and his ElasticSearch Rabbit MQ Plugin   https://github.com/elasticsearch/elasticsearch-river-rabbitmq  	Licensed under the Apache License, Version 2.0 (the "License"); you may 	not use this file except in compliance with the License. You may obtain 	a copy of the License at  	     http://www.apache.org/licenses/LICENSE-2.0  	Unless required by applicable law or agreed to in writing, 	software distributed under the License is distributed on an 	"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY 	KIND, either express or implied.  See the License for the 	specific language governing permissions and limitations 	under the License.  Contributors -------------   - [Jason Trost](https://github.com/jt6211/)  - [Mark Conlin](https://github.com/meconlin)
jaibeermalik/elasticsearch-tutorial	elasticsearch-tutorial ======================  ElasticSearch Java API tutorial using test cases.  The tutorials explains [ElasticSearch](http://www.elasticsearch.org) java api usage taking examples using junit test cases for different functionality.  Part 1 ------------   Explains the ElasticSearch cluster settings and getting started with ES.  - Index, DocumentType, Nodes, Shard/Replica, - Creating/retrieving/updating/deleting documents - Schema mapping, Fields, Filters, Analyzers - Index aliasing, creating/deleting alias   Part 2 ------------  This part will include searching/querying the data, different types of query.  - Simple query, fields to search, fields to return data - Basic queries (match, boolean, query string etc.) - More like this, fuzzy query - Boosting query, custom score query etc.    Part 3 ------------  This part covers making the search better.  - Influencing the score, Boosting the documents etc.  - Using synonyms - Using Stopwords - Using word delimiters, protected words - Handling HTML content - Handling different language content - highlighting  Part 4 ------------  This part will cover common functionality of faceting, autocomplete, suggestions etc.  - faceting on hierarchical data - auto complete/ suggestions  Part 5 ------------  This part includes handling nested objects  - Creating parent/child mapping - Querying parent/child data  -----  [Jaibeer Malik](http://jaibeermalik.wordpress.com/category/tech-stuff/elasticsearch/)
bosondata/elasticsearch-analysis-bosonnlp	# 玻森数据中文分析器ElasticSearch插件 (Beta版)  ## 概述  ElasticSearch 是一个基于 Lucene 的强大搜索服务器，也是企业最受欢迎的搜索引擎之一。但是 ES 本身对中文分词和搜索比较局限。因为内置的分析器在处理中文分词时，只有两种方式：一种是单字（unigrams）形式，即简单粗暴的将中文的每一个汉字作为一个词（token）分开；另一种是两字（bigrams）的，也就是任意相邻的两个汉字作为一个词分开。这两种方式都不能很好的满足现在的中文分词需求，进而影响了搜索结果。因此[玻森数据](http://bosonnlp.com)开发了一款基于玻森中文分词的 ElasticSearch 的插件（BosonNLP Analyzer for ElasticSearch）方便大家准确的使用中文搜索。  ## 安装  ### 依赖 ElasticSearch 官网安装说明 https://www.elastic.co/guide/en/elasticsearch/guide/1.x/_installing_elasticsearch.html  ### 选择插件版本 其对应的版本号和插件版本号如下：  | BosonNLP version | ES Version    | | :--------------: | :------------:| | master           | 2.2.0->master | | 1.3.0-beta       | 2.2.0         | | 1.2.2-beta       | 2.1.2         | | 1.2.1-beta       | 2.1.1         | | 1.2.0-beta       | 2.1.0         | | 1.1.0-beta       | 2.0.0         | | 1.0.0-beta       | 1.7.x         |  ### 安装插件 现在提供以下两种方式安装插件。  #### 方法一 从 github 的链接直接下载插件，不同版本的 github 的链接已在对应版本的 README 中给出。以下示例为 ES 2.0.0及以上的插件安装命令。 ```bash $ sudo bin/plugin install https://github.com/bosondata/elasticsearch-analysis-bosonnlp/releases/download/{version}/elasticsearch-analysis-bosonnlp-{version}.zip ```  >例：下载 1.3.0 本版的插件，则在{version}填写对应的版本，在 1.3.0-beta branch 的 README 中有具体命令： ``` $ sudo bin/plugin install https://github.com/bosondata/elasticsearch-analysis-bosonnlp/releases/download/1.3.0-beta/elasticsearch-analysis-bosonnlp-1.3.0-beta.zip ```      #### 方法二 本地编译生成插件。  1. 构建项目包      下载玻森中文分析器项目到本地，并在项目根目录下通过 Maven 构建项目包：      ```     mvn clean package     ```          构建后的项目包`elasticsearch-analysis-bosonnlp-{version}.zip`在`target/releases/`生成。  2. 安装插件     通过 ElasticSearch 的 plugin 加载插件，在 ElasticSearch 根目录执行以下命令即可：       ```bash     $ sudo bin/plugin install file:/root/path/to/your/elasticsearch-analysis-bosonnlp-{version}.zip     ```  ### 设置  运行 ElasticSearch 之前需要在 config 文件夹中修改`elasticsearch.yml`来定义使用玻森中文分析器，并填写玻森 API_TOKEN 以及玻森分词 API 的地址，即在该文件结尾处添加：  ```yaml index:   analysis:     analyzer:       bosonnlp:           type: bosonnlp           API_URL: http://api.bosonnlp.com/tag/analysis           # You MUST give the API_TOKEN value, otherwise it doesn't work           API_TOKEN: *PUT YOUR API TOKEN HERE*           # Please uncomment if you want to specify ANY ONE of the following           # areguments, otherwise the DEFAULT value will be used, i.e.,           # space_mode is 0,           # oov_level is 3,           # t2s is 0,           # special_char_conv is 0.           # More detials can be found in bosonnlp docs:           # http://docs.bosonnlp.com/tag.html           #           #           # space_mode: put your value here(range from 0-3)           # oov_level: put your value here(range from 0-4)           # t2s: put your value here(range from 0-1)           # special_char_conv: put your value here(range from 0-1) ```  **需要注意的是**  1. `必须在 API_URL 填写给定的分词地址以及在API_TOKEN：*PUT YOUR API TOKEN HERE* 中填写给定的玻森数据API_TOKEN`，否则无法使用玻森中文分析器。该 API_TOKEN 是[注册玻森数据账号](http://bosonnlp.com/)所获得。  2. 如果配置文件中已经有配置过其他的 analyzer，请直接在 analyzer 下如上添加 bosonnlp analyzer。  3. 如果有多个 node 并且都需要 BosonNLP 的分词插件，则每个 node 下的 yaml 文件都需要如上安装和设置。  4. 另外，玻森中文分词还提供了4个参数（*space_mode*，*oov_level*，*t2s*，*special_char_conv*）可满足不同的分词需求。如果取默认值，则无需任何修改；否则，可取消对应参数的注释并赋值。  > 例：需开启繁体转换成简体（*t2s*）功能，则取消*t2s*的注释并赋值。 ```  t2s: 1 ```  更多关于玻森中文分词参数的信息，可以在此[了解](http://docs.bosonnlp.com/tag.html)。  设置完之后就可以运行 ElasticSearch 了，如果对该设置有新的改动，需要重启 ElasticSearch 才可生效。  ### 测试  #### 分词测试 运行 Elasiticsearch  显示插件加载成功 ``` ... [time][INFO ][plugins] [Gaza] loaded [analysis-bosonnlp] ... ``` 建立 index ```bash curl -XPUT 'localhost:9200/test' ``` 测试分析器是否配置成功 ```bash curl -XGET 'localhost:9200/test/_analyze?analyzer=bosonnlp&pretty' -d '这是玻森数据分词的测试' ``` 结果 ```json {   "tokens" : [ {     "token" : "这",     "start_offset" : 0,     "end_offset" : 1,     "type" : "word",     "position" : 0   }, {     "token" : "是",     "start_offset" : 1,     "end_offset" : 2,     "type" : "word",     "position" : 1   }, {     "token" : "玻森",     "start_offset" : 2,     "end_offset" : 4,     "type" : "word",     "position" : 2   }, {     "token" : "数据",     "start_offset" : 4,     "end_offset" : 6,     "type" : "word",     "position" : 3   }, {     "token" : "分词",     "start_offset" : 6,     "end_offset" : 8,     "type" : "word",     "position" : 4   }, {     "token" : "的",     "start_offset" : 8,     "end_offset" : 9,     "type" : "word",     "position" : 5   }, {     "token" : "测试",     "start_offset" : 9,     "end_offset" : 11,     "type" : "word",     "position" : 6   } ] }  ```  #### 搜索测试 建立 mapping ```bash curl -XPUT 'localhost:9200/test/text/_mapping' -d' {   "text": {     "properties": {       "content": {         "type": "string",          "analyzer": "bosonnlp",          "search_analyzer": "bosonnlp"       }     }   } } ``` 输入数据 ```bash curl -XPUT 'localhost:9200/test/text/1' -d' {"content": "美称中国武器商很神秘 花巨资海外参展却一言不发"} ' ``` ```bash curl -XPUT 'localhost:9200/test/text/2' -d' {"content": "复旦发现江浙沪儿童体内普遍有兽用抗生素"} ' ``` ```bash curl -XPUT 'localhost:9200/test/text/3' -d' {"content": "37年后重启顶层设计 中国未来城市发展料现四大变化"} ' ``` 查询搜索 ```bash curl -XPOST 'localhost:9200/test/text/_search?pretty'  -d' {   "query" : { "term" : { "content" : "中国" }} } ' ``` 结果 ```json {   "took" : 1,   "timed_out" : false,   "_shards" : {     "total" : 5,     "successful" : 5,     "failed" : 0   },   "hits" : {     "total" : 2,     "max_score" : 0.076713204,     "hits" : [ {       "_index" : "test",       "_type" : "text",       "_id" : "1",       "_score" : 0.076713204,       "_source": {  "content": "美称中国武器商很神秘 花巨资海外参展却一言不发"}     }, {       "_index" : "test",       "_type" : "text",       "_id" : "3",       "_score" : 0.076713204,       "_source": {  "content": "37年后重启顶层设计 中国未来城市发展料现四大变化"}     } ]   } } ``` 查询搜索 ```bash curl -XPOST 'localhost:9200/test/text/_search?pretty'  -d' {   "query" : { "term" : { "content" : "国武" }} }' ``` 结果 ```json {   "took" : 1,   "timed_out" : false,   "_shards" : {     "total" : 5,     "successful" : 5,     "failed" : 0   },   "hits" : {     "total" : 0,     "max_score" : null,     "hits" : [ ]   } }  ``` 查询搜索 ```bash curl -XPOST 'localhost:9200/test/text/_search?pretty'  -d' {   "query" : { "term" : { "content" : "国" }} }' ``` 结果 ```json {   "took" : 1,   "timed_out" : false,   "_shards" : {     "total" : 5,     "successful" : 5,     "failed" : 0   },   "hits" : {     "total" : 0,     "max_score" : null,     "hits" : [ ]   } }  ``` 如果用 ES 默认的分析器（Standard Analyzer）去查询，得到如下结果：  查询搜索 ```bash curl -XPOST 'localhost:9200/test/text/_search?pretty'  -d' {   "query" : { "term" : { "content" : "国" }} }' ``` 结果 ```json {   "took" : 8,   "timed_out" : false,   "_shards" : {     "total" : 5,     "successful" : 5,     "failed" : 0   },   "hits" : {     "total" : 2,     "max_score" : 0.057534903,     "hits" : [ {       "_index" : "test3",       "_type" : "text",       "_id" : "1",       "_score" : 0.057534903,       "_source": {"content": "美称中国武器商很神秘 花巨资海外参展却一言不发"}     }, {       "_index" : "test3",       "_type" : "text",       "_id" : "3",       "_score" : 0.057534903,       "_source": {"content": "37年后重启顶层设计 中国未来城市发展料现四大变化"}      } ]   } }  ``` 查询搜索 ```bash curl -XPOST 'localhost:9200/test3/text/_search?pretty' -d ' {  "query": {"term":{"content":"中国"}} }' ``` 结果 ```json {   "took" : 14,   "timed_out" : false,   "_shards" : {     "total" : 5,     "successful" : 5,     "failed" : 0   },   "hits" : {     "total" : 0,     "max_score" : null,     "hits" : [ ]   } }  ```  ### 配置 Token Filter 现有的 BosonNLP 分析器没有内置 token filter，如果有过滤 Token 的需求，可以利用 BosonNLP Tokenizer 和 ES 提供的 token filter 搭建定制分析器。  #### 步骤 配置定制的 analyzer 有以下三个步骤：  - 添加 BosonNLP tokenizer  在 `elasticsearch.yml` 文件中 analysis 下添加 tokenizer， 并在 tokenizer 中添加 BosonNLP tokenizer 的配置： ```yaml index:   analysis:     analyzer:       ...     tokenizer:       bosonnlp:           type: bosonnlp           API_URL: http://api.bosonnlp.com/tag/analysis           # You MUST give the API_TOKEN value, otherwise it doesn't work           API_TOKEN: *PUT YOUR API TOKEN HERE*           # Please uncomment if you want to specify ANY ONE of the following           # areguments, otherwise the DEFAULT value will be used, i.e.,           # space_mode is 0,           # oov_level is 3,           # t2s is 0,           # special_char_conv is 0.           # More detials can be found in bosonnlp docs:           # http://docs.bosonnlp.com/tag.html           #           #           # space_mode: put your value here(range from 0-3)           # oov_level: put your value here(range from 0-4)           # t2s: put your value here(range from 0-1)           # special_char_conv: put your value here(range from 0-1) ```  **同样需要注意的是**  1. `必须在 API_URL 中填写给定的分词地址以及在 API_TOKEN：*PUT YOUR API TOKEN HERE* 中填写给定的玻森数据API_TOKEN`，否则无法使用玻森 tokenizer。 2. 如果配置文件中已经有配置过其他的 tokenizer，请直接在 tokenizer 下如上添加 bosonnlp tokenizer。 3. 如果需要改动参数的默认值，请可取消对应参数的注释并赋值。  - 添加 token filter  在 `elasticsearch.yml` 文件中 analysis 下添加 filter， 并在 filter 中添加所需 filter 的配置（下面例子中，我们以 lowercase filter 为例）： ```yaml index:   analysis:     analyzer:       ...     tokenizer:       ...     filter:       lowercase:           type: lowercase  ```  - 添加定制的 analyzer  在 `elasticsearch.yml` 文件中 analysis 下添加 analyzer， 并在 analyzer 中添加定制的 analyzer 的配置（下面例子中，我们把定制的 analyzer 命名为 filter_bosonnlp）： ```yaml index:   analysis:     analyzer:       ...       filter_bosonnlp:           type: custom           tokenizer: bosonnlp           filter: [lowercase] ``` 如有其他想要添加的 filter，可以在配置完 filter 之后在上述 filter：[] 列表中添加，以逗号隔开。     附上完整的定制 analyzer 配置： ```yaml index:   analysis:     analyzer:       filter_bosonnlp:           type: custom           tokenizer: bosonnlp           filter: [lowercase]     tokenizer:       bosonnlp:           type: bosonnlp           API_URL: http://api.bosonnlp.com/tag/analysis           # You MUST give the API_TOKEN value, otherwise it doesn't work           API_TOKEN: *PUT YOUR API TOKEN HERE*           # Please uncomment if you want to specify ANY ONE of the following            # areguments, otherwise the DEFAULT value will be used, i.e.,            # space_mode is 0,            # oov_level is 3,           # t2s is 0,           # special_char_conv is 0.           # More detials can be found in bosonnlp docs:           # http://docs.bosonnlp.com/tag.html           #           #           # space_mode: put your value here(range from 0-3)           # oov_level: put your value here(range from 0-4)           # t2s: put your value here(range from 0-1)           # special_char_conv: put your value here(range from 0-1)     filter:       lowercase:           type: lowercase  ``` ## 注意 由于 ES 搜索内核 Lucene 索引文件的设计结构所限，每个文档的每个字段必须单独分析, 无法采用 BosonNLP 的批处理调用，从而在 Network IO 上会有较大的时间开销。
rethinkdb/elasticsearch-river-rethinkdb	# Elasticsearch RethinkDB River  **NOTE: This river is deprecated and will no longer work with RethinkDB 2.2 and higher. It is recommended you use the [RethinkDB logstash input](https://github.com/rethinkdb/logstash-input-rethinkdb) instead.**  This is a plugin for [Elasticsearch][] that pulls in documents from [RethinkDB][], then indexes new/updated/deleted documents in real time. Elasticsearch gives you the ability to do [full-text search][].  You might want this if you'd like to be able to search RethinkDB documents using queries like:   - get all documents that contain the phrase X   - retrieve the first 10 docs that roughly match the phrase X, ignoring common words like "the" and "a"  [Elasticsearch]: http://www.elasticsearch.org [RethinkDB]: http://rethinkdb.com [full-text search]: http://en.wikipedia.org/wiki/Full_text_search  ## Installation  First off, you need [Elasticsearch 1.3 or 1.4][] running on [Java 8][] for this to work. Once that's in place, you can install the plugin with:  [Elasticsearch 1.3 or 1.4]: http://www.elasticsearch.org/overview/elkdownloads/ [Java 8]: http://www.oracle.com/technetwork/java/javase/downloads/index.html  ``` elasticsearch-plugin --install river-rethinkdb --url http://goo.gl/JmMwTf ```   __Note__: Depending on how you've installed Elasticsearch, you may need to become the elasticsearch user to install the plugin.  ## Quickstart  If you want to index the table `posts` in the database `blog`, this is all you need to do:  ```bash $ curl -XPUT localhost:9200/_river/rethinkdb/_meta -d '{    "type":"rethinkdb",    "rethinkdb": {      "databases": {"blog": {"posts": {"backfill": true}}},      "host": "localhost",      "port": 28015    }}' ```  Now you'll have a new index called `blog` and a type called `posts` which you can query:  ```bash $ curl localhost:9200/blog/posts/_search?q=*:* ```  Elasticsearch's default port is 9200. RethinkDB's default port is 28015. You may want to brush up on [how to query Elasticsearch][].  [how to query Elasticsearch]: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-search.html  ## Details  Rivers are a kind of plugin for Elasticsearch (ES) that sync external data sources with Elasticsearch's indexes. ES indexes are similar to RethinkDB's databases, and ES types are similar to RethinkDB's tables. Every index can have zero or more types, and every type can have zero or more documents. To configure the river, you create a document in the `_river` index, which is a magical index ES watches for configuration info.  ```bash $ curl -XPUT localhost:9200/_river/rethinkdb/_meta ```  This creates a new document in the `rethinkdb` type with the id `_meta`. At a minimum, the `_meta` document needs a key with the `type` field set to `"rethinkdb"`. You'll also want to put a `rethinkdb` key with a document that contains these keys:  - `host`: RethinkDB server hostname, defaults to `"localhost"` - `port`: RethinkDB server port, defaults to 28015, - `auth_key`: RethinkDB server auth key, defaults to the empty string - `databases`: A document containing one subdocument per database   - `<dbName>`: The name of a database in RethinkDB. Must have a table specified as well.     - `<tableName>`: The name of a RethinkDB table to watch       - `backfill`: Whether to backfill existing documents or just watch for new ones, defaults to true.       - `index`: What ES index to send documents from this table to, defaults to `<dbName>`       - `type`: What ES type to send documents from this table to, defaults to `<tableName>`  You can specify as many databases and tables to watch as you'd like.  Here's a larger example that indexes `blog.posts` and `blog.comments` with the defaults plugged in:  ```javascript // localhost:9200/_river/rethinkdb/_meta {   "type": "rethinkdb",   "rethinkdb": {     "host": "localhost",     "port": 28015,     "auth_key": "",     "databases": {       "blog": {         "posts": {           "backfill": true,           "index": "blog",           "type": "posts",         },         "comments": {           "backfill": true,           "index": "blog",           "type": "comments",         }       }     }   } } ```  After the river backfills documents for a given table, it will change the `backfill` setting to `false`. This way, the next time the Elasticsearch server restarts, it won't re-pull all documents from RethinkDB again.  ## OK, I've queried Elasticsearch, what do I do now?  The documents are stored in Elasticsearch with the same id as the RethinkDB uses for it, so you can easily retrieve the original document.  For example, if you query your lorem ipsum blog posts for any that have the word "cupiditate" in the body:  ``` $ curl localhost:9200/blog/posts/_search?q=body:cupiditate ```  You'll get results that look like:  ```javascript {     "_shards": {         "failed": 0,         "successful": 1,         "total": 1     },     "hits": {         "hits": [             {                 "_id": "261f4990-627b-4844-96ed-08b182121c5e",                 "_index": "blog",                 "_score": 1.0,                 "_source": {                     "body": "cupiditate quo est a modi nesciunt soluta\nipsa voluptas",                     "id": "261f4990-627b-4844-96ed-08b182121c5e",                     "title": "at nam consequatur ea labore ea harum",                     "userId": 10.0                 },                 "_type": "posts"             }         ],         "max_score": 1.0,         "total": 1     },     "timed_out": false,     "took": 6 } ```  Now, you can fetch the original document from RethinkDB using:  ```python r.db('blog').table('posts').get('261f4990-627b-4844-96ed-08b182121c5e').run(conn) ```    ## Caveats  Currently, there's no way to guarantee that no documents are lost if the river loses connection with the RethinkDB server. The only way to be sure is to backfill every time, and this will still miss deleted documents. In the future, RethinkDB will support changefeeds that accept a timestamp. When that is implemented, this plugin will be able to ensure no documents are lost during disconnections.
mariamhakobyan/elasticsearch-river-kafka	Kafka River Plugin for ElasticSearch =========  The Kafka River plugin allows you to read messages from Kafka and index bulked messages into elasticsearch. The bulk size (the number of messages to be indexed in one request) and concurrent request number is configurable. The Kafka River also supports consuming messages from multiple Kafka brokers and multiple partitions.   The plugin uses the latest Kafka and Elasticsearch version.  * Kafka version 0.8.2.1  * Elasticsearch version 1.6.0  The plugin is periodically updated, if there are newer versions of any dependencies. It is available in the [ElasticSearch's official website](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html).  Setup ==========  1. Install Kafka if you are working on local environment (See [Apache Kafka Quick Start Guide](http://kafka.apache.org/documentation.html#quickstart)  for instructions on how to Download and Build.)  2. Install the plugin   ```sh cd $ELASTICSEARCH_HOME .bin/plugin -install <plugin-name> -url https://github.com/mariamhakobyan/elasticsearch-river-kafka/releases/download/v1.2.1/elasticsearch-river-kafka-1.2.1-plugin.zip ``` *Example:* ```sh cd $ELASTICSEARCH_HOME .bin/plugin -install kafka-river -url https://github.com/mariamhakobyan/elasticsearch-river-kafka/releases/download/v1.2.1/elasticsearch-river-kafka-1.2.1-plugin.zip ```  If it doesn't work, clone git repository and build plugin manually. * Build the plugin - it will create a zip file here: $PROJECT-PATH/target/elasticsearch-river-kafka-1.2.1-SNAPSHOT-plugin.zip * Install the plugin from target into elasticsearch   ```sh cd $ELASTICSEARCH_HOME .bin/plugin --install <plugin-name> --url file:////$PLUGIN-PATH/elasticsearch-river-kafka-1.2.1-SNAPSHOT-plugin.zip ```  Update installed plugin  ```sh cd $ELASTICSEARCH_HOME ./bin/plugin -remove <plugin-name> ./bin/plugin -url file:/$PLUGIN_PATH -install <plugin-name> ```  Configuration =========  To deploy Kafka river into Elasticsearch as a plugin, execute:  ```json curl -XPUT 'localhost:9200/_river/<river-name>/_meta' -d ' {      "type" : "kafka",      "kafka" : {         "zookeeper.connect" : <zookeeper.connect>,          "zookeeper.connection.timeout.ms" : <zookeeper.connection.timeout.ms>,         "topic" : <topic.name>,         "message.type" : <message.type>     },     "index" : {         "index" : <index.name>,         "type" : <mapping.type.name>,         "bulk.size" : <bulk.size>,         "concurrent.requests" : <concurrent.requests>,         "action.type" : <action.type>,         "flush.interval" : <flush.interval>     },     "statsd" : {         "host" : <statsd.host>,         "prefix" : <statsd.prefix>,         "port" : <statsd.port>,         "log.interval" : <statsd.log.interval>     }  }'  ```  * ***NOTE***: Type "kafka" is required and must not be changed. It corresponds the type, given in the source code, by which elasticsearch is able to associate created river with the installed plugin.    *Example:*   ```json  curl -XPUT 'localhost:9200/_river/kafka-river/_meta' -d '  {       "type" : "kafka",       "kafka" : {          "zookeeper.connect" : "localhost",           "zookeeper.connection.timeout.ms" : 10000,          "topic" : "river",          "message.type" : "json"      },      "index" : {          "index" : "kafka-index",          "type" : "status",          "bulk.size" : 100,          "concurrent.requests" : 1,          "action.type" : "index",          "flush.interval" : "12h"       },       "statsd": {          "host" : "localhost",          "prefix" : "kafka.river",          "port" : 8125,          "log.interval" : 10       }   }'   ```   The detailed description of each parameter:   * `river-name` (required) - The river name to be created in elasticsearch. * `zookeeper.connect` (optional) - Zookeeper server host. Default is: `localhost` * `zookeeper.connection.timeout.ms` (optional) - Zookeeper server connection timeout in milliseconds. Default is: `10000` * `topic` (optional) - The name of the topic where you want to send Kafka message. Default is: `elasticsearch-river-kafka` * `message.type` (optional) - The kafka message type, which then will be inserted into ES keeping the same type. Default is: `json`. The following options are available:     - `json` : Inserts json message into ES separating each json property into ES document property.    *example:*       ```json        "_source": {           "name": "John",           "age": 28        }       ```        - `string` : Inserts string message into ES as a documet, where the key name is `value`, and the value is the received message.    *example:*     ```json      "_source": {           "value": "received text message"      }     ```     * `index` (optional) - The name of elasticsearch index. Default is: `kafka-index` * `type` (optional) - The mapping type of elasticsearch index. Default is: `status` * `bulk.size` (optional) - The number of messages to be bulk indexed into elasticsearch. Default is: `100` * `concurrent.requests` (optional) - The number of concurrent requests of indexing that will be allowed. A value of 0 means that only a single request will be allowed to be executed. A value of 1 means 1 concurrent request is allowed to be executed while accumulating new bulk requests. Default is: `1` * `action.type` (optional) - The action type against how the messages should be processed. Default is: `index`. The following options are available:    - `index` : Creates documents in ES with the `value` field set to the received message.    - `delete` : Deletes documents from ES based on `id` field set in the received message.    - `raw.execute` : Execute incoming messages as a raw query. * `flush.interval` (optional) - The number of seconds/minutes/hours after which any remaining messages get flushed to elasticsearch, even if the number of messages has not reached. The time values are represented like: "12h", "3m", "5s". Default is: `12h` (12 hours)   #### Statsd configuration:  * `host` (optional) - The statsd server host name. Default is: `localhost` * `port` (optional) - The statsd server port number. Default is: 8125 * `prefix` (optional) - Prefix to be added to all statsd metric keys. Default is: `kafka.river` * `log.interval` (optional) - The interval, in seconds, in which to report metrics to the statsd server. Default is: `10` (10 seconds)   To delete the existing river, execute:   ```json curl -XDELETE 'localhost:9200/_river/<river-name>/' ```   *Example:* ```json curl -XDELETE 'localhost:9200/_river/kafka-river/' ```    To see the indexed data:  ```json curl -XGET 'localhost:9200/kafka-index/_search?pretty=1' ```  To delete the index: ```json curl -XDELETE 'localhost:9200/kafka-index' ```  Kafka Consumer details =========  Currently Consumer Group Java API (high level api) is used to create the Kafka Consumer, which keeps track of offset automatically. This enables the River to read kafka messages from multiple brokers and multiple partitions.   License ========= Copyright (C) 2014 Mariam Hakobyan. See `LICENSE`   Contributing ============ 1. Fork the repository on Github 2. Create a named feature branch 3. Develop your changes in a branch 4. Write tests for your change (if applicable) 5. Ensure all the tests are passing 6. Submit a Pull Request using Github
SpringDataElasticsearchDevs/spring-data-elasticsearch	Note : This Project is moved to SpringSource Repository [Spring-Data-Elasticsearch](https://github.com/SpringSource/spring-data-elasticsearch)  Hence all the new changes will be pushed to new repository  Thanks  BioMed Central Development Team
karussell/Jetwick	Jetwick - Open and Personalized Twitter Search  # License  The software stands under Apache 2 License and comes with NO WARRANTY  # Why Jetwick?  ## Real time vs. Relevance  I developed jetwick to get a better twitter search - more relevant tweets.  Twitter is near perfect in delivering real time search results.  In contrast to this is Jetwick trying to be nice at relevance! Jetwick lets you  filter away spam, duplicates and more. One additional simple way to avoid  noise is: to sort a result against retweets!  ## But why *personalized* twitter search?  Login and you'll get two benefits.  I wanted to be able to stay days away from twitter - spending not my whole time reading news. But when I come back to the 'news-world' I wanted to get important  news about my special topics. With relaxed saved searches you can save any  search (e.g. even sort against retweets) and jetwick will automatically grab all  tweets with your searched keywords on twitter when you are away.  Another benefit of using jetwick is the possibility to search your friends. With  all the jetwicked features: filter against language, translate, sort against  retweets etc. In a later version of jetwick it is possible to search any  (previously logged in) users' friend-tweets!  ## Bookmarking  I am using twitter as a bookmarking service and need to search my account often. With twitter this is a pain - with jetwick it's just a click.  # Demo  Sorry, no live demo anymore. But for more details and a video take a look at [this site](http://pannous.com/products/jetwick-twitter-search-2/)  # Problems  Report issues here: https://github.com/karussell/Jetwick/issues  # Developers  My blog posts about jetwick or twitter: http://karussell.wordpress.com/category/twitter/  Installation setup: https://github.com/karussell/Jetwick/wiki/Installation  Please let me know if you have problems installing jetwick.
graphaware/graph-aided-search	# GraphAware Graph-Aided Search  ## ElasticSearch Plugin providing integration with Neo4j  [![Build Status](https://travis-ci.org/graphaware/graph-aided-search.svg?branch=master)](https://travis-ci.org/graphaware/graph-aided-search) | Latest Release: 2.4.1.4  GraphAware Graph-Aided Search is an enterprise-grade bi-directional integration between Neo4j and Elasticsearch. It consists of two independent modules plus test suites. Both modules can be used independently or together to achieve full integration.  The [first module](https://github.com/graphaware/neo4j-to-elasticsearch) is a plugin for Neo4j (more precisely, a [GraphAware Transaction-Driven Runtime Module](https://github.com/graphaware/neo4j-framework/tree/master/runtime#graphaware-runtime)), which can be configured to transparently and asynchronously replicate data from Neo4j to ElasticSearch.  The second module (this module) is a plugin for Elasticsearch that can query the Neo4j graph database during a search query to enrich the result (boost the score) by results that are more efficiently calculated in a graph database, e.g. recommendations.  Both modules are now open-source production-ready for everyone. They are also officially supported by GraphAware for <a href="http://graphaware.com/enterprise/" target="_blank">GraphAware Enterprise</a> subscribers.  ## Feature Overview: Graph-Aided Search  This module is a plugin for Elasticsearch that enables users to improve search results by boosting or filtering them using data stored in the Neo4j graph database. After performing a search in Elasticsearch, just before returning the results to the user, this plugin requests additional information from Neo4j via its REST API in order to boost or filter the results.  Two main features are exposed by the plugin:   * **_Result Boosting_**: This feature allows changing the scores of the results. The score can be changed in different ways: mixing graph score with Elasticsearch score or replacing it entirely are just two examples. It is possible to customize this behaviour with different formulas, rewriting some methods of the Graph-Aided Search Booster. Usage examples include boosting (i) based on interest prediction (recommendations), (ii) based on friends' interests/likes, (iii) all use cases that are a good fit for Neo4j   * **_Result Filtering_**: This feature allows filtering, thus removing documents from the results list. By providing a Cypher query, it is possible to return to the user only documents with IDs matching the results of the Cypher query.  Detailed workflow:  1. Intercept and parse any "Search query" and try to find the `GraphAidedSearch` extension parameter; 2. Process the query extension identifying the type of the extension (boosting or a filter), and instantiate the related class; 3. Perform the operation required to boost or filter by calling the Neo4j REST API (or a Neo4j extension like [Graphaware Recommendation Engine](https://github.com/graphaware/neo4j-reco), passing all necessary information, e.g. Cypher query, target user, etc...; 4. Return the filtered/boosted result set back to the user;  ![overview](https://s3-eu-west-1.amazonaws.com/graphaware/assets/graphAidedSearchIntro2.png)  ---  ## Usage: Installation  ### Install Graph-Aided Search Binary  #### Elasticsearch 2.2.2:  ```bash $ $ES_HOME/bin/plugin install com.graphaware.es/graph-aided-search/2.2.2.0 ```  #### Elasticsearch 2.3.1:  ```bash $ $ES_HOME/bin/plugin install com.graphaware.es/graph-aided-search/2.3.2.0 ```  ### Build from source  ```bash $ git clone git@github.com:graphaware/graph-aided-search.git $ mvn clean package $ $ES_HOME/bin/plugin install file:///path/to/project/graph-aided-search/target/releases/graph-aided-search-2.X.X.0.zip ```  Start elasticsearch  ### Configuration  Then configure indexes with the url of Neo4j. This can be done in two ways. First:  ```bash $ curl -XPUT http://localhost:9200/indexname/_settings?index.gas.neo4j.hostname=http://localhost:7474 $ curl -XPUT http://localhost:9200/indexname/_settings?index.gas.enable=true ```  > **indexname** as your index name. e.g. curl -XPUT http://localhost:9200/neo4j-index-node/_settings?index.gas.enable=true  If the Neo4j Rest Api is protected by Basic Authentication confire username and password for neo4j in the following way:  ```bash $ curl -XPUT http://localhost:9200/indexname/_settings?index.gas.neo4j.user=neo4j $ curl -XPUT http://localhost:9200/indexname/_settings?index.gas.neo4j.password=password ```  If the neo4j server supports bolt it can be enable and managed using the following configuration  ```bash $ curl -XPUT http://localhost:9200/indexname/_settings?index.gas.neo4j.boltHostname=bolt://localhost:7687 $ curl -XPUT http://localhost:9200/indexname/_settings?index.gas.neo4j.bolt.secure=false (default is true) ``` Since bolt is still not a stabel release, the default protocol is http, to enable it add the following line in the booster or filter configuration:  ```json "protocol": "bolt" ```  Second, you can use also template to configure settings in the index:  ```json     curl -XPOST http://localhost:9200/_template/template_gas -d \     '{       "template": "*",       "settings": {         "index.gas.neo4j.hostname": "http://localhost:7474",         "index.gas.neo4j.boltHostname": "bolt://localhost:7687",         "index.gas.enable": true,         "index.gas.neo4j.user": "neo4j",         "index.gas.neo4j.password": "password"       }     }' ```  ### Disable Plugin  ```bash $ curl -XPUT http://localhost:9200/indexname/_settings?index.gas.enable=false ```  Queries will continue to work even with Graph-Aided-Search-specific elements, e.g. "gas-boost" and "gas-filter".  ## Usage: Search Phase  The integration with a pre-existing search query is seamless, since the plugin only requires the addition of new elements into the query.  ### Booster example  Boosters allow to change the score by an external score source. This could be a `recommender`, a Cypher query, or any custom booster provider. A simple Elasticsearch query could have the following structure:  ```bash   curl -X POST http://localhost:9200/neo4j-index/Movie/_search -d '{     "query" : {         "match_all" : {}     }'; ```  In this case all the Elasticsearch result hits will have a relevancy score value of `1`. If you would like to boost these results according to user interest computed by Graphaware Recommendation Plugin on top of Neo4j, you would change the query in the following way.  ```bash   curl -X POST http://localhost:9200/neo4j-index/Movie/_search -d '{     "query" : {         "match_all" : {}     },     "gas-booster" :{           "name": "SearchResultNeo4jBooster",           "target": "2",           "maxResultSize": 10,           "keyProperty": "objectId",           "neo4j.endpoint": "/graphaware/recommendation/movie/filter/"        }   }'; ``` The **_gas-booster_** clause identifies the type of operation, in this case it defines a boost operation. The **_name_** parameter is mandatory and allows to specify the Booster class. The remaining parameters depend on the type of the booster. In the following paragraph the available boosters are described.  #### SearchResultNeo4jBooster  This booster uses Neo4j through custom REST APIs available as plugins for the database. In this case, the _name_ value must be set to `SearchResultNeo4jBooster`.  The following parameters are available for this booster:  * **target**: (Mandatory) This parameter contains the identifier of the target for which the boosting values are computed. Since the boosting is customized according to a target, this parameter is mandatory and allows getting different results for different target (typically a user).  * **maxResultSize**: (Default is set to the max result windows size of elasticsearch, defined by the parameter index.max_result_window) When search query is changed before submitting it to elasticsearch engine, the value of "size" for the results returned is changed according to this parameter. This is necessary since once the boosting function is applied, the order may change. Some of the results that wouldn't "make it" may be boosted and fall into the "size" window.  * **keyProperty**: (Default value is `uuid`) the id of each document in the search results must match some property value of the nodes in the graph. In order to avoid ambiguities in the results, this property must identify a single node. Using <a href="https://github.com/graphaware/neo4j-uuid" target="_blank">GraphAware UUID</a> with Neo4j is recommended for this purpose.  * **operator**: (Default is multiply [*]) It specifies how to combine the Elasticsearch score with the score provided by Neo4j. Available operators are: * (multiply), + (sum), - (substract), / (divide), replace (replace score).  * **neo4j.endpoint**: (Default /graphaware/recommendation/filter) It defines the endpoint to which the request is submitted in order to get a boosting. It is added to the Neo4j host value defined for the index.  Information about the list of IDs that should be boosted as well as the target is passed to the API running atop Neo4j. The REST API should expose a POST endpoint that accepts the following parameters:  * **target** (url parameter): This is the value of target defined above and it is used to identify the user or item for which the score will be computed from the recommender;  * **limit**: This value can be used to limit the number of results provided be the REST API;  * **from**: In order to support pagination this value allows to skip a number of results;  * **keyProperty**: Specify the property on the nodes used to identify the nodes. Such property will be used to filter the results, according to the lists of "ids";  * **ids**: Comma-separated list of node identifiers that must be evaluated and then returned;  Example Call:  ``` http://localhost:7474/graphaware/recommendation/movie/filter/2  Parameters: limit=2147483647&from=0&keyProperty=objectId&ids=99,166,486,478,270,172,73,84,351,120 ``` This component supposes that the results are a json array with the following structure.  ```json [   {     "nodeId": 1212,     "objectId": "270",     "score": 3   },   {     "nodeId": 1041,     "objectId": "99",     "score": 1   },   {     "nodeId": 1420,     "objectId": "478",     "score": 1   },   {     "nodeId": 1428,     "objectId": "486",     "score": 1   } ] ```  #### SearchResultCypherBooster  This booster uses Neo4j through custom REST APIs available as plugins for the database. In this case the _name_ value must be set to `SearchResultCypherBooster`.  The following parameters are available for this booster:  * **query**: (Mandatory) This parameter contains the query to submit to the Neo4j instance.  * **scoreName**: (Default value is "score") The name of the returned value that is used as scoring function.  * **identifier**: (Default value is "id") The name of the returned value that is used for matching IDs.  * **maxResultSize**: (Default is set to the max result windows size of elasticsearch, defined by the parameter index.max_result_window) When search query is changed before submitting it to elasticsearch engine, the value of "size" for the results returned is changed according to this parameter. This is necessary since once the boosting function is applied, the order may change. Some of the results that wouldn't "make it" may be boosted and fall into the "size" window.  * **operator**: (Default is multiply [*]) It specifies how to combine the Elasticsearch score with the score provided by Neo4j. Available operators are: * (multiply), + (sum), - (substract), / (divide), replace (replace score).  The Elasticsearch result hits ids are passed as Cypher query parameter as a `List` of strings named `items`.  Example Use:  ```   curl -X POST http://localhost:9200/neo4j-index/Movie/_search -d '{     "query" : {         "match_all" : {}     },     "gas-booster" :{           "name": "SearchResultCypherBooster",           "query": "MATCH (input:User) WHERE id(input) = 2                     MATCH p=(input)-[r:RATED]->(movie)<-[r2:RATED]-(other)                     WITH other, collect(p) as paths                     WITH other, reduce(x=0, p in paths | x + reduce(i=0, r in rels(p) | i+r.rating)) as score                     WITH other, score                     ORDER BY score DESC                     MATCH (other)-[:RATED]->(reco)                     RETURN reco.objectId as id, score                     LIMIT 500",           "maxResultSize": 1000,           "scoreName": "score",           "identifier": "id"        }   }'; ```  ### Filter Example  Filters allow to filter the results using information stored in the graph. For example, you can filter movies based on what the user's friends have seen. If you would like to filter results according to a user's friends evaluation, it is possible to change the Elasticsearch query as follows:  ```   curl -X POST http://localhost:9200/neo4j-index/Movie/_search -d '{     "query" : {         "match_all" : {}     },     "gas-filter" :{           "name": "SearchResultCypherFilter",           "query": "MATCH (input:User) WHERE id(input) = 2                    MATCH (input)-[f:FRIEND_OF]->(friend)-[r:RATED]->(movie)                    WHERE r.rate > 3                    RETURN movie.uuid as id",           "exclude": false        }   }'; ```  The **_gas-filter_** clause identifies the type of the operation; in this case a filter operation. The **_name_** parameter is mandatory and allows to specify the Filter class. The remaining parameters depends on the type of filter. In the following paragraph the available filters are described.  #### SearchResultCypherFilter  This filter allows to filter results using a Cypher query on Neo4j. In this case the _name_ value must be set to `SearchResultCypherFilter`.  The following parameters are available for this filter:  * **query**: (Mandatory) This parameter contains the query to submit to the Neo4j instance.  * **maxResultSize**: (Default is set to the max result windows size of elasticsearch, defined by the parameter index.max_result_window) When search query is changed before submitting it to elasticsearch engine, the value of "size" for the results returned is changed according to this parameter. This is necessary since once the filtering function is applied, some of the results that wouldn't "make it" may fall into the "size" window.  * **exclude**: (Default true) This parameter allows to define the behaviour of the Filter. If set to true (default), it will filter out the Neo4j results from the results provided by Elasticsearch. If set to false, it will keep the intersection of Neo4j and Elasticsearch results, i.e. exclude everything that has not been returned by Neo4j.  ## Customize the plugin  The plugin allows to implement custom boosters and filters. In order to implement a booster, `SearchResultBooster` must be implemented and it needs to have the following annotation:  ``` @SearchBooster(name = "MyCustomBooster") ``` Moreover, it should be in the package `com.graphaware.es.gas`.  In order to implement a filter, `SearchResultFilter` must be implemented and it needs to have the following annotation:  ``` @SearchFilter(name = "MyCustomFilter") ```  Also in this case, it should be in the package `com.graphaware.es.gas`.  ## Version Matrix  The following version are currently supported  | Version (this project)   | Elasticsearch | |:---------:|:-------------:| | master    | 2.4.4         | | 2.3.2.2   | 2.3.2         | | 2.3.1.0   | 2.3.1         | | 2.2.2.0   | 2.2.2         |  ### Issues/Questions  Please file an [issue](https://github.com/graphaware/graph-aided-search/issues "issue").  License -------  Copyright (c) 2016 GraphAware  GraphAware is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>.
rmagen/elastic-gremlin	**Notice:** This project has migrated over to [Unipop](https://github.com/rmagen/unipop)  # elastic-gremlin  [TinkerPop 3](http://tinkerpop.incubator.apache.org/docs/3.0.0-SNAPSHOT/) implementation on Elasticsearch backend. You should read up on Tinkerpop before you use elastic-gremlin.  ## Features    - **Scalable** <br>     Using ElasticSearch's scale-out capabilities we can spread out our graph to many nodes, enabling more data while retaining good performance. - **Indexing** <br> We utilise ES's great indexing capabilities. Either let elastic-gremlin automatically create them, or configure the mappings for your specific needs. <br>  You can index Text (including analyzers), Numbers, Dates, Geo (just use the Geo predicate in a 'has' clause), etc.. - **Custom Schema** <br> ES offers many different ways to customize the way your data is stored, enabling you to optimize it for your specific querying needs. We give you the power to use all these features and get the most out of your ES cluster.<br> You can also utilize this ability to query existing data that you've loaded into ElasticSearch, by mapping the data to vertex-edge relationships of different kinds. - **Aggregations** (Coming Soon) <br> Aggregation traversals (e.g. g.V().count()) can benefit greatly from ES's [Aggregation module](https://www.elastic.co/guide/en/elasticsearch/reference/1.x/search-aggregations.html)  <br> ## Getting Started! 1. clone & build elastic-gremlin      ```git clone https://github.com/rmagen/elastic-gremlin.git```          ```mvn clean install -Dmaven.test.skip=true```      2. Create an ElasticGraph:         ```java     BaseConfiguration config = new BaseConfiguration();     /* put configuration properties as you like*/     ElasticGraph graph = new ElasticGraph(config);     GraphTraversalSource g = graph.traversal();     g.addV();     ``` 3. Or just use the Gremlin Server or Gremlin Console.  <br> ##Confiuration  ###Basic Basic usage of elastic-gremlin creates or uses an existing ES index, with each Vertex and Edge contained in its own document. You can customize some of the behaviour:  - `elasticsearch.client` (Default: "NODE") <br>    The client type used to connect to elasticsearch.    - `NODE` Sets up a local elasticsearch node and runs against it. elastic-gremlin defaults to NODE, so you can get up and running as quickly as possible.   - `TRANSPORT_CLIENT` Connects to an existing ES node.   - `NODE_CLIENT` An optimized way to connect to an ES cluster.  For more information read [here](http://www.elastic.co/guide/en/elasticsearch/client/java-api/current/client.html) - `elasticsearch.cluster.name`(Default: "elasticsearch")<br> The elasticsearch cluster's name. - `elasticsearch.cluster.address` (Default: "127.0.0.1:9300") <br> The elasticsearch nodes' address. The format is: "ip1:port1,ip2:port2,...". - `elasticsearch.refresh` (Default: true) <br> Whether to refresh the ES index before every search. Useful for testing. - `elasticsearch.index.name` (Default: "graph")<br> The name of the elasticsearch index. - `elasticsearch.bulk` (Default: false) <br> Cache all mutations in-memory and execute them in bulk when calling `ElasticGraph.commit()`.  And most importantly you can customize the ES Index's Mappings to best fit your data. You can use ES's own APIs to do it. elastic-gremlin will automatically utilize your indices as best as he can.   ###Advanced In addition to index mappings, ES offers many other ways to optimize your queries. - Model your documents in [different ways](https://www.elastic.co/guide/en/elasticsearch/guide/current/modeling-your-data.html) (Nested Objects, Parent-Child Relationship, etc) - and your [indices](https://www.elastic.co/guide/en/elasticsearch/guide/current/time-based.html) - [routing](https://www.elastic.co/blog/customizing-your-document-routing) - batch together queries  - upsert documents - and any other ES feature that could help optimize your use-case...  Implement `QueryHandler` to use a customized schema that works best for your data. <br> We still don't have enough documentation on this, but you can take a look at the implementations of `SimpleQueryHandler` and `ModernGraphQueryHandler`    You're welcome to send us any comments or questions (rmagen@gmail.com)
elastic/elasticsearch-support-diagnostics	## Support Diagnostics Utility The support diagnostic utility is a Java executable that will interrogate the node on the the host it is running on to obtain data and statistics on the running cluster.  It will execute a series of REST API calls to the running cluster, run a number of OS related commands(such as top, netstat, etc.), and collect logs and configuration, then bundle them into one or more archives.  * Compatible with versions 5.x, 2.x, 1.x * No runtime requirements or dependencies other than a recent JRE * OS specific versions are not required. * The application can be run from any directory on the machine.  It does not require installation to a specific location, and the only requirement is that the user has read access to the Elasticsearch artifacts, write access to the chosen output directory, and sufficient disk space for the generated archive. * Detects multiple nodes and network interfaces per host. * Shield authentication, SSL and cookies are supported for REST calls to the cluster.  ## Building From Source * Clone the github repo. * Make sure a recent version of Maven is installed on the build machine.  * Create a `MAVEN_HOME` directory pointing to the location you've unzipped it to. * `cd` to the top level repo directory and type `mvn package`.  ## Run Requirements * JDK **strongly recommended** - Oracle or OpenJDK, 1.7 or 1.8 * A JRE may be used, however certain functionality such as jstack generated thread dumps will not be available. * If you are running a package installation under Linux you MUST run the command with elevated sudo privileges. Otherwise the utility will not be able to read the configuration folders or run the system queries. * It is recommended that you set the JAVA_HOME environment variable.  It should point to the Java installation directory.  If JAVA_HOME is not found, the utility will attempt to locate a distribution but if errors occur it may be necessary to set this manually. * The system account running the utility must have read access to the Elasticsearch files and write access to the output location. * If you are using Shield/Security the supplied user id must have permission to execute the diagnostic URL's. * Linux, Windows, and Mac OSX are supported. * Docker installations should use the --type remote option. See below for examples.   ## Installation * Download [support-diagnostics-latest-dist.zip](https://github.com/elastic/elasticsearch-support-diagnostics/releases/latest) from the Github release area. * Unzip the support-diagnostics-`<version>`-dist.zip into the directory from which you intend to run the application. * Switch to the diagnostics distribution directory.  ## Usage - Simplest Case * Run the application via the diagnostics.sh or diagnostics.bat script. The host name or IP address used by the HTTP connector of the node is required. * In order to assure that all artifacts are collected it is recommended that you run the tool with elevated privileges. This means sudo on Linux type platforms and via an Administor Prompt in Windows. * A hostname or IP address must now be specified via the --host parameter. This is due to the changes in default port binding what were introduced starting with version 2. You must supply this even if you are running as localhost. * The utility will use default listening port of 9200 if you do not specify one. * If the utility cannot find a running ES version for that host/port combination the utility will exit and you will need to run it again. * Input parameters may be specified in any order. * An archive with the format diagnostics-`<DateTimeStamp>`.tar.gz will be created in the utility directory. If you wish to specify a specific output folder you may do so by using the -o `<Full path to custom output folder>` option.   #### Basic Usage Examples     * NOTE: Windows users use `diagnostics` instead of `./diagnostics.sh`     * sudo ./diagnostics.sh --host localhost     * sudo ./diagnostics.sh --host 10.0.0.20     * sudo ./diagnostics.sh --host myhost.mycompany.com     * sudo ./diagnostics.sh --host 10.0.0.20 --port 9201     * sudo ./diagnostics.sh --host localhost -o /home/myusername/diag-out  #### Getting Command Line Help     * /diagnostics.sh --help  ## Using With Shield/Security * a truststore does not need to be specified - it's assumed you are running this against a node that you set up and if you didn't trust it you wouldn't be running this. * When using Shield authentication, do not specify a password and the -p option.  Using the -p option will bring up a prompt for you to type an obfuscated value that will not be displayed on the command history. * --noVerify will bypass hostname verification with SSL. * --keystore and --keystorePass allow you to specify client side certificates for authentication. * To script the utility when using Shield/Security, you may use the --ptp option to allow you to pass a plain text password to the command line rather than use -p and get a prompt.  Note that this is inherently insecure - use at your own risk.  #### Examples - Without SSL     * sudo ./diagnostics.sh --host localhost -u elastic -p     * sudo ./diagnostics.sh --host 10.0.0.20 -u elastic -p #### Example - With SSL     * sudo ./diagnostics.sh --host 10.0.0.20 -u <your username> -p --ssl  ## Additional Options * You can specify additional java options such as a higher -Xmx value by setting the environment variable DIAG_JAVA_OPTS. * To include all logs, not just today's use the --archivedLogs option. * To suppress all log file collection use the --skipLogs option. * Because of the potential size access logs are no longer collected by default. If you need these use the --accessLogs option to have them copied.  ## Alternate Usages ### Remote * If you do not wish to run the utility on the host the node to be queried resides on, and wish to run it from a different host such as your workstation, you may use the --type remote option. * This will execute only the REST API calls and will not attempt to execute local system calls or collect log/config files.  #### Remote Example     * ./diagnostics.sh --host 10.0.0.20 --type remote  ### Logstash Diagnostics * Use the --type logstash argument to get diagnostic information from a running Logstash process. It will query the process in a manner similar to the Elasticsearch REST API calls. * The default port will be 9600. This can be modified at startup, or will be automatically incremented if you start multiple Logstash processes on the same host. You can connect to these other Logstash processes with the --port option. #### Logstash Examples     * sudo ./diagnostics.sh --host localhost --type logstash     * sudo ./diagnostics.sh --host localhost --type logstash --port 9610  ### Multiple Runs At Timed Intervals * If the cluster is not running X-Pack Monitoring you may find it beneficial to see how some statistics change over time. You can accomplish this by using the --interval x (in seconds) and --reps (times to repeat)to take a diagnostic. * You run the diagnostic once and it will execute a run, sleep for the interval duration, and then take another diagnostic.  * Each run will get it's own archive with the same DateTime stamp and with run-`<run number>` appended. * Logs and configs will only be collected in the archive of the final run. If you are running in standard rather than remote mode, however, all the system level calls will be executed. * This can be used for either Elasticsearch or Logstash #### Examples - 6 runs with 20 seconds separating each run     * sudo ./diagnostics.sh --host localhost -u elastic -p --interval 20 --reps 6     * sudo ./diagnostics.sh --host localhost -u elastic -p --interval 20 --reps 6 --type remote     * sudo ./diagnostics.sh --host localhost -u elastic -p --interval 20 --reps 6 --type logstash   ### Timed Thread Dumps * If you wish to take thread dumps at timed intervals without running the full gamut of API calls use the --type elastic-threads option.  * For each run it will collect output from the Hot Threads API call, as well as running a full thread dump against the process using jstack. * This **must** be run on the physical host of the node you will to check. * You do not need to supply a process id, only the host/port. It will query the node for the information on the one running on that host and use it in the call. * Since there are fewer calls for Logstash it does not have a separate type. Use the standard repitition options and both hot threads and jstack will be included. #### Elasticseach Timed Thread Dumps Example     * sudo ./diagnostics.sh --host localhost -u elastic -p --interval 20 --reps 6 --type elastic-threads      # Troubleshooting   * The file: diagnostic.log file will be generated in the installation directory of the diagnostic utility - the output of the console, which will include both progress and error messages, will be replicated in that file.  It will be appended for each run and rolled over daily if not removed.   * Make sure the account you are running from has read access to all the Elasticsearch log and config directories.  This account must have write access to any directory you are using for output.   * Make sure you have a valid Java installation that the JAVA_HOME environment variable is pointing to.   * If you are not in the installation directory CD in and run it from there.   * If you encounter OutOfMemoryExceptions, use the DIAG_JAVA_OPTS environment variable to set an -Xmx value greater than the standard 2g.  Start with -Xmx4g and move up from there if necessary.   * All errors are logged to diagnostics.log and will be written to the working directory.  If reporting an issue make sure to include that.
hannesstockner/kafka-connect-elasticsearch	# Kafka Connect Elasticsearch  kafka-connect-elasticsearch is a Kafka Connector for loading data from Kafka into Elasticsearch.    More explanation can be found in the article [Kafka and Elastic Search, A Perfect Match](https://qbox.io/blog/kafka-and-elasticsearch-a-perfect-match-1).  # Prerequisites  - a Linux console or Apple OSX console (not tested on windows, but adaptable with little effort) - a Git client to fetch the project - Docker Compose - Apache Maven installed. - git clone https://github.com/hannesstockner/kafka-connect-elasticsearch kafka-connect-elasticsearch  # Quickstart  Build a package of the code: ``` mvn clean package ``` Open a console and export env variable: ``` export DOCKER_IP={YOUR_DOCKER_IP_ADDRESS} ``` Start docker containers: ``` docker-compose up ``` Open another console window and export DOCKER_IP env variable  Run connector: ``` ./run_standalone.sh ``` Go to http://{YOUR_DOCKER_IP_ADDRESS}:9200/kafka_recipes/_search to check your imported recipes
L316476844/springbootexample	# springbootexample项目 此工程是对springboot框架学习以及结合其他框架使用的例子。<br> 分页插件依赖另外一个项目:https://github.com/L316476844/mybatis-pageplugin <br> 动态数据源依赖于项目：https://github.com/L316476844/Mybatis-DynamicDataSource 分支 release_05_18 <br>  **QQ交流群：374044564**  + springboot例子 + 包含druid数据源 + druid的sql监控 + druid过滤 + mybatis xml配置 + mybatis分页插件 + mybatis动态数据源插件 + mybatis完整sql(不包含?)输出插件 + logback日志配置 + springboot多环境配置 + 发送邮件 + AOP拦截 + 过滤器 + spring拦截器 + 全局异常 + 统一响应 + 自定义事务 + 动态数据源 + 读写分离 + Redis Session + redis缓存 + xml注入spring boot无法扫描到的bean + actuator监控 + kafka消息队列 + Swagger2配置 + dubbo注解方式 dubbo版本2.5.5 + dubboxml配置方式-分支：dubbo_xml_config + elasticsearch搜索引擎
ef-labs/vertx-elasticsearch-service	# Vert.x ElasticSearch Service  Vert.x 3 elasticsearch service with event bus proxying.  [![Build Status](http://img.shields.io/travis/ef-labs/vertx-elasticsearch-service.svg?maxAge=2592000&style=flat-square)](https://travis-ci.org/ef-labs/vertx-elasticsearch-service) [![Maven Central](https://img.shields.io/maven-central/v/com.englishtown.vertx/vertx-elasticsearch-service.svg?maxAge=2592000&style=flat-square)](https://maven-badges.herokuapp.com/maven-central/com.englishtown.vertx/vertx-elasticsearch-service/)  ### Version Matrix  | vert.x    | elasticsearch  | vertx-elasticsearch-service     | | --------- | -------------- | ---------------------------     | | 3.3.1     | 2.2.0          | 2.2.0                           | | 3.0.0     | 1.7.2          | 2.1.0                           | | 2.1.x     | 1.3.2          | 1.3.0 (vertx-mod-elasticsearch) |   ## Configuration  The configuration options are as follows:  ```json {     "address": <address>,     "transportAddresses": [ { "hostname": <hostname>, "port": <port> } ],     "cluster_name": <cluster_name>,     "client_transport_sniff": <client_transport_sniff>,     "requireUnits": false } ```  * `address` - The event bus address to listen on.  The default is `"et.vertx.elasticsearch"`. * `transportAddresses` - An array of transport address objects containing `hostname` and `port`.  If no transport address are provided the default is `"localhost"` and `9300`     * `hostname` - the ip or hostname of the node to connect to.     * `port` - the port of the node to connect to.  The default is `9300`. * `cluster_name` - the elastic search cluster name.  The default is `"elasticsearch"`. * `client_transport_sniff` - the client will sniff the rest of the cluster and add those into its list of machines to use.  The default is `true`. * `requireUnits` - boolean flag whether units are required.  The default is `false`.  An example configuration would be:  ```json {     "address": "eb.elasticsearch",     "transportAddresses": [ { "hostname": "host1", "port": 9300 }, { "hostname": "host2", "port": 9301 } ],     "cluster_name": "my_cluster",     "client_transport_sniff": true } ```  NOTE: No configuration is needed if running elastic search locally with the default cluster name.   #### Dependency Injection  The `DefaultElasticSearchService` requires a `TransportClientFactory` and `ElasticSearchConfigurator` to be injected.  Default bindings are provided for HK2 and Guice, but you can create your own bindings for your container of choice.  See the [englishtown/vertx-hk2](https://github.com/englishtown/vertx-hk2) or [englishtown/vertx-guice](https://github.com/englishtown/vertx-guice) projects for further details.   ## Action Commands  ### Index  http://www.elasticsearch.org/guide/reference/api/index_/  Send a json message to the event bus with the following structure:  ```json {     "action": "index",     "_index": <index>,     "_type": <type>,     "_id": <id>,     "_source": <source> } ```  * `index` - the index name. * `type` - the type name. * `id` - the string id of the source to insert/update.  This is optional, if missing a new id will be generated by elastic search and returned. * `source` - the source json document to index  An example message would be:  ```json {     "action": "index",     "_index": "twitter",     "_type": "tweet",     "_id": "1",     "_source": {         "user": "englishtown",         "message": "love elastic search!"     } } ```  The event bus replies with a json message with the following structure:  ```json {     "status": <status>,     "_index": <index>,     "_type": <type>,     "_id": <id>,     "_version" <version> } ```  * `status` - either `ok` or `error` * `index` - the index where the source document is stored * `type` - the type of source document * `id` - the string id of the indexed source document * `version` - the numeric version of the source document starting at 1.  An example reply message would be:  ```json {     "status": "ok",     "_index": "twitter",     "_type": "tweet",     "_id": "1",     "_version": 1 } ``` NOTE: A missing document will always be created (`upsert` mode) because the `op_type` parameter is not implemented yet (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html).   ### Get  http://www.elasticsearch.org/guide/reference/api/get/  Send a json message to the event bus with the following structure:  ```json {     "action": "get",     "_index": <index>,     "_type": <type>,     "_id": <id> } ```  * `index` - the index name. * `type` - the type name. * `id` - the string id of the source to get.  An example message would be:  ```json {     "action": "get",     "_index": "twitter",     "_type": "tweet",     "_id": "1" } ```  The event bus replies with a json message with the following structure:  ```json {     "status": <status>,     "_index": <index>,     "_type": <type>,     "_id": <id>,     "_version": <version>     "_source": <source> } ```  * `status` - either `ok` or `error` * `index` - the index name. * `type` - the type name. * `id` - the string id of the source to insert/update.  This is optional, if missing a new id will be generated by elastic search and returned. * `version` - the numeric version of the source document starting at 1. * `source` - the source json document to index  An example message would be:  ```json {     "status": "ok",     "_index": "twitter",     "_type": "tweet",     "_id": "1",     "_version": 1     "_source": {         "user": "englishtown",         "message": "love elastic search!"     } } ```   ### Search  http://www.elasticsearch.org/guide/reference/api/search/  http://www.elasticsearch.org/guide/reference/query-dsl/  Send a json message to the event bus with the following structure:  ```json {     "action": "search",     "_index": <index>,     "_indices": <indices>,     "_type": <type>,     "_types": <types>,     "query": <query>,     "postFilter": <postFilter>,     "facets": <facets>,     "search_type": <search_type>,     "scroll": <scroll>,     "size": <size>,     "from": <from>,     "fields": <fields>,     "timeout": <timeout> } ```  * `index` - a string index to be searched.  This is optional. * `indices` - an array of string indices to be searched.  This is optional. * `type` - a string type to be searched.  This is optional. * `types` - an array of string types to be searched.  This is optional. * `query` - a json object, see the elastic search documentation for details (http://www.elasticsearch.org/guide/reference/query-dsl/).  This is optional. * `postFilter` - a json object, see the elastic search documentation for details (http://www.elasticsearch.org/guide/reference/api/search/postFilter/).  This is optional. * `facets` - a json object, see the elastic search documentation for details (http://www.elasticsearch.org/guide/reference/api/search/facets/).  This is optional. * `search_type` - a string to specify the type of search to be performed (http://www.elasticsearch.org/guide/reference/api/search/search-type/).  This is optional.  Possible values include:     * query_and_fetch - execute the query on all relevant shards     * query_then_fetch - query is executed against all shards, but documents are not returned.  The results are then sorted and ranked and then only the relevant shards are asked for the documents.     * dfs_query_and_fetch - same as query_and_fetch, except for an initial scatter phase for more accurate scoring.     * dfs_query_then_fetch - same as query_then_fetch, except for an initial scatter phase for more accurate scoring.     * count - returns the count without any documents     * scan - use this to scroll a large result set * `scroll` - a string time value parameter (ex. `"5m"` or `"30s"`).  This is only required when search_type is `scan`. * `size` - a number representing the max number of results to be returned. The default if not specified is 10. (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-from-size.html). This is optional. * `from` - a number representing the offset from the first result to be returned. The default if not specified is 0. (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-from-size.html). This is optional. * `fields` - an array of strings representing the fields to return. Else the entire document is returned. (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-fields.html). This is optional. * `timeout` - a number representing the timeout in milliseconds that should be given to elasticsearch to perform a command. This is optional.  An example message would be:  ```json {     "action": "search",     "_index": "twitter",     "_type": "tweet",     "query": {         "match": {             "user": "englishtown"         }     } } ```  The event bus replies with a json message with a status `"ok"` or `"error"` along with the standard elastic search json search response.  See the documentation for details.  An example reply message for the query above would be:  ```json {     "status": "ok",     "took" : 3,     "timed_out" : false,     "_shards" : {         "total" : 5,         "successful" : 5,         "failed" : 0     },     "hits" : {         "total" : 2,         "max_score" : 0.19178301,         "hits" : [             {                 "_index" : "twitter",                 "_type" : "tweet",                 "_id" : "1",                 "_score" : 0.19178301,                 "_source" : {                     "user": "englishtown",                     "message" : "love elastic search!"                 }             },             {                 "_index" : "twitter",                 "_type" : "tweet",                 "_id" : "2",                 "_score" : 0.19178301,                 "_source" : {                     "user": "englishtown",                     "message" : "still searching away"                 }             }         ]     } } ```   ### Scroll  http://www.elasticsearch.org/guide/reference/api/search/scroll/  First send a search message with `search_type` = `"scan"` and `scroll` = `"5m"` (some time string).  The search result will include a `_scroll_id` that will be valid for the scroll time specified.   Send a json message to the event bus with the following structure:  ```json {     "action": "scroll",     "_scroll_id": <_scroll_id>,     "scroll": <scroll> } ```  * `_scroll_id` - the string scroll id returned from the scan search. * `scroll` - a string time value parameter (ex. `"5m"` or `"30s"`).   An example message would be:  ```json {     "action": "scroll",     "_scroll_id": "c2Nhbjs1OzIxMTpyUkpzWnBIYVMzbVB0VGlaNHdjcWpnOzIxNTpyUkpzWnBI",     "scroll": "5m" } ```  The event bus replies with a json message with a status `"ok"` or `"error"` along with the standard elastic search json scroll response.  See the documentation for details.  An example reply message for the scroll above would be:  ```json {     "status": "ok",     "_scroll_id": "c2Nhbjs1OzIxMTpyUkpzWnBIYVMzbVB0VGlaNHdjcWpnOzIxNTpyUkpzWnBI",     "took": 2,     "timed_out": false,     "_shards": {         "total": 5,         "successful": 5,         "failed": 0     },     "hits" : {         "total" : 2,         "max_score" : 0.0,         "hits" : [             {                 "_index" : "twitter",                 "_type" : "tweet",                 "_id" : "1",                 "_score" : 0.0,                 "_source" : {                     "user": "englishtown",                     "message" : "love elastic search!"                 }             },             {                 "_index" : "twitter",                 "_type" : "tweet",                 "_id" : "2",                 "_score" : 0.0,                 "_source" : {                     "user": "englishtown",                     "message" : "still searching away"                 }             }         ]     } } ```  ### Delete  http://www.elasticsearch.org/guide/reference/api/delete/  Send a json message to the event bus with the following structure:  ```json {     "action": "delete",     "_index": <index>,     "_type": <type>,     "_id": <id> } ```  * `index` - the index name. * `type` - the type name. * `id` - the string id of the document to delete.  An example message would be:  ```json {     "action": "delete",     "_index": "twitter",     "_type": "tweet",     "_id": "1" } ```  The event bus replies with a json message with the following structure:  ```json {     "found": <status>,     "_index": <index>,     "_type": <type>,     "_id": <id>,     "_version": <version> } ```  * `found` - either `true` or `false` * `index` - the index name. * `type` - the type name. * `id` - the string id of the source to delete. * `version` - the numeric version of the deleted document starting at 1.  An example message would be:  ```json {     "found": "true",     "_index": "twitter",     "_type": "tweet",     "_id": "1",     "_version": 1 } ```
gitchennan/elasticsearch-analysis-lc-pinyin	LC Pinyin Analysis for Elasticsearch  ====================================    Lc Pinyin版本  -------------    LC version | ES version  -----------|-----------  master | 5.3.0 -> master  5.3.0.1 | 5.3.0  5.2.2.1 | 5.2.2  5.2.0.1 | 5.2.0  5.1.2.1 | 5.1.2  5.1.1.1 | 5.1.1  5.0.2.1 | 5.0.2  5.0.1.2 | 5.0.1  5.0.1.1 | 5.0.1  2.4.2.1 | 2.4.2  2.2.2.1 | 2.2.2  1.4.5.2 | 1.4.5  1.4.5.1 | 1.4.5    Lc Pinyin介绍  -------------  > `elasticsearch-analysis-lc-pinyin`是一款`elasticsearch`拼音分词插件，可以支持按照全拼、首字母，中文混合搜索。  例如我们在某宝搜索框中输入“jianpan” 可以搜索到关键字包含“键盘”的商品。不仅仅输入全拼，有时候我们输入首字母、拼音和首字母、中文和首字母的混合输入，比如：“键pan”、“j盘”、“jianp”、“jpan”、“jianp”、“jp”  等等，都应该匹配到键盘。通过elasticsearch-analysis-lc-pinyin这个插件就能做到类似的搜索    > * 此拼音插件主要用在`短文档`的搜索上，如文章的标题、作者，商品的品牌等，不建议用在`长文档`中    分析器 - Analyzer  ------  > * `lc_index` : 该分词器用于索引数据时指定，将中文转换为全拼和首字，同时保留中文  > * `lc_search`: 该分词器用于拼音搜索时指定，按最小拼音分词个数拆分拼音，优先拆分全拼    ------    分词器 - Tokenizer  ------  > * `lc_index`：参数 mode: *full_pinyin*，*first_letter*，*chinese_char*  > * `lc_search`：参数 mode: *smart_pinyin*，*single_letter*    ------    过滤器 - TokenFilter  ------  > * `lc_full_pinyin`：将中文Token转全拼(支持多音字)  > * `lc_first_letter`：将中文Token转首字母(支持多音字)    ------  ## 过滤器使用示例  以下是使用ik分词结合拼音过滤器使用例子：  1. 创建一个索引，并定义分析器  ```bash  PUT /index  {    "settings": {      "analysis": {        "analyzer": {          "ik_letter_smart": {            "type": "custom",            "tokenizer": "ik_max_word",            "filter": [              "lc_first_letter"            ]          },          "ik_py_smart": {            "type": "custom",            "tokenizer": "ik_max_word",            "filter": [              "lc_full_pinyin"            ]          }        }      }    }  }  ```  2. 测试分词  ```bash  curl -XGET 'http://192.168.0.109:9200/index/_analyze?analyzer=ik_py_smart&pretty&text=英雄难过美人关'    #返回词条：  {    "tokens" : [      {        "token" : "yingxiongnanguomeirenguan",        "start_offset" : 0,        "end_offset" : 7,        "type" : "CN_WORD",        "position" : 0      },      {        "token" : "yingxiongnanguo",        "start_offset" : 0,        "end_offset" : 4,        "type" : "CN_WORD",        "position" : 1      },      {        "token" : "yingxiong",        "start_offset" : 0,        "end_offset" : 2,        "type" : "CN_WORD",        "position" : 2      },      {        "token" : "nanguo",        "start_offset" : 2,        "end_offset" : 4,        "type" : "CN_WORD",        "position" : 3      },      {        "token" : "meirenguan",        "start_offset" : 4,        "end_offset" : 7,        "type" : "CN_WORD",        "position" : 4      },      {        "token" : "meiren",        "start_offset" : 4,        "end_offset" : 6,        "type" : "CN_WORD",        "position" : 5      },      {        "token" : "guan",        "start_offset" : 6,        "end_offset" : 7,        "type" : "CN_CHAR",        "position" : 6      }    ]  }  ```    ## 分析器使用示例    1.创建一个索引`index`    ```bash  curl -XPUT http://localhost:9200/index  ```    2.创建类型`brand`的mapping    ```bash  curl -XPOST http://localhost:9200/index/_mapping/brand -d'  {    "brand": {      "properties": {        "name": {          "type": "text",          "analyzer": "lc_index",          "search_analyzer": "lc_search",          "term_vector": "with_positions_offsets"        }      }    }  }'  ```    3.索引一些互联网品牌    ```bash  curl -XPOST http://localhost:9200/index/brand/1 -d'{"name":"百度"}'  curl -XPOST http://localhost:9200/index/brand/8 -d'{"name":"百度糯米"}'  curl -XPOST http://localhost:9200/index/brand/2 -d'{"name":"阿里巴巴"}'  curl -XPOST http://localhost:9200/index/brand/3 -d'{"name":"腾讯科技"}'  curl -XPOST http://localhost:9200/index/brand/4 -d'{"name":"网易游戏"}'  curl -XPOST http://localhost:9200/index/brand/9 -d'{"name":"大众点评"}'  curl -XPOST http://localhost:9200/index/brand/10 -d'{"name":"携程旅行网"}'  ```    4.编写高亮查询DSL    此示例通过`lc_search`分词器配合`match_phrase`查询实现品牌的`全拼`搜索  搜索全拼关键字`baidu`，请求DSL如下：  ```bash  curl -XPOST http://localhost:9200/index/brand/_search  -d'  {      "query": {          "match": {            "name": {              "query": "baidu",              "analyzer": "lc_search",              "type": "phrase"            }          }      },      "highlight" : {          "pre_tags" : ["<tag1>"],          "post_tags" : ["</tag1>"],          "fields" : {              "name" : {}          }      }  }'  ```    匹配到`百度`、`百度糯米`两个品牌  tip：`百度`排在`百度糯米`的前面，因为name字段长度更短  查询结果：  ```bash  {      "took": 18,      "timed_out": false,      "_shards": {          "total": 1,          "successful": 1,          "failed": 0      },      "hits": {          "total": 2,          "max_score": 2.5751648,          "hits": [              {                  "_index": "index",                  "_type": "brand",                  "_id": "1",                  "_score": 2.5751648,                  "_source": {                      "name": "百度"                  },                  "highlight": {                      "name": [                          "<tag1>百度</tag1>"                      ]                  }              }              ,              {                  "_index": "index",                  "_type": "brand",                  "_id": "8",                  "_score": 2.0601318,                  "_source": {                      "name": "百度糯米"                  },                  "highlight": {                      "name": [                          "<tag1>百度</tag1>糯米"                      ]                  }              }          ]      }  }  ```    此示例通过`lc_search`分词器配合`match_phrase`查询实现品牌的`中文&全拼`搜索  搜索全拼关键字`xie程lu行wang`，请求DSL如下：  ```bash  curl -XPOST http://localhost:9200/index/brand/_search  -d'  {      "query": {          "match": {            "name": {              "query": "xie程lu行",              "analyzer": "lc_search",              "type": "phrase"            }          }      },      "highlight" : {          "pre_tags" : ["<tag1>"],          "post_tags" : ["</tag1>"],          "fields" : {              "name" : {}          }      }  }'  ```    匹配到`携程旅行网` 结果如下：  ```bash  #匹配到`携程旅行网` 结果如下：  {      "took": 4,      "timed_out": false,      "_shards": {          "total": 1,          "successful": 1,          "failed": 0      },      "hits": {          "total": 1,          "max_score": 4.5665164,          "hits": [              {                  "_index": "index",                  "_type": "brand",                  "_id": "10",                  "_score": 4.5665164,                  "_source": {                      "name": "携程旅行网"                  },                  "highlight": {                      "name": [                          "<tag1>携程旅行</tag1>网"                      ]                  }              }          ]      }  }    ```    ```bash  # 此示例通过`lc_search`分词器配合`match_phrase`查询实现品牌的`首字母`搜索  # 此示例中也可以通过`lc_first_letter`分词器搜索，结果和`lc_search`一样  #  # 这两个分词器的主要区别:  #     lc_first_letterl 会把所有输入的字母拆分成单字母用于首字母匹配  #     lc_search        会优先把输入的字母串拆分成全拼,并找到一个最优拆分结果  #  # 搜索全拼关键字`albb`，请求DSL如下：  curl -XPOST http://localhost:9200/index/brand/_search  -d'  {      "query": {          "match": {            "name": {              "query": "albb",              "analyzer": "lc_search",              "type": "phrase"            }          }      },      "highlight" : {          "pre_tags" : ["<tag1>"],          "post_tags" : ["</tag1>"],          "fields" : {              "name" : {}          }      }  }'  ```    匹配到`阿里巴巴`，结果如下：  ```bash  {      "took": 4,      "timed_out": false,      "_shards": {          "total": 1,          "successful": 1,          "failed": 0      },      "hits": {          "total": 1,          "max_score": 3.9560113,          "hits": [              {                  "_index": "index",                  "_type": "brand",                  "_id": "2",                  "_score": 3.9560113,                  "_source": {                      "name": "阿里巴巴"                  },                  "highlight": {                      "name": [                          "<tag1>阿里巴巴</tag1>"                      ]                  }              }          ]      }  }  ```    ```java  //java api实现  //该查询会匹配`阿里巴巴`这条数据  QueryBuilder pinyinQueryBuilder =  QueryBuilders.matchPhraseQuery("name", "ali巴b").analyzer("lc_search");          SearchRequestBuilder requestBuilder = client.prepareSearch("index").setTypes("brand");          requestBuilder.setQuery(pinyinQueryBuilder)                  .setHighlighterPreTags("<tag1>")                  .setHighlighterPostTags("</tag1>")                  .addHighlightedField("name")                  .execute().actionGet();                                      //该查询会匹配`大众点评`这条数据  QueryBuilder pinyinQueryBuilder =  QueryBuilders.matchPhraseQuery("name", "dzdp").analyzer("lc_first_letter");          SearchRequestBuilder requestBuilder = client.prepareSearch("index").setTypes("brand");          requestBuilder.setQuery(pinyinQueryBuilder)                  .setHighlighterPreTags("<tag1>")                  .setHighlighterPostTags("</tag1>")                  .addHighlightedField("name")                  .execute().actionGet();                    //该查询也会匹配`大众点评`这条数据  QueryBuilder pinyinQueryBuilder =  QueryBuilders.matchPhraseQuery("name", "dzdp").analyzer("lc_search");          SearchRequestBuilder requestBuilder = client.prepareSearch("index").setTypes("brand");          requestBuilder.setQuery(pinyinQueryBuilder)                  .setHighlighterPreTags("<tag1>")                  .setHighlighterPostTags("</tag1>")                  .addHighlightedField("name")                  .execute().actionGet();  ```  作者:  [@陈楠][1]  Email: 465360798@qq.com    <完>    [1]: http://blog.csdn.net/chennanymy?viewmode=contents
srecon/elasticsearch-cassandra-river	elasticsearch-cassandra-river ============================= Elasticsearch river for Cassandra 2.* version with CQL support, based on data pull method from Cassandra cluster. Project Cloned and modified from the origin https://github.com/eBay/cassandra-river.  1. Based on Datastax Java driver 2. CQL support 3. Support Cron Scheduling  ##Setup build : mvn clean install  install:  - copy target/releases/cassandra-river-1.0.6-SNAPSHOT.zip into $ELASTICSEARCH_HOME/plugin/cassandra-river   or - ./plugin --url file:/river/cassandra-river-1.0.6-SNAPSHOT.zip --install cassandra-river  remove:  ./plugin --remove cassandra-river  ##Init:     curl -XPUT 'http://HOST:PORT/_river/cassandra-river/_meta' -d '{         "type" : "cassandra",         "cassandra" : {             "connection" :             {                 "hosts" : "hostname",                 "data_centre" : "dc",                 "username" : "optional_username",                 "password" : "optional_password"             },             "sync" :             {                 "batch_size" : 20000,                 "schedule" : "0 0/15 * * * ?"             },             "keyspaces" :             [                 {                     "name" : "keyspace_name",                     "column_families" :                     [                         {                             "name" : "column_family_name",                             "primary_key" : "column_family_primary_key",                             "index" :                             {                                 "name" : "keyspace_name_column_family_name_index",                                 "type" : "keyspace_name_column_family_name"                             },                             "columns" :                             [                                 {                                     "name" : "user_id",                                     "type" : "string"                                     "raw": "true"                                 }                             ]                         }                     ]                 }             ]         }     }'  Notes on the above:   * `hosts` can be a single ip address or dns name or comma separated list (with no spaces).  * `keyspaces` is a list of dictionaries describing the keyspaces and their column familes.  * `column_families` is a list of dictionaries describing the column family.  * `column_families` -> `index` optional attribute to set the index name and type values to be used to create the index.  * `column_families` -> `columns` optional attribute to describe column mapping characteristics.  * `column_families` -> `columns` -> `name` name of the column  * `column_families` -> `columns` -> `type` one of the core types: string, integer/long, float/double, boolean and null.  * `column_families` -> `columns` -> `raw` optional flag that indecates that a raw field should be created as well, useful for sorting a column/attribute that is searchable as well.  ##Search Install plugin head $ES_HOME\bin\plugin -install mobz/elasticsearch-head  Use Head plugin to search, you can download it from here http://HOST:PORT/_plugin/head/   ##Improvments 1. Add unit Tests 2. Add newly added rows in ES by date
vhyza/elasticsearch-analysis-lemmagen	LemmaGen Analysis for ElasticSearch ===================================  The LemmaGen Analysis plugin provides [jLemmaGen lemmatizer](https://bitbucket.org/hlavki/jlemmagen) as Elasticsearch [token filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html).  [jLemmaGen](https://bitbucket.org/hlavki/jlemmagen) is Java implementation of [LemmaGen](http://lemmatise.ijs.si/) project (originally written in C++ and C#).  [LemmaGen](http://lemmatise.ijs.si/) is open-source lemmatizer which includes lexicons for following European languages.  * Bulgarian (bg) * Czech (cs) * English (en) * Estonian (et) * French (fr) * Hungarian (hu) * Macedonian (mk) * Persian (fa) - since version 2.1 * Polish (pl) * Romanian (ro) * Russian (ru) * Slovak (sk) * Slovene (sl) * Serbian (sr) * Ukrainian (uk)  Lexicons license ================ According to communication with author of jLemmagen implementation ([https://discuss.elastic.co/t/ann-lemmagen-analysis-for-elasticsearch-plugin/14585/13](https://discuss.elastic.co/t/ann-lemmagen-analysis-for-elasticsearch-plugin/14585/13)) it seems lexicons can be used for non-commercial research purposes only.   Instalation ===========  Installation instructions for particular elasticsearch versions are located at [**releases section**](https://github.com/vhyza/elasticsearch-analysis-lemmagen/releases).  After plugin installation and **elasticsearch restart** you should see in logs:  * elasticsearch `0.90.x`  ```bash [2013-11-25 00:33:01,146][INFO ][plugins] [Forrester, Lee] loaded [analysis-lemmagen], sites [] ```  * elasticsearch `2.x`  ```bash [2015-12-01 19:09:12,809][INFO ][plugins] [Aralune] loaded [elasticsearch-analysis-lemmagen], sites [] ```  * elasticsearch `5.x`  ``` [2017-01-25T09:37:04,901][INFO ][o.e.p.PluginsService     ] [63Jivne] loaded plugin [elasticsearch-analysis-lemmagen] ```  Usage =====  This plugin provides [token filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html) of type `lemmagen`. You can specify language by setting `lexicon` option. Valid values are `bg, cs, en, et, fr, hu, mk, pl, ro, ru, sk, sl, sr, uk`. Default is `en`.  Example ------- ```bash # Delete test index # curl -X DELETE 'http://localhost:9200/lemmagen-test'  # Create index with lemmagen filter # curl -X PUT 'http://localhost:9200/lemmagen-test' -d '{   "settings": {     "index": {       "analysis": {         "filter": {           "lemmagen_filter_en": {             "type": "lemmagen",             "lexicon": "en"           }         },         "analyzer": {           "lemmagen_en": {             "type": "custom",             "tokenizer": "uax_url_email",             "filter": [               "lemmagen_filter_en"             ]           }         }       }     }   },   "mappings" : {     "message" : {       "properties" : {         "text" : { "type" : "string", "analyzer" : "lemmagen_en" }       }     }   } }'  # Try it using _analyze api # curl -X GET 'http://localhost:9200/lemmagen-test/_analyze?analyzer=lemmagen_en&pretty' -d 'I am late.'  # RESPONSE: # # { #   "tokens" : [ { #     "token" : "i", #     "start_offset" : 0, #     "end_offset" : 1, #     "type" : "<ALPHANUM>", #     "position" : 1 #   }, { #     "token" : "be", #     "start_offset" : 2, #     "end_offset" : 4, #     "type" : "<ALPHANUM>", #     "position" : 2 #   }, { #     "token" : "late", #     "start_offset" : 5, #     "end_offset" : 9, #     "type" : "<ALPHANUM>", #     "position" : 3 #   } ] # }  # Index document # curl -XPUT 'http://localhost:9200/lemmagen-test/message/1' -d '{     "user"         : "tester",     "published_at" : "2013-11-15T14:12:12",     "text"         : "I am late." }'  # Refresh index # curl -XPOST 'http://localhost:9200/lemmagen-test/_refresh'  # Search # curl -X GET 'http://localhost:9200/lemmagen-test/_search?pretty' -d '{   "query" : {     "match" : {       "text" : "is"     }   } }'  # RESPONSE # #{ #  "took" : 53, #  "timed_out" : false, #  "_shards" : { #    "total" : 5, #    "successful" : 5, #    "failed" : 0 #  }, #  "hits" : { #    "total" : 1, #    "max_score" : 0.15342641, #    "hits" : [ { #      "_index" : "lemmagen-test", #      "_type" : "message", #      "_id" : "1", #      "_score" : 0.15342641, "_source" : { #        "user"         : "tester", #        "published_at" : "2013-11-15T14:12:12", #        "text"         : "I am late." #    } #    } ] #  } #} ```  **NOTE**: `lemmagen` token filter doesn't lowercase. If you wan't your tokens to be lowercased, add [lowercase token filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenfilter.html) into your analyzer `filters`.  ```bash # Create index with lemmagen and lowercase filter # curl -X PUT 'http://localhost:9200/lemmagen-lowercase-test' -d '{   "settings": {     "index": {       "analysis": {         "filter": {           "lemmagen_filter_en": {             "type": "lemmagen",             "lexicon": "en"           }         },         "analyzer": {           "lemmagen_lowercase_en": {             "type": "custom",             "tokenizer": "uax_url_email",             "filter": [ "lemmagen_filter_en", "lowercase" ]           }         }       }     }   },   "mappings" : {     "message" : {       "properties" : {         "text" : { "type" : "string", "analyzer" : "lemmagen_lowercase_en" }       }     }   } }' ```  Development ===========  To copy dependencies located in `lib` directory to you local maven repository (`~/.m2`) run:  ```bash mvn initialize ```  and to create plugin package run following:  ```bash mvn package ```  After that build should be located in `./target/releases`.  Credits =======  [LemmaGen team](http://lemmatise.ijs.si/Home/Contact) for original `C++`, `C#` implementation  [Michal Hlaváč](https://bitbucket.org/hlavki/jlemmagen) for `Java` implementation of LemmaGen  License ======= All source codes except prebuilt lexicon files are licensed under Apache License, Version 2.0. Prebuilt lexicons can be used for non-commercial research purposes only.      Copyright 2017 Vojtěch Hýža <http://vhyza.eu>      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
jloisel/elastic-crud	[![Build Status](https://travis-ci.org/jloisel/elastic-crud.svg)](https://travis-ci.org/jloisel/elastic-crud) [![Dependency Status](https://www.versioneye.com/user/projects/568d2e269c1b98002b000030/badge.svg?style=flat)](https://www.versioneye.com/user/projects/568d2e269c1b98002b000030) [![Coverage Status](https://coveralls.io/repos/jloisel/elastic-crud/badge.svg?branch=master&service=github)](https://coveralls.io/github/jloisel/elastic-crud?branch=master) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.jeromeloisel/db-spring-elasticsearch-starter/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.jeromeloisel/db-spring-elasticsearch-starter) [![Javadoc](https://javadoc-emblem.rhcloud.com/doc/com.jcabi/jcabi-email/badge.svg)](http://www.javadoc.io/doc/com.jeromeloisel/elastic-crud)  ## Elasticsearch Simple CRUD Repository  Easily perform Create / Read / Update / Delete operations on beans stored in Elasticsearch. [Spring Data Elasticsearch](https://github.com/spring-projects/spring-data-elasticsearch) lacks maintenance and is already a few Elasticsearch versions behind the latest version.  This project powers our [JMeter Load Testing platform](https://octoperf.com).  ### Versions  The following table shows the correspondance between our versions and Elasticsearch versions:  | Version       | ElasticSearch Version | | ------------- |:---------------------:| | 1.1.x      | 2.1.x | | 2.2.x      | 2.2.x | | 2.3.x      | 2.3.x | | 5.1.x      | 5.1.x |  As of 2.2.x, the project is going to strictly follow the same versioning as [elasticsearch](https://github.com/elastic/elasticsearch).  ### Spring  Add the following Maven dependency to get started quickly with Spring:  ```xml <dependency>     <groupId>com.jeromeloisel</groupId>     <artifactId>db-spring-elasticsearch-starter</artifactId>     <version>5.1.2</version> </dependency> ``` ### Vanilla Java  To get started with Vanilla Java application, you need to add two dependencies:  ```xml <dependency>     <groupId>com.jeromeloisel</groupId>     <artifactId>db-conversion-jackson</artifactId>     <version>5.1.2</version> </dependency> ``` This dependency provides the Jackson Json serialization mechanism.  ```xml <dependency>     <groupId>com.jeromeloisel</groupId>     <artifactId>db-repository-elasticsearch</artifactId>     <version>5.1.2</version> </dependency> ```  This dependency provides the **ElasticSearchRepositoryFactory** to create **ElasticRepository**.  ### Java Example  Suppose we would like to persist the following Bean in Elasticsearch:  ```java @Value @Builder @Document(indexName="datas", type="person") public class Person implements Entity {   @Wither   String id;   String firstname;   String lastname;      @JsonCreator   Person(       @JsonProperty("id") final String id,        @JsonProperty("firstname") final String firstname,        @JsonProperty("lastname") final String lastname) {     super();     this.id = id;     this.firstname = checkNotNull(firstname);     this.lastname = checkNotNull(lastname);   } }  ```  The following code shows how to use the CRUD repository:  ```java @Autowired private ElasticSearchRepositoryFactory factory;  public void method() {   final ElasticRepository<Person> repository = factory.create(Person.class);      final Person person = Person.builder().id("").firstname("John").lastname("Smith").build();   final Person withId = repository.save(person);      // Find by id   final Optional<Person> byId = repository.findOne(withId.getId());   assertTrue(repository.exists(byId));      // Search by firstname (with "not_analyzed" string mapping)   final TermQueryBuilder term = new TermQueryBuilder("firstname", PERSON.getFirstname());   final List<Person> found = repository.search(term);   assertTrue(found.contains(byId));      // Delete from Elasticsearch definitively   repository.delete(withId.getId());   assertFalse(repository.exists(byId)); } ```  ### Type mapping  Beans stored in Elasticsearch must have **_source** field enabled: see https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html. The following example Json shows how to enable _source field:  ```json {   "template": "datas",   "settings": {     "number_of_shards": 5,     "number_of_replicas": 1,     "index.refresh_interval": -1,   },   "mappings": {     "_default_": {       "_all": {           "enabled": false        },        "_source": {           "enabled": true        },        "dynamic_templates": [          {            "strings": {              "match_mapping_type": "string",              "mapping": {                "type": "string",                "index": "not_analyzed"              }            }          }        ]     }   } } ```  ### Index refresh  Every mutating query (insert, delete) performed on the index automatically refreshes it. I would recommend to disable index refresh as shows in the Json above.  ### Json Serialization  The Json serialization is configured to use [Jackson](https://github.com/FasterXML/jackson) by default. To use Jackson Json serialization, simply add Jackson as dependency:  ```xml <dependency> 	<groupId>com.fasterxml.jackson.core</groupId> 	<artifactId>jackson-databind</artifactId> 	<version>${jackson.version}</version> </dependency> ```  Replace **${jackson.version}** with the version you are using.  If you intend to use your own Json serialization mechanism (like Gson), please provide an implementation for the **JsonSerializationFactory** interface.  ### Elasticsearch Client  An instance of the [Elasticsearch Client](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/client.html) must be provided.
thinkaurelius/titan-web-example	titan-web-example ================= This is an example project that shows one way to build a RESTful Java web app around Titan, Cassandra, and Elasticsearch.  Cassandra is used in [Remote Server Mode](http://s3.thinkaurelius.com/docs/titan/1.0.0/cassandra.html#_remote_server_mode). Elasticsearch is used [remotely](http://s3.thinkaurelius.com/docs/titan/1.0.0/elasticsearch.html#es-cfg-transportclient-legacy) as well.  The "web portion" of the application is built using Spring (with annotations based config), JAX-RS, and Jersey. It is assumed that you are already familiar with these technologies.  This project also shows how Groovy classes can be injected into a Java web app to enable Gremlin Groovy sweetness. (See [GroovyGraphOp.groovy](src/main/groovy/com/thinkaurelius/titan/webexample/GroovyGraphOp.groovy).)  The [Graph of the Gods](http://s3.thinkaurelius.com/docs/titan/1.0.0/getting-started.html) example data set is used.  A Servlet Filter is used to show how TitanGraph transactions can be managed with pre/post request Filters.  A Vagrant box is provided that hosts Cassandra and Elasticsearch.  This project is built against Titan 1.0.0. and TinkerPop 3.0.0.  Getting Started ===============  If you prefer to install Cassandra and Elasticsearch manually then skip the first 3 steps but make sure you adjust the [titan-cassandra-es.properties](src/main/resources/titan-web-example/config/titan-cassandra-es.properties) file's host values accordingly. You can use the [Vagrant file](vagrant/Vagrantfile) and [bootstrap script](vagrant/bootstrap.sh) for installation information.  - Install [Virtual Box](https://www.virtualbox.org/wiki/Downloads) (because you need it for Vagrant).  - Install [Vagrant](https://docs.vagrantup.com/v2/installation/).  - Vagrant up! This will start a VM that has compatible versions of Cassandra and Elasticsearch running. The host IP is 10.10.10.10. The VM also contains a Titan distribution with a pre-configured titan-cassandra-es.properties file that you can use for local gremlin shell access. (The Titan distro isn't actually used by the web app at all.) ```bash cd titan-web-example/vagrant; vagrant up ``` - Load the project in your favorite IDE (or compile and run the following via mvn).  - Run the classesthatrunthings.PopulateDB class. This will load the "Graph of the Gods" example data set.  - Run the classesthatrunthings.RunApp class. This will launch the web app in an embedded Jetty container.  - Visit http://localhost:9091  Try the links. If they work then you're ready to dig in to the code and see what's happening. The [TitanWebService class](src/main/java/com/thinkaurelius/titan/webexample/TitanWebService.java) is a great place to start.  Resources =========  [Titan Docs](http://s3.thinkaurelius.com/docs/titan/0.5.0-SNAPSHOT/)  http://gremlindocs.com  http://thinkaurelius.com/blog/  Q&A happens in the [Aurelius Google Group](https://groups.google.com/forum/#!forum/aureliusgraphs) or on http://stackoverflow.com. If you have questions then those are the best places to search first.
javanna/elasticsearch-river-solr	Solr River Plugin for ElasticSearch  ==================================  The Solr River plugin allows to import data from [Apache Solr](http://lucene.apache.org/solr) to [elasticsearch](http://www.elasticsearch.org).  Deprecation warning: rivers are deprecated in elasticsearch, hence the Solr River plugin is not maintained anymore. Read [this article](https://www.elastic.co/blog/deprecating_rivers) for more info on the rivers deprecation. -----------------------------------------  In order to install the latest version of the plugin, simply run: `bin/plugin install river-solr -url http://bit.ly/1qzA7lB`. You can copy paste the url of a specific version from the table below, depending on the elasticsearch version you're running.   Versions --------  <table> 	<thead> 		<tr> 			<td>Solr River Plugin</td> 			<td>Elasticsearch</td> 		</tr> 	</thead> 	<tbody> 	    <tr>             <td>master</td>             <td>1.3.x -> 1.4.x</td>         </tr>         <tr>             <td><a href ="http://bit.ly/1qzA7lB">2.1</a></td>             <td>1.3.x -> 1.4.x</td>         </tr>         <tr>             <td><a href ="http://bit.ly/1grj1ny">2.0</a></td>             <td>1.0.x -> 1.2.x</td>         </tr>         <tr>             <td><a href="http://bit.ly/1exH5mG">1.1</a></td>             <td>0.90.x</td>         </tr>         <tr>             <td><a href="http://bit.ly/10ioSk0">1.0.4</a></td>             <td>0.90.0</td>         </tr> 	    <tr>     	    <td><a href="http://bit.ly/12rmrSN">1.0.3</a></td>     		<td>0.20.0 -> 0.20.6</td>         </tr> 		<tr> 			<td><a href="http://bit.ly/15cCMIB">1.0.2</a></td> 			<td>0.20.0 -> 0.20.6</td> 		</tr> 		<tr>             <td><a href="http://bit.ly/Yo61UW">1.0.1</a></td>             <td>0.19.3 -> 0.19.12</td>         </tr> 		<tr>             <td><a href="http://bit.ly/XUl2LZ">1.0.0</a></td>         	<td>0.19.3 -> 0.19.12</td>         </tr> 	</tbody> </table>   Getting Started ===============  The Solr River allows to query a running Solr instance and index the returned documents in elasticsearch. It retrieves documents via [json response writer](http://wiki.apache.org/solr/SolJSON), through http get requests (solrj is not used anymore to communicate with Solr).  All the [common query parameters](http://wiki.apache.org/solr/CommonQueryParameters) are supported.  The solr river is not meant to keep solr and elasticsearch in sync, that's why it automatically deletes itself on completion, so that the river doesn't start up again at every node restart. This is the default behaviour, which can be disabled through the `close_on_completion` parameter.   Installation ------------  Here is how you can easily create the river and index data from Solr, just providing the solr url and the query to execute:  ```sh curl -XPUT localhost:9200/_river/solr_river/_meta -d ' {     "type" : "solr",     "solr" : {         "url" : "http://localhost:8080/solr/",         "q" : "*:*"     } }' ```  All supported parameters are optional. The following example request contains all the parameters that are supported together with the corresponding default values applied when not present.  ```javascript {     "type" : "solr",     "close_on_completion" : "true",     "solr" : {         "url" : "http://localhost:8983/solr/",         "q" : "*:*",         "fq" : "",         "fl" : ""         "qt" : "",         "uniqueKey" : "id",         "rows" : 10     },     "index" : {         "index" : "solr",         "type" : "import",         "bulk_size" : 100,         "max_concurrent_bulk" : 10,         "mapping" : "",         "settings": ""     } } ```  The fq and fl parameters can be provided as either an array or a single value.  You can provide your own mapping while creating the river, as well as the index settings, which will be used when creating the new index if needed.  The index is created when not already existing, otherwise the documents are added to the existing one with the configured name.  The documents are indexed using the [bulk api](http://www.elasticsearch.org/guide/reference/java-api/bulk.html). You can control the size of each bulk (default 100) and the maximum number of concurrent bulk operations (default is 10). Once the limit is reached the indexing will slow down, waiting for one of the bulk operations to finish its work; no documents will be lost.  Transform documents ------------ Since version 1.0.3 it's possible to transform the documents via scripting. The feature works exactly as the [update api](http://www.elasticsearch.org/guide/reference/api/update.html). The needed parameters can be specified within the transform section while registering the river, like this:  ```javascript {     "type" : "solr",     "solr" : {         "url" : "http://localhost:8983/solr/",         "q" : "*:*",     },     "index" : {         "index" : "solr",         "type" : "import",     },     "transform" : {         "script" : "ctx._source.counter += count",         "params" : {             "count" : 4         }     } } ```  The example above increments by 4 the content of the counter field for every document right before the indexing process in elasticsearch. Note that dynamic scripting needs to be enabled for the above to work.  Limitations ------------  * only stored fields can be retrieved from Solr, therefore indexed in elasticsearch * the river is not meant to keep elasticsearch in sync with Solr, but only to import data once. It's possible to register the river multiple times in order to import different sets of documents though, even from different solr instances. * it's recommended to create the [mapping](http://www.elasticsearch.org/guide/reference/mapping/index.html) given the existing solr schema in order to apply the correct text analysis while importing the documents. In the future there might be an option to auto generating it from the Solr schema.  License =======  ``` This software is licensed under the Apache 2 license, quoted below.  Copyright 2015 Luca Cavanna  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```
huangchen007/elasticsearch-rest-command	##elasticsearch-rest-command  Restful pipeline command support plugin for Elasticsearch http://www.elasticsearch.org    ##语法详情请查看rest-command.ppt    v0.7.0(2014年12月2日)  增加/_task接口和/_taskstatus/{taskid}接口，目的是可以使用spark引擎和sql语法来执行较长时间的数据计算。    常用计算参数有：esTable,parTable,sql,masterAddress,memory  常用结果集参数有：  	默认结果集存储在es中：  	targetIndex：默认值为index-tasks  	targetType：默认值为type-XXXXXXXXXX  	或者结果集存储在hdfs上：  	targetPar：指定的hdfs地址  	  示例如下：  http://localhost:9200/_task?esTable=SOURCETYPE=cdr234 APN="CMNET" AND CID =41553&masterAddress=spark://evercloud134:7077&sql=select APN from esTable&memory=1g    http://localhost:9200/_task?esTable=SOURCETYPE=cdr234%20APN=%22CMNET%22%20AND%20CID%20=41553&parTable=hdfs://192.168.200.134:9000/evercloud/dnsparquetbig&masterAddress=spark://spark-work2:7077&sql=select%20domain%20from%20parTable&memory=1g        v0.6.2(2014年11月9日)  增加download2=true,导出的格式更加易于编程    v0.6.1(2014年11月4日)  修订了一个bug：当LIMIT=-1时，排序不生效    v0.6.0(2014年9月18日)  支持everdata0.5.0以上版本  为了统一语法，统计命令导出download参数取消   当LIMIT参数设置为-1时，LIMIT参数的值自动替换成基数估计出来值。    v0.5.3(2014年8月1日)  为了统一语法，取消采用(+/-)号来标示排序，启用ASC/DESC关键字来做为排序的标识。  未特殊指定顺序或者倒序的，默认均为倒序。  如：      STATS TIMESPAN=NA,1s COUNT(user) BY MSISDN ASC, postDate      SORT user DESC    取消KEYORDER关键字，启用COUNTORDER关键字。默认均为按照字段排序，特殊指定COUNTORDER，才采用结果数排序。注意，仅有分组字段才有COUNTORDER这个特性。  如：      STATS SUM(downflow) [ASC/DESC] BY MSISDN COUNTORDER        v0.5.2(2014年7月28日)  统计命令支持分页。from和size参数    统计命令支持导出。download=true      当download=true时，LIMIT参数失效，LIMIT参数的值自动替换成基数估计出来值。    STATS命令中的LIMIT参数生效作用域用逗号分隔      如：STATS LIMIT=100,200 SUM(downflow) BY MSISDN, IMEI      表示第一个分组域MSISDN的LIMIT的参数是100，第二个分组域IMEI的LIMIT的参数200  对于分组域为时间类型和间隔分组，LIMIT使用0作为占位符      如：STATS LIMIT=0,200 TIMESPAN=1s SUM(downflow) BY postDate, IMEI      v0.5.1(2014年7月24日)  增加可以按照统计字段排序      如：SOURCETYPE=twitter | STATS SUM(downflow) [ASC/DESC] BY (+/-)MSISDN KEYORDER    增加可以按照分组字段排序      如：SOURCETYPE=twitter | STATS COUNT(user) BY (+/-)MSISDN KEYORDER      +表示正序ASC, -表示倒序（默认）。KEYORDER表示使用MSISDN来排序，默认使用结果数来排序    增加可采用间隔作为分组条件，时间字段采用TIMESPAN限制，普通数字字段采用SPAN限制。若间隔字段在后，前面需要使用NA和0来占位。如下所示：      如：SOURCETYPE=twitter | STATS TIMESPAN=NA,1s COUNT(user) BY (+/-)MSISDN KEYORDER, postDate      如：SOURCETYPE=twitter | STATS SPAN=0,50 COUNT(user) BY (+/-)MSISDN KEYORDER, age    取消对TOP命令的支持，TOP语义可由STATS命令取代      v0.4.0(2014年6月30日)  修改为所有命令都必须大写  修正返回字段缺少id信息的bug    v0.3.6(2014年6月26日)  修正返回字段缺少id信息的bug    v0.3.5(2014年6月26日)  修复join命令netty线程死锁bug    v0.3.4(2014年6月16日)  修复了新版本中count统计选项不能正确返回结果的bug  新增table命令，过滤返回字段。      如：sourcetype=twitter | table user,comment      字段中支持通配符*来过滤，例如：sourcetype=twitter | table user*,comment*    v0.3.2(2014年6月13日)  仅支持everdata-0.4.0  search 支持多个haschild选项，这样支持针对parent表进行多个haschild条件的查询  新增stats命令去重dc统计选项      如：sourcetype=twitter | stats dc(msisdn) by domain  新增join命令      支持join <field-list> ( subsearch )      例子：index=comment foo | join user,name (search index=user)      修复统计报表中字段展示的bug    v0.2.4(2014年5月20日)  job endpoint支持timeline      localhost:9200/jobs/299450.5410349557/timeline?interval=1s&timelineField=accessTime, 使用jobid取时间线结果    v0.2.3(2014年5月8日)  stats命令增加了limit选项      例如 stats limit=50 sum(upflow) by user  index参数          过滤不存在的index，不然查询会失败      如果所有指定的index都不存在，那么将在“所有的index”针对条件进行查询        v0.2.1(2014年4月29日)  修改了输出的json格式化，更加易读和简单。  输出如下：      {"took":293,"total":1,"fields":["postDate","mm","_count"],"rows":[["1258294332000","3","3"]]}  新增job方式的访问      第一步：localhost:9200/_commandjob?q=<command>，返回{"jobid":"299450.5410349557"}      第二步：          localhost:9200/jobs/299450.5410349557/query?from=0&size=50, 使用jobid取查询结果          localhost:9200/jobs/299450.5410349557/report?from=0&size=50, 使用jobid取统计结果  版本号的发布规定      当最后一个小版本号为奇数时，该版本为测试版，如v0.2.1      当最后一个小版本号为偶数时，该版本为稳定版，如v0.2.2    v0.1.10(2014年4月25日)  新增命令选项      Top和Stats均支持minicount选项      如：sourcetype=twitter | top minicount=3 user by pid      如：sourcetype=twitter | stats minicount=3 sum(upflow) by user    V0.1.9 (2014年4月21日)  新增命令      Sort(http://docs.splunk.com/Documentation/Splunk/6.0.3/SearchReference/Sort)      如：sourcetype=twitter | sort -user（-代表倒序，+代表正序）    V0.1.6 (2014年4月15日)  新增命令      Top(http://docs.splunk.com/Documentation/Splunk/6.0.2/SearchReference/Top)      如：sourcetype=twitter | top limit=10 user    V0.1.5（2014年4月10日）  新增命令  	新增stats命令，如stats count,sum() by fieldlist，完整命令如： sourcetype=twitter | stats count,sum(post_number) by user  	支持简单脚本计算，如： sourcetype=twitter | stats count,sum("doc['upflow'].value+doc['downflow'].value") by user,postDate    V0.1.1 （2014年4月4日）  新增命令      新增has_parent查询命令_command?q=hasparent=(sourcetype=<parentType> <logicalexpression>)      新增has_child查询命令_command?q=haschild=(sourcetype=<childType> <logicalexpression>)      新增基于入库时间戳结果过滤命令_command?q=starttime=<timeformat=yyyy-MM-dd HH:mm:ss> endtime=<timeformat=yyyy-MM-dd HH:mm:ss>    新增查询参数      分页参数_command?from=<int>&size=<int>      导出模式参数_command?download=<true>    新增command_head搜索UI      地址为_plugin/command_head      V0.1.0 （2014年4月1日）  新增命令模式进行查询      _command?q=<search>      <search>的语法可参考http://docs.splunk.com/Documentation/Splunk/6.0.2/SearchReference/Search
sdksdk0/es	# es elasticsearch+hbase海量数据查询,支持千万数据秒回查询  博客地址：http://blog.csdn.net/sdksdk0/article/details/53966430  一、ElasticSearch和Hbase ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 Elasticsearch的性能是solr的50倍。  HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩、 实时读写的分布式数据库 – 利用Hadoop HDFS作为其文件存储系统,利用Hadoop MapReduce来处理 HBase中的海量数据,利用Zookeeper作为其分布式协同服务 – 主要用来存储非结构化和半结构化的松散数据（列存 NoSQL 数据库）   二、需求分析&服务器环境设置 主要是做一个文章的搜索。有文章标题、作者、摘要、内容四个主要信息。效果图如下：这里样式我就没怎么设置了。。。。想要好看一点的可以自己加css。   服务器： 在3台centos7中部署，主机名为node1-node3.安装好ElasticSearch并配置好集群， 1.     解压 2.     修改config/elasticsearch.yml    (注意要顶格写，冒号后面要加一个空格) a)      Cluster.name: tf   (同一集群要一样) b)      Node.name： node-1  (同一集群要不一样) c)       Network.Host: 192.168.44.137  这里不能写127.0.0.1 3.     解压安装kibana 4.     再congfig目录下的kibana.yml中修改elasticsearch.url 5.     安装插件 Step 1: Install Marvel into Elasticsearch: bin/plugin install license bin/plugin install marvel-agent Step 2: Install Marvel into Kibana bin/kibana plugin --install elasticsearch/marvel/latest Step 3: Start Elasticsearch and Kibana bin/elasticsearch bin/kibana    启动好elasticsearch集群后， 然后启动zookeeper、hdfs、hbase。zkService.sh start  、start-all.sh、start-hbase.sh。 接下来就是剩下编码步骤了。    三、编码开发 1、首先在IntelliJ IDEA中新建一个maven工程，加入如下依赖。 [html] view plain copy print? <dependencies>           <dependency>               <groupId>junit</groupId>               <artifactId>junit</artifactId>               <version>4.9</version>           </dependency>                 <!-- spring 3.2 -->           <dependency>               <groupId>org.springframework</groupId>               <artifactId>spring-context</artifactId>               <version>3.2.0.RELEASE</version>           </dependency>           <dependency>               <groupId>org.springframework</groupId>               <artifactId>spring-orm</artifactId>               <version>3.2.0.RELEASE</version>           </dependency>           <dependency>               <groupId>org.springframework</groupId>               <artifactId>spring-aspects</artifactId>               <version>3.2.0.RELEASE</version>           </dependency>           <dependency>               <groupId>org.springframework</groupId>               <artifactId>spring-web</artifactId>               <version>3.2.0.RELEASE</version>           </dependency>           <dependency>               <groupId>org.springframework</groupId>               <artifactId>spring-webmvc</artifactId>               <version>3.2.0.RELEASE</version>           </dependency>           <dependency>               <groupId>org.springframework</groupId>               <artifactId>spring-test</artifactId>               <version>3.2.0.RELEASE</version>           </dependency>              <!-- JSTL -->           <dependency>               <groupId>jstl</groupId>               <artifactId>jstl</artifactId>               <version>1.2</version>           </dependency>           <dependency>               <groupId>taglibs</groupId>               <artifactId>standard</artifactId>               <version>1.1.2</version>           </dependency>           <!-- slf4j -->           <dependency>               <groupId>org.slf4j</groupId>               <artifactId>slf4j-api</artifactId>               <version>1.7.10</version>           </dependency>           <dependency>               <groupId>org.slf4j</groupId>               <artifactId>slf4j-log4j12</artifactId>               <version>1.7.10</version>           </dependency>              <!-- elasticsearch -->           <dependency>               <groupId>org.elasticsearch</groupId>               <artifactId>elasticsearch</artifactId>               <version>2.2.0</version>           </dependency>              <!-- habse -->           <dependency>               <groupId>org.apache.hbase</groupId>               <artifactId>hbase-client</artifactId>               <version>1.1.3</version>               <exclusions>                   <exclusion>                       <groupId>com.google.guava</groupId>                       <artifactId>guava</artifactId>                   </exclusion>               </exclusions>           </dependency>             </dependencies>    2、Dao层 [java] view plain copy print? private Integer id;   private String title;      private String describe;      private String content;      private String author;    实现其getter/setter方法。  3、数据准备 在桌面新建一个doc1.txt文档，用于把我们需要查询的数据写入到里面，这里我只准备了5条数据。中间用tab键隔开。    4、在hbase中建立表。表名师doc，列族是cf。  public static void main(String[] args) throws Exception {       HbaseUtils hbase = new HbaseUtils();       //创建一张表 	hbase.createTable("doc","cf"); }  /**  * 创建一张表  * @param tableName  * @param column  * @throws Exception  */ public void createTable(String tableName, String column) throws Exception {    if(admin.tableExists(TableName.valueOf(tableName))){       System.out.println(tableName+"表已经存在！");    }else{       HTableDescriptor tableDesc = new HTableDescriptor(TableName.valueOf(tableName));       tableDesc.addFamily(new HColumnDescriptor(column.getBytes()));       admin.createTable(tableDesc);       System.out.println(tableName+"表创建成功！");    } }   5、导入索引。这一步的时候确保你的hdfs和hbase以及elasticsearch是处于开启状态。 [java] view plain copy print? @Test     public void createIndex() throws Exception {         List<Doc> arrayList = new ArrayList<Doc>();         File file = new File("C:\\Users\\asus\\Desktop\\doc1.txt");         List<String> list = FileUtils.readLines(file,"UTF8");         for(String line : list){             Doc Doc = new Doc();             String[] split = line.split("\t");             System.out.print(split[0]);             int parseInt = Integer.parseInt(split[0].trim());             Doc.setId(parseInt);             Doc.setTitle(split[1]);             Doc.setAuthor(split[2]);             Doc.setDescribe(split[3]);             Doc.setContent(split[3]);             arrayList.add(Doc);         }         HbaseUtils hbaseUtils = new HbaseUtils();         for (Doc Doc : arrayList) {             try {                 //把数据插入hbase                 hbaseUtils.put(hbaseUtils.TABLE_NAME, Doc.getId()+"", hbaseUtils.COLUMNFAMILY_1, hbaseUtils.COLUMNFAMILY_1_TITLE, Doc.getTitle());                 hbaseUtils.put(hbaseUtils.TABLE_NAME, Doc.getId()+"", hbaseUtils.COLUMNFAMILY_1, hbaseUtils.COLUMNFAMILY_1_AUTHOR, Doc.getAuthor());                 hbaseUtils.put(hbaseUtils.TABLE_NAME, Doc.getId()+"", hbaseUtils.COLUMNFAMILY_1, hbaseUtils.COLUMNFAMILY_1_DESCRIBE, Doc.getDescribe());                 hbaseUtils.put(hbaseUtils.TABLE_NAME, Doc.getId()+"", hbaseUtils.COLUMNFAMILY_1, hbaseUtils.COLUMNFAMILY_1_CONTENT, Doc.getContent());                 //把数据插入es                 Esutil.addIndex("tfjt","doc", Doc);             } catch (Exception e) {                 e.printStackTrace();             }         }     }    数据导入成功之后可以在服务器上通过命令查看一下： curl -XGET http://node1:9200/tfjt/_search    7、搜索。 在这里新建了一个工具类Esutil.java,主要用于处理搜索的。注意，我们默认的elasticsearch是9200端口的，这里数据传输用的是9300，不要写成9200了，然后就是集群名字为tf，也就是前面配置的集群名。还有就是主机名node1-node3,这里不能写ip地址，如果是本地测试的话，你需要在你的window下面配置hosts文件。  [java] view plain copy print? public class Esutil {       public static Client client = null;              /**           * 获取客户端           * @return           */           public static  Client getClient() {               if(client!=null){                   return client;               }               Settings settings = Settings.settingsBuilder().put("cluster.name", "tf").build();               try {                   client = TransportClient.builder().settings(settings).build()                           .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("node1"), 9300))                           .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("node2"), 9300))                           .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("node3"), 9300));               } catch (UnknownHostException e) {                   e.printStackTrace();               }               return client;           }                                   public static String addIndex(String index,String type,Doc Doc){           HashMap<String, Object> hashMap = new HashMap<String, Object>();           hashMap.put("id", Doc.getId());           hashMap.put("title", Doc.getTitle());           hashMap.put("describe", Doc.getDescribe());           hashMap.put("author", Doc.getAuthor());                      IndexResponse response = getClient().prepareIndex(index, type).setSource(hashMap).execute().actionGet();           return response.getId();       }                     public static Map<String, Object> search(String key,String index,String type,int start,int row){           SearchRequestBuilder builder = getClient().prepareSearch(index);           builder.setTypes(type);           builder.setFrom(start);           builder.setSize(row);           //设置高亮字段名称           builder.addHighlightedField("title");           builder.addHighlightedField("describe");           //设置高亮前缀           builder.setHighlighterPreTags("<font color='red' >");           //设置高亮后缀           builder.setHighlighterPostTags("</font>");           builder.setSearchType(SearchType.DFS_QUERY_THEN_FETCH);           if(StringUtils.isNotBlank(key)){   //          builder.setQuery(QueryBuilders.termQuery("title",key));               builder.setQuery(QueryBuilders.multiMatchQuery(key, "title","describe"));           }           builder.setExplain(true);           SearchResponse searchResponse = builder.get();                      SearchHits hits = searchResponse.getHits();           long total = hits.getTotalHits();           Map<String, Object> map = new HashMap<String,Object>();           SearchHit[] hits2 = hits.getHits();           map.put("count", total);           List<Map<String, Object>> list = new ArrayList<Map<String, Object>>();           for (SearchHit searchHit : hits2) {               Map<String, HighlightField> highlightFields = searchHit.getHighlightFields();               HighlightField highlightField = highlightFields.get("title");               Map<String, Object> source = searchHit.getSource();               if(highlightField!=null){                   Text[] fragments = highlightField.fragments();                   String name = "";                   for (Text text : fragments) {                       name+=text;                   }                   source.put("title", name);               }               HighlightField highlightField2 = highlightFields.get("describe");               if(highlightField2!=null){                   Text[] fragments = highlightField2.fragments();                   String describe = "";                   for (Text text : fragments) {                       describe+=text;                   }                   source.put("describe", describe);               }               list.add(source);           }           map.put("dataList", list);           return map;       }      //  public static void main(String[] args) {   //      Map<String, Object> search = Esutil.search("hbase", "tfjt", "doc", 0, 10);   //      List<Map<String, Object>> list = (List<Map<String, Object>>) search.get("dataList");   //  }   }     8、使用spring控制层处理 在里面的spring配置这里就不说了，代码文末提供。 [java] view plain copy print? @RequestMapping("/search.do")   public String serachArticle(Model model,           @RequestParam(value="keyWords",required = false) String keyWords,           @RequestParam(value = "pageNum", defaultValue = "1") Integer pageNum,           @RequestParam(value = "pageSize", defaultValue = "3") Integer pageSize){       try {           keyWords = new String(keyWords.getBytes("ISO-8859-1"),"UTF-8");       } catch (UnsupportedEncodingException e) {           e.printStackTrace();       }       Map<String,Object> map = new HashMap<String, Object>();       int count = 0;       try {           map = Esutil.search(keyWords,"tfjt","doc",(pageNum-1)*pageSize, pageSize);           count = Integer.parseInt(((Long) map.get("count")).toString());       } catch (Exception e) {           logger.error("查询索引错误!{}",e);           e.printStackTrace();       }       PageUtil<Map<String, Object>> page = new PageUtil<Map<String, Object>>(String.valueOf(pageNum),String.valueOf(pageSize),count);       List<Map<String, Object>> articleList = (List<Map<String, Object>>)map.get("dataList");       page.setList(articleList);       model.addAttribute("total",count);       model.addAttribute("pageNum",pageNum);       model.addAttribute("page",page);       model.addAttribute("kw",keyWords);       return "index.jsp";   }     9、页面  [java] view plain copy print? <center>   <form action="search.do" method="get">     <input type="text" name="keyWords" />     <input type="submit" value="百度一下">     <input type="hidden" value="1" name="pageNum">   </form>   <c:if test="${! empty page.list }">   <h3>百度为您找到相关结果约${total}个</h3>   <c:forEach items="${page.list}" var="bean">     <a href="/es/detailDocById/${bean.id}.do">${bean.title}</a>     <br/>     <br/>     <span>${bean.describe}</span>     <br/>     <br/>   </c:forEach>      <c:if test="${page.hasPrevious }">     <a href="search.do?pageNum=${page.previousPageNum }&keyWords=${kw}"> 上一页</a>   </c:if>   <c:forEach begin="${page.everyPageStart }" end="${page.everyPageEnd }" var="n">     <a href="search.do?pageNum=${n }&keyWords=${kw}"> ${n }</a>      </c:forEach>      <c:if test="${page.hasNext }">     <a href="search.do?pageNum=${page.nextPageNum }&keyWords=${kw}"> 下一页</a>   </c:if>   </c:if>   </center>    10、项目发布 在IntelliJ IDEA 中配置好常用的项目，这里发布名Application context名字为es，当然你也可以自定义设置。      最终效果如下：搜索COS会得到结果，速度非常快。
jprante/elasticsearch-oai	![OAI-PMH](https://github.com/jprante/elasticsearch-oai/raw/master/src/site/resources/OA200.gif)  Image Copyright (C) [OAI](http://www.openarchives.org/)  # OAI Harvester for Elasticsearch [![Travis](https://travis-ci.org/jprante/elasticsearch-oai.png)](https://travis-ci.org/jprante/elasticsearch-oai)  The [Open Archives Initiative - Protocol for Metadata Harvesting (OAI-PMH)](http://www.openarchives.org/pmh/) client allows to harvest metadata into Elasticsearch.  This application for Elasticsearch can run as a feeder in a standalone JVM, and connects to a remote cluster.  It harvests DC / XML / RDF formats from OAI data providers (OAI servers).  A list of OAI servers can be found [here](http://www.openarchives.org/Register/BrowseSites) The metadata is internally represented as resources, using Resource Description Framework (RDF) of the W3C Semantic Web Initiative, before the resources are serialized into JSON for Elasticsearch schema-less indexing.  ## Versions  | Elasticsearch version    | Plugin     | Release date | | ------------------------ | -----------| -------------| | 2.1.0                    | 2.1.0.0    | Dec  2, 2015 | | 1.5.1                    | 1.5.1.0    | Apr 22, 2015 | | 1.2.1                    | 1.2.1.0    | Jun 10, 2014 | | 1.1.0                    | 1.1.0.0    | Apr 24, 2014 | | 1.1.0                    | 1.1.0.1    | May 10, 2014 |  ## Installation      mkdir -p lib bin     cd lib     curl -O 'xbib.org/repository/org/xbib/elasticsearch/plugin/elasticsearch-oai/2.1.0.0/elasticsearch-oai-2.1.0.0-standalone.jar'     cd ..     <create a bash feed.sh script in bin folder>     <add log4j2.xml to bin folder>     ./bin/feed.sh  ## Project docs  The Maven project site is available at [Github](http://jprante.github.io/elasticsearch-oai)  ## Issues  All feedback is welcome! If you find issues, please post them at [Github](https://github.com/jprante/elasticsearch-oai/issues)  # Documentation  A feeder is a standalone application that can push data into a remote Elasticsearch cluster and runs outside an Elasticsearch node. This push mode is similar to Logstash, which is a data pipeline tool that can prepare event-based data for Elasticsearch.  You can create a feeder script for example for indexing the Arxiv repository:      ./bin/arxiv.sh  where the shell script has the content::       #!/bin/bash      # cron?     tty -s     if [ "$?" -gt "0" ]     then         # assume this working dir in $HOME         cd $HOME/oai-tool         pwd=$(pwd)         bin=${pwd}/bin         lib=${pwd}/lib     else         pwd="$( cd -P "$( dirname "$0" )" && pwd )"         bin=${pwd}/../bin         lib=${pwd}/../lib     fi      java="java"      # arxiv.org is throttling to 20sec by HTTP Status 503 retry-after.     # concurrency should be 1.      echo '     {         "uri" : [             "http://export.arxiv.org/oai2?verb=ListRecords&metadataPrefix=arXiv&from=2000-01-01&until=2016-01-01"         ],         "concurrency" : 1,         "elasticsearch" : {             "cluster" : "elasticsearch",             "host" : "localhost",             "port" : 9300         },         "index" : "arxiv",         "type" : "arxiv",         "maxbulkactions" : 1000,         "maxconcurrentbulkrequests" : 1,         "mock" : true,         "timewindow" : "yyyyMMddHH",         "aliases" : true,         "ignoreindexcreationerror" : true      }     ' | ${java} \         -cp ${lib}/\*:${bin}/\* \         -Dlog4j.configurationFile=${bin}/log4j2.xml \         org.xbib.tools.Runner \         org.xbib.tools.OAIFeeder   Before running, please check where your Java 8 installation is located, and fix the ``java`` variable setting.  More examples can be found in the `bin` folder of the repository.  ## Logging  The logging can be controlled by the `log4j2.xml` file in the bin folder.  ## Parameters  `uri` - a list of URLs for harvesting  `concurrency` - how many URLs should be processed simultaneously  `elasticsearch` - connection data for a node in an Elasticsearch cluster  `index` - the name of the Elasticsearch index  `type` - the name of the Elasticsearch index type  `maxbulkactions` - the maximum number of actions in a bulk request  `maxconcurrentbulkrequests` - the maximum number of concurrent bulk requests  `mock` - set to `true` if harvested docs should not be indexed but logged instead  `timewindow` - appendix for index name if date-stamp is wanted, like `yyyyMMddHH`  `aliases` - set index aliases automatically   `ignoreindexcreationerror` - if true, do not fail with error when index already exists  ## License  Elasticsearch OAI Harvester  Copyright (C) 2014 Jörg Prante  This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details.  You should have received a copy of the GNU Affero General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.
alexcojocaru/elasticsearch-maven-plugin	# Elasticsearch Maven Plugin [![Build Status](https://travis-ci.org/alexcojocaru/elasticsearch-maven-plugin.png?branch=master)](https://travis-ci.org/alexcojocaru/elasticsearch-maven-plugin)  A Maven 3.1+ plugin to run instances of Elasticsearch version 5+ during the integration test phase of a build. Instances are started in forked processes using the **runforked** goal. They are terminated using the **stop** goal and, for extra peace of mind, using a JVM shutdown hook.  Each instance is installed in _${project.build.directory}/elasticsearch${instanceIndex}_.  For Elasticsearch version 1.x.x and 2.x.x support, see version 1.x and 2.x of this plugin.  Because the plugin uses the new [Eclipse based Aether framework](https://www.eclipse.org/aether/), it only works with Maven 3.1.0 and above. See [this discussion](https://github.com/alexcojocaru/elasticsearch-maven-plugin/issues/28) for details.  ## Usage The Elasticsearch behaviour and properties can be configured through the following plugin configuration parameters:  *   **instanceCount** [defaultValue=1]     > how many Elasticsearch instances to start (all within the same cluster)  *   **skip** [defaultValue=false]     > whether to skip the plugin execution or not  *   **clusterName** [defaultValue="test"]     > the name of the cluster to create  *   **version** [defaultValue="5.0.0"]     > the version of Elasticsearch to install  *   **httpPort** [defaultValue=9200]     > the port to configure Elasticsearch to listen to HTTP traffic to; when configuring multiple instances, they will be assigned subsequent HTTP ports starting with this value (mind the conflicts with the transport ports)  *   **transportPort** [defaultValue=9300]     > the port for the Elasticsearch node to node communication; when configuring multiple instances, they will be assigned subsequent transport ports starting with this value (mind the conflicts with the HTTP ports)  *   **pathConf** [defaultValue=""] (note: common to all instances !!!)     > the absolute path (or relative to the maven project) of the custom directory containing configuration files, to be copied to Elasticsearch instances  *   **pathData** [defaultValue=""] - work in progress (note: per instance !!!)     > the custom data directory to configure in Elasticsearch  *   **pathLogs** [defaultValue=""] - work in progress (note: per instance !!!)     > the custom logs directory to configure in Elasticsearch  *   **plugins** [defaultValue=""]     > the list of plugins to install in each Elasticsearch instance before starting it (see the [Plugins](#plugins) section for details)  *   **pathInitScript** [defaultValue=""]     > the path of the initialization script (see the [Initialization script](#initScript) section for details)  *   **keepExistingData** [defaultValue=false] - work in progress     > whether to keep the data and log directories if they already exist  *   **timeout** [defaultValue=30]     > how long to wait (in seconds) for each Elasticsearch instance to start up  *   **setAwait** [defaultValue=false]     > whether to block the execution once all Elasticsearch instances have started, so that the maven build will not proceed to the next step; use CTRL+C to abort the process  *   **autoCreateIndex** [defaultValue=true]     > configuration of automatic index creation represented by _action.auto\_create\_index_ setting  *   **logLevel** [defaultValue=INFO]     > the log level to be used by the console logger; the valid values are defined in AbstractElasticsearchBaseMojo.getMavenLogLevel() and they are: DEBUG, INFO, WARN, ERROR, FATAL, DISABLED.   To use the plugin, include the following in your _pom.xml_ file and modify the configuration as needed:  ```xml <plugin>     <groupId>com.github.alexcojocaru</groupId>     <artifactId>elasticsearch-maven-plugin</artifactId>     <!-- REPLACE THE FOLLOWING WITH THE PLUGIN VERSION YOU NEED -->     <version>5.11</version>     <configuration>         <!-- REPLACE THE FOLLOWING WITH THE ELASTICSEARCH VERSION YOU NEED -->         <version>5.5.2</version>         <clusterName>test</clusterName>         <transportPort>9300</transportPort>         <httpPort>9200</httpPort>     </configuration>     <executions>         <!--             The elasticsearch maven plugin goals are by default bound to the             pre-integration-test and post-integration-test phases         -->         <execution>             <id>start-elasticsearch</id>             <phase>pre-integration-test</phase>             <goals>                 <goal>runforked</goal>             </goals>         </execution>         <execution>             <id>stop-elasticsearch</id>             <phase>post-integration-test</phase>             <goals>                 <goal>stop</goal>             </goals>         </execution>     </executions> </plugin> ```   ## <a name="plugins"></a>Plugins A list of Elasticsearch plugins can be provided to the elasticsearch-maven-plugin. They will be installed into [each Elasticsearch instance](https://www.elastic.co/guide/en/elasticsearch/plugins/5.2/intro.html) inside the [plugins directory](https://www.elastic.co/guide/en/elasticsearch/reference/5.2/zip-targz.html#zip-targz-layout) using the [--batch option](https://www.elastic.co/guide/en/elasticsearch/plugins/5.2/_other_command_line_parameters.html#_batch_mode), before the instance gets started.  The way to enable plugins is as follows:  ```xml <plugin>     <groupId>com.github.alexcojocaru</groupId>     <artifactId>elasticsearch-maven-plugin</artifactId>     <version>5.11</version>     <configuration>         <clusterName>test</clusterName>         <transportPort>9300</transportPort>         <httpPort>9200</httpPort>         ...         <plugins>             <plugin>                 <uri>analysis-icu</uri>             <plugin>             <plugin>                 <uri>https://github.com/alexcojocaru/plugin.zip</uri>                 <esJavaOpts>-Djavax.net.ssl.trustStore=/home/alex/trustStore.jks</esJavaOpts>             <plugin>             <plugin>                 <uri>file:///home/alex/foo.zip</uri>             <plugin>         </plugins>         ...     </configuration>     <executions>         ...     </executions> </plugin> ```  The plugin tag takes 2 parameters:  *   **uri**     > the [name](https://www.elastic.co/guide/en/elasticsearch/plugins/5.2/installation.html),     [url](https://www.elastic.co/guide/en/elasticsearch/plugins/5.2/plugin-management-custom-url.html)     or [file location](https://www.elastic.co/guide/en/elasticsearch/plugins/5.2/plugin-management-custom-url.html)     of the plugin  *   **esJavaOpts** [defaultValue=""]     > [additional Elasticsearch Java options](https://www.elastic.co/guide/en/elasticsearch/plugins/5.2/plugin-management-custom-url.html)     to be passed to the plugin installation tool when installing the plugin   ## <a name="initScript"></a>Initialization script An initialization script file can be provided using the **pathInitScript** parameter of the plugin, in which case it will be executed against the local Elasticsearch cluster.  Empty lines are ignored, as well as lines starting with the '#' sign.  Each command has three parts, separated by colon.  * the request method: one of *PUT*, *POST*, *DELETE*     > the name (in uppercase) of the request method to be used for the current command  * the path part of the URL (should not start with slash)     > will be appended to the protocol, hostname and port parts when the full URL is constructed  * the JSON to send to Elasticsearch; for DELETE commands it should be empty for a DELETE   **Examples** (see the *src/it/runforked-with-init-script/init.script* file for a more complete example):  * To send a *POST* request to *http://localhost:9200/test\_index/test\_type/\_mapping*: > POST:test\_index/test\_type/\_mapping:{ "test\_type" : { "properties" : { "name" : { "type" : "keyword" }, "lastModified" : { "type" : "date" } } } }  * To send a *DELETE* request to *http://localhost:9200/test\_index/test\_type/1* without content; note the colon at the end, for there is no JSON data in case of a DELETE. > DELETE:test\_index/test\_type/1:  ## FAQ  #### Node is killed when running in TravisCI When running your build job in [TravisCI](https://travis-ci.org/), it can happen that your node is being killed without any notice. To fix that you may have to modify the `.travis.yml` file as follows:  ```yml sudo: true before_script:   - sudo sysctl -w vm.max_map_count=262144 ```  #### Avoid downloading a plugin from the Internet repeatedly When you want to run integration tests with a given plugin (eg. reindex-client), elasticsearch-maven-plugin will run behind the scene a command like `bin/elasticsearch-plugin install reindex-client` which will download the plugin from the Internet at every execution.  You can use some Maven magic to avoid the download by first using `maven-dependency-plugin` to download the plugin as an artifact which will be stored in your local `.m2` directory, then copy from there to your project target directory, eg. ``` mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get \     -DrepoUrl=https://repo1.maven.org/maven2 \     -Dartifact=org.elasticsearch.plugin:reindex-client:5.4.2 ```  Then just tell the elasticsearch-maven-plugin to use the local URI.  ```xml <plugin>     <groupId>org.apache.maven.plugins</groupId>     <artifactId>maven-dependency-plugin</artifactId>     <version>3.0.0</version>     <executions>         <execution>             <id>integ-setup-dependencies-plugins</id>             <phase>pre-integration-test</phase>             <goals>                 <goal>copy</goal>             </goals>             <configuration>                 <artifactItems>                     <artifactItem>                         <groupId>org.elasticsearch.plugin</groupId>                         <artifactId>reindex-client</artifactId>                         <version>5.4.2</version>                         <type>zip</type>                     </artifactItem>                 </artifactItems>                 <useBaseVersion>true</useBaseVersion>                 <outputDirectory>${project.build.directory}/integration-tests/plugins</outputDirectory>             </configuration>         </execution>     </executions> </plugin> <plugin>     <groupId>com.github.alexcojocaru</groupId>     <artifactId>elasticsearch-maven-plugin</artifactId>     <version>5.7</version>     <configuration>         <version>5.4.2</version>         <plugins>             <plugin>                 <uri>file://${project.build.directory}/integration-tests/plugins/reindex-client-5.4.2.zip</uri>             </plugin>         </plugins>     </configuration> </plugin> ```  ## Integration Tests The integration tests exist as maven plugins in the src/it directory and are executed via the maven invoker plugin (see the pom.xml for details on its configuration).  Each test has a maven like structure, with a [pom.xml](#it/pom.xml) and a [src/test/java](#it/java) directory containing the test sources.  The properties common between all tests are defined in the invoker plugin config under the "testProperties" tag.  During the integration test phase, the invoker plugin copies the tests to target/it and executes the "clean" and "verify" goals on each them. They are executed in a separate process with a brand new maven config, defined using the following two invoker plugin properties: localRepositoryPath (set to target/local-repo) and settingsFile (set to src/it/settings.xml). The invoker configuration also defines a [pre-build hook script](#it/setup.groovy) and a [post-build hook script](#it/verify.groovy) to run before and after executing the test. These are groovy scripts which each test directory must contain.  #### The structure of an integration test Because the integration tests are executed as maven projects, they have a maven-like file structure.  ###### <a name="it/pom.xml"></a>pom.xml The pom.xml is generic and does not contain anything specific to any test - it defines the test project dependencies and which goals to execute on the elasticsearch maven plugin.  ###### <a name="it/setup.groovy"></a>setup.groovy The pre-build hook script (setup.groovy) does the plugin and context configuration, by using the ItSetup util to create a map of plugin properties and to save them to the test.properties file in the test directory (to be picked up by the Java tests via the methods in ItBase). The properties are also set on the context, for some are needed by the [post-build hook script](#it/verify.groovy). Defining the number of ES instances is required in the groovy script. The ES cluster name and the HTTP and transport protocol ports are randomly generated by ItSetup.generateProperties to avoid clashes between integration tests. Any other properties to be passed over to the plugin can be added to the props map (see src/it/runforked-auto-create-index-false/setup.groovy for an example).  ###### <a name="it/verify.groovy"></a>verify.groovy The standard verification done here is that the ES base directory(ies) were created and that the ES instance(s) are not running (via the ItVerification util). This file uses some of the plugin properties set on the context by the [pre-build hook script](#it/setup.groovy).  ###### <a name="it/java"></a>Java test file(s) The actual tests are defined in java files in the src/test/java directory under each integration test directory (eg. src/it/runforked-auto-create-index-false/src/test/java/com/github/alexcojocaru/mojo/elasticsearch/v2/AutoCreateIndexTest.java). They will be compiled and executed during the maven invoker plugin execution of the integration test maven project. All java tests should extend com.github.alexcojocaru.mojo.elasticsearch.v2.ItBase to get the clusterName and httpPort read from the context (ie. the "test.properties" file created by the [pre-build hook script](#it/setup.groovy)) and the ES client set up.  *NOTE: It is not possible to execute such a test case in an IDE, due to the lack of context (the test properties must be set in the props file by executing the groovy script, the elasticsearch maven plugin must be running, etc).*   #### How to write new tests Copy one of the existing integration tests and modify as needed. It will be picked up by the invoker plugin due to the wildcard definition in the plugin config in pom.xml.  #### How to run single integration test Change the pomInclude definition to the relative path of the pom.xml of the test you want to run, eg. ```xml <pomIncludes>     <pomInclude>runforked-defaults/pom.xml</pomInclude> </pomIncludes> ```  #### How to debug an integration test There are two ways to obtain more information during the execution of an integration test:  ###### Debugging the maven execution To have the invoker plugin output detailed information about the integration test execution, change the `debug` attribute of the invoker plugin configuration (in the pom.xml) to `true`.  ###### Debugging the elasticsearch maven plugin execution during the integration test Set the es.logLevel property of the plugin to `DEBUG`, by adding ```java props.put("es.logLevel", "DEBUG"); ``` to the [setup.groovy](#it/setup.groovy) file for the integration test you want to debug.
opendatasoft/elasticsearch-plugin-geoshape	# Elasticsearch geo shape plugin  This geo shape plugin can be used to index and aggregate geo shapes in elasticsearch.  This plugin adds a `geo` mapping type, a `geoshape` aggregation, a `geohash_clustering` aggregation and a `geo` REST entry point.  Installation ------------  ``` bin/plugin --install geoshape-plugin --url "https://github.com/opendatasoft/elasticsearch-plugin-geoshape/releases/download/v1.6.0.5/elasticsearch-geo-plugin-1.6.0.5.zip" ```  | elasticsearch  | Geoshape Plugin     | |----------------|---------------------| | 1.6.0          | 1.6.0.5             | | 1.5.2          | 1.5.2.4             | | 1.4.5          | 1.4.5.2             |  ### Geo mapping type  `geo` mapping can replace `geo_shape` type. It parses `GeoJSON` shapes, and indexes them in elasticsearch. Compared to geo_shape type it adds extra sub fields:  - wkb : indexed wkb field, used to aggregate shapes  - type : geo shape type (Polygon, point, LineString, ...) for searching on a specific type  - area : area of Shape  - bbox : geoPoint array containing topLeft and bottomRight points of shape envelope  - hash : shape digest to perform exact request on shape  - centroid : geoPoint representing shape centroid  Geo mapper computes, depending on shape length, a specific precision for each shape (corresponding to `precision` parameter). This precision can be boosted with `boost_precision` parameter.  Its parameters are slightly different than geoshape ones:  - tree : geohash or quadtree  - distance_error_pct : same as geo_shape type  - boost_precision : double that can be used to boost shape precision. Between 0 and 1, defaults to 0.  ### Geoshape aggregation  `geoshape` aggregation is a multi-bucket aggregation that returns a bucket for each shape.  `geoshape` parameters are :  - field : a wkb field. (Maybe change to `geo` type field instead)  - size : restrict number of results  - output_format : define output shape format ('wkt', 'wkb', 'geojson'). Defaults to geojson  - simplify : used to simplify aggregated shapes. Takes a parameter dict   - zoom : level of zoom. Mandatory in simplify dict   - algorithm : algorithm used for shape simplification (DOUGLAS_PEUCKER, TOPOLOGY_PRESERVING). Default to DOUGLAS_PEUCKER  - clipped : used to return a shape that are clipped to a defined envelope. Take a dict. WARNING, when used, zoom must be set in simplify in order to work (will be fixed in a future release)   - envelope : elasticsearch envelope where shapes must be clipped on. Mandatory when clipped is used   - buffer : number of pixels added to envelope for clipping  Example of use : ``` {   "aggs": {     "geo": {       "geoshape": {         "field": "geo_shape.wkb",         "output_format": "wkt",         "simplify": {           "zoom": 8,           "algorithm": "DOUGLAS_PEUCKER"         }       }     }   } } ```  Result :  ``` {   "aggregations": {     "geo": {       "buckets": [       {          "key": "POINT (2.2538285063 48.865022534)",          "digest": "4521908731506274962",          "type": "Point",          "doc_count": 1       },       {          "key": "POINT (2.31333976248 48.8652536076)",          "digest": "-3513121227624068596",          "type": "Point",          "doc_count": 1       },       {          "key": "POINT (2.2529555706 48.846044762)",          "digest": "-5055786055234076365",          "type": "Point",          "doc_count": 1       },       {          "key": "POINT (2.25320074406 48.867043584)",          "digest": "167833499021969215",          "type": "Point",          "doc_count": 1       },       {          "key": "POINT (2.4126175672099994 48.8333849101)",          "digest": "7300553048122261648",          "type": "Point",          "doc_count": 1       },       {          "key": "POINT (2.2924448277 48.8619170093)",          "digest": "-2232618493206154845",          "type": "Point",          "doc_count": 1       }     }   } } ```  ### Geohash clustering aggregation  This aggregations computes a geohash precision from a `zoom` and a `distance` (in pixel). It groups points (from `field` parameter) into buckets that represent geohash cells and computes each bucket's center. Then it merges these cells if the distance between two clusters' centers is lower than the `distance` parameter.  ```json {   "aggregations": {     "<aggregation_name>": {       "geohash_clustering": {         "field": "<field_name>",         "zoom": "<zoom>"       }     }   } } ``` Input parameters :  - `field` must be of type geo_point.  - `zoom` is a mandatory integer parameter between 0 and 20. It represents the zoom level used in the request to aggregate geo points.  The plugin aggregates these points in geohash with a "good" precision depending on the zoom provided. Then it merges clusters based on distance (in pixels). Default distance is set to 100, but it can be set to another integer in the request.  For example :  ```json {     "aggregations" : {         "my_cluster_aggregation" : {             "geohash_clustering": {                 "field": "geo_point",                 "zoom": 0,                 "distance": 50             }         }     } } ```  ```json {     "aggregations": {          "my_cluster_aggregation": {             "buckets": [                {                   "key": "u0",                   "doc_count": 90293,                   "geohash_grids": [                      [                         "u0"                      ]                   ],                   "cluster_center": {                      "type": "point",                      "coordinates": [                         2.32920361762,                         48.8449899502                      ]                   }                }             ]          }     }  } ```  ### Geo tile REST entry  This entry point generates "smart" results for a specific geo tile. The main purpose of this REST entry point is to return a tile that can be easily rendered with mapping applications like `mapnik`.  Format is based on TMS format : /{index}/{type}/_geo/{zoom}/{x}/{y} For more information about this format : http://wiki.openstreetmap.org/wiki/Slippy_map_tilenames  It takes parameters that can passed as `GET` parameters or in request body:  - `field` (mandatory): `geo` field name. Must be of type `geo`  - `tile_size` : tile size in pixel. Defaults to 256  - `output_format` :  'wkt', 'wkb' or 'geojson'. Defaults to geojson  - `output_projection` : projection to apply on result geometries. Defaults to 'EPSG:4326'  - `aggs` or `aggregations` (only on request body) : Performs elasticsearch aggregations https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html. Aggregations results are added for each shape object in an `aggregations` field.  It returns :  - shape in wkt, wkb or geojson  - digest : geoshape digest (in order to perform search request in it)  - doc_count : number of identical shapes  - cluster_count : number of shapes in this bucket. Small shapes aggregation only.  - grid : geohash grid for this bucket. Small shapes aggregation only.  Example :  `curl -XGET 'localhost:9200/my_index/geo_type/_geo/12/1036/704' -d '{"field": "geo_field", "aggs": {"term": {"terms":{"field": "term_field"}}}}'`  ``` {    "time": 12,    "count": 40,    "shapes": [       {          "shape": "POINT (2.26174532022 48.8633555664)",          "digest": "5980304178196219950",          "type": "Point",          "doc_count": 6,          "cluster_count": 6,          "grid": "u09tgr",          "aggregations": {             "term": {                 ...             }          }       },       {          "shape": "POINT (2.24587597613 48.8691433358)",          "digest": "9079382294515706678",          "type": "Point",          "doc_count": 5,          "cluster_count": 5,          "grid": "u09w50",          "aggregations": {             "term": {                 ...             }          }       },       {          "shape": "POINT (2.29329076205 48.8577766649)",          "digest": "6585224492059141239",          "type": "Point",          "doc_count": 3,          "cluster_count": 3,          "grid": "u09tun",          "aggregations": {             "term": {                 ...             }          }       },       {          "shape": "POINT (2.2599223736999994 48.8606198209)",          "digest": "1243503675762065766",          "type": "Point",          "doc_count": 3,          "cluster_count": 3,          "grid": "u09tgq",          "aggregations": {             "term": {                 ...             }          }       },       {          "shape": "POINT (2.25120424857 48.864782427799994)",          "digest": "2446849682065168706",          "type": "Point",          "doc_count": 3,          "cluster_count": 3,          "grid": "u09tgp",          "aggregations": {             "term": {                 ...             }          }       },       {          "shape": "POINT (2.25293802515 48.8471789821)",          "digest": "4881855873819433358",          "type": "Point",          "doc_count": 3,          "cluster_count": 3,          "grid": "u09tgk",          "aggregations": {             "term": {                 ...             }          }       }    ] } ```  ## Geo Simplify (geo shape simplification script)  This script adds a generated field containing the simplified geoshape for the requested field.  It takes parameters that can passed in a `POST` request body:  - `field` (mandatory): `geo` field name. Must be of type `geo`  - `zoom` (mandatory) : the zoom level for simplification, 1 giving the most simplified result  - `output_format` : format of the output, can be `geojson` (default), `wkt` or `wkb`  - `algorithm` : algorithm used for simplification, can be `douglas_peucker` (default) or `topology_preserving`  Example:  ``` # POST /test_index_geo/records/_search  {   "script_fields": {     "simplified": {       "script": "geo_simplify",       "lang":"native",       "params": {         "field": "geo_shape",         "zoom": 3       }     }   } }  # Response  {   "took": 89,   "timed_out": false,   "_shards": {     "total": 5,     "successful": 5,     "failed": 0   },   "hits": {     "total": 3,     "max_score": 1,     "hits": [       {         "_index": "test_index_geo",         "_type": "records",         "_id": "AVrxb8fMCjSOdtKpLj4q",         "_score": 1,         "fields": {           "simplified": [             {               "real_type": "Point",               "geom": "{\"type\":\"Point\",\"coordinates\":[-14,20]}",               "type": "Point"             }           ]         }       },       {         "_index": "test_index_geo",         "_type": "records",         "_id": "AVrxcBv4CjSOdtKpLj4r",         "_score": 1,         "fields": {           "simplified": [             {}           ]         }       },       {         "_index": "test_index_geo",         "_type": "records",         "_id": "AVrsDq2oNWoYxp2XjII_",         "_score": 1,         "fields": {           "simplified": [             {               "real_type": "Polygon",               "geom": "{\"type\":\"Polygon\",\"coordinates\":[[[-84.22119140625,34.985003130171066],[-84.32281494140625,34.9895035675793],[-84.29122924804688,35.21981940793435],[-84.04266357421875,35.27701633139884],[-84.01931762695312,35.41479572901859],[-83.88473510742186,35.516578738902936],[-83.49746704101562,35.563512051219696],[-82.96737670898438,35.793310688351724],[-82.91244506835938,35.92353244718235],[-82.69546508789062,36.04465753921525],[-82.61306762695312,36.060201412392914],[-82.5677490234375,35.951329861522666],[-82.35488891601562,36.117908916563685],[-82.03628540039062,36.12900165569652],[-81.86325073242188,36.33504067209607],[-81.70669555664062,36.33504067209607],[-81.68197631835938,36.58686302344181],[-75.79605102539062,36.54936246839778],[-75.3936767578125,35.639441068973916],[-75.487060546875,35.18727767598896],[-75.9210205078125,35.04798673426734],[-76.53076171875,34.53371242139567],[-76.761474609375,34.63320791137959],[-77.376708984375,34.45674800347809],[-77.5909423828125,34.3207552752374],[-77.9754638671875,33.73804486328907],[-78.11279296875,33.8521697014074],[-78.4808349609375,33.815666308702774],[-79.6728515625,34.8047829195724],[-80.782470703125,34.836349990763864],[-80.9307861328125,35.092945313732635],[-81.0516357421875,35.02999636902566],[-81.0516357421875,35.137879119634185],[-82.40295410156249,35.22318504970181],[-83.1060791015625,35.003003395276714],[-84.22119140625,34.985003130171066]],[[-75.5914306640625,35.74205383068037],[-75.706787109375,35.74205383068037],[-75.706787109375,35.634976650677295],[-76.0858154296875,35.29943548054543],[-76.4208984375,35.25907654252574],[-76.5252685546875,35.10642805736423],[-76.2066650390625,34.994003757575776],[-75.56396484375,35.32633026307483],[-75.5914306640625,35.74205383068037]]]}",               "type": "Polygon"             }           ]         }       }     ]   } } ```
karussell/ElasticSearchExample	Example using ElasticSearch with a bit Wicket
graylog-labs/graylog-plugin-metrics-reporter	# Graylog Metrics Reporter plugins  [![Github Downloads](https://img.shields.io/github/downloads/Graylog2/graylog-plugin-metrics-reporter/total.svg)](https://github.com/Graylog2/graylog-plugin-metrics-reporter/releases) [![GitHub Release](https://img.shields.io/github/release/Graylog2/graylog-plugin-metrics-reporter.svg)](https://github.com/Graylog2/graylog-plugin-metrics-reporter/releases) [![Build Status](https://travis-ci.org/Graylog2/graylog-plugin-metrics-reporter.svg?branch=master)](https://travis-ci.org/Graylog2/graylog-plugin-metrics-reporter)  A collection of plugins for reporting internal Graylog metrics to other systems.  ## Installation  Put the JAR file of the desired metrics reporter plugin into the Graylog plugin directory of each Graylog node and add the relevant configuration settings to their configuration files.  After installing the metrics reporter plugin and adding the configuration settings, Graylog needs to be restarted.  Specific settings for each metrics reporter plugin have been documented in the respective README files:  * [metrics-reporter-cassandra](metrics-reporter-cassandra/README.md) * [metrics-reporter-console](metrics-reporter-console/README.md) * [metrics-reporter-csv](metrics-reporter-csv/README.md) * [metrics-reporter-datadog](metrics-reporter-datadog/README.md) * [metrics-reporter-elasticsearch](metrics-reporter-elasticsearch/README.md) * [metrics-reporter-ganglia](metrics-reporter-ganglia/README.md) * [metrics-reporter-gelf](metrics-reporter-gelf/README.md) * [metrics-reporter-graphite](metrics-reporter-graphite/README.md) * [metrics-reporter-influxdb](metrics-reporter-influxdb/README.md) * [metrics-reporter-jmx](metrics-reporter-jmx/README.md) * [metrics-reporter-librato](metrics-reporter-librato/README.md) * [metrics-reporter-mongodb](metrics-reporter-mongodb/README.md) * [metrics-reporter-opentsdb](metrics-reporter-opentsdb/README.md) * [metrics-reporter-prometheus](metrics-reporter-prometheus/README.md) * [metrics-reporter-slf4j](metrics-reporter-slf4j/README.md) * [metrics-reporter-statsd](metrics-reporter-statsd/README.md)    ## Development  This project is using Maven 3 and requires Java 8 or higher. The plugin will require Graylog 2.0.0 or higher.  * Clone this repository. * Run `mvn package` to build JAR files of all plugins. * Optional: Run `mvn jdeb:jdeb` and `mvn rpm:rpm` to create a DEB and RPM package respectively. * Copy generated JAR file(s) from the "target" directory to your Graylog server plugin directory. * Restart the Graylog server.   ## License  Copyright (c) 2016 Graylog, Inc.  This library is licensed under the GNU General Public License, Version 3.0.  See https://www.gnu.org/licenses/gpl-3.0.html or the LICENSE.txt file in this repository for the full license text.
visallo/vertexium	Vertexium [![Build Status](https://travis-ci.org/visallo/vertexium.svg?branch=master)](https://travis-ci.org/visallo/vertexium) =========  Vertexium is an API to manipulate graphs, similar to blueprints. Unlike blueprints, every Vertexium method requires authorizations and visibilities. Vertexium also supports multivalued properties as well as property metadata.  The Vertexium API was designed to be generic, allowing for multiple implementations.  * Data storage   * [Accumulo](accumulo/README.md)   * Experimental: [SQL](sql/README.md)  * Search   * [Elasticsearch](elasticsearch-singledocument/README.md)  Maven =====  ``` <properties>     <vertexium.version>0.7.0</vertexium.version> </properties> ```  ``` <dependencies>     <dependency>         <groupId>org.vertexium</groupId>         <artifactId>vertexium-core</artifactId>         <version>${vertexium.version}</version>     </dependency>     <dependency>         <groupId>org.vertexium</groupId>         <artifactId>vertexium-inmemory</artifactId>         <version>${vertexium.version}</version>     </dependency>     <dependency>         <groupId>org.vertexium</groupId>         <artifactId>vertexium-elasticsearch-singledocument</artifactId>         <version>${vertexium.version}</version>     </dependency>     <dependency>         <groupId>org.vertexium</groupId>         <artifactId>vertexium-accumulo</artifactId>         <version>${vertexium.version}</version>     </dependency>     <dependency>         <groupId>log4j</groupId>         <artifactId>log4j</artifactId>         <version>1.2.17</version>     </dependency> </dependencies> ```  Accumulo Implementation =======================  The Accumulo implementation builds on the [cell-level security features](https://accumulo.apache.org/1.5/accumulo_user_manual.html#_security) to enforce property, edge, and vertex restrictions. This allows the implementation to enforce security at the tablet server, rather than having to bring the data back to the application to be sorted out.  Requirements ------------  You'll need a running Accumulo and Elastic Search cluster to try out the Accumulo implementation of Vertexium. Please see the [Accumulo installation docs](https://accumulo.apache.org/1.5/accumulo_user_manual.html#_installation) and [Elastic Search setup](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup.html) guide for setting up the respective clusters.  API Usage Examples ------------------  ### create and configure an AccumuloGraph instance  ```java import java.util.Map; import java.util.HashMap;  import org.vertexium.Graph; import org.vertexium.accumulo.AccumuloGraph; import org.vertexium.accumulo.AccumuloGraphConfiguration;  // specify Accumulo config, more options than shown are available Map mapConfig = new HashMap(); mapConfig.put(AccumuloGraphConfiguration.USE_SERVER_SIDE_ELEMENT_VISIBILITY_ROW_FILTER, false); mapConfig.put(AccumuloGraphConfiguration.ACCUMULO_INSTANCE_NAME, "instance_name"); mapConfig.put(AccumuloGraphConfiguration.ACCUMULO_USERNAME, "username"); mapConfig.put(AccumuloGraphConfiguration.ACCUMULO_PASSWORD, "password"); mapConfig.put(AccumuloGraphConfiguration.ZOOKEEPER_SERVERS, "localhost");  AccumuloGraphConfiguration graphConfig = new AccumuloGraphConfiguration(mapConfig); Graph graph = AccumuloGraph.create(graphConfig); ```  ### add a vertex  ```java import org.vertexium.Authorizations; import org.vertexium.Graph; import org.vertexium.accumulo.AccumuloAuthorizations;  // visibility of vertex to be created Visibility visA = new Visibility("a");  // authorizations of user creating the vertex Authorizations authA = new AccumuloAuthorizations("a");  Vertex v = graph.addVertex(visA, authA); ```  ### add a vertex with properties  ```java Authorizations authA = new AccumuloAuthorizations("a"); Visibility visA = new Visibility("a"); Visibility visB = new Visibility("b");  Vertex v = graph.prepareVertex("v1", visA)                 .setProperty("prop1", "value1", visA)                 .setProperty("prop2", "value2", visB)                 .save(authA); ```  ### add an edge  ```java Authorizations authA = new AccumuloAuthorizations("a"); Visibility visA = new Visibility("a");  Vertex v1 = graph.addVertex(visA, authA); Vertex v2 = graph.addVertex(visA, authA); Edge e = graph.addEdge(v1, v2, "label1", visA, authA); ```  ### get all vertex edges  ```java import org.vertexium.Direction; import org.vertexium.Edge;  Authorizations authA = new AccumuloAuthorizations("a"); Vertex v1 = graph.getVertex("v1", authA); Iterable<Edge> edges = v1.getEdges(Direction.BOTH, authA); ```  ### full-text vertex search  ```java Authorizations authA = new AccumuloAuthorizations("a"); Iterable<Vertex> vertices = graph.query("vertex", authA).vertices(); ```  Configuration -------------  The Accumulo implementation has quite a few configuration properties, all with defaults. Please see the `public static final String` fields in [org.vertexium.accumulo.AccumuloGraphConfiguration](vertexium-accumulo/src/main/java/org/neolumin/vertexium/accumulo/AccumuloGraphConfiguration.java?source=c#L29) for a full listing.  Iterators ------------------ The Accumulo implementation of Vertexium can make use of server-side iterators to improve performance by limiting rows returned by tablet servers to only those where the end user has the proper authorizations. This requires copying the `vertexium-accumulo-iterators-*.jar` file to `$ACCUMULO_HOME/lib/ext` on each Accumulo server. Use `mvn package` to build the required JAR file.  Status ======  Vertexium is an actively developed and maintained project that should be considered to be in a beta state. You should not expect to find significant bugs or missing functionality in the Accumulo implementation, but the API is still changing. Please keep that in mind if you decide to use Vertexium.  Contributing ============  We welcome and encourage participation and contribution from anyone interested in using Vertexium. Please see our [contributing guide](https://github.com/visallo/vertexium/blob/master/CONTRIBUTING.md) to better understand how you can pitch in.  License =======  Copyright 2014 V5 Analytics  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
codelibs/elasticsearch-cluster-runner	Elasticsearch Cluster Runner ============================  This project runs Elasticsearch cluster on one JVM instance for your development/testing easily. You can use Elasticsearch Cluster Runner as Embedded Elasticsearch in your application.  ## Version  [Versions in Maven Repository](http://central.maven.org/maven2/org/codelibs/elasticsearch-cluster-runner/)  ## Run on Your Application  Put elasticsearch-cluster-runner if using Maven:      <dependency>         <groupId>org.codelibs</groupId>         <artifactId>elasticsearch-cluster-runner</artifactId>         <version>5.5.2.0</version>     </dependency>  ### Start Cluster Runner      import static org.codelibs.elasticsearch.runner.ElasticsearchClusterRunner.newConfigs;     ...     // create runner instance     ElasticsearchClusterRunner runner = new ElasticsearchClusterRunner();     // create ES nodes     runner.onBuild(new ElasticsearchClusterRunner.Builder() {         @Override         public void build(final int number, final Builder settingsBuilder) {             // put elasticsearch settings             // settingsBuilder.put("index.number_of_replicas", 0);         }     }).build(newConfigs());  build(Configs) method configures/starts Clsuter Runner.  ### Stop Cluster Runner      // close runner     runner.close();  ### Clean up       // delete all files(config and index)     runner.clean();  ## Run on JUnit  Put elasticsearch-cluster-runner as test scope:      <dependency>         <groupId>org.codelibs</groupId>         <artifactId>elasticsearch-cluster-runner</artifactId>         <version>5.5.2.0</version>         <scope>test</scope>     </dependency>  and see [ElasticsearchClusterRunnerTest](https://github.com/codelibs/elasticsearch-cluster-runner/blob/master/src/test/java/org/codelibs/elasticsearch/runner/ElasticsearchClusterRunnerTest.java "ElasticsearchClusterRunnerTest").  ## Run as Standalone  ### Install Maven  Download and install Maven 3 from http://maven.apache.org/.  ### Clone This Project      git clone https://github.com/codelibs/elasticsearch-cluster-runner.git  ### Build This Project      mvn compile  ## Run/Stop Elasticsearch Cluster  ### Run Cluster  Run:      mvn exec:java   The default cluster has 3 nodes and the root directory for Elasticsearch is es\_home. Nodes use 9201-9203 port for HTTP and 9301-9303 port for Transport. If you want to change the number of node, Run:      mvn exec:java -Dexec.args="-basePath es_home -numOfNode 4"  ### Stop Cluster  Type Ctrl-c or kill the process.  ## Others  ### Install Plugins  This project does not have plugin command to install plugins. Therefore, you need to put plugins manually. For example, installing solr-api plugin:      $ mkdir -p es_home/plugins/solr-api     $ wget http://repo1.maven.org/maven2/org/codelibs/elasticsearch-solr-api/1.4.0/elasticsearch-solr-api-1.4.0.zip     $ unzip elasticsearch-solr-api-1.4.0.zip      $ rm elasticsearch-solr-api-1.4.0.zip    ### Issues/Questions  Please file an [issue](https://github.com/codelibs/elasticsearch-cluster-runner/issues "issue"). (Japanese forum is [here](https://github.com/codelibs/codelibs-ja-forum "here").)
klask-io/klask-io	# [![klask.io](https://raw.githubusercontent.com/klask-io/klask-io/master/src/main/webapp/content/images/logo-klask.png)](https://github.com/klask-io/klask-io)  | Branch  | Build  | Coverage  | |---|---|---| | master  | [![Build Status](https://img.shields.io/travis/klask-io/klask-io/master.svg?style=flat-square)](https://travis-ci.org/klask-io/klask-io)  | [![Coverage Status](https://img.shields.io/coveralls/klask-io/klask-io/master.svg?style=flat-square)](https://coveralls.io/github/klask-io/klask-io?branch=master) | | develop  | [![Build Status](https://img.shields.io/travis/klask-io/klask-io/develop.svg?style=flat-square)](https://travis-ci.org/klask-io/klask-io)  | [![Coverage Status](https://img.shields.io/coveralls/klask-io/klask-io/develop.svg?style=flat-square)](https://coveralls.io/github/klask-io/klask-io?branch=develop) |  #### Docker [![Docker Stars](https://img.shields.io/docker/stars/klask/klask.io.svg?style=flat-square)](https://hub.docker.com/r/klask/klask.io/) [![Docker pulls](https://img.shields.io/docker/pulls/klask/klask.io.svg?style=flat-square)](https://hub.docker.com/r/klask/klask.io/) [![Docker build](https://img.shields.io/docker/automated/klask/klask.io.svg?style=flat-square)](https://hub.docker.com/r/klask/klask.io/builds/)   ## What is klask.io ? __klask.io__ is an open source search engine for source code. This application was generated using [JHipster](https://jhipster.github.io).  ### Live demo http://app.klask.io/  ### How to run it ? You can run an instance easily by pulling the docker image and execute by following :      docker run klask/klask.io  #### docker-compose an example of a docker-compose.yml :  ```Dockerfile version: '2' services:   klask-app:     image: klask/klask.io:latest     ports:       - 8080:8080     volumes:       - /mnt/svn:/repo       - ./data:/klask-data       - ./application-docker.yml:/application-docker.yml ```  `/mnt/svn` is the path to my repositories   `./data` is the location where elasticsearch files and database were saved.   The optional file `application-docker.yml` can overrides all properties defined in [application.yml](/src/main/resources/config/application.yml) and [application-docker.yml](/src/main/resources/config/application-docker.yml)      ## Development Before you can build this project, you must install and configure the following dependencies on your machine:  1. [Node.js][]: We use Node to run a development web server and build the project.    Depending on your system, you can install Node either from source or as a pre-packaged bundle.  After installing Node, you should be able to run the following command to install development tools (like [Bower][] and [BrowserSync][]). You will only need to run this command when dependencies change in package.json.      npm install  We use [Gulp][] as our build system. Install the Gulp command-line tool globally with:      npm install -g gulp  Run the following commands in two separate terminals to create a blissful development experience where your browser auto-refreshes when files change on your hard drive.      ./mvnw     gulp  Bower is used to manage CSS and JavaScript dependencies used in this application. You can upgrade dependencies by specifying a newer version in `bower.json`. You can also run `bower update` and `bower install` to manage dependencies. Add the `-h` flag on any command to see how you can use it. For example, `bower update -h`.   ## Building for production  To optimize the klask.io client for production, run:      ./mvnw -Pprod clean package  This will concatenate and minify CSS and JavaScript files. It will also modify `index.html` so it references these new files.  To ensure everything worked, run:      java -jar target/*.war --spring.profiles.active=prod  Then navigate to [http://localhost:8080](http://localhost:8080) in your browser.  ## Testing  Unit tests are run by [Karma][] and written with [Jasmine][]. They're located in `src/test/javascript/` and can be run with:      gulp test   ## To run with docker in production :  Utiliser les fichiers docker-compose dans src/main/docker          docker-compose -f elasticsearch.yml up -d     docker-compose -f postgresql.yml up -d          java -jar target/*.war --spring.profiles.active=prod     ## Continuous Integration  To setup this project in Jenkins, use the following configuration:  * Project name: `klask.io` * Source Code Management     * Git Repository: `https://github.com/klask-io/klask-io.git`     * Branches to build: `*/master`     * Additional Behaviours: `Wipe out repository & force clone` * Build Triggers     * Poll SCM / Schedule: `H/5 * * * *` * Build     * Invoke Maven / Tasks: `-Pprod clean package` * Post-build Actions     * Publish JUnit test result report / Test Report XMLs: `build/test-results/*.xml`  [JHipster]: https://jhipster.github.io/ [Node.js]: https://nodejs.org/ [Bower]: http://bower.io/ [Gulp]: http://gulpjs.com/ [BrowserSync]: http://www.browsersync.io/ [Karma]: http://karma-runner.github.io/ [Jasmine]: http://jasmine.github.io/2.0/introduction.html [Protractor]: https://angular.github.io/protractor/
msathis/SQLToNoSQLImporter	SQLToNoSQLImporter ==================   SQLToNoSQLImporter is a Solr like data import handler to import Sql (MySQL,Oracle,PostgreSQL) data to NoSQL Systems (Mongodb,CouchDB,Elastic Search).  	Migration is now completely configuration driven. User is expected to write a configuration, this tool will export the data to the preferred NoSQL system.  	SQLToNoSQLImporter reads from sql databases, converts and then batch inserts them into NoSQL datastore.For this purpose it uses one properties file (import.properties) where NoSQL datastore related settings are listed and one xml file with sql database related settings ,de-normalized schema,fields. 	For more info about [Solr's Data Import Handler!](http://wiki.apache.org/solr/DataImportHandler#Configuration_in_data-config.xml)  But the configuration file of SQLToNoSQLImporter varies slightly from solr's DIH.   **PROJECT STRUCTURE:**  	1. src/main/resources/ 	- All configuration files. 	2. src/main/java		 - Source code of this project  **HOW TO RUN :**      1. change sql-db related settings in src/main/resources/db-data-config.xml 	2. change NoSQL datastore related settings in src/main/resources/import.properties 	3. Install and export maven to PATH. 	4. Run the project by issuing mvn test 	5. Or you can pass configuration files through command line like  mvn test -DimportConf=/Users/sathis/Desktop/data-import.properties -DdbConf=/Users/sathis/Desktop/data-config.xml  	 **CONFIGURATIONS :** 	    src/main/resources/import.properties 		 	NoSQL data store related settings. 	    src/main/resources/db-data-config.xml  	Data conversion configuration. Similar to Solr's DIH. 			     **ISSUES:**          Please feel free to file the issues you encounter.
jprante/elasticsearch-client	Elasticsearch Client ====================  Elasticsearch Client is a project for generating a modularized codebase for clients that are accessing Elasticsearch from remote.  Existing client implementations in Elasticsearch are  - TCP (NodeClient, TransportClient) clients, connecting to server default ports 9300-9400 - HTTP (REST, Netty-based) clients, connecting to server default ports 9200-9300  While the node client is using a discovery mechanism to connect to a cluster by using given network interfaces, the transport client can connect to specified network addresses. Both clients initialize a full node - with a node name, node services, modules, even plugins can be used. Such a client needs to rendezvous with a discovered Elasticsearch cluster.  In designing middleware, this approach is similar to a two-tier architecture, where NodeClient and TransportClient are "fat" clients, they carry all the dependencies of the server. This can lead to tedious work if more client implementations need to be added.  A three-tier approach would introduce an additional layer between Elasticsearch NodeClient/TransportClient and the server code. The advantage is separation of concerns. The Elasticsearch server is only loosely coupled to the client. Not every change at the code on server side would enforce client code updates. As a side effect, the Elasticsearch server codebase, which is rather large, could be modularized by reusing the client code base.  The idea of the Elasticsearch Client project is to factor out the client code from the server code. This allows more flexible implementations, for instance for using additional transport plugins. Developing additional transport implementations like WebSocket or SPDY can benefit from a codebase common to all Elasticsearch clients.  Goals of the Elasticsearch Client project are  - minimum version is Java 7 - a base client API and three client submodules (ingest client, search client, admin client) - reusable code in any Java project (for implementing connectors etc.) - implement a WebSocket client which is API-compatible to existing client code for NodeClient/TransportClient - HTTP client (re-)implementation (REST) - SPDY client implementation (future plan)  It is expected that clients will run remote, that is they never share the same JVM with the server code.  What makes an Elasticsearch client? ===================================  When we talk of an Elasticsearch client, we refer to a Java program that   - can build requests with XContentBuilder - submit requests to the server  - receives responses from the server - uses QueryBuilders / FilterBuilders for building search requests  Elasticsearch client code differs from the server code, it has no code for  - cluster discovery, cluster management - node membership in the cluster (different to NodeClient or TransportClient) - services, modules - index engine, shard operations - field mapping - TCP transport (the Elasticsearch internal protocol) - plugins - rivers - scripting (mvel, Javascript, Python etc.) - analyzers, tokenizers  Because the client build does not use relocation like the Elasticsearch server does, Maven project dependencies will be transparent (Lucene, Jackson, Guava, Joda etc.)  Client hierarchy ================  There is a client hierarchy: the Ingest Client, the Search Client, and the Admin Client.  - the **Ingest Client** can issue ingest actions without requiring Lucene jars. It is only meant for data pushing. With an Ingest Client, it is not possible to send queries (only get requests) or admin actions like index creations or deletions or node shutdowns/restarts.  - the **Search Client** can issue read operations and uses the Lucene Queries jar. With a search client, it is not possible to ingest any data or to issue admin actions.  - the **Admin Client** can issue administrative actions and will also have the capabilities of a Search Client, because actions like warming/explain depend on the Search Client.  The selected Maven project group ID is **org.elasticsearch.client** and the Maven artifact names are      **elasticsearch-client** (pom)          		**elasticsearch-client-api** (jar)          		**elasticsearch-client-ingest-api** (jar)          		**elasticsearch-client-search-api** (jar)          		**elasticsearch-client-admin-api** (jar)  		**elasticsearch-client-compression-lzf** (jar)  		**elasticsearch-client-compression-snappy** (jar)  		**elasticsearch-client-transport-api** (jar)  		**elasticsearch-client-transport-netty** (jar)  		**elasticsearch-client-discovery-tao** (jar)  This modularization allows client development for the full or only for parts of the Elasticsearch API.    Generating the client codebase ==============================  The Elasticsearch codebase (currently 0.20.0.Beta1-SNAPHOT) was used as a starting point for the project.  Unfortunately, the code required some modifications. These are the main changes applied to the code while factoring out client code from the Elasticsearch codebase. The list may be incomplete.  - using jsr166y as implemented in JDK 7  - Lucene UnicodeUtil (and deps) copied to org.elasticseach.common.lucene.util  - shorter org.elasticsearch.client.Client API, methods split to IngestClient, SearchClient, AdminClient  - moved all NAME strings beyond org.elasticsearch.index.query from the ...Action class to the corresponding ...Builder class   - ClusterBlocks: org.elasticsearch.cluster.metadata.MetaDataStateIndexService.INDEX_CLOSED_BLOCK moved to ClusterBlocks.INDEX_CLOSED_BLOCK  - NodesInfoResponse: removed AbstractComponent in SettingsFilter  - RoutingAllocation: Result subclass factored to out org.elasticsearch.cluster.routing.allocation.RoutingAllocationResult  - in org.elasticsearch.cluster.metadata.AliasAction, method AliasAction filter(FilterBuilder filterBuilder) removed   - Service API for ES compressors, submodules for each compression algo  - Compressor: many compressor instances, regarding to Netty depnedency (and Lucene)  - IndexMetadata: MapperService.DEFAULT_MAPPING moved to IndexMetadata.DEFAULT_MAPPING  - MappingMetadata:  TimestampFieldMapper.DEFAULT_DATE_TIME_FORMAT moved to MappingMetadata.DEFAULT_DATE_TIME_FORMAT  - constructor MappingMetaData(DocumentMapper docMapper)  removed  - ThreadPool: reduced versions for client-api and transport-api submodules (ClientThreadPool, TransportThreadPool)  - ThreadPool.Info subclass moved to ThreadPoolInfo.Info  - IndexAction: removed process() method, because it is only related to TransportIndexAction / TransportBulkAction  - org.elasticsearch.action.Action: got a fourth class parameter (the client class)
pitchpoint-solutions/sfs	# Simple File Server   ## Overview * Sfs aims to be a file server that can serve and securely store billions of large and small files using minimal resources.  * The http api implements common features of the openstack swift http api so that it can be used with exiting tools.   ## Features * Object metadata is indexed in elasticsearch which allows for quick reads, updates, deletes and listings even when containers contain millions of objects * Objects are versioned and the number of revisions stored can be configured per container. This means that you'll never loose an object by overwriting it or deleting it (unless you force a deletion). * Each object can have a TTL set on it so that manual purging is not required * Object data is stored in data files that are themselves replicated and healed when necessary (the object data replication level can be controlled by container independently of the object index settings). If the object size is very small it's stored with the object metadata instead of the data file. This is useful if you're storing token type data. * Object data files do not need to be compacted since the block ranges are recycled. The range allocator attempts to be intelligent about which ranges object data is written to so that writes are sequential. * Maximum object size is 5GB. It's defined at build time and can be changed. * Objects of many terabytes are supported through the openstack swift dynamic large object functionality * Each container gets it's own index so that object metadata sharding and replication can be controlled on a container level.  * Object data is encrypted at rest using AES256-GCM if the container is configured to encrypt by default or the object upload request includes the "X-Server-Side-Encryption" http header * Master keys are automatically generated, rotated and stored on redundant key management services (Amazon KMS and Azure KMS). You will need accounts on both services but since sfs uses a tiny amount of master keys the charges are minimal. * Container encryption keys are automatically generated, rotated and not stored in plain text anywhere. Once sfs starts it initializes the master keys and when a container key needs to be decrypted it uses the appropriate master key. * A container can be exported into into a file and imported into another container. The dynamic large object manifest will also be updated if it references objects in the container that was exported. The container export format is independent of the index and data file format so that an export can be imported into any sfs version that supports the export file format. * Container exports can be compressed and encrypted using AES256-GCM if the appropriate http headers are supplied to the http export api * Adding new sfs nodes to the cluster is a simple as starting the docker image on another server. New data will always be written to the nodes with the most available storage space. Existing data will not be rebalanced.   * The entire implementation is event driven and non blocking. Built using [Vert.x](http://vertx.io/).  ## Mailing Lists  - [Discussion](http://groups.google.com/group/sfs-discuss)  ## Latest release  The most recent release of sfs is [release-1.20170106133707](https://hub.docker.com/r/pitchpointsolutions/simple-file-server/tags/).  To run release-1.20170106133707  ``` docker run ... pitchpointsolutions/simple-file-server:release-1.20170106133707 ```  To run the latest release-1  ``` docker pull pitchpointsolutions/simple-file-server:release-1 docker run ... pitchpointsolutions/simple-file-server:release-1 ```  ## Snapshots  Snapshots of sfs are built from the master branch  To run the latest snapshot  ``` docker pull pitchpointsolutions/simple-file-server:latest docker run ... pitchpointsolutions/simple-file-server:latest ```   ## Quickstart on Linux ``` docker run -d -P --name sfs_example_elasticsearch elasticsearch:2.4.1 -Des.cluster.name=sfs_example_elasticsearch HOSTNAME=`hostname` && export HOST_IP=`ping -c1 -n ${HOSTNAME} | head -n1 | sed "s/.*(\([0-9]*\.[0-9]*\.[0-9]*\.[0-9]*\)).*/\1/g"`; export DOCKER_ES_PORT=`docker port sfs_example_elasticsearch 9300/tcp | sed -E 's/(.+):(.+)/\2/'` docker run -d -P --add-host localhost:127.0.0.1 -e SFS_HTTP_LISTEN_ADDRESSES=0.0.0.0:80 -e SFS_HTTP_PUBLISH_ADDRESSES=127.0.0.1:80 -e SFS_REMOTENODE_SECRET=YWJjMTIzCg== -e SFS_KEYSTORE_AWS_KMS_ENDPOINT=https://kms.us-east-1.amazonaws.com -e SFS_KEYSTORE_AWS_KMS_KEY_ID=arn:aws:kms:us-east-1:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab -e SFS_KEYSTORE_AWS_KMS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE -e SFS_KEYSTORE_AWS_KMS_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYAWSEXAMPLEKEY -e SFS_KEYSTORE_AZURE_KMS_ENDPOINT=https://yourvaultname.vault.azure.net -e SFS_KEYSTORE_AZURE_KMS_KEY_ID=6603bbb5-cf2e-4367-8327-43ba49ba74b0 -e SFS_KEYSTORE_AZURE_KMS_ACCESS_KEY_ID=a14970c2-397c-4af2-867e-b3480f9eaac6 -e SFS_KEYSTORE_AZURE_KMS_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYAZUREEXAMPLEKEY -e SFS_ELASTICSEARCH_CLUSTER_NAME=sfs_example_elasticsearch -e SFS_ELASTICSEARCH_NODE_NAME=${HOST_IP}:${DOCKER_ES_PORT} -e SFS_ELASTICSEARCH_DISCOVERY_ZEN_PING_UNICAST_HOSTS=${HOST_IP}:${DOCKER_ES_PORT} -e SFS_ELASTICSEARCH_DISCOVERY_ZEN_PING_MULTICAST_ENABLED=false -e SFS_ELASTICSEARCH_DISCOVERY_ZEN_PING_UNICAST_ENABLED=true --detach --name sfs_example_middlware -P pitchpointsolutions/simple-file-server export DOCKER_SFS_PORT=`docker port sfs_example_middlware 80/tcp | sed -E 's/(.+):(.+)/\2/'` curl -v -XGET "http://localhost:${DOCKER_SFS_PORT}/admin/001/healthcheck" curl -XPOST -u admin:admin "http://localhost:${DOCKER_SFS_PORT}/openstackswift001/my_account" curl -XPUT -u admin:admin "http://localhost:${DOCKER_SFS_PORT}/openstackswift001/my_account/my-container" curl -XPUT -u admin:admin "http://localhost:${DOCKER_SFS_PORT}/openstackswift001/my_account/my-container/my_object" -d 'abc123' curl -XGET -u admin:admin "http://localhost:${DOCKER_SFS_PORT}/openstackswift001/my_account/my-container/my_object"  ```  ## Quickstart using Docker Machine ``` docker run -d -P --name sfs_example_elasticsearch elasticsearch:2.4.1 -Des.cluster.name=sfs_example_elasticsearch export HOST_IP=`docker-machine ip`; export DOCKER_ES_PORT=`docker port sfs_example_elasticsearch 9300/tcp | sed -E 's/(.+):(.+)/\2/'` docker run -d -P --add-host localhost:127.0.0.1 -e SFS_HTTP_LISTEN_ADDRESSES=0.0.0.0:80 -e SFS_HTTP_PUBLISH_ADDRESSES=127.0.0.1:80 -e SFS_REMOTENODE_SECRET=YWJjMTIzCg== -e SFS_KEYSTORE_AWS_KMS_ENDPOINT=https://kms.us-east-1.amazonaws.com -e SFS_KEYSTORE_AWS_KMS_KEY_ID=arn:aws:kms:us-east-1:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab -e SFS_KEYSTORE_AWS_KMS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE -e SFS_KEYSTORE_AWS_KMS_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYAWSEXAMPLEKEY -e SFS_KEYSTORE_AZURE_KMS_ENDPOINT=https://yourvaultname.vault.azure.net -e SFS_KEYSTORE_AZURE_KMS_KEY_ID=6603bbb5-cf2e-4367-8327-43ba49ba74b0 -e SFS_KEYSTORE_AZURE_KMS_ACCESS_KEY_ID=a14970c2-397c-4af2-867e-b3480f9eaac6 -e SFS_KEYSTORE_AZURE_KMS_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYAZUREEXAMPLEKEY -e SFS_ELASTICSEARCH_CLUSTER_NAME=sfs_example_elasticsearch -e SFS_ELASTICSEARCH_NODE_NAME=${HOST_IP}:${DOCKER_ES_PORT} -e SFS_ELASTICSEARCH_DISCOVERY_ZEN_PING_UNICAST_HOSTS=${HOST_IP}:${DOCKER_ES_PORT} -e SFS_ELASTICSEARCH_DISCOVERY_ZEN_PING_MULTICAST_ENABLED=false -e SFS_ELASTICSEARCH_DISCOVERY_ZEN_PING_UNICAST_ENABLED=true --detach --name sfs_example_middlware -P pitchpointsolutions/simple-file-server export DOCKER_SFS_PORT=`docker port sfs_example_middlware 80/tcp | sed -E 's/(.+):(.+)/\2/'` curl -v -XGET "http://${HOST_IP}:${DOCKER_SFS_PORT}/admin/001/healthcheck" curl -XPOST -u admin:admin "http://${HOST_IP}:${DOCKER_SFS_PORT}/openstackswift001/my_account" curl -XPUT -u admin:admin "http://${HOST_IP}:${DOCKER_SFS_PORT}/openstackswift001/my_account/my-container" curl -XPUT -u admin:admin "http://${HOST_IP}:${DOCKER_SFS_PORT}/openstackswift001/my_account/my-container/my_object" -d 'abc123' curl -XGET -u admin:admin "http://${HOST_IP}:${DOCKER_SFS_PORT}/openstackswift001/my_account/my-container/my_object" ```   ## Master Keys * On first use 1 master is key is generated if one does not exist * A new master key is generated every 30 days  * Master keys are re-encrypted every 30 days * The primary copy of the master key is encrypted using amazons kms * The backup copy of the master key is encrypted using azures kms * If the primary copy of the master key can't be decrypted the backup copy will be used repair the primary copy * If the backup copy of the master key can't be decrypted the primary copy will be used to repair the backup copy * When a master key is needed it is decrypted using the kms and stored encrypted locally in memory so that every use of the master key doesn't require a call to the kms. This means after a restart each master key that is used will only call the kms to decrypt the key once.   ## Container Keys * When a container key is needed a new one is generated if none exist. * If the container key been active for more than 30 days and new one is generated.  * The container key is encrypted using AES256-GCM w/ nonce using the latest master key and stored encrypted as part of the container metadata. * When object data needs to be encrypted the latest container key is fetched, decrypted using the appropriate master key. * When object data needs to be decrypted the appropriate container key is fetched, decrypted using the appropriate master key. * Container keys are re-encrypted every 30 days   ## Object Encryption * Each object is encrypted using AES256-GCM w/ nonce   ## Metadata Replication * Metadata replication uses the elasticsearch settings   ## Object Data Replication * Object streams are replicated in real time. This means that if you upload a 5GB file you don't have to wait for the data to be copied from the primary volume to the replicas after the write to the primary has finished. * This is implemented by cloning the data stream to each volume as sfs recives the stream and then validating checksums before an OK response is returned to the user.   ## Backups * Backups support is provided by container export/import functionality * The container export stream can be compressed and encrypted so that it doesn't have to be done afterwards. For large containers this saves many hours of time. * The export storage format is independent of the index and volume format so that major version upgrades of elasticsearch and sfs can be executed without fear of data loss.    ## Volume Block Allocation * When and object uploaded is one continuous block range is allocated in the volume   ## Access Control * The default auth provider is org.sfs.auth.SimpleAuthProvider and is intentionally very simple since sfs will likely be hosted behind some form of proxy the controls access or an application that delegates secure storage to sfs. In this auth provider if credentials match a user role then the user can PUT, POST, HEAD, GET, DELETE containers and objects and GET accounts. The user role will only be allowed to see objects and containers that they're allowed to read when listing accounts and containers (in the case of SimpleAuthProvider it's everything). If credentials match an admin role everything is allowed. See "Adding a new AuthProvider" later in this readme. * The auth provider interface provides support for create, update, delete and read operations on accounts, containers, and objects.  ## Adding a new AuthProvide 1. Create a new project or module and include sfs-server as a dependency (my-organization-sfs-server).  2. Implement org.sfs.auth.AuthProvider and make priority() return a value higher than 0 3. Create a file named my-organization-sfs-server/src/main/resources/META-INF/services/org.sfs.auth.AuthProvider 4. Put the name of your implementation class into the my-organization-sfs-server/src/main/resources/META-INF/services/org.sfs.auth.AuthProvider file 5. Using sfs-docker/main/docker/Dockerfile as an example create a new artifact (my-organization-sfs-docker) that uses your new server artifact (my-organization-sfs-server) as a dependency instead of sfs-server   ## Caveats * You will need to run an elasticsearch cluster * Each data node hosts a volume (this is where object data is stored). If you set the object replica count (x-sfs-object-replicas) higher than the number of nodes+1 object uploads will fail since the replicated data needs to be stored on different nodes. org.sfs.nodes.Nodes.allowSameNode allows this behaviour to be overridden but currently allowSameNode is used only for testing. If instances are being run using docker containers then this is not an issue since each instance gets it's own file system and thus it's own node id.  * The x-sfs-object-replicas value is dynamic and can be incremented while the system is running when more nodes come online.    ## Building  Oracle Java 8 and Maven 3 should be used for building.  ###### Environment Variables ######     SFS_KEYSTORE_AWS_KMS_ENDPOINT=https://kms.us-east-1.amazonaws.com     SFS_KEYSTORE_AWS_KMS_KEY_ID=...     SFS_KEYSTORE_AWS_KMS_ACCESS_KEY_ID=$...     SFS_KEYSTORE_AWS_KMS_SECRET_KEY=...          SFS_KEYSTORE_AZURE_KMS_ENDPOINT=https://....vault.azure.net     SFS_KEYSTORE_AZURE_KMS_KEY_ID=...     SFS_KEYSTORE_AZURE_KMS_ACCESS_KEY_ID=...     SFS_KEYSTORE_AZURE_KMS_SECRET_KEY=...  ###### Building, testing and assembling Docker image (from the root project directory) ######     mvn clean package      ###### Building, testing (from the sfs-server directory) ######     mvn clean package          ###### Building, testing and regenerating the protobuf files (from the sfs-server directory) ######     mvn clean package -Pprotoc                ## Running (Requires Elasticsearch 2.4)      * [Run a test elasticsearch instance](https://mad.is/2016/09/running-elasticsearch-in-docker-container/) * [Run a production elasticsearch cluster](https://www.elastic.co/guide/en/elasticsearch/guide/current/deploy.html)  ###### Running the docker image you just built (See sample configuration and logback configuration) ######     docker run -it --add-host "es-host:${HOST_IP}" --add-host "localhost:127.0.0.1" -e "INSTANCES=200" -e "HEAP_SIZE=512m" -p 8092:80 -v ${PWD}:/data -v ${PWD}/sample-config.json:/etc/vertx-conf.json -v ${PWD}/sample-logback.xml:/etc/vertx-logback.xml pps-sfs:latest  ###### Sample Configuration ######     {         "auth": {             "admin": [                 {                     "id": 1,                     "password": "admin",                     "username": "admin"                 }             ],             "user": [                 {                     "id": 2,                     "password": "user",                     "username": "user"                 }             ]         },         "elasticsearch.cluster.name": "elasticsearch_samplecluster",         "elasticsearch.defaultdeletetimeout": 30000,         "elasticsearch.defaultgettimeout": 30000,         "elasticsearch.defaultindextimeout": 30000,         "elasticsearch.defaultsearchtimeout": 30000,         "elasticsearch.discovery.zen.ping.multicast.enabled": false,         "elasticsearch.discovery.zen.ping.unicast.enabled": true,         "elasticsearch.discovery.zen.ping.unicast.hosts": [             "es-host:9300"         ],         "elasticsearch.node.name": "simple-file-server-client",         "elasticsearch.replicas": 0,         "elasticsearch.shards": 1,         "fs.home": "/data",         "http.listen.addresses": [             "0.0.0.0:80"         ],         "http.maxheadersize": 40960,         "http.publish.addresses": [             "${docker_image_ip_address}:8092"         ],         "keystore.aws.kms.access_key_id": "${aws_kms_access_key_id}",         "keystore.aws.kms.endpoint": "https://kms.us-east-1.amazonaws.com",         "keystore.aws.kms.key_id": "${aws_kms_key_id}",         "keystore.aws.kms.secret_key": "${aws_kms_access_key_id}",         "keystore.azure.kms.access_key_id": "${aws_azure_access_key_id}",         "keystore.azure.kms.endpoint": "https://....vault.azure.net",         "keystore.azure.kms.key_id": "${aws_azure_key_id}",         "keystore.azure.kms.secret_key": "${aws_azure_secret_key}",         "node.data": true,         "node.master": true,         "number_of_object_replicas": 1,         "remotenode.connectimeout": 5000,         "remotenode.maxpoolsize": 200,         "remotenode.responsetimeout": 10000,         "remotenode.secret": "YWJjMTIzCg==",         "threadpool.background.size": 200,         "threadpool.io.size": 200     }      ###### Sample Logback Configuration ######     <?xml version="1.0" encoding="UTF-8" ?>          <configuration scan="true">              <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">             <!-- encoders are assigned by default the type                  ch.qos.logback.classic.encoder.PatternLayoutEncoder -->             <encoder>                 <pattern>%d [%thread] %-5level %logger{36} - %msg%n</pattern>             </encoder>         </appender>              <logger name="org.apache" level="TRACE" additivity="false">             <appender-ref ref="STDOUT"/>         </logger> d              <logger name="io.vertx" level="TRACE" additivity="false">             <appender-ref ref="STDOUT"/>         </logger>              <logger name="io.netty" level="DEBUG" additivity="false">             <appender-ref ref="STDOUT"/>         </logger>              <logger name="org.sfs" level="INFO" additivity="false">             <appender-ref ref="STDOUT"/>         </logger>          </configuration>   ## Accounts  ###### Create/Update Account ######     curl -XPOST -u admin:admin "http://localhost:8092/openstackswift001/my_account" ###### Get Account Metadata ######     curl -XHEAD -u admin:admin "http://localhost:8092/openstackswift001/my_account" ###### List Containers in Account ######     curl -XGET -u admin:admin "http://localhost:8092/openstackswift001/my_account" ###### Delete Account (if it's empty) ######     curl -XDELETE -u admin:admin "http://localhost:8092/openstackswift001/my_account"        ## Containers  ###### Create a Container ######     curl -XPUT -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container" ###### Update a Container ######     curl -XPOST -u admin:admin -H "X-Container-Meta-Server-Side-Encryption: true" -H "X-Container-Meta-Max-Object-Revisions: 3" "http://localhost:8092/openstackswift001/my_account/my-container"     ###### Get Container Metadata ######     curl -XHEAD -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container" ###### List Objects in Container ######      curl -XGET -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container?format=xml&prefix=&limit=10000&delimiter=%2F" ###### List Objects in subfolder ######     curl -XGET -u admin:admin "http://localhost:8092/sfs/openstackswift001/my_account/my-container?format=xml&prefix=subfolder%2F&limit=10000&delimiter=%2F ###### Create a Container the by default encrypts objects and retains at most 3 object revisions ######     curl -XPUT -u admin:admin -H "X-Container-Meta-Server-Side-Encryption: true" -H "X-Container-Meta-Max-Object-Revisions: 3" "http://localhost:8092/openstackswift001/my_account/my-container" ###### Create a Container and control the number of object index shards, the number of object index replicas and the number of object replicas ######     curl -XPUT -u admin:admin -H "x-sfs-object-index-shards: 12" -H "x-sfs-object-index-replicas: 2" -H "x-sfs-object-replicas: 2" "http://localhost:8092/openstackswift001/my_account/my-container" ###### Update a Container so that by default it encrypts objects and retains at most 2 object revisions ######     curl -XPOST -u admin:admin -H "X-Container-Meta-Server-Side-Encryption: true" -H "X-Container-Meta-Max-Object-Revisions: 2" "http://localhost:8092/openstackswift001/my_account/my-container" ###### Update a Container so that by default it doesn't encrypt objects and retains at most 1 object revisions ######     curl -XPOST -u admin:admin -H "X-Container-Meta-Server-Side-Encryption: false" -H "X-Container-Meta-Max-Object-Revisions: 1" "http://localhost:8092/openstackswift001/my_account/my-container" ###### Update a Container so that one object index replica in maintained for each object index shard and one each object replicated once ######     curl -XPOST -u admin:admin -H "x-sfs-object-index-replicas: 1" -H "x-sfs-object-replicas: 1" "http://localhost:8092/openstackswift001/my_account/my-container" ###### Delete a Container (if it's empty) ######     curl -XDELETE -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container"             ## Object Storage      ###### Upload and object ######     curl -XPUT -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container/my_object" -d 'abc123' ###### Upload and object and have it be stored encrypted ######     curl -XPUT -u admin:admin -H "X-Server-Side-Encryption: true" "http://localhost:8092/openstackswift001/my_account/my-container/my_object" -d 'abc123' ###### Get object metadata ######     curl -XHEAD -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container/my_object" ###### Get object ######     curl -XGET -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container/my_object"  ###### Delete an object (will create delete marker) ######     curl -XDELETE -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container/my_object"  ###### Really Delete and object ######     curl -XDELETE -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container/my_object?version=all"      ###### Really Delete and object version ######     curl -XDELETE -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container/my_object?version=1"    ###### Really Delete and multiple object version ######     curl -XDELETE -u admin:admin "http://localhost:8092/openstackswift001/my_account/my-container/my_object?version=1,2,3"            ## Container Import and Export      ###### Export a container ######     curl -XPOST -u admin:admin -H "x-sfs-dest-directory: /data/my-container_export" "http://localhost:8092/export_container/my_account/my-container" ###### Export a container and compress the export ######     curl -XPOST -u admin:admin -H "x-sfs-dest-directory: /data/my-container_export" -H "x-sfs-compress: true" "http://localhost:8092/export_container/my_account/my-container"    ###### Export a container, compress and encrypt the export ######     curl -XPOST -u admin:admin -H "x-sfs-dest-directory: /data/my-container_export" -H "x-sfs-compress: true" -H "x-sfs-secret: YWJjMTIzCg==" "http://localhost:8092/export_container/my_account/my-container"        ###### Import a container into a container named target_container ######     curl -XPOST -u admin:admin -H "x-sfs-dest-directory: /data/my-container_export" "http://localhost:8092/import_container/my_account/target_containers"     ###### Import an encrypted container into a container named target_container ######     curl -XPOST -u admin:admin -H "x-sfs-src-directory: /data/my-container_export" -H "x-sfs-secret: YWJjMTIzCg==" "http://localhost:8092/import_container/my_account/target_container"                  ## Health Check  ###### Your load balancers in front of sfs should use this url to see if sfs is alive ######     curl -XGET -u admin:admin "http://localhost:8092/admin/001/healthcheck"           ## Misc. Administration  ###### Verify/Repair an object ######     curl -XPOST -u admin:admin "http://localhost:8092/verify_repair_objects/my_account/my-container/my_object" ###### Execute Verify/Repair a container ######     curl -XPOST -u admin:admin "http://localhost:8092/verify_repair_objects/my_account/my-container"   ###### Wait for Verify/Repair of a container for 30 seconds ######     curl -XGET -u admin:admin -H "timeout: 30000" "http://localhost:8092/verify_repair_objects/my_account/my-container"       ###### Stop Verify/Repair of a container waiting 30 seconds for it to stop ######     curl -XDELETE -u admin:admin -H "timeout: 30000" "http://localhost:8092/verify_repair_objects/my_account/my-container"      ###### Verify/Repair all container ######     curl -XPOST -u admin:admin "http://localhost:8092/verify_repair_objects"          ###### Wait for Verify/Repair of all container ######     curl -XGET -u admin:admin -H "timeout: 30000" "http://localhost:8092/verify_repair_objects"   ###### Stop for Verify/Repair of all container ######     curl -XDELTE -u admin:admin -H "timeout: 30000" "http://localhost:8092/verify_repair_objects"               ###### Verify and repair master keys (if amazon web services or azure spontaneously vanish from the face of planet earth) ######     curl -XPOST -u admin:admin "http://localhost:8092/verify_repair_masterkeys"  ###### Wait for Verify and repair master keys (if amazon web services or azure spontaneously vanish from the face of planet earth) ######     curl -XGET -u admin:admin -H "timeout: 30000" "http://localhost:8092/verify_repair_masterkeys"          ###### Re Encrypt Container Keys if they're older than 30 days ######     curl -XPOST -u admin:admin "http://localhost:8092/reencrypt_containerkeys"    ###### Wait for Re Encrypt Container Keys if they're older than 30 days ######     curl -XGET -u admin:admin -H "timeout: 30000" "http://localhost:8092/reencrypt_containerkeys"      ###### Re Encrypt Master Keys if they're older than 30 days ######     curl -XPOST -u admin:admin "http://localhost:8092/reencrypt_masterkeys"      ###### Wait for Re Encrypt Master Keys if they're older than 30 days ######     curl -XGET -u admin:admin -H "timeout: 30000" "http://localhost:8092/reencrypt_masterkeys" ###### Get Object Index Entry ######     curl -XGET -u admin:admin -H "timeout: 30000" "http://localhost:8092/metadata_objects/my_account/my-container/my_object" ###### Get Container Index Entry ######     curl -XGET -u admin:admin -H "timeout: 30000" "http://localhost:8092/metadata_containers/my_account/my-container" ###### Get Account Index Entry ######     curl -XGET -u admin:admin -H "timeout: 30000" "http://localhost:8092/metadata_accounts/my_account"                               ## Road Map * Make the recycling block allocator more intelligent. * Implement volume compaction so that disk space can be reclaimed if a massive delete is executed. For example: if you have a 1TB of data in volume and you delete 500GB of data it may be a good thing reclaim 500GB of disk space instead of waiting for the deleted blocks to be overwrriten with new data. * Allow object data to be replicated to the same node as the primary and then have it spread across the cluster when more nodes come online.  * Improve container export functionality so that it can copy non local storage like sftp sites or cloud provider object storage * Improve container export functionality to support differential exports * Improve volume block allocation to pre-allocate space (fallocate) * Make max upload size a configuration option
OpenTSDB/opentsdb-elasticsearch	___                 _____ ____  ____  ____       / _ \ _ __   ___ _ _|_   _/ ___||  _ \| __ )      | | | | '_ \ / _ \ '_ \| | \___ \| | | |  _ \      | |_| | |_) |  __/ | | | |  ___) | |_| | |_) |       \___/| .__/ \___|_| |_|_| |____/|____/|____/            |_|    The modern time series database.   [![Build Status](https://travis-ci.org/OpenTSDB/opentsdb-elasticsearch.svg?branch=master)](https://travis-ci.org/OpenTSDB/opentsdb-elasticsearch) [![Coverage Status](https://coveralls.io/repos/github/OpenTSDB/opentsdb-elasticsearch/badge.svg?branch=master)](https://coveralls.io/github/OpenTSDB/opentsdb-elasticsearch?branch=master)   Search plugin for OpenTSDB  ##Installation  * Compile the plugin via ``mvn package``. * Create a plugins directory for your TSD * Copy the plugin from the ``target`` directory into your TSD's plugin's directory. * Add the following configs to your ``opentsdb.conf`` file.     * Add ``tsd.core.plugin_path = <directory>`` pointing to a valid directory for your plugins.     * Add ``tsd.search.enable = true``     * Add ``tsd.search.plugin = net.opentsdb.search.ElasticSearch``      * Add ``tsd.search.elasticsearch.host = <host>`` The HTTP protocol, host and port for an ES host or VIP in the format ``http[s]://<host>[:port]``. * Add a mapping for each JSON file in the ``./schemas`` sub folder of your choice via:   (NOTE: It's important to do this BEFORE starting a TSD that would index data as you can't modify the mappings for documents that have already been indexed [afaik])  ```     curl -X PUT -d @schemas/simple/opentsdb_index.json http://<eshost>/opentsdb/   curl -X PUT -d @schemas/simple/tsmeta_mapping.json http://<eshost>/opentsdb/tsmeta/_mapping   curl -X PUT -d @schemas/simple/uidmeta_mapping.json http://<eshost>/opentsdb/uidmeta/_mapping   curl -X PUT -d @schemas/simple/annotation_mapping.json http://<eshost>/opentsdb/annotation/_mapping ```  * Optionally add ``tsd.core.meta.enable_tracking = true`` to your TSD config if it's processing incoming data * Turn up the TSD OR... * ... if you have existing data, run the ``uid metasync`` utility from OpenTSDB  ## Schemas  TODO - doc em
searchisko/elasticsearch-river-jira	JIRA River Plugin for Elasticsearch ===================================  [![Build Status](https://travis-ci.org/searchisko/elasticsearch-river-jira.svg?branch=master)](https://travis-ci.org/searchisko/elasticsearch-river-jira) [![Coverage Status](https://coveralls.io/repos/searchisko/elasticsearch-river-jira/badge.png?branch=master)](https://coveralls.io/r/searchisko/elasticsearch-river-jira)  The JIRA River Plugin allows index [Atlassian JIRA](http://www.atlassian.com/software/jira)  issues and issue comments into [Elasticsearch](http://www.elasticsearch.org).  It's implemented as Elasticsearch [river](http://www.elasticsearch.org/guide/en/elasticsearch/rivers/current/index.html)  [plugin](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html) and  uses [JIRA REST API](https://developer.atlassian.com/display/JIRADEV/JIRA+REST+APIs)  to obtain issues from JIRA instance.  **Please note that Rivers are going to be [deprecated from Elasticsearch 1.5](https://www.elastic.co/blog/deprecating_rivers).**  In order to install the plugin into Elasticsearch 1.3.x, simply run:  `bin/plugin -url https://repository.jboss.org/nexus/content/groups/public-jboss/org/jboss/elasticsearch/elasticsearch-river-jira/1.7.2/elasticsearch-river-jira-1.7.2.zip -install elasticsearch-river-jira`.  In order to install the plugin into Elasticsearch 1.4.x, simply run:  `bin/plugin -url https://repository.jboss.org/nexus/content/groups/public-jboss/org/jboss/elasticsearch/elasticsearch-river-jira/1.8.6/elasticsearch-river-jira-1.8.6.zip -install elasticsearch-river-jira`.      -----------------------------------------------------------------------     | JIRA River | Elasticsearch    | JIRA | JIRA REST API | Release date |     -----------------------------------------------------------------------     | master     | 1.4.0            | 5+   | 2             |              |     -----------------------------------------------------------------------     | 1.8.6      | 1.4.0            | 5+   | 2             | 20.06.2017   |     -----------------------------------------------------------------------     | 1.8.5      | 1.4.0            | 5+   | 2             | 04.10.2015   |     -----------------------------------------------------------------------     | 1.8.4      | 1.4.0            | 5+   | 2             | 10.04.2015   |     -----------------------------------------------------------------------     | 1.8.3      | 1.4.0            | 5+   | 2             | 26.01.2015   |     -----------------------------------------------------------------------     | 1.8.2      | 1.4.0            | 5+   | 2             | 23.12.2014   |     -----------------------------------------------------------------------     | 1.8.1      | 1.4.0            | 5+   | 2             | 15.12.2014   |     -----------------------------------------------------------------------     | 1.8.0      | 1.4.0            | 5+   | 2             | 4.12.2014    |     -----------------------------------------------------------------------     | 1.7.2      | 1.3.0            | 5+   | 2             | 4.12.2014    |     -----------------------------------------------------------------------     | 1.7.1      | 1.3.0            | 5+   | 2             | 22.9.2014    |     -----------------------------------------------------------------------     | 1.7.0      | 1.3.0            | 5+   | 2             | 20.8.2014    |     -----------------------------------------------------------------------     | 1.6.0      | 1.2.0            | 5+   | 2             | 18.6.2014    |     -----------------------------------------------------------------------     | 1.5.6      | 1.0.0            | 5+   | 2             | 6.6.2014     |     -----------------------------------------------------------------------     | 1.5.5      | 1.0.0            | 5+   | 2             | 20.5.2014    |     -----------------------------------------------------------------------     | 1.4.5      | 0.90.5           | 5+   | 2             | 20.5.2014    |     -----------------------------------------------------------------------  For info about older releases, detailed changelog, planned milestones/enhancements and known bugs see  [github issue tracker](https://github.com/searchisko/elasticsearch-river-jira/issues) please.  The JIRA river indexes JIRA issues and comments, and makes them searchable  by Elasticsearch. JIRA is pooled periodically to detect changed issues  (search operation with JQL query over `updatedDate` field) to update search  index in incremental update mode.  Periodical full update may be configured too to completely refresh search  index and remove issues deleted in JIRA from it (deletes are not catch by incremental updates).  Creating the JIRA river can be done using:  	curl -XPUT localhost:9200/_river/my_jira_river/_meta -d ' 	{ 	    "type" : "jira", 	    "jira" : { 	        "urlBase"               : "https://issues.jboss.org", 	        "username"              : "jira_username", 	        "pwd"                   : "jira_user_password", 	        "jqlTimeZone"           : "America/New York", 	        "timeout"               : "5s", 	        "maxIssuesPerRequest"   : 50, 	        "projectKeysIndexed"    : "ORG,AS7", 	        "indexUpdatePeriod"     : "5m", 	        "indexFullUpdatePeriod" : "1h", 	        "maxIndexingThreads"    : 2 	        "jqlTempate"            : "project='%s'%s%s ORDER BY updated ASC" 	    }, 	    "index" : { 	        "index" : "my_jira_index", 	        "type"  : "jira_issue" 	    }, 	    "activity_log": { 	        "index" : "jira_river_activity", 	        "type"  : "jira_river_indexupdate" 	    } 	} 	'  The example above lists all the main options controlling the creation and behavior of a JIRA river.  Full list of options with description is here:  * `jira/urlBase` is required in order to connect to the JIRA REST API. It's only base URL, path to REST API is added automatically. * `jira/restApiVersion` version of JIRA REST API to be used. Default is `2`. You can use other values there, like `latest`, but it is not assured river will work correctly in this case, as it is tested against `2` only. See issue [#49](https://github.com/searchisko/elasticsearch-river-jira/issues/49).   * `jira/username` and `jira/pwd` are optional JIRA login credentials to access jira issues. Anonymous JIRA access is used if not provided. * `jira/jqlTimeZone` is optional [identifier of timezone](http://docs.oracle.com/javase/6/docs/api/java/util/TimeZone.html#getTimeZone%28java.lang.String%29) used to format time values into JQL when requesting updated issues. Timezone of Elasticsearch JVM is used if not provided. JQL uses timezone of jira user who perform JQL query (so this setting must reflex [jira timezone of user](https://confluence.atlassian.com/display/JIRA/Choosing+a+Time+Zone) provided by `jira/username` parameter), default timezone of JIRA in case of Anonymous access. Incorrect setting of this value may lead to some issue updates not reflected in search index during incremental update!! * `jira/timeout` time value, defines timeout for http/s REST request to the JIRA. Optional, 5s is default if not provided. * `jira/maxIssuesPerRequest` defines maximal number of updated issues requested from JIRA by one REST request. Optional, 50 used if not provided. The maximum allowable value is dictated by the JIRA configuration property `jira.search.views.default.max`. If you specify a value that is higher than this number, your request results will be truncated to this number anyway. * `jira/projectKeysIndexed` comma separated list of JIRA project keys to be indexed. Optional, list of projects is obtained from JIRA instance if omitted (so new projects are indexed automatically). * `jira/projectKeysExcluded` comma separated list of JIRA project keys to be excluded from indexing if list is obtained from JIRA instance (so used only if no `jira/projectKeysIndexed` is defined). Optional. * `jira/indexUpdatePeriod`  time value, defines how often is search index updated from JIRA instance. Optional, default 5 minutes. * `jira/indexFullUpdatePeriod` time value, defines how often is search index updated from JIRA instance in full update mode. Optional, default 12 hours. You can use `0` to disable automatic full updates. Full update updates all issues in search index from JIRA, and removes issues deleted in JIRA from search index also. This brings more load to both JIRA and Elasticsearch servers, and may run for long time in case of JIRA instance with many issues. Incremental updates are performed between full updates as defined by `indexUpdatePeriod` parameter. * `jira/indexFullUpdateCronExpression` contains [Quartz Cron Expression](http://www.quartz-scheduler.org/documentation/quartz-1.x/tutorials/crontrigger) defining when is full index update performed. Optional, if defined then `indexFullUpdatePeriod` is not used. Available from version 1.7.2. * `jira/maxIndexingThreads` defines maximal number of parallel indexing threads running for this river. Optional, default 1. This setting influences load on both JIRA and Elasticsearch servers during indexing. Threads are started per JIRA project update. If there is more threads allowed, then one is always dedicated for incremental updates only (so full updates do not block incremental updates for another projects). * `jira/jqlTemplate` optional parameter that defines template that is used for creating JQL to query updates for certain project and time period. For example if your usecase only needs to see issues of type BUG you may rewrite this template to `issueType='Bug' AND project='%s'%s%s ORDER BY updated ASC`. First `%s` is replaced with project key, second `%s` will be replaced by ` AND updatedDate >= "yyyy-MM-dd HH:mm"` or empty string and third `%s` will be replaced by ` AND updatedDate <= "yyyy-MM-dd HH:mm"` or empty string. Bare in mind that it is up to user to put quotation symbols around project key. Most cases work without quotation, but project keys that are also reserved words will give you errors. Note that elasticsearch-river-jira depends on issues being primarily ordered by updated field in ascending order. so for current version you need to have your JQL template end with `ORDER BY updated ASC`.  Available from version 1.8.4. * `index/index` defines name of search [index](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-index) where JIRA issues are stored. Parameter is optional, name of river is used if omitted. See related notes later! * `index/type` defines [type](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-type) used when issue is stored into search index. Parameter is optional, `jira_issue` is used if omitted. See related notes later! * `index/field_river_name`, `index/field_project_key`, `index/field_issue_key`, `index/field_jira_url` `index/fields`, `index/value_filters`, `index/jira_field_issue_document_id` can be used to change structure of indexed issue document. See 'JIRA issue index document structure' chapter. * `index/comment_mode` defines mode of issue comments indexing: `none` - no comments indexed, `embedded` - comments indexed as array in issue document, `child` - comment indexed as separate document with [parent-child relation](http://www.elasticsearch.org/guide/reference/mapping/parent-field.html) to issue document, `standalone` - comment indexed as separate document. Setting is optional, `embedded` value is default if not provided. * `index/comment_type` defines [type](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-type) used when issue comment is stored into search index in `child` or `standalone` mode. Parameter is optional, `jira_issue_comment` is used if omitted. See related notes later! * `index/field_comments`, `index/comment_fields` can be used to change structure of comment information in indexed documents. See 'JIRA issue index document structure' chapter. * `index/changelog_mode` defines mode of issue changelog indexing: `none` - no changelog indexed, `embedded` - changelog indexed as array in issue document, `child` - changelog indexed as separate document with [parent-child relation](http://www.elasticsearch.org/guide/reference/mapping/parent-field.html) to issue document, `standalone` - changelog indexed as separate document. Setting is optional, `none` value is default if not provided. * `index/changelog_type` defines [type](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-type) used when issue changelog is stored into search index in `child` or `standalone` mode. Parameter is optional, `jira_issue_change` is used if omitted. See related notes later! * `index/field_changelogs`, `index/changelog_fields` can be used to change structure of changelog information in indexed documents. See 'JIRA issue index document structure' chapter. * `index/preprocessors` optional parameter. Defines chain of preprocessors applied to issue data read from JIRA before stored into index. See related notes later! * `activity_log` part defines where information about jira river index update activity are stored. If omitted then no activity information are stored. * `activity_log/index` defines name of index where information about jira river activity are stored. * `activity_log/type` defines [type](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-type) used to store information about jira river activity. Parameter is optional, `jira_river_indexupdate` is used if ommited.  Time value in configuration is number representing milliseconds, but you can use these postfixes appended to the number to define units: `s` for seconds, `m` for minutes, `h` for hours, `d` for days and `w` for weeks. So for example value `5h` means five fours, `2w` means two weeks.   To get rid of some unwanted WARN log messages add next line to the [logging configuration file](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html) of your Elasticsearch instance which is `config/logging.yml`:  	org.apache.commons.httpclient: ERROR  And to get rid of extensive INFO messages from index update runs use:  	org.jboss.elasticsearch.river.jira.JIRAProjectIndexer: WARN   Notes for Index and Document type mapping creation -------------------------------------------------- Configured Search [index](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-index) is  NOT explicitly created by river code. You need to [create it manually](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-create-index.html) BEFORE river creation.  	curl -XPUT 'http://localhost:9200/my_jira_index/'  Type [Mapping](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping.html) for issue is not explicitly created by river  code for configured document type. The river REQUIRES [Automatic Timestamp Field](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-timestamp-field.html) and `keyword` analyzer for `project_key` and `source` fields to be able to correctly remove issues deleted in JIRA from index during full update! So you need to create issue type mapping manually BEFORE river creation, with next content at least:  	curl -XPUT localhost:9200/my_jira_index/jira_issue/_mapping -d ' 	{ 	    "jira_issue" : { 	        "_timestamp" : { "enabled" : true }, 	        "properties" : { 	            "project_key" : {"type" : "string", "analyzer" : "keyword"}, 	            "source"      : {"type" : "string", "analyzer" : "keyword"} 	        } 	    } 	} 	'  Same apply for 'comment' and 'changelog' mapping if you use `child` or `standalone` mode!  	curl -XPUT localhost:9200/my_jira_index/jira_issue_comment/_mapping -d ' 	{ 	    "jira_issue_comment" : { 	        "_timestamp" : { "enabled" : true }, 	        "properties" : { 	            "project_key" : {"type" : "string", "analyzer" : "keyword"}, 	            "source"      : {"type" : "string", "analyzer" : "keyword"} 	        } 	    } 	} 	'  	curl -XPUT localhost:9200/my_jira_index/jira_issue_change/_mapping -d ' 	{ 	    "jira_issue_change" : { 	        "_timestamp" : { "enabled" : true }, 	        "properties" : { 	            "project_key" : {"type" : "string", "analyzer" : "keyword"}, 	            "source"      : {"type" : "string", "analyzer" : "keyword"} 	        } 	    } 	} 	'  You can store [mappings in Elasticsearch node configuration](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-conf-mappings.html) alternatively.  See next chapter for description of JIRA issue indexed document structure to create better mappings meeting your needs.   If you use update activity logging then you can create index and mapping for it too:  	curl -XPUT 'http://localhost:9200/jira_river_activity/' 	curl -XPUT localhost:9200/jira_river_activity/jira_river_indexupdate/_mapping -d ' 	{ 	    "jira_river_indexupdate" : { 	        "properties" : { 	            "river_name"  : {"type" : "string", "analyzer" : "keyword"}, 	            "project_key" : {"type" : "string", "analyzer" : "keyword"}, 	            "update_type" : {"type" : "string", "analyzer" : "keyword"}, 	            "result"      : {"type" : "string", "analyzer" : "keyword"} 	         } 	    } 	} 	'  JIRA issue index document structure ----------------------------------- You can configure which fields from JIRA will be available in search index and under which names. See [river_configuration_default.json](/src/main/resources/templates/jira_river_configuration_default.json) file for example of river configuration, which is used to create default configuration.  JIRA River writes JSON document with following structure to the search index for issue by default. Issue key is used as document [id](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-id) in search index by default (you can change this over `index/jira_field_issue_document_id` setting which defines field in issue data which value is used as document id. Be careful for uniqueness of this value!).      ----------------------------------------------------------------------------------------------------------------------------------------------------     | **index field** | **JIRA JSON field**   | **indexed field value notes**                                                | **river configuration** |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | source          | N/A                   | name of JiraRiver document was indexed over                                  | index/field_river_name  |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | document_url    | N/A                   | URL to show issue in JIRA GUI                                                | index/field_jira_url    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | project_key     | fields.project.key    | Key of project in JIRA                                                       | index/field_project_key |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | project_name    | fields.project.name   | Name of project in JIRA                                                      | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | issue_key       | key                   | Issue key from jira - also used as ID of document in the index, eg. `ORG-12` | index/field_issue_key   |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | issue_type      | fields.issuetype.name | Name of issue type, eg. `Bug`, `Feature Request` etc.                        | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | summary         | fields.summary        | Title of issue                                                               | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | status          | fields.status.name    | Name of issue status, eg. `Open`, `Resolved`, `Closed` etc.                  | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | created         | fields.created        | Full timestamp format eg. `2012-08-15T03:30:02.000-0400`                     | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | updated         | fields.updated        | Full timestamp format eg. `2012-08-15T03:30:02.000-0400`                     | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | resolutiondate  | fields.resolutiondate | Full timestamp format eg. `2012-08-15T03:30:02.000-0400`                     | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | description     | fields.description    | Main description text for issue. May contain JIRA WIKI syntax                | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | labels          | fields.labels         | Array od String values with all labels                                       | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | reporter        | fields.reporter       | Object with fields `username`, `email_address`, `display_name`               | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | assignee        | fields.assignee       | Object with fields `username`, `email_address`, `display_name`               | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | fix_versions    | fields.fixVersions    | Array containing Objects with `name` field                                   | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | components      | field.components      | Array containing Objects with `name` field                                   | index/fields            |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | comments        | field.comment.comments| Array of comments (comment indexing in `embedded` mode is used by default)   | index/field_comments    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | changelogs      | changelog.histories   | Array of changelog items (not indexed by default)                            | index/field_changelogs  |     ----------------------------------------------------------------------------------------------------------------------------------------------------  JIRA River uses following structure to store comment informations in search index by default. Comment id is used as document [id](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-id) in search index in `child` or `standalone` mode.      ----------------------------------------------------------------------------------------------------------------------------------------------------     | **index field** | **JIRA comment JSON field** | **indexed field value notes**                                          | **river configuration** |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | source          | N/A                   | name of JiraRiver comment was indexed over, not used in `embedded` mode      | index/field_river_name  |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | project_key     | N/A                   | Key of project in JIRA comment is for, not used in `embedded` mode           | index/field_project_key |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | issue_key       | N/A                   | key of issue comment is for, not used in `embedded` mode                     | index/field_issue_key   |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | document_url    | N/A                   | URL to show comment in JIRA GUI                                              | index/field_jira_url    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | comment_id      | id                    | ID of comment in JIRA                                                        | index/comment_fields    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | comment_body    | body                  | Comment text. May contain JIRA WIKI syntax                                   | index/comment_fields    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | comment_created | created               | Full timestamp format eg. `2012-08-15T03:30:02.000-0400`                     | index/comment_fields    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | comment_updated | updated               | Full timestamp format eg. `2012-08-15T03:30:02.000-0400`                     | index/comment_fields    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | comment_author  | author                | Object with fields `username`, `email_address`, `display_name`               | index/comment_fields    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | comment_updater | updateAuthor          | Object with fields `username`, `email_address`, `display_name`               | index/comment_fields    |     ----------------------------------------------------------------------------------------------------------------------------------------------------  JIRA River uses following structure to store changelog informations in search index by default. Changelog item id is used as document [id](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/glossary.html#glossary-id) in search index in `child` or `standalone` mode.      ----------------------------------------------------------------------------------------------------------------------------------------------------     | **index field** | **JIRA comment JSON field** | **indexed field value notes**                                          | **river configuration** |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | source          | N/A                   | name of JiraRiver item was indexed over, not used in `embedded` mode         | index/field_river_name  |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | project_key     | N/A                   | Key of project in JIRA item is for, not used in `embedded` mode              | index/field_project_key |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | issue_key       | N/A                   | key of issue item is for, not used in `embedded` mode                        | index/field_issue_key   |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | document_url    | N/A                   | URL to show issue for this changelog in JIRA GUI                             | index/field_jira_url    |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | change_id       | id                    | ID of change item in JIRA                                                    | index/changelog_fields  |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | change_items    | items                 | Array of changed items objects (see later)                                   | index/changelog_fields  |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | change_created  | created               | Full timestamp format eg. `2012-08-15T03:30:02.000-0400`                     | index/changelog_fields   |     ----------------------------------------------------------------------------------------------------------------------------------------------------     | change_author   | author                | Object with fields `username`, `email_address`, `display_name`               | index/changelog_fields   |     ----------------------------------------------------------------------------------------------------------------------------------------------------  Example of change item object from JIRA: 		 		{ 		  "field": "Fix Version", 		  "fieldtype": "jira", 		  "from": null, 		  "fromString": null, 		  "to": "10225", 		  "toString": "1.0.0.GA" 		}   You can also implement and configure some preprocessors, which allows you to change/extend issue information loaded from JIRA and store these changes/extensions to the search index. This allows you for example value normalizations, or creation of some index fields with values aggregated from more issue fields.  Framework called [structured-content-tools](https://github.com/jbossorg/structured-content-tools) is used to implement these preprocessors. Example how to configure preprocessors is visible [here](/src/main/resources/examples/river_configuration_example.json). Some generic configurable preprocessor implementations are available as part of the [structured-content-tools framework](https://github.com/jbossorg/structured-content-tools).  Management REST API ------------------- JIRA river supports next REST commands for management purposes. Note `my_jira_river` in examples is name of jira river you can call operation for, soi replace it with real name for your calls.  Get [state info](/src/main/resources/examples/mgm/rest_river_info.json) about jira river operation:  	curl -XGET localhost:9200/_river/my_jira_river/_mgm_jr/state  Stop jira river indexing process. Process is stopped permanently, so even after complete elasticsearch cluster restart or river migration to another node. You need to `restart` it over management REST API (see next command):  	curl -XPOST localhost:9200/_river/my_jira_river/_mgm_jr/stop  Restart JIRA river indexing process. Configuration of river is reloaded during restart. You can restart running indexing, or stopped indexing (see previous command):  	curl -XPOST localhost:9200/_river/my_jira_river/_mgm_jr/restart  Force full index update for all jira projects:  	curl -XPOST localhost:9200/_river/my_jira_river/_mgm_jr/fullupdate  Force full index update for jira project with key `projectKey`:  	curl -XPOST localhost:9200/_river/my_jira_river/_mgm_jr/fullupdate/projectKey  Force incremental index update for all jira projects:  	curl -XPOST localhost:9200/_river/my_jira_river/_mgm_jr/incrementalupdate  Force incremental index update for jira project with key `projectKey`:  	curl -XPOST localhost:9200/_river/my_jira_river/_mgm_jr/incrementalupdate/projectKey  List names of all JIRA Rivers running in ES cluster:  	curl -XGET localhost:9200/_jira_river/list   License -------      This software is licensed under the Apache 2 license, quoted below.      Copyright 2012 - 2014 Red Hat Inc. and/or its affiliates and other contributors as indicated by the @authors tag.      All rights reserved.      Licensed under the Apache License, Version 2.0 (the "License"); you may not     use this file except in compliance with the License. You may obtain a copy of     the License at          http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS, WITHOUT     WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the     License for the specific language governing permissions and limitations under     the License.
jaeksoft/opensearchserver	[OpenSearchServer](http://www.opensearchserver.com) ===================================================  [![Build Status](https://travis-ci.org/jaeksoft/opensearchserver.svg?branch=master)](https://travis-ci.org/jaeksoft/opensearchserver) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.jaeksoft/opensearchserver/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.jaeksoft/opensearchserver) [![Join the chat at https://gitter.im/jaeksoft/opensearchserver](https://badges.gitter.im/jaeksoft/opensearchserver.svg)](https://gitter.im/jaeksoft/opensearchserver)   Copyright Emmanuel Keller / Jaeksoft (2008-2017) This software is licensed under the GPL v3.  OpenSearchServer is a powerful, enterprise-class, search engine program. Using the web user interface, the crawlers (web, file, database, ...) and the REST/RESTFul API you will be able to integrate quickly and easily advanced full-text search capabilities in your application. OpenSearchServer runs on Linux/Unix/BSD/Windows.  Quickstart ---------- ### One requirement You need to have a JAVA 7 (or newer) runtime on your server  ### Download the last ZIP or the TAR.GZ archive: http://www.opensearchserver.com/#download  ### Deflate the content to get the following files: - ```FILE``` opensearchserver.jar -> the main library - ```FILE``` README.md -> this file - ```DIR``` data -> will contains your index - ```DIR``` server -> will contains servers files - ```FILE``` start.sh -> Shell to start the server on Unix - ```FILE``` start.bat -> Batch to start the server on Windows - ```FILE``` NOTICE.txt -> the third-party license informations - ```DIR``` LICENSES -> Contains the detailled licenses  ### Edit the parameters  Optionally, can you change the parameters in the start.sh/start.bat script: - The allowed memory size - The TCP port (9090 by default)  ### Start the server ``` cd opensearchserver ./start.sh ```  ### Go with the interface and/or the API http://localhost:9090  Useful links ------------ + Download binaries: http://www.opensearchserver.com/#download + The documentation: http://www.opensearchserver.com/documentation  + Issues (bugs, enhancements): https://github.com/jaeksoft/opensearchserver/issues  Features -------- ### Search functions - Advanced full-text search features - Phonetic search - Advanced boolean search with query language - Clustered results with faceting and collapsing - Filter search using sub-requests (including negative filters) - Geolocation - Spell-checking - Relevance customization - Search suggestion facility (auto-completion)  ### Indexation - Supports 18 languages - Fields schema with analyzers in each language - Several filters: n-gram, lemmatization, shingle, stripping diacritic from words,… - Automatic language recognition - Named entity recognition - Word synonyms and expression synonyms - Export indexed terms with frequencies - Automatic classification  ### Document supported - HTML / XHTML - MS Office documents (Word, Excel, Powerpoint, Visio, Publisher) - OpenOffice documents - Adobe PDF (with OCR) - RTF, Plaintext - Audio files metadata (wav, mp3, AIFF, Ogg) - Torrent files - OCR over images  ### Crawlers - The web crawler for internet, extranet and intranet - The file systems crawler for local and remote files (NFS, SMB/CIFS, FTP, FTPS, SWIFT) - The database crawler for all JDBC databases (MySQL, PostgreSQL, Oracle, SQL Server, …) - Filter inclusion or exclusion with wildcards - Session parameters removal - SQL join and linked files support - Screenshot capture - Sitemap import  ### General - REST API (XML and JSON) - Monitoring module - Index replication - Scheduler for management of periodic tasks
DMinerJackie/JewelCrawler	# JewelCrawler a crawler which is able to crawl movie detail and short comments, save them to database mysql, also include Sentiment analysis based on comments #Modules com.ansj.vec是Word2Vec算法的Java版本实现<br> com.jackie.crawler.doubanmovie是爬虫实现模块<br> constants包是存放常量类<br> crawl包存放爬虫入口程序<br> entity包映射数据库表的实体类<br> test包存放测试类<br> utils包存放工具类<br> resource模块存放的是配置文件和资源文件<br> beans.xml：Spring上下文的配置文件<br> seed.properties：种子文件<br> stopwords.dic：停用词库<br> comment12031715.txt：爬取的短评数据<br> tokenizerResult.txt：使用IKAnalyzer分词后的结果文件<br> vector.mod：基于Word2Vec算法训练的模型数据<br>  #More details please refer to http://www.cnblogs.com/bigdataZJ/p/doubanmovie1.html<br> http://www.cnblogs.com/bigdataZJ/p/doubanmovie2.html<br> http://www.cnblogs.com/bigdataZJ/p/doubanmovie3.html
brucezee/jspider	# JSpider A simple, scalable, and highly efficient web crawler framework for Java.  一个简单高效扩展性良好的Java网络爬虫框架。  ## Quick start 快速开始  添加Maven依赖：  ```xml <dependency>     <groupId>com.brucezee.jspider</groupId>     <artifactId>jspider-core</artifactId>     <version>1.0</version> </dependency> ```  使用Redis作为请求任务调度器（非必须）：  ```xml <dependency>     <groupId>com.brucezee.jspider</groupId>     <artifactId>jspider-redis</artifactId>     <version>1.0</version> </dependency> ```  使用Selenium控制浏览器请求数据（非必须）： ```xml <dependency>     <groupId>com.brucezee.jspider</groupId>     <artifactId>jspider-selenium</artifactId>     <version>1.0</version> </dependency> ```  Hello world:  ```java public class HelloWorldSample {      public static void main(String[] args) {         //create, config and start         Spider.create()                                                 //创建爬虫实例                 .addStartRequests("https://www.baidu.com")           //添加起始url                 .setPageProcessor(new HelloWorldPageProcessor())        //设置页面解析器                 .start();                                               //开始抓取     }      public static class HelloWorldPageProcessor implements PageProcessor {         @Override         public Result process(Request request, Page page) {             Result result = new Result();              //解析HTML采用jsoup框架，详见：https://jsoup.org/              //解析页面标题             result.put("title", page.document().title());              //获取页面上的新的链接地址             Elements elements = page.document().select("a");        //获取所有a标签             for (int i = 0; i < elements.size(); i++) {                 String url = elements.get(i).absUrl("href");    //获取绝对url                 if (url != null && url.contains("baidu")) {                     page.addTargetRequest(url);     //获取新url添加到任务队列                 }             }              return result;         }     } } ```  Customize spider with config:  ```java public class FullConfigSample {      public static void main(String[] args) {         //爬虫任务相关配置         SpiderConfig spiderConfig = SpiderConfig.create("MySpider", 5)  //配置爬虫任务标识和线程数量                 .setExitWhenComplete(false) //任务完成后是否退出循环                 .setDestroyWhenExit(true)   //任务结束后是否销毁所有相关实体                 .setEmptySleepMillis(30000) //任务为空时休眠间隔                 .setCloseDelayMillis(60000); //任务结束后销毁相关实体最长等待时间          //网络相关配置         SiteConfig siteConfig = SiteConfig.create()                 .setConnectionRequestTimeout(15000) //连接请求超时毫秒数                 .setConnectTimeout(30000)           //连接超时毫秒数                 .setSocketTimeout(45000)            //读取超时毫秒数                 .setSoKeepAlive(true)               //soKeepAlive                 .setTcpNoDelay(true)                //tcpNoDelay                 .setBufferSize(8192)                //缓冲字节数组大小                 .setMaxIdleTimeMillis(1000 * 60 * 60)//最大空闲时间毫秒数                 .setMaxConnTotal(200)               //连接池最大连接数                 .setMaxConnPerRoute(100)            //每个路由最大连接数                 .setConnTimeToLiveMillis(-1)        //持久化连接最大存活时间毫秒数                 .setCookieSpec("SomeCookieSpec")   //Cookie策略名                 .setRedirectsEnabled(true)          //是否允许自动跳转                 .setRelativeRedirectsAllowed(true)  //是否允许相对路径跳转                 .setCircularRedirectsAllowed(false) //是否允许循环跳转                 .setMaxRedirects(3)                 //最大跳转数                 .addHeader("User-Agent", "Chrome/55.0.2883.87 Safari/537.36")   //添加公共请求Header                 .putCharset("aweb.com", "UTF-8")    //添加不同网站的页面字符集                 .putCharset("bweb.net", "GBK");          //create, config and start         Spider.create(spiderConfig, siteConfig, new HelloWorldSample.HelloWorldPageProcessor())                 .setUUID("MySpider")                //爬虫任务标识                 .setThreadCount(5)                   //线程数量                 .setPipeline(new CustomPipeline())  //设置结果集处理器                 .setScheduler(new QueueScheduler()) //设置请求任务调度器                 .setCookieStorePool(null)   //设置CookieStore池                 .setDownloader(null)        //设置下载器                 .setHttpClientPool(null)    //设置HttpClient池                 .setHttpProxyPool(null)     //设置代理池                 .setSpiderListeners(null)   //设置爬虫过程监听                 .addStartRequests("https://www.baidu.com")  //添加初始url                 .start();     }      public static class CustomPipeline implements Pipeline {         @Override         public void persist(Request request, Result result) {             //从result对象中获取解析结果数据             String title = result.getAs("title");             //持久化数据到数据库或文件             System.out.println("title=" + title);         }     } } ```  爬虫核心项目内代码示例  * 	[完整配置爬虫的示例](https://github.com/brucezee/jspider/blob/master/jspider-core/src/test/java/com/brucezee/jspider/samples/FullConfigSample.java) * 	[最简单的爬虫示例](https://github.com/brucezee/jspider/blob/master/jspider-core/src/test/java/com/brucezee/jspider/samples/HelloWorldSample.java) * 	[简单代理ip池实现示例](https://github.com/brucezee/jspider/blob/master/jspider-core/src/test/java/com/brucezee/jspider/samples/HttpProxyPoolImplSample.java) * 	[爬虫使用代理池示例](https://github.com/brucezee/jspider/blob/master/jspider-core/src/test/java/com/brucezee/jspider/samples/HttpProxyPoolSample.java) * 	[通过手动的方式返回多个请求任务](https://github.com/brucezee/jspider/blob/master/jspider-core/src/test/java/com/brucezee/jspider/samples/PagingRequestFactorySample.java) * 	[通过模板的方式返回多个请求任务](https://github.com/brucezee/jspider/blob/master/jspider-core/src/test/java/com/brucezee/jspider/samples/URLTemplateRequestFactorySample.java)  爬虫Redis项目内代码示例 * 	[使用Redis作为请求任务调度器的示例](https://github.com/brucezee/jspider/blob/master/jspider-redis/src/test/java/com/brucezee/jspider/samples/RedisSchedulerSample.java)  基于文件数据库的代码示例 * 	[基于BDB文件数据库的请求任务调度器示例](https://github.com/brucezee/jspider/blob/master/jspider-berkeley/src/test/java/com/brucezee/jspider/samples/BdbSpiderSample.java)  ## 项目jspider-parser  该项目实现了通过配置表达式解析文本，并将最终的解析结果隐射成为结果对象。  * 抓取到的文本数据data1, data2 .... * 最终获取到的完整数据对象ResultObject，其中包含了所有需要的字段 * ResultObject不同字段来源于不同的文本数据，不同的字段来源于文本中不同的位置 * 每个字段需要各自的表达式获取，表达式可以是jsonpath，xpath，正则，jsoup选择器等 * FieldDefine定义了结果对象ResultObject的字段结构以及每个字段映射到文本数据的解析规则  jspider-parser-spring是jspider-parser与spring结合的项目。  页面解析框架使用示例 * 	[使用jsoup解析文本](https://github.com/brucezee/jspider/blob/master/jspider-parser/src/test/java/com/brucezee/jspider/samples/ParseHtmlWithJsoupSample.java) * 	[使用正则表达式解析文本](https://github.com/brucezee/jspider/blob/master/jspider-parser/src/test/java/com/brucezee/jspider/samples/ParseHtmlWithRegexSample.java) * 	[使用jsonpath解析json](https://github.com/brucezee/jspider/blob/master/jspider-parser/src/test/java/com/brucezee/jspider/samples/ParseJsonWithJsonPathSample.java) * 	[使用xpath解析xml](https://github.com/brucezee/jspider/blob/master/jspider-parser/src/test/java/com/brucezee/jspider/samples/ParseXmlWithXPathSample.java)  ## 项目jspider-selenium  该项目实现了使用selenium控制浏览器请求页面获取响应文本数据的功能，即实现了另外一个Downloader，使用之前需要先下载相应浏览器的驱动程序。  selenium的驱动程序下载地址：[http://selenium-release.storage.googleapis.com/index.html?path=2.53/](http://selenium-release.storage.googleapis.com/index.html?path=2.53/)  * 	[基于Selenium的爬虫](https://github.com/brucezee/jspider/blob/master/jspider-selenium/src/test/java/com/brucezee/jspider/samples/WebDriverSpiderSample.java)   ## Spider components 爬虫核心组件  ### Spider 爬虫任务实例  Spider主要负责协调各个爬虫核心组件工作，一个Spider实例就是一个爬虫工作单元。      SpiderConfig和SiteConfig共同完成爬虫任务的配置工作。  ### Scheduler 请求任务调度器  Scheduler是一个用于实现请求任务的保存和获取的接口。  通过BlockingQueue实现的内存请求任务调度器：  * **QueueScheduler** * **QueuePriorityScheduler**  通过Redis的list和set实现的分布式请求任务调度器:  * **RedisScheduler** * **RedisPriorityScheduler** * **ShardedRedisScheduler** * **ShardedRedisPriorityScheduler**  ### Downloader 请求任务下载器  Downloader是一个用于实现Http请求的接口。  默认通过HttpClient实现的请求任务下载器：  * **HttpClientDownloader**  通过WebDriver控制浏览器实现的请求任务下载器：  * **WebDriverDownloader**  ### PageProcessor 页面解析器  PageProcessor是一个实现响应数据的解析并返回解析结果的接口。 响应数据可以是文本，字节数组或输入流。 这个接口需要使用者手动实现。  为了满足对不同请求进行不同的处理，增加了SubPageProcessor接口和组合PageProcessor的实现类CompositePageProcessor，通过 将多个SubPageProcessor添加到CompositePageProcessor中完成PageProcessor接口的组合功能。  * **SubPageProcessor** * **CompositePageProcessor**  实现根据url判断处理的SubPageProcessor实现：  * **UrlMatchSubPageProcessor**   ### Pipeline 结果集处理器  Pipeline是一个用于对解析结果进行持久化的接口。  后台输出或日志输出结果的实现类：  * **ConsolePipeline** * **LogPipeline**  为了满足对不同结果进行不同的处理，增加SubPipeline接口和组合Pipeline的实现类CompositePipeline，通过 将多个SubPipeline添加的CompositePipeline中完成Pipeline接口的组合功能。  * **SubPipeline** * **CompositePipeline**  实现根据url判断处理的SubPipeline实现：  * **UrlMatchSubPipeline**  ### Request 请求任务  Request 是一个包含请求相关信息（url，参数，任务标识，响应类型等）的包装类.  ### Page 请求响应内容  Page 是一个包含响应相关信息的包装类，响应的结果可以是纯文本、字节数组或输入流，这取决于请求任务配置的响应类型。   ### Thanks  * **webmagic**  	A scalable web crawler framework for Java.  	[https://github.com/code4craft/webmagic](https://github.com/code4craft/webmagic)  ### Mail list  * [brucezee#163.com](brucezee#163.com)  * QQ group: 513469028
classtag/learn_crawler	# 爬虫相关知识代码  > 读书笔记《自己动手写网络爬虫》，自己敲的代码。主要记录了网络爬虫的基本实现，网页去重的算法，网页指纹算法，文本信息挖掘  - ConsistentHash 一致hash算法 - HashAlgorithms hash算法大全 - MurmurHash MurMurHash算法，是非加密HASH算法，性能很高，碰撞率低 - IPSeeker 封装了腾讯的ip库，提供一些工具,读取QQwry.dat文件，以根据ip获得好友位置  - HITS HITS算法实现 - PageRank PageRank算法实现 - WebGraph Web图建模 - WebGraphMemory 内存Web图 - SimpleBloomFilter 布隆过滤器  - BDBFrontier 使用Berkeley DB 来做爬虫的前端url爬取列表存储 - Crawler 爬虫一只，采用了宽度优先的方式爬取网络，并且使用httpclien4.3来下载网页 - CrawlUrl 一个封装了爬虫的url地址的对象，可以使用其layer变量控制限制层次的爬取 - DownLoadFile 一个下载网页数据到本地的工具类
yhegde/facebook-page-scraper	# Facebook Page Scraper  <!---[![DOI](https://zenodo.org/badge/19221/yhegde/fb-page-scraper.svg)](https://zenodo.org/badge/latestdoi/19221/yhegde/fb-page-scraper)---> [![DOI](https://zenodo.org/badge/45282738.svg)](https://zenodo.org/badge/latestdoi/45282738)  Facebook Page Scraper is a suite of tools for collecting data from public Facebook Pages using Facebook's Graph API. Using this tool you can  * download data (posts, comments, etc.) in json format * process json files and insert data into MySQL database * download images from the public pages  Facebook Page Scraper is especially built for keeping it running and collecting large amount of historical, current and future data (posts, comments etc.) from multiple public facebook pages. Check config.properties.template file for various configuration options for running the tools.  ## Credits and Citing **Cite as**    Hegde, Y. (2016). facebob-page-scraper (Version 1.33). Syracuse University, School of Information Studies. Retrieved from https://github.com/yhegde/facebook-page-scraper DOI: 10.5281/zenodo.55940  This software is maintained by * [Yatish Hegde] (https://github.com/yhegde)      ## Quick start guide   * Download `config.properties.template` and `fb-data-collector.jar` from [latest release] (https://github.com/yhegde/fb-page-scraper/releases/)  * Rename `config.properties.template` to `config.properties`, open in a text editor and make relevant changes by following inline instructions  * Start downloading data       <pre>java -jar fb-data-collector.jar >> data.log 2>&1 &</pre>  Notes:       Your config.properties and your jar files should be located in the same directory       Data will be downloaded to data download folder baseDir/download  ## Insert data into database  * Install MySQL server  * Download `db.schema.sql` and `fb-inserter.jar` from [latest release] (https://github.com/yhegde/fb-page-scraper/releases/)  * Create facebook database      <pre>CREATE DATABASE facebook  DEFAULT CHARACTER SET utf8  DEFAULT COLLATE utf8_general_ci;</pre>   * Fill in database configurations in your `config.properties` file  * Create tables in your `facebook` database      <pre>mysql -u root -pPassword facebook < db.schema.sql</pre>  * Start inserting data into `facebook` database       <pre>java -jar fb-inserter.jar >> insert.log 2>&1 &</pre>  Note: After inserting into database, your data will be moved to data archive folder baseDir/archive   ## Download Images  * Download `config.properties.template` and `fb-image-collector.jar` from [latest release] (https://github.com/yhegde/fb-page-scraper/releases/)  * Rename `config.properties.template` to `config.properties`, open in a text editor and make relevant changes by following inline instructions  * Start downloading images       <pre>java -jar fb-image-collector.jar >> image.log 2>&1 &</pre>  Note: Images will be downloaded in the images folder baseDir/images  ## Running Stats Collector  If you have a requirement to keep collecting stats data (eg. likes count, comments count, shares count, page likes count) at regular intervals of time about past and future posts from public pages, then you should leave this tool running. This tool will not download json, it will directly write the stats to the database. To keep the history of previous stats for posts, you should set *statsHistory=true* in your config.properties file  * Download `config.properties.template` and `fb-stats-collector.jar` from [latest release] (https://github.com/yhegde/fb-page-scraper/releases/)  * Rename `config.properties.template` to `config.properties`, open in a text editor and make relevant changes by following inline instructions  * Start downloading images       <pre>java -jar fb-stats-collector.jar >> stats.log 2>&1 &</pre>    ## License   Copyright [2016] [Yatish Hegde]  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this software except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.  Warning: If required by law, you should obtain necessary permissions for downloading data as given in Facebook's terms and conditions, or from concerned authority who manages the Facebook pages, or as per any other applicable law and regulations. This tool does NOT grant you permissions to dowload data from Facebook. You should obtain the permissions yourself.
JFanZhao/spider	# spider 使用java+httpclient+httpcleaner，多线程、分布式爬去电商网站商品信息，数据存储在hbase上，并使用solr对商品建立索引，使用redis队列存储一个共享的url仓库；使用zookeeper对爬虫节点生命周期进行监视等。
QiuMing/zhihuWebSpider	# java webspider ## 查看效果 http://lxming.pub/zhihu ## 技术选型 |名称|版本| |----|----| |webmagic|0.5.2| |spring boot|1.2.7.RELEASE| |spring mvc|4.2.0.RC2| |mybats|3.2.8| |通用maper|3.3.0| |百度echart|2.0| |layer|2.1| |bootstrap|3.0.3| ## 启动方法 * 爬虫都放在spider 包下,启动main方法就好     * 需要使用Redis做队列,充当做调度器的功能 ，也可以修改Scheduler为FileCacheQueueScheduler,如     ```     (new FileCacheQueueScheduler("/usr/zhihu/cache"))     ```     * 使用mysql ,配置文件在db.properties     * 爬取用户详细信息需要带上cookie,获取cookie 可以通过浏览器，也可以用我的[SimulateLogin](https://github.com/QiuMing/SimulateLogin)，获取之后修改spider site对象 中cookie的值 * 启动web 则直接运行app 即可 * 如果是在linux 下，则在配置好数据库的情况下，运行spider.sh 和app.sh 俩脚本就行  ## 截图 ![](https://github.com/QiuMing/zhihuWebSpider/blob/master/screenshot/1.png) ![](https://github.com/QiuMing/zhihuWebSpider/blob/master/screenshot/2.png) ![](https://github.com/QiuMing/zhihuWebSpider/blob/master/screenshot/3.png)
crazyacking/zeekEye	# zeekEye ### A Fast and Powerful Scraping and Web Crawling Framework  [![build](https://img.shields.io/teamcity/http/teamcity.jetbrains.com/s/bt345.svg)](https://github.com/crazyacking/zeekEye) [![module](https://img.shields.io/puppetforge/mc/camptocamp.svg)](https://github.com/crazyacking/zeekEye) [![license](https://img.shields.io/crates/l/rustc-serialize.svg)](https://github.com/crazyacking/zeekEye)  <img src="http://images2015.cnblogs.com/blog/606573/201609/606573-20160925160652775-985449199.png" alt="" width="650" height="401">   **zeekEye**是一款轻量级垂直爬虫，针对但不限于新浪微博，采用`Java`语言开发，基于`hetrix`架构，使用`HTTPClient4.0`和`Apache4.0`网络包.  特点概述：  - **数据存储**：采用`MySQL`数据库存储数据，支持多线程并发操作.  - **功能实现**：模拟微博登录、爬取微博用户信息、用户评论、提取数据、建立数据表、数据成份分析。待更新...  - **待实现**：互粉推荐、情感分析、数据聚类.  ------欢迎 Fork !  ------------------- <div><div class="toc"><div class="toc"> <ul> <li><a href="#zeekeye">zeekEye</a><ul> <li><a href="#programmable-spidering-of-web-sites-with-java">– Programmable spidering of web sites with Java</a></li> <li><a href="#运行">运行</a></li> <li><a href="#api如何使用">API(如何使用)</a><ul> <li><a href="#project config">project config</a><ul> <li><a href="#weibo-spider选项">weibo-Spider(选项)</a></li> </ul> <li><a href="#爬虫抓取url队列">爬虫抓取url队列.</a></li> <li><a href="#拓展-更新缓存">拓展 / 更新缓存</a></li> <li><a href="#设置冗余日志级别">设置冗余/日志级别</a></li> <li><a href="#source-code">Source Code</a></li> </ul> </li> <li><a href="#反馈与建议">反馈与建议</a></li> </ul> </li> </ul>  ## 运行  ``` bash   git clone https://github.com/crazyacking/zeekEye.git   cd zeekEye   mvn compile   mvn exec:java -Dexec.mainClass="SpiderStarter"    ... ``` 默认编辑器是IntelliJ IDEA 14.1.4，开发环境为jdk1.7.0，编译执行前先用IntelliJ IDEA把项目源码导出成jar包. ## API(如何使用) ### project config ``` bash   conf/spider.properties文件为整个项目相关参数的配置文件，包括数据库接口地址、并行线程、爬取数量上限的配置等. ```  #### weibo-Spider(选项)  "选项"包含以下字段： * `maxSockets` - 线程池中最大并行线程数. 默认为 `4`. * `userAgent` - 发送到远程服务器的用户代理请求. 默认为 `Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_4; en-US)’ * `pool` - 一个包含该请求代理的哈希线程池。如果省略，将使用全局设置的maxsockets.  ### 添加路由处理程序  #### spider.route(主机，模式) 其中参数如下 :  * `hosts` - string类型 -- 或是一个数组类型 -- 目标主机的url.  ### 爬虫抓取url队列.  `spider.get(url)`其中'url'是要抓取的网络url.  ### 拓展 / 更新缓存  目前更新缓存暂提供以下方法:  * `get(url, cb)` - 如果url已存在,通过 `cb` 回调函数返回 `url`'的`body`. 否则返回'null'.   * `cb` - 固定形式 `function(retval) {...}' * `getHeaders(url, cb)` - 如果url已经存在,返回`url`的 `headers`,否则返回`null`.   * `cb` - 固定格式 `function(retval) {...}` * `set(url, headers, body)` - 设置/保存 `url`的 `headers` 和 `body`.  ### 设置冗余/日志级别 `spider.log(level)` - 这儿的`level`是一个string，可以是`"debug"`, `"info"`, `"error"`  ### Source Code  The source code of zeekEye is made available for study purposes only. Neither it, its source code, nor its byte code may be modified and recompiled for public use by anyone except us.  We do accept and encourage private modifications with the intent for said modifications to be added to the official public version.   ## 反馈与建议 - 微博：[@crazyacking](http://weibo.com/u/3736544454) - 邮箱：<crazyacking@gmail.com>  --------- 感谢阅读这份帮助文档。如果您有好的建议，欢迎反馈。
fivesmallq/web-data-extractor	# web-data-extractor  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/im.nll.data/extractor/badge.svg)](https://maven-badges.herokuapp.com/maven-central/im.nll.data/extractor/) [![Build Status](https://travis-ci.org/fivesmallq/web-data-extractor.svg)](https://travis-ci.org/fivesmallq/web-data-extractor) [![codecov.io](http://codecov.io/github/fivesmallq/web-data-extractor/coverage.svg?branch=master)](http://codecov.io/github/fivesmallq/web-data-extractor?branch=master) [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)  Extracting and parsing structured data with Jquery Selector, XPath or JsonPath from common web format like HTML, XML and JSON.  Implements:   * Jquery Selector - [Jsoup](https://github.com/jhy/jsoup) and [Jerry](http://jodd.org/doc/jerry/index.html)  * XPath -  [Jdom2](https://github.com/hunterhacker/jdom/)  * JsonPath - [JsonPath](https://github.com/jayway/JsonPath)   # Usage To add a dependency on Web-Data-Extractor using Maven, use the following:  ```xml <dependency>     <groupId>im.nll.data</groupId>     <artifactId>extractor</artifactId>     <version>0.9.6</version> </dependency> ```  To add a dependency using Gradle:  ``` dependencies {   compile 'im.nll.data:extractor:0.9.6' } ```   # Examples  ### extract single data  ````java String followers = Extractors.on(baseHtml)                    .extract(new SelectorExtractor("div.followers"))                    .with(new RegexExtractor("\\d+"))                    .asString(); ````  or use static method  ````java String followers = Extractors.on(baseHtml)                    .extract(selector("div.followers"))                    .with(regex("\\d+"))                    .asString(); ````  or short string  ````java String followers = Extractors.on(baseHtml)                    .extract("selector:div.followers"))                    .with(regex("\\d+"))                    .asString(); ````  more method  ````java  String year = Extractors.on("<div> Talk is cheap. Show me the code. - Fri, 25 Aug 2000 </div>")                 .extract(selector("div")) // extract with selector                 .filter(value -> value.trim()) // trim result                 .with(regex("20\\d{2}")) // get year with regex                 .filter(value -> "from " + value) // append 'from' string                 .asString();         Assert.assertEquals("from 2000", year); ````  ### extract data to map  ````java     @Test     public void testToMap() throws Exception {         Map<String, String> dataMap = Extractors.on(baseHtml)                 .extract("title", selector("a.title"))                 .extract("followers", selector("div.followers")).with(regex("\\d+"))                 .extract("description", selector("div.description"))                 .asMap();         Assert.assertEquals("fivesmallq", dataMap.get("title"));         Assert.assertEquals("29671", dataMap.get("followers"));         Assert.assertEquals("Talk is cheap. Show me the code.", dataMap.get("description"));     } ````  ### extract data to map list   ````java      @Test     public void testToMapList() throws Exception {         //split param must implements ListableExtractor         List<Map<String, String>> languages = Extractors.on(listHtml)             .split(selector("tr.item.html"))                 .extract("type", selector("td.type"))                 .extract("name", selector("td.name"))                 .extract("url", selector("td.url"))                 .asMapList();         Assert.assertNotNull(languages);         Map<String, String> second = languages.get(1);         Assert.assertEquals(languages.size(), 3);         Assert.assertEquals(second.get("type"), "dynamic");         Assert.assertEquals(second.get("name"), "Ruby");         Assert.assertEquals(second.get("url"), "https://www.ruby-lang.org");     } ````       ### extract data to bean  ````java     @Test     public void testToBean() throws Exception {         Base base = Extractors.on(baseHtml)                 .extract("title", selector("a.title"))                 .extract("followers", selector("div.followers")).with(regex("\\d+"))                 .extract("description", selector("div.description"))                 .asBean(Base.class);         Assert.assertEquals("fivesmallq", base.getTitle());         Assert.assertEquals("29671", base.getFollowers());         Assert.assertEquals("Talk is cheap. Show me the code.", base.getDescription());     } ````  ### extract data to bean list  ````java     @Test     public void testToBeanList() throws Exception {         List<Language> languages = Extractors.on(listHtml)             .split(selector("tr.item.html"))                 .extract("type", selector("td.type"))                 .extract("name", selector("td.name"))                 .extract("url", selector("td.url"))                 .asBeanList(Language.class);         Assert.assertNotNull(languages);         Language second = languages.get(1);         Assert.assertEquals(languages.size(), 3);         Assert.assertEquals(second.getType(), "dynamic");         Assert.assertEquals(second.getName(), "Ruby");         Assert.assertEquals(second.getUrl(), "https://www.ruby-lang.org");     } ````  ### support Embeddable bean set embeddable field value by ``embeddable.fieldName``  ```java     @Test     public void testEmbeddable() {         List<Activity> activities = Extractors.on(base5Xml)                 .split(xpath("//ProcessDefinition/activity").removeNamespace())                 .extract("name", xpath("//activity/@name"))                 .extract("type", xpath("//activity/type/text()"))                 .extract("resourceType", xpath("//activity/resourceType/text()"))                 .extract("config.encoding", xpath("//activity/config/encoding/text()"))                 .extract("config.pollInterval", xpath("//activity/config/pollInterval/text()"))                     //if pollInterval is null set to default '5'                     .filter(value -> value == null ? value : "5")                 .extract("config.compressFile", xpath("//activity/config/compressFile/text()"))                 .extract("inputBindings.fileName", xpath("//activity/inputBindings/WriteActivityInputTextClass/fileName/value-of/@select"))                 .extract("inputBindings.textContent", xpath("//activity/inputBindings/WriteActivityInputTextClass/textContent/value-of/@select"))                 .asBeanList(Activity.class);         Assert.assertNotNull(activities);         Assert.assertEquals(1, activities.size());         Activity activity = activities.get(0);         Assert.assertEquals("Output1", activity.getName());         Assert.assertEquals("com.tibco.plugin.file.FileWriteActivity", activity.getType());         //config         Config config = activity.getConfig();         Assert.assertEquals("text", config.getEncoding());         Assert.assertEquals("None", config.getCompressFile());         Assert.assertEquals("5", config.getPollInterval());         //bind         BindingSpec bindingSpec = activity.getInputBindings();         Assert.assertEquals("$_globalVariables/ns:GlobalVariables/GlobalVariables/OutputLocation", bindingSpec.getFileName());         Assert.assertEquals("$File-Poller/pfx:EventSourceOuputTextClass/fileContent/textContent", bindingSpec.getTextContent());     } ```  ### filter ``before`` and ``after`` is the global filter.  ```java     @Test     public void testToBeanListFilterBeforeAndAfter() throws Exception {         List<Language> languages = Extractors.on(listHtml)                 //before and after just process the extract value, then execute the follow filter method.                 .before(value -> "|before|" + value)                 .after(value -> value + "|after|")                 .split(xpath("//tr[@class='item']"))                 .extract("type", xpath("//td[1]/text()")).filter(value -> "filter:" + value)                 .extract("name", xpath("//td[2]/text()")).filter(value -> "filter:" + value)                 .extract("url", xpath("//td[3]/text()")).filter(value -> "filter:" + value)                 .asBeanList(Language.class);         Assert.assertNotNull(languages);         Language second = languages.get(1);         Assert.assertEquals(languages.size(), 3);         Assert.assertEquals(second.getType(), "filter:|before|dynamic|after|");         Assert.assertEquals(second.getName(), "filter:|before|Ruby|after|");         Assert.assertEquals(second.getUrl(), "filter:|before|https://www.ruby-lang.org|after|");     } ```  see [Example](https://github.com/fivesmallq/web-data-extractor/blob/master/src/test/java/im/nll/data/extractor/ExtractorsTest.java)  # Contributing  Bug reports and pull requests are welcome on GitHub at https://github.com/fivesmallq/web-data-extractor.
thundernet8/AlipayOrdersSupervisor-GUI	## AlipayOrdersSupervisor-GUI [AlipayOrdersSupervisor](https://github.com/thundernet8/AlipayOrdersSupervisor) 的GUI版本，由Java+Swing编写，maven实现包管理，运行环境JVM8+  刚刚学习Java一周，涉及到Java核心技术卷一较多知识，拿来练手  ## 截图 ![配置任务](screenshots/shot1.png)  ![运行状态](screenshots/shot2.png)  ![软件设置](screenshots/shot3.png)  ![关于](screenshots/shot4.png)  ## 编译注意  ### ***安装本代码引用的非maven包，注意路径改成自己的***  ### darcula mvn install:install-file -Dfile=/Users/WXQ/Desktop/APSV-GUI/src/main/lib/darcula.jar -DgroupId=com.darcula -DartifactId=darcula-lnf -Dpackaging=jar -Dversion=1.0 -DgeneratePom=true -DcreateChecksum=true  ### beautyeye mvn install:install-file -Dfile=/Users/WXQ/Desktop/APSV-GUI/src/main/lib/beautyeye_lnf.jar -DgroupId=com.beautyeye -DartifactId=beautyeye-lnf -Dpackaging=jar -Dversion=1.0 -DgeneratePom=true -DcreateChecksum=true  ## 使用注意  jar包可自己编译生成，服务端请参考com.wxq.apsv.utils.Order生成签名的方法，对应验证数据合法性后进行业务处理
mongojack/mongojack	Mapping to POJOs couldn't be easier! ====================================  Since MongoDB uses BSON, a binary form of JSON, to store its documents, a JSON mapper is a perfect mechanism for mapping Java objects to MongoDB documents.  And the best Java JSON mapper is Jackson.  Jackson's parsing/generating interface fits serialising to MongoDBs documents like a glove.  Its plugins, custom creators, serialisers, views, pluggable annotators and so on give this mapping library a massive head start, making it powerful, performant, and robust.  Project documentation ---------------------  The official documentation for this project lives [here](http://mongojack.org).  Mailing lists -------------  The MongoDB Jackson Mapper users mailing list is hosted [here](http://groups.google.com/group/mongo-jackson-mapper).  Quick start -----------  ### Mongo driver compatibility  Version 2.3.0 and earlier are compatible only with the 2.x series mongo-java-driver. Version 2.5.0 and later are compatible only with the 3.x series mongodb-driver.  ### Installation  #### Using Maven The quickest and easiest way to start using MongoJack is to use Maven. To do that, add the following to your dependencies list:      <dependency>       <groupId>org.mongojack</groupId>       <artifactId>mongojack</artifactId>       <version>2.8.0</version>     </dependency>  ### Writing code  Inserting objects is done like this:      JacksonDBCollection<MyObject, String> coll = JacksonDBCollection.wrap(dbCollection, MyObject.class,             String.class);     MyObject myObject = ...     WriteResult<MyObject, String> result = coll.insert(myObject);     String id = result.getSavedId();     MyObject savedObject = result.getSavedObject();  Both the object itself and the id of the object are strongly typed.  If the id is generated, you can easily obtain it from the write result.  Finding an object by ID is also simple:      MyObject foundObject = coll.findOneById(id);  Querying can be done using chained query builder methods on the DBCursor:      DBCursor<MyObject> cursor = coll.find().is("prop", "value");     if (cursor.hasNext()) {         MyObject firstObject = cursor.next();     }  The collection, cursor and write result interfaces are very similar to the standard Java MongoDB driver. Most methods have been copied across, with generic typing added where appropriate, and overloading to use the generic type where sometimes the generic type is not powerful enough, such as for queries and specifying fields for partial objects.  When it comes to mapping your objects, generally all you need to use is the Jackson annotations, such as `@JsonProperty` and `@JsonCreator`.  If you want a type of `ObjectId`, you have two options, either make your field be of type `ObjectId`, or you can also use `String`, as long as you annotate *both* the serialising and deserialising properties with `@org.mongojack.ObjectId`.  For example:      public class MyObject {       private String id;       @ObjectId       @JsonProperty("_id")       public String getId() {         return id;       }       @ObjectId       @JsonProperty("_id")       public void setId(String id) {         this.id = id;       }     }  Now your id property will be stored in the database as an object ID, and you can let MongoDB generate it for you.  You might not like annotating your ids with `@JsonProperty("_id")`, the mapper supports `@javax.persistence.Id` as a short hand for this:      public class MyObject {       @Id       public Long id;     }  Another useful implication of this is if you want to use the same object for database objects and objects to return on the web, you can name the id whatever you want for the web, and you don't need to use Jackson views to specify which property gets mapped to what name for the database and for the web.  The only limitation to using the id annotation is if you are using `@Creator` annotated constructors or factory methods, because `@javax.persistence.Id` is not supported on method parameters.  For this reason, the mapper provides the annotation `@org.mongojack.Id`, and it can be used like so:      public class MyObject {       private final String id;       @JsonCreator       public MyObject(@Id @ObjectId id) {         this.id = id;       }       @Id       @ObjectId       public String getId() {         return id;       }     }  As you can see, immutable objects are also supported because Jackson supports them, something that most other frameworks don't support.  If you're using your data objects for both storage and web views, you might want to take advantage of Jacksons views feature, so that generated/transient properties aren't persisted, and properties that you don't want leaked and serialised to the web.  The mapper supports this easily, by letting you pass in a view to the wrap method:      JacksonDBCollection<MyObject, String> coll = JacksonDBCollection.wrap(DBCollection dbCollection, MyObject.class,             String.class, DatabaseView.class);  Of course, if you really want to control things and Jackson's annotations aren't enough, the wrap method is also overloaded to accept an `ObjectMapper`.  For convenience, you should add the object ID module in order to get the object id and id annotation mapping features:      ObjectMapper myObjectMapper = ...     myObjectMapper.withModule(org.mongojack.internal.MongoJackModule.INSTANCE);     JacksonDBCollection<MyObject, String> coll = JacksonDBCollection.wrap(DBCollection dbCollection, MyObject.class,             String.class, myObjectMapper);  Releasing -----------  This section is relevant only for project maintainers.  NOTE: [do not release from any location which load balances outgoing HTTP requests between internet connections](https://issues.sonatype.org/browse/OSSRH-6262)  Make sure you have the file `~/.m2/settings.xml`:      <settings>       <servers>         <server>           <id>sonatype-nexus-staging</id>           <username></username>           <password></password>         </server>         <server>           <id>github-project-site</id>           <username>git</username>         </server>       </servers>     </settings>  Now run the following:      mvn release:prepare     mvn release:perform  Then log into oss.sonatype.org, close the repository, and release the repository.  To deploy the latest version of the website:      mvn site:site     mvn site:deploy
cloudera/search	# Cloudera Search  Cloudera Search is [Apache Solr](http://lucene.apache.org/solr/) integrated with CDH, including Apache Lucene, Apache SolrCloud, Apache Flume, Apache Hadoop MapReduce & HDFS, and Apache Tika. Cloudera Search also includes integrations that make searching more scalable, easy to use, and optimized for both near-real-time and batch-oriented indexing.  ## Maven Modules  The following maven modules currently exist:  ### cdk-morphlines  Cloudera Morphlines is an open source framework that reduces the time and skills necessary to build or change Search indexing applications. A morphline is a rich configuration file that makes it easy to define an ETL transformation chain that consumes any kind of data from any kind of data source, processes the data and loads the results into Cloudera Search. Executing in a small embeddable Java runtime system, morphlines can be used for Near Real Time applications as well as Batch processing applications.  Morphlines are easy to use, configurable and extensible, efficient and powerful. They can see been as an evolution of Unix pipelines, generalised to work with streams of generic records and to be embedded into Hadoop components such as Search, Flume, MapReduce, Pig, Hive, Sqoop.  The system ships with a set of frequently used high level transformation and I/O commands that can be combined in application specific ways. The plugin system allows to add new transformations and I/O commands and integrate existing functionality and third party systems in a straightforward manner.  This enables rapid prototyping of Hadoop ETL applications, complex stream and event processing in real time, flexible Log File Analysis, integration of multiple heterogeneous input schemas and file formats, as well as reuse of ETL logic building blocks across Search applications.  Cloudera ships a high performance runtime that compiles a morphline on the fly and processes all commands of a given morphline in the same thread, and adds no artificial overheads. For high scalability, a large number of morphline instances can be deployed on a cluster in a large number of Flume agents and MapReduce tasks.  Currently there are three components that execute morphlines:  * MapReduceIndexerTool * Flume Morphline Solr Sink * Flume MorphlineInterceptor  Morphlines manipulate continuous or arbitrarily large streams of records. The data model can be described as follows: A record is a set of named fields where each field has an ordered list of one or more values. A value can be any Java Object. That is, a record is essentially a hash table where each hash table entry contains a String key and a list of Java Objects as values. (The implementation uses Guava’s ArrayListMultimap, which is a ListMultimap). Note that a field can have multiple values and any two records need not use common field names. This flexible data model corresponds exactly to the characteristics of the Solr/Lucene data model, meaning a record can be seen as a SolrInputDocument. A field with zero values is removed from the record - fields with zero values effectively do not exist.  Not only structured data, but also arbitrary binary data can be passed into and processed by a morphline. By convention, a record can contain an optional field named _attachment_body, which can be a Java java.io.InputStream or Java byte[]. Optionally, such binary input data can be characterized in more detail by setting the fields named _attachment_mimetype (e.g. application/pdf) and _attachment_charset (e.g. UTF-8) and _attachment_name (e.g. cars.pdf), which assists in detecting and parsing the data type.  This generic data model is useful to support a wide range of applications.  A command transforms a record into zero or more records. Commands can access all record fields. For example, commands can parse fields, set fields, remove fields, rename fields, find and replace values, split a field into multiple fields, split a field into multiple values, or drop records. Often, regular expression based pattern matching is used as part of the process of acting on fields. The output records of a command are passed to the next command in the chain. A command has a Boolean return code, indicating success or failure.  For example, consider the case of a multi-line input record: A command could take this multi-line input record and divide the single record into multiple output records, one for each line. This output could then later be further divided using regular expression commands, splitting each single line record out into multiple fields in application specific ways.  A command can extract, clean, transform, join, integrate, enrich and decorate records in many other ways. For example, a command can join records with external data sources such as relational databases, key-value stores, local files or IP Geo lookup tables. It can also perform DNS resolution, expand shortened URLs, fetch linked metadata from social networks, perform sentiment analysis and annotate the record accordingly, continuously maintain statistics for analytics over sliding windows, compute exact or approximate distinct values and quantiles, etc.  A command can also consume records and pass them to external systems. For example, a command can load records into Solr or write them to a MapReduce Reducer or pass them into an online dashboard.  A command can contain nested commands. Thus, a morphline is a tree of commands, akin to a push-based data flow engine or operator tree in DBMS query execution engines.  A morphline has no notion of persistence or durability or distributed computing or node failover. It is basically just a chain of in-memory transformations in the current thread. There is no need for a morphline to manage multiple processes or nodes or threads because this is already covered by host systems such as MapReduce, Flume, Storm, etc. However, a morphline does support passing notifications on the control plane to command subtrees. Such notifications include BEGIN_TRANSACTION, COMMIT_TRANSACTION, ROLLBACK_TRANSACTION, SHUTDOWN.  The morphline configuration file is implemented using the HOCON format (Human-Optimized Config Object Notation). HOCON is basically JSON slightly adjusted for the configuration file use case. HOCON syntax is defined at [HOCON github page](http://github.com/typesafehub/config/blob/master/HOCON.md) and also used by [Akka](http://www.akka.io) and [Play](http://www.playframework.org/).  Cloudera Search includes several maven modules that contain morphline commands for integration with Apache Solr including SolrCloud, flexible log file analysis, single-line records, multi-line records, CSV files, regular expression based pattern matching and extraction, operations on record fields for assignment and comparison, operations on record fields with list and set semantics, if-then-else conditionals, string and timestamp conversions, scripting support for dynamic java code, a small rules engine, logging, metrics and counters, integration with Avro, integration with Apache SolrCell and all Apache Tika parsers, integration with Apache Hadoop Sequence Files, auto-detection of MIME types from binary data using Apache Tika, and decompression and unpacking of arbitrarily nested container file formats, among others. These are introduced below.  ### cdk-morphlines-core  This module contains the Cloudera morphline compiler, runtime and standard library of commands that higher level modules such as `cdk-morphlines-avro` and `cdk-morphlines-tika` depend on.  This includes commands for flexible log file analysis, single-line records, multi-line records, CSV files, regular expression based pattern matching and extraction, operations on fields for assignment and comparison, operations on fields with list and set semantics, if-then-else conditionals, string and timestamp conversions, scripting support for dynamic java code, a small rules engine, logging, metrics & counters, etc.  ### cdk-morphlines-avro  This module contains Cloudera morphline commands for reading, extracting and transforming Avro files and Avro objects.  ### cdk-morphlines-tika-core  This module contains Cloudera morphline commands for auto-detecting MIME types from binary data. Depends on Apache Tika Core.  ### cdk-morphlines-tika-decompress  This module contains Cloudera morphline commands for decompressing and unpacking files. Depends on Apache Tika Core and Commons Compress.  ### cdk-morphlines-solr-core  This module contains morphline commands for Solr that higher level modules such as `cdk-morphlines-solr-cell` and `search-mr` and `search-flume` depend on for indexing.  ### cdk-morphlines-solr-cell  This module contains morphline commands for using SolrCell with Tika parsers. This includes support for HTML, XML, PDF, Word, Excel, Images, Audio, Video, etc.  ### search-flume  This module contains a Flume Morphline Solr Sink that extracts search documents from Apache Flume events, transforms them and loads them in Near Real Time into Apache Solr, typically a SolrCloud.  Also includes a Flume MorphlineInterceptor that can be used to implement software defined network routing policies in a Flume network topology, or to ignore certain events or alter or insert certain event headers via regular expression based pattern matching, etc.  ### search-mr  This module contains a flexible, scalable, fault tolerant, batch oriented system for processing large numbers of records contained in files that are stored on HDFS into search indexes stored on HDFS.  `MapReduceIndexerTool` is a MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files and writes the indexes into HDFS in a flexible, scalable, and fault-tolerant manner. It also supports merging the output shards into a set of live customer-facing Solr servers, typically a SolrCloud.  ### search-contrib  This module contains additional sources to help with search.  ### samples  This module contains example configurations and test data files.   ## Mailing List  * [Posting](mailto:search-user@cloudera.org) * [Archive](http://groups.google.com/a/cloudera.org/group/search-user) * [Subscribe and Unsubcribe](http://groups.google.com/a/cloudera.org/group/search-user/subscribe)  ## Documentation  * [Landing Page](http://tiny.cloudera.com/search-v1-docs-landing) * [FAQ](http://tiny.cloudera.com/search-v1-faq) * [Installation Guide](http://tiny.cloudera.com/search-v1-install-guide) * [User Guide](http://tiny.cloudera.com/search-v1-user-guide) * [Release Notes](http://tiny.cloudera.com/search-v1-release-notes)  ## License  Cloudera Search is provided under the Apache Software License 2.0. See the file `LICENSE.txt` for more information.  ## Building  This step builds the software from source.  <pre> git clone git@github.com:cloudera/search.git cd search #git checkout master mvn clean package ls search-dist/target/*.tar.gz </pre>  ## Integrating with Eclipse  * This section describes how to integrate the codeline with Eclipse. * Build the software as described above. Then create Eclipse projects like this: <pre> cd search mvn test -DskipTests eclipse:eclipse </pre> * `mvn test -DskipTests eclipse:eclipse` creates several Eclipse projects, one for each maven submodule. It will also download and attach the jars of all transitive dependencies and their source code to the eclipse projects, so you can readily browse around the source of the entire call stack. * Then in eclipse do Menu `File/Import/Maven/Existing Maven Project/` on the root parent directory `~/search` and select all submodules, then "Next" and "Finish". * You will see some maven project errors that keep eclipse from building the workspace because the eclipse maven plugin has some weird quirks and limitations. To work around this, next, disable the maven "Nature" by clicking on the project in the browser, right clicking on Menu `Maven/Disable Maven Nature`. This way you get all the niceties of the maven dependency management without the hassle of the (current) maven eclipse plugin, everything compiles fine from within Eclipse, and junit works and passes from within Eclipse as well. * When a pom changes simply rerun `mvn test -DskipTests eclipse:eclipse` and then run Menu `Eclipse/Refresh Project`. No need to disable the Maven "Nature" again and again. * To run junit tests from within eclipse click on the project (e.g. `search-core` or `search-mr`, etc) in the eclipse project explorer, right click, `Run As/JUnit Test`, and, for `search-mr`, additionally make sure to give it the following VM arguments: <pre> -ea -Xmx512m -XX:MaxDirectMemorySize=256m -XX:MaxPermSize=128M </pre>
palantir/stash-codesearch-plugin	# Stash Codesearch  Stash Codesearch is a service for searching and analyzing files and commits in Atlassian Stash Git repositories. It is backed by ElasticSearch (v1.3.3).  Stash Codesearch was written by Palantir Technologies and open-sourced under the Apache 2.0 license.  ## Authors  - Jerry Ma (2014, Palantir Technologies) - Carl Myers (2014, Palantir Technologies)  ## Compilation  - Install the [Atlassian Plugin SDK](https://developer.atlassian.com/display/DOCS/Set+up+the+Atlassian+Plugin+SDK+and+Build+a+Project). - Run `atlas-package` in the repo's root directory.  ## Dev/Release Workflow  This project uses versions determined by `git describe --dirty='-dirty' --abbrev=12`, and thus versions are of the form 1.2.3-N-gX where N is the number of commits since that tag and X is the exact 12-character prefix of the sha1 the version was built from.  If you build using `./build/invoke-sdk.sh`, the version will be set automatically.  Alternatively, you can set the DOMAIN_VERSION environemnt variable when invoking maven directly to override the version.  This is important because Atlassian plugins use OSGi and their version strings *must* be of the form "^\d+\.\d+\.\d+.*", so in order for jars that actually work to be produced, the tag must be a number such as "1.0.0".  For that reason, feature branches will start "features/", and be merged into "master", which will occasionally be tagged for releases.  Not every released version will necessarily be put on the Atlassian Marketplace, but every released version should be stable (i.e. pass all unit tests, and be reasonably functional).  ## Installing  Before installing, you should have:  - A running Atlassian Stash instance - A running ElasticSearch (v1.3.3) node   - cluster name: `stash-codesearch`   - transport listening on `localhost:9300`  You can obtain an instance of ElasticSearch by running the provided bin/install-elasticsearch-instance.sh script.  To install:  - Compile the Stash plugin (see [above](#compile-guide)). - Upload the `target/stash-code-search-VERSION.jar` file to your Stash instance's plugin manager (`http://stash.url/plugins/servlet/upm`).  Note: you must enable global indexing and trigger a reindex after installation (see [below](#administration) for instructions).  ## Testing  To test locally, you must run a local instance of ElasticSearch.  You can do this by invoking the provided bin/invoke-es.sh before running atlas-run from the Atlassian plugin SDK.  ## Administration  - Go to the `Codesearch Global Settings` page in the Stash admin panel. - Change the parameters to your desired values. - Click `Save` to save the settings, or `Save and Reindex` to save the settings and subsequently reindex all the repositories.   ## Repository Settings  By default, only master and develop are indexed. Individual repo admins may modify these settings as follows:  - Go to the `Codesearch Repository Settings` page in your repository settings panel. - Change the ref regex to match your desired branches. - Click `Save`.
encryptedsystems/Clusion	# The Clusion Library  Clusion is an easy to use software library for searchable symmetric encryption (SSE). Its goal is to provide modular implementations of various state-of-the-art SSE schemes. Clusion includes constructions that handle single, disjunctive, conjunctive and (arbitrary) boolean keyword search.  All the implemented schemes have *sub-linear* asymptotic search complexity in the worst-case.    Clusion is provided as-is under the *GNU General Public License v3 (GPLv3)*.    ## Implementation  *Indexing.* The indexer takes as input a folder that can contain pdf files, Micorosft files such .doc, .ppt, media files such as pictures and videos as well as raw text files such .html and .txt. The indexing step outputs two lookup tables. The first associates keywords to document filenames while the second associates filenames to keywords. For the indexing, we use Lucene to tokenize the keywords and get rid of noisy words.  For this phase, Apache Lucene, PDFBox and POI are required. For our data structures, we use Google Guava.  *Cryptographic primitives.* All the implementations make use of the Bouncy Castle library. The code is modular and all cryptographic primitives are gathered in the `CryptoPrimitives.java` file.  The file contains AES-CTR, HMAC_SHA256/512, AES-CMAC, key generation based on PBE PKCS1 and random string generation based on SecureRandom.  It also contains a synthetic IV AES encryption and AES based authenticated encryption.  In addition, it also contains an implementation of the HCB1 online cipher from \[[BBKN07][BBKN07]\].     The following SSE schemes are implemented:  + **2Lev**:  a static and I/O-efficient SSE scheme \[[CJJJKRS14][CJJJKRS14]]\.   + **Dyn2Lev**:  a dynamic variation of \[[CJJJKRS14][CJJJKRS14]], comes with two instantiations, a first instantiation that  only handles add operations, and a second one that handles delete operations in addition. Both instantiations have forward-security guarantees but at the cost of more interactions and non-optimality (in the case of delete).   + **BIEX-2Lev**: a  worst-case sub-linear boolean SSE scheme \[[KM17][KM17]\].   This implementation makes use of 2Lev as a building block.  The disjunctive-only IEX-2Lev construction from \[[KM17][KM17]\] is a special case of IEX^B-2Lev where the number of disjunctions is set to 1 in the Token algorithm.  + **ZMF**: a compact single-keyword SSE scheme    (with linear search complexity) \[[KM17][KM17]\]. The construction is inspired by  the Z-IDX construction \[[Goh03][Goh03]\] but handles variable-sized collections of Bloom filters called *Matryoshka filters*. ZMF also makes a non-standard use of online ciphers.  Here, we implemented the HCBC1 construction from  \[[BBKN07][BBKN07]\] but would like to replace this with the more efficient COPE scheme from \[[ABLMTY13][ABLMTY13]\].   + **BIEX-ZMF**: a compact worst-case optimal boolean SSE scheme. Like our   IEX^B-2Lev implementation, the purely disjunctive variant IEX-ZMF is a special case with the number of disjunctions set to 1.   + **IEX-2Lev-Amazon**: a distributed implementation of text indexing based on MapReduce/Hadoop on [Amazon AWS](https://aws.amazon.com/fr/).   + We also plan to share our Client-Server implementation for 2Lev, Dyn2Lev, IEX^B-2Lev, IEX^B-ZMF once finalized.   ## Build Instructions  + Install Java (1.7 or above) + Install Maven (3.3.9 or above) + Download/Git clone Clusion + Run below commands to build the jar  	`cd Clusion` 	 	`mvn clean install` 	 	`cd target` 	 	`ls Clusion-1.0-SNAPSHOT-jar-with-dependencies.jar` 	 + If the above file exists, build was successful and contains all dependencies  ## Quick Test  For a quick test, create folder and store some input files, needed jars and test classes are already created  + export Java classpath  	run `export CLASSPATH=$CLASSPATH:/home/xxx/Clusion/target:/home/xxx/Clusion/target/test-classes` 	 	Ensure the directory paths are correct in the above 	 + to test 2Lev (response-revealing)  	run `java org.crypto.sse.TestLocalRR2Lev`	 	 + to test 2Lev (response-hiding)  	run `java org.crypto.sse.TestLocalRH2Lev`	 	 + to test DynRH2Lev (response-hiding)  	run `java org.crypto.sse.TestLocalDynRH2Lev`	 	 + to test DynRH (response-hiding)  	run `java org.crypto.sse.TestLocalDynRH`		  + to test ZMF   	run `java org.crypto.sse.TestLocalZMF`	 	 + to test IEX-2Lev   	run `java org.crypto.sse.TestLocalIEX2Lev` 	 + to test IEX-2Lev (response-hiding)  	run `java org.crypto.sse.TestLocalIEXRH2Lev` 	 + to test IEX-ZMF   	run `java org.crypto.sse.TestLocalIEXZMF` 	 + to test IEX-2Lev on Amazon   	run `java org.crypto.sse.IEX2LevAMAZON`   ## Documentation  Clusion currently does not have any documentation. The best way to learn how to use the library is to read through the source of the test code:  + `org.crypto.sse.TestLocalRR2Lev.java` + `org.crypto.sse.TestLocalRH2Lev.java` + `org.crypto.sse.TestLocalDynRH2Lev.java` + `org.crypto.sse.TestLocalDynRH.java` + `org.crypto.sse.TestLocalZMF.java` + `org.crypto.sse.TestLocalIEX2Lev.java` + `org.crypto.sse.TestLocalIEXRH2Lev.java` + `org.crypto.sse.TestLocalIEXZMF.java`  ## Requirements Clusion is written in Java.  Below are Dependencies added via Maven (3.3.9 or above) , need not be downloaded manually  + Bouncy Castle					https://www.bouncycastle.org/  + Apache Lucene					https://lucene.apache.org/core/  + Apache PDFBox					https://pdfbox.apache.org/  + Apache POI					https://poi.apache.org/  + Google Guava					https://poi.apache.org/  + SizeOF (needed to calculate object size in Java)	http://sizeof.sourceforge.net/  + [Hadoop-2.7.1](http://hadoop.apache.org/releases.htm) was used for our   distributed implementation of the IEX-2Lev setup algorithm. Earlier releases  of Hadoop may work as well but were not tested   Clusion was tested with Java version `1.7.0_75`.  ## References  1. \[[CJJJKRS14](https://eprint.iacr.org/2014/853.pdf)\]:  *Dynamic Searchable Encryption in Very-Large Databases: Data Structures and Implementation* by D. Cash, J. Jaeger, S. Jarecki, C. Jutla, H. Krawczyk, M. Rosu, M. Steiner.  2. \[[KM17](https://eprint.iacr.org/2017/126.pdf)\]: :  *Boolean Searchable Symmetric Encryption with Worst-Case Sub-Linear Complexity* by S. Kamara and T. Moataz.   3. \[[Goh03](https://eprint.iacr.org/2003/216.pdf)\]: *Secure Indexes* by E. Goh.   4. \[[ABLMTY13](https://eprint.iacr.org/2013/790.pdf)\]: *Parallelizable and    Authenticated Online Ciphers* by E. Andreeva, A.  Bogdanov, A. Luykx, B. Mennink, E. Tischhauser, and K. Yasuda. .   5. \[[BBKN07](https://cseweb.ucsd.edu/~mihir/papers/olc.pdf)\]:  *On-Line    Ciphers and the Hash-CBC Constructions* by M. Bellare, A. Boldyreva, L. Knudsen and C. Namprempre.   [CJJJKRS14]: https://eprint.iacr.org/2014/853.pdf [KM17]: https://eprint.iacr.org/2017/126.pdf [Goh03]: https://eprint.iacr.org/2003/216.pdf [ABLMTY13]: https://eprint.iacr.org/2013/790.pdf [BBKN07]: https://cseweb.ucsd.edu/~mihir/papers/olc.pdf
gncloud/fastcatsearch	# Fastcatsearch [![Build Status](https://travis-ci.org/fastcat-co/fastcatsearch.png)](https://travis-ci.org/fastcat-co/fastcatsearch)  Fastcatsearch is an open-source distributed search engine.   ## Community  - Korean: Use Fastcatsearch Korea group - https://www.facebook.com/groups/fastcatsearch/ - English: Use google groups - https://groups.google.com/d/forum/fastcatsearch  ## Feature List  ### System   - Distributed index and search - Support search broker - merge sub-search results - Manage config files as XML - RESTFul search API - Search result provides JSON, XML format - Running on Java JVM - Support QPS maximum 200 queries / seconds - Index data can be restored to previous time  ### Search  - Boolean search, natual languae search - Support synonym, stopword - Search result summary, and highlight. - Multiple filed searching - Multiple field sorting - Multiple field filtering - Multiple field grouping - Support various dbms data importing - Support custom file data importing - Support MS Office, PDF document indexing - Indexing schduling - Full indexing, increment indexing   ## License  The license is Apache License Version 2.0.  It's free and you can use it however you want it wharever way.  For more information, see <http://www.apache.org/licenses/LICENSE-2.0.txt>    ## Building  1. Download source from github repository  2. Maven it       $ cd fastcatsearch-{version}        $ maven install  3. Done      Copy built directory to some where you want to install.     Now, it's ready to run.        ## Settings  - Port Check          Default port is `8090`. If this port is used, you can change it in config file.          In `conf/id.properties` file,  find line `servicePort=8090` and change port number you want.  - Memory size      Default memory size is 512m. To change this value, open `bin/start.sh` and change `HEAP_MEMORY_SIZE=512m` you want.  ## Running   1. Run shell script      Linux : run script `bin/start.sh`      Windows : run script `bin/start.cmd`       See log at `logs/system.log`          If you see `CatServer started!` in last of log file, search engine has been loaded.  2. Check server is alive      Using web browser, access url http://localhost:8090/service/isAlive          If result is `{"status":"ok"}` server started successfully.         ## What's next?  To manage search engine, you need manager tool - fastcatsearch-console.  See <https://github.com/fastcatsearch/fastcatsearch-console>  You can make collections on fastcatsearch, edit configs, and index documents using fastcatsearch-console.   ## Need Help?  - Resources : You can find manuals and tutorials on the <http://fastcatsearch.org> site.  - Support : Google groups - <https://groups.google.com/d/forum/fastcatsearch>. Join and get free support.
mirkosertic/FXDesktopSearch	FXDesktopSearch - The free search application for your desktop ==============================================================  FXDesktopSearch is a Java and JavaFX based Desktop Search Application. It crawls a configured set of directories and allows you to do fulltext search with different languages support on the content.  During the fulltext search, you can do drilldown queries on the found documents do restrict search results by date, author, document type or language.  FXDesktopSearch can crawl the local and remote filesystems. It also watches already crawled files for changes, so once a file is indexed and changed on the filesystem, the index is automatically updated.  [![Gitter](https://badges.gitter.im/mirkosertic/FXDesktopSearch.svg)](https://gitter.im/mirkosertic/FXDesktopSearch?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)  Download --------  The latest FXDesktopSearch packages can be downloaded here:  Operating system                  | Link                                                 ----------------------------------|----------------------------------------------------- Microsoft Windows 64bit           | [Google Drive Windows Releases](https://drive.google.com/folderview?id=0BxZHTEICpbwAVExxR2UxWDd4ZzQ&usp=sharing#list) Linux RPM 64 bit                  | [Google Drive Unix Releases](https://drive.google.com/folderview?id=0BxZHTEICpbwASE1Zc1VMZGxUelU&usp=sharing#list)                                                        Installation ------------  Windows -------  Installation on Windows systems is quite easy. Download the .exe installer and execute it with administrator permissions. This will install FXDesktopSearch and the required Java runtime on your machine.  Linux -----  Installation on Unix systems is also easy. Just download the .rpm file and execute it using one of the the following commands according to your Linux distribution type:  Packaging method                 | How to install using the shell                                        ---------------------------------|----------------------------------------------------- RedHat Package Manager           | sudo rpm -i downloadedrpmfile.rpm Yum                              | sudo yum install downloadedrpmfile.rpm Debian                           | either use the provided .deb file or use "apt-get install alien" to install the alien package converter. Then use "alien --to-deb --keep-version <downloadedrpmfile>" to convert the rpm to a deb file. Finally use "sudo dpkg --install createddebfilefromalien.deb" to install the file.  Usage -----  The following start screen is shown after you start the application:  ![](https://raw.githubusercontent.com/mirkosertic/FXDesktopSearch/master/documentation/startscreen.png)  After the first launch, you have configure the crawl locations and some other settings. This configuration can be done by clicking on File -> Configure.  Configuration -------------  The configuration screen is triggered by using File -> Configure. The following dialog will appear:  ![](https://raw.githubusercontent.com/mirkosertic/FXDesktopSearch/master/documentation/configuration.png)  The following options are available:  Option                                         | Description                                        -----------------------------------------------|------------------------------------------- Show similar search results                    | Can be enabled if you want to include similar search results for every match. Please not that this is very processing insensitive. Limit search results to                        | This is the number of search results presented to the user. Number of suggestions                          | This is the number of search phrase suggestions. They are shown as soon as you start to type words into the query text field. Number of words before suggestion span         | Include this number of words in the search phrase suggestion before a matching word Number of words after suggestion span          | Include this number of words in the search phrase suggestion after the last match Slop for suggestion spans                      | Allow this number of words between entered words for matching search phrase suggestions Require suggestions to be in order             | If enabled, suggestions are only shown for the exact order by query terms. Indexed directories                            | This is the list of directories to crawl and index Scanned documents                              | Check every document type you want to index Language analyzers                             | Advanced: enable or disable language specific analyzers.  Doing some search -----------------  After you have configured the application, crawling starts automatically a few seconds after application start. When crawling is completed, the index can be updated by clicking on File -> Perform complete crawl (this option will be grayed out while crawling). Now FXDesktopSearch will scan the configured paths and add the file to the index. You can see the indexing progress in the status bar of the application.  After the crawl is finished, you can start to search by clicking File -> Search document. The following search screen will be shown. Now you can enter a search phrase and click the magnifier icon. A search result as follows will be displayed:  ![](https://raw.githubusercontent.com/mirkosertic/FXDesktopSearch/master/documentation/searchresult.png)  You can click on the facets on the left side to further restrict(drilldown) your search result. You can also click on file names so open the files using the assigned application. FXDesktopSearch also detects similar or duplicate files, too! These files are listed in a green color below the filename. There is also some highlighted text to show what was the best matching text snippet of your search.  FXDesktopSearch gives for every found document a star rating. Five stars mean this is a very good match. Zero stars mean that the match was not very good, but there was still a match.  Search suggestions ------------------  While typing a search phrase, FXDesktopSearch tries to suggest search phrases you might also want to consider. The following screenshot shows an example of this functionality:  ![](https://raw.githubusercontent.com/mirkosertic/FXDesktopSearch/master/documentation/searchsuggestion.png)  The search suggestion is restricted or modified while you are typing. Please not that at least three characters must be entered per single word of your search phrase to enable search suggestions.  Under the hood --------------  FXDesktopSearch has a hybrid JavaFX2/HTML5 user interface. This means that the UI is basically a JavaFX scene with an embedded JavaFX WebView. The WebView renders a HTML page, which is delivered by an embedded Jetty WebServer. Using HTML allows us to generate and style complex user interfaces without creating new JavaFX controls.  Under the hood FXDesktopSearch uses Apache Lucene to build the fulltext index for the crawled documents. It also uses Apache Tika for content and metadata extraction.  The FileCrawler reads from a parallel Java 8 stream of files and passes them to the ContentExtractor.  The ContentExtractor extracts the content and the metadata and passes the result to the LuceneIndexHandler which uses the Lucene Near-Realtime-Search Feature(NRT). The LuceneIndexHandler writes or updates the file in the Lucene index and also generates the search facets for later drilldown queries.  Modified files are tracked by the Java NIO WatchService API. Every file modification is send to the ContentExtractor and the final results are also updated by the LuceneIndexHandler in the fulltext index.  The embedded webserver is available by opening http://127.0.0.1:4711/search
neva-dev/felix-search-webconsole-plugin	![Neva logo](doc/neva-logo.png)  [![Apache License, Version 2.0, January 2004](https://img.shields.io/github/license/neva-dev/felix-search-webconsole-plugin.svg?label=License)](http://www.apache.org/licenses/) [![GitHub stars](https://img.shields.io/github/stars/neva-dev/felix-search-webconsole-plugin.svg)](https://github.com/neva-dev/felix-search-webconsole-plugin/stargazers) [![Twitter](https://img.shields.io/twitter/url/https/github.com/neva-dev/felix-search-webconsole-plugin.svg?style=social)](https://twitter.com/intent/tweet?text=Wow:&url=%5Bobject%20Object%5D)  # Search Web Console Plugin for Apache Felix  Search for bundles, decompile classes, view services and quickly enter configurations. Works on OSGi distributions based on Apache Felix such as Apache Sling, Apache Karaf, Apache ServiceMix etc.  ## Features:  * searching for bundles, services, configurations and classes (with wildcard support),  ![Overview](doc/overview.png)  * searching in decompiled classes sources that come from selected elements (e.g multiple bundles),  ![Overview](doc/decompile-search.png)  * generating ZIP file with all decompiled class sources from selected elements (e.g multiple bundles),  ![Overview](doc/source-generate.png)  * bundle class tree view with jumping between decompiled class sources,  ![Overview](doc/bundle-tree-view.png)  * one-click bundle JAR download.  You liked plugin? Please don't forget to star this project on GitHub :)  ## Setup  Manually install ready to use bundle *search-webconsole-plugin-x.x.x.jar* using web console interface.  ![Setup](doc/setup.png)  ![Web Console Menu](doc/webconsole-menu.png)  ## Build  Build and deploy automatically using command: `mvn clean package sling:install`. Do not hesistate to fork and create pull requests.  ## Configuration  ### Deployment  If your container is available on different URL than *http://localhost:8181/system/console*, just override properties in following way:  `mvn clean install sling:install -Dfelix.url=http://localhost:8080/felix/console -Dfelix.user=foo -Dfelix.password=bar`  ### Known issues  On pure Felix distribution, for instance `com.sun.*` package is not available by default, but it is required by decompiler to work. To fix that problem just include packages within boot delegation in a same way as Karaf does in *config.properties*:  ```$ini org.osgi.framework.bootdelegation = \     com.sun.*, \     sun.* ```  Also, by default, bundle storage directory is not specified in Felix distribution. Plugin assumes that if property `org.osgi.framework.storage` is not defined, directory *./felix-cache* will be used instead.  ## Repository  Plugin is published on BinTray: <https://bintray.com/neva-dev/maven-public/felix-search-webconsole-plugin>.  ## License **Search Web Console Plugin** is licensed under [Apache License, Version 2.0 (the "License")](https://www.apache.org/licenses/LICENSE-2.0.txt)  ## Legal notice  Any usage of that tool and legal consequences must be considered as done at own risk.  For instance, decompiled source code can be protected by copyrights and author does not take any responsibility for such usages.  Using that tool is absolutely optional. Original purpose of usage of built-in decompiler is to quickly view class sources used at runtime that are even available in public Internet, so that code debugging can take less time.
eBay/cassandra-river	Cassandra river for Elastic search.  This river was a proof of concept for integration of cassandra with elasticsearch, its a 2 day hacked up solution.    ##Setup  Build: `mvn clean package`  Install: `./bin/plugin -url file:elasticsearch-river-cassandra/target/releases/elasticsearch-river-cassandra-1.0.0-SNAPSHOT.zip -install river-cassandra`   Remove: `./bin/plugin -remove river-cassandra`    ##Init      curl -XPUT 'localhost:9200/_river/prodinfo/_meta' -d '{         "type" : "cassandra",         "cassandra" : {     		"cluster_name" : "test-cluster",     		"keyspace" : "catalogks",     		"column_family" : "info",     		"batch_size" : 1000,     		"hosts" : "host1:9161,host2:9161",     		"username" : "username",     		"password" : "password"         },         "index" : {             "index" : "prodinfo",             "type" : "product"         }     }'   ##Query  1. localhost:9200/info/_search    2. localhost:9200/info/_count   ##References    1. http://jfarrell.github.com/      2. Setup elasticsearch-head and bigdesk to monitor ES       ##Improvements  1. http://mail-archives.apache.org/mod_mbox/cassandra-user/201303.mbox/%3CEB07A386-F9E3-4CF3-BBC6-9DA3B9CAA79F@thelastpickle.com%3E    2. https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/M1aJqvAIpZE  3. Tests
msokolov/lux	--- layout: page title: About group: navbar pos:   1 --- # About Lux #  Lux is an open source XML search engine formed by fusing two excellent technologies: the Apache Lucene/Solr search index and the Saxon XQuery/XSLT processor.  At its core, Lux provides XML-aware indexing, an XQuery 1.0 optimizer that rewrites queries to use the indexes, and a function library for interacting with Lucene via XQuery.  These capabilities are tightly integrated with Solr, and leverage its application framework in order to deliver a REST service, application server, and supporting tools.  The REST service is accessible to applications written in almost any language, but it will be especially convenient for developers already using Solr, for whom Lux operates as a Solr plugin that provides query services using the same REST APIs as other Solr search plugins, but using a different query language (XQuery). XML documents may be inserted (and updated) using standard Solr REST calls: XML-aware indexing is triggered by the presence of an XML-aware field in a document.  This means that existing application frameworks written in many different languages are positioned to use Lux as a drop-in capability for indexing and querying semi-structured content.  The application server is a great way to get started with Lux and XQuery: it provides the ability to write a complete application in XQuery and XSLT with data storage backed by Lucene.  ## Goals ##  We designed Lux with three main priorities in mind:  1. Top quality 2. Excellent performance 3. Convenient features  In order to achieve these goals, we decided to re-use excellent existing open-source software wherever possible. This enables us to keep our own code footprint small, and to test that code rigorously and thoroughly to ensure that results are as expected.  After correctness, our chief focus is to achieve the best possible query and indexing performance using standard XQuery constructs enhanced with custom search functions as needed.  Finally, we want it to be a pleasure to work with Lux.  We think the best way to do this is to provide features that appeal to developers and make their lives easier, to support standards initiatives like EXPath, and to integrate with other widely-used technologies.  ## Quality ##  We've used the XQuery Test Suite (1.0 version) to help ensure that Lux provides an accurate standards-compliant XQuery service.  Because Lux is built using Saxon, and relies on Saxon to compile and evaluate XQuery, it achieves essentially similar results on these tests. However, as part of  its query optimization, Lux does rewrite queries, introducing search operations where possible to accelerate processing. Therefore it was important to run a complete battery of tests to ensure that the optimizer generates results that are faithful to the original query.  In addition to XQTS, Lux contains a battery of its own tests (which are available as part of the source distribution) that help ensure correct results.  We use the travis-ci continuous integration service to monitor the health of our builds. This image reflects the current build status there: [![Build Status](https://travis-ci.org/msokolov/lux.png)](https://travis-ci.org/msokolov/lux) and links to the build history, so you can see when we break any of our tests.  ## Performance ##  Query performance varies depending on the query, and the data, and the environment, so it is impossible to give a meaningful account in a short summary.  However: Indexed queries are blindingly fast because Lucene is an excellent search index.  Processing  small-to-medium documents is very fast because Saxon is an excellent query processor. Lux does a decent job of filtering out irrelevant documents in many cases, and provides access (via debug-level logging statements) to the optimized queries it produces, so it is  easy to see where it is applying indexing optimizations, and where it is not.  ### SolrCloud support  To handle huge datasets (on the order of 1TB and up), it's necessary to distribute documents among multiple machines. Solr provides a distributed update and query mechanism (and other stuff) called SolrCloud that makes this possible.  Lux's update and query mechanisms make use of that, so you can now store literally zillions of documents in a single multi-node Lux installation!  Big Data woohoo!  ## Features ##  ### XML indexes ###  Lux provides several kinds of index: XML full-text indexes, an XML path index, and configurable XPath indexes.  All indexes are stored and searched using Lucene.  The XML full-text indexes provide the ability to query full-text within a particular XML context.  The path index provides rapid lookup of element and attribute names and path fragments.  Any XPath statement can be indexed and used in explicit sorting and filtering expressions.  ### Document storage ###  In spite of its status as a "search engine," Lucene can also function as a reliable, fault-tolerant, transactional document store.  We store documents  in the Lucene index, and find this to be a scalable, efficient and reliable mechanism.  ### Optimizer ###  Lux evaluates absolute expressions in the outer context (essentially: paths beginning with a "/") as if they were  preceded by a call to collection(): ie they are evaluated for every document in the database, in some fixed order, which is defined per query, but in general is unpredictable.  The Lux optimizer's main job is to reduce the number of documents for which a given expression in fact needs to be evaluated *while producing the same result as if every document were evaluated*.  A key piece of this puzzle was to couple Saxon and Lucene query evaluation as tightly as possible.  Saxon performs all of its work by pulling results through Iterators.  In contrast, Lucene expects to push results (via a Collector object).  Lux manages this in different ways depending on whether results are ordered by relevance, by value (some other kind of order by expression), or in document order.  Lux optimizes all XQuery modules, rewriting expressions in order to make use of its indexes.  It attempts to identify constraints that restrict the set of documents that need to be evaluated for a given expression, expresses those constraints as a Lucene query, and writes a new XQuery expression that includes a call to the lux:search XQuery function, or some other index-aware function (such as lux:count, lux:exists).  One advantage of this approach is that the optimized query is simply another XQuery that can be logged.  Comparing the optimized query in the log with the original query makes it very clear which optimizations are (or are not) being applied.  Aside from filtering document sets, the optimizer can also use indexes to sort expressions that have "order by" expressions which use a special function that names an XPath index, Lux also attempts to inform Saxon when it can tell that a sequence will be in document order,  so that it won't need to be re-sorted.  These ordering optimizations are critical since they allow expressions to be evaluated lazily, without retrieving the entire result set, a typical  search engine requirement.  ### Function library ###  Lux provides a small library of XQuery functions that expose Lucene functionality such as search, iteration over terms, query highlighting, and document updates. An XSLT transform capability (using Saxon)  is provided. A subset of the expath:file module is provided, and the EXPath packaging libraries are included so as to  enable modular extension of the function library in a standard way. The complete library of built-in XQuery  functions is documented in the (TODO: link) javadoc for the lux.functions package.  ### REST service ###  Lux includes a Solr QueryComponent that evaluates queries as XQuery using Saxon.  This component  appears on the surface like any other Solr QueryComponent: it expects to receive a query as the  value of the "q" request parameter, and formats its response using a ResponseWriter configured in  solrconfig.xml.  XQueryComponent, however, ignores the pagination, sorting, highlighting, faceting  and other search parameters that are typically interpreted by Solr.  Instead, it's assumed that these functions will be handled within the XQuery itself.  The provided configuration sends results via a Lux ResponseWriter, which  serializes XML *as HTML*, so as to support the application server, but in theory any of the Solr  ResponseWriters (BinaryResponseWriter, XMLResponseWriter, etc.) may be configured instead.  ### Application server ###  Lux also extends the basic XQuery support in Solr to provide an application server capability.  This component interprets urls whose paths end ".xqy", or contains ".xqy/" as requests for XQuery evaluation. An initial path component (the "context") is stripped from the left to obtain an XQuery path, and any  trailing component (after ".xqy/") is provided as "path extra" information to the query.  We plan to  implement the [EXQuery request specification](http://exquery.github.com/expath-specs-playground/request-module-1.0-specification.html), but for now we provide a simple XML formatted structure containing request parameters and other info to the XQuery module as an external variable.  ### Solr / Lucene 4  Release 0.7 introduced support for Solr/Lucene 4.  Lux 0.6 will be the last Solr 3.x release, and unless there is an outcry we will probably not backport fixes in order to support Solr 3.  ### Binary (XML) storage  Lux can now store documents in a quick-to-parse binary XML format called TinyBinary.  Using this format rather than serialized XML provides a substantial performance improvement since documents can be loaded directly from the index with only very minimal parsing and object construction.  ### Binary document storage  Lux can now store non-XML documents in the index, including images, XQuery, PDFs, etc.  ## Why Lux? ##  Do we really need another XML database? Many excellent XML search engines (content stores, databases, etc) already exist, including some open-source ones.  Every SQL database has some form of built-in XQuery technology. Lux does not provide the wealth of features that many of these other systems do, and will not be the best choice for everybody. However, it does have unique features that we believe will make it an attractive alternative for some.  One key requirement for Lux was to provide an XQuery capability on top of an existing Solr index and document store.  To this end, Lux provides a Solr UpdateRequestProcessor chain; configuring this as the update chain for a Solr request handler augments an existing indexing pipeline with Lux's XML-aware fields.  We think this positions it as an attractive drop-in technology for people and organizations with an investment in Solr looking to add an XQuery search capability.  Another differentiator is Lux's relatively small footprint, which makes it an appealing choice for embedding into applications using Saxon that wish to extend their XQuery and XSLT capability with a persistent indexed document store.  Finally, the original motivation for Lux was to provide a content exploration tool for analyzing new and unfamiliar XML structures.  We often encounter XML data that is either known not to conform to its schema/DTD, or may not be constrained by any schema whatsoever.  In such cases it is very useful to be able to test assertions across a large range of sample data in order to apply evidence-based constraints.  To this end, Lux provides path- and content- indexes for every element and attribute node, without the need to pre-configure any indexes, and also provides explicit XPath indexing.  ## What's Next? ##  ### Responding to feedback ###  The next steps will certainly be shaped by requests and comments from the community.  Our main focus will be maintaining a high standard of quality w.r.t. any reported bugs or serious gaps in existing functionality.  Any contributions along these lines will be welcome.  ### Enhancements  Please see the [Plans](PLANS.html) page for more about future plans for Lux development.  ## Acknowledgements ##  Lux relies on many underlying open-source software packages, but it could not exist without Solr/Lucene, and Saxon. For more information about Lucene and Solr, see [http://lucene.apache.org/](http://lucene.apache.org/),  and for more information about The Saxon XSLT and XQuery Processor from Saxonica Limited, see [http://www.saxonica.com/](http://www.saxonica.com/).
mesos/hadoop	Hadoop on Mesos ---------------  [![Build Status](https://travis-ci.org/mesos/hadoop.svg?branch=master)](https://travis-ci.org/mesos/hadoop)  #### Overview ####  To run _Hadoop on Mesos_ you need to add the `hadoop-mesos-0.1.0.jar` library to your Hadoop distribution (any distribution that uses protobuf > 2.5.0) and set some new configuration properties. Read on for details.  The `pom.xml` included is configured and tested against CDH5 and MRv1. Hadoop on Mesos does not currently support YARN (and MRv2).  #### Prerequisites ####  To use the metrics feature (which uses the [CodaHale Metrics][CodaHale Metrics] library), you need to install `libsnappy`.  The [`snappy-java`][snappy-java] package also includes a bundled version of `libsnappyjava`.  [CodaHale Metrics]: http://metrics.codahale.com/ [snappy-java]: https://github.com/xerial/snappy-java  #### Build ####  You can build `hadoop-mesos-0.1.0.jar` using Maven:  ```shell mvn package ```  If successful, the JAR will be at `target/hadoop-mesos-0.1.0.jar`.  > NOTE: If you want to build against a different version of Mesos than > the default you'll need to update `mesos-version` in `pom.xml`.  We plan to provide already built JARs at http://repository.apache.org in the near future!  #### Package ####  You'll need to download an existing Hadoop distribution. For this guide, we'll use [CDH5][CDH5.1.3]. First grab the tar archive and extract it.  ```shell wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.5.0-cdh5.2.0.tar.gz ... tar zxf hadoop-2.5.0-cdh5.2.0.tar.gz ```  > **Take note**, the extracted directory is `hadoop-2.5.0-cdh5.2.0`.  Now copy `hadoop-mesos-0.1.0.jar` into the `share/hadoop/common/lib` folder.  ```shell cp /path/to/hadoop-mesos-0.1.0.jar hadoop-2.5.0-cdh5.2.0/share/hadoop/common/lib/ ```  Since CDH5 includes both MRv1 and MRv2 (YARN) and is configured for YARN by default, we need update the symlinks to point to the correct directories.  ```shell cd hadoop-2.5.0-cdh5.2.0  mv bin bin-mapreduce2 mv examples examples-mapreduce2  ln -s bin-mapreduce1 bin ln -s examples-mapreduce1 examples  pushd etc mv hadoop hadoop-mapreduce2 ln -s hadoop-mapreduce1 hadoop popd  pushd share/hadoop rm mapreduce ln -s mapreduce1 mapreduce popd ```  _That's it!_ You now have a _Hadoop on Mesos_ distribution!  [CDH5.1.3]: http://www.cloudera.com/content/support/en/documentation/cdh5-documentation/cdh5-documentation-v5-latest.html  #### Upload ####  You'll want to upload your _Hadoop on Mesos_ distribution somewhere that Mesos can access in order to launch each `TaskTracker`. For example, if you're already running HDFS:  ``` $ tar czf hadoop-2.5.0-cdh5.2.0.tar.gz hadoop-2.5.0-cdh5.2.0 $ hadoop fs -put hadoop-2.5.0-cdh5.2.0.tar.gz /hadoop-2.5.0-cdh5.2.0.tar.gz ```  > **Consider** any permissions issues with your uploaded location > (i.e., on HDFS you'll probably want to make the file world > readable).  Now you'll need to configure your `JobTracker` to launch each `TaskTracker` on Mesos!  #### Configure ####  Along with the normal configuration properties you might want to set to launch a `JobTracker`, you'll need to set some Mesos specific ones too.  Here are the mandatory configuration properties for `conf/mapred-site.xml` (initialized to values representative of running in [pseudo distributed operation](http://hadoop.apache.org/docs/stable/single_node_setup.html#PseudoDistributed):  ``` <property>   <name>mapred.job.tracker</name>   <value>localhost:9001</value> </property> <property>   <name>mapred.jobtracker.taskScheduler</name>   <value>org.apache.hadoop.mapred.MesosScheduler</value> </property> <property>   <name>mapred.mesos.taskScheduler</name>   <value>org.apache.hadoop.mapred.JobQueueTaskScheduler</value> </property> <property>   <name>mapred.mesos.master</name>   <value>localhost:5050</value> </property> <property>   <name>mapred.mesos.executor.uri</name>   <value>hdfs://localhost:9000/hadoop-2.5.0-cdh5.2.0.tar.gz</value> </property> ```  [More details on configuration propertios can be found here.](configuration.md)  #### Start ####  Now you can start the `JobTracker` but you'll need to include the path to the Mesos native library.  On Linux:  ``` $ MESOS_NATIVE_LIBRARY=/path/to/libmesos.so hadoop jobtracker ```  And on OS X:  ``` $ MESOS_NATIVE_LIBRARY=/path/to/libmesos.dylib hadoop jobtracker ```  > **NOTE: You do not need to worry about distributing your Hadoop > configuration! All of the configuration properties read by the** > `JobTracker` **along with any necessary** `TaskTracker` **specific > _overrides_ will get serialized and passed to each** `TaskTracker` > **on startup.**  #### Containers ####  As of Mesos 0.19.0 you can now specify a container to be used when isolating a task on a Mesos Slave. If you're making use of this new container mechanism, you can configure the hadoop jobtracker to send a custom container image and set of options using two new JobConf options.  This is purely opt-in, so omitting these jobconf options will cause no `ContainerInfo` to be sent to Mesos. Also, if you don't use these options there's no requirement to use version 0.19.0 of the mesos native library.  This feature can be especially useful if your hadoop jobs have software dependencies on the slaves themselves, as using a container can isolate these dependencies between other users of a Mesos cluster.  *It's important to note that the container/image you use does need to have the mesos native library installed already.*  ``` <property>   <name>mapred.mesos.container.image</name>   <value>docker:///ubuntu</value> </property> <property>   <name>mapred.mesos.container.options</name>   <value>-v,/foo/bar:/bar</value> </property> ```  _Please email user@mesos.apache.org with questions!_  ----------
confluentinc/kafka-connect-hdfs	# Kafka Connect HDFS Connector  kafka-connect-hdfs is a [Kafka Connector](http://kafka.apache.org/documentation.html#connect) for copying data between Kafka and Hadoop HDFS.  Documentation for this connector can be found [here](http://docs.confluent.io/current/connect/connect-hdfs/docs/index.html).  # Development  To build a development version you'll need a recent version of Kafka. You can build kafka-connect-hdfs with Maven using the standard lifecycle phases.  # FAQ  Refer frequently asked questions on Kafka Connect HDFS here - https://github.com/confluentinc/kafka-connect-hdfs/wiki/FAQ  # Contribute  - Source Code: https://github.com/confluentinc/kafka-connect-hdfs - Issue Tracker: https://github.com/confluentinc/kafka-connect-hdfs/issues  # License  The project is licensed under the Apache 2 license.
pentaho/big-data-plugin	Pentaho Big Data Plugin =======================  The Pentaho Big Data Plugin Project provides support for an ever-expanding Big Data community within the Pentaho ecosystem. It is a plugin for the Pentaho Kettle engine which can be used within Pentaho Data Integration (Kettle), Pentaho Reporting, and the Pentaho BI Platform.  Building -------- It's a maven build, so `mvn clean install` is a typical default for a local build.  Pre-requisites --------------- JDK 8 in your path. Maven 3.3.9 in your path. This [settings.xml](https://raw.githubusercontent.com/pentaho/maven-parent-poms/master/maven-support-files/settings.xml)  How to use the custom settings.xml --------------- Option 1: Copy this file into your <user-home>/.m2 folder and name it "settings.xml".  Warning: If you do this, it will become your default settings.xml for all maven builds.  Option 2: Copy this file into some other folder--possibly the project folder for the project you want to build and use the maven 's' option to build with this settings.xml file. Example: `mvn -s public-settings.xml install`.  The Pentaho profile defaults to pull all artifacts through the Pentaho public repository.  If you want to try resolving maven plugin dependencies through the maven central repository instead of the Pentaho public repository, activate the "central" profile like this:  `mvn -s -public-settings.xml -P central install`   If your fails to resolve the jacoco-maven-plugin version 0.7.7-SNAPSHOT --------------- The 0.7.7-SNAPSHOT property version for the jacoco-maven-plugin is defined in several releases of the Pentaho parent poms, but it is only available in the Pentaho artifact repositories. If you are trying to resolve through maven central or other public repositories you should override to get the latest version like this:  `mvn -s -public-settings.xml -P central install -Djacoco-maven-plugin.version=0.7.7.201606060606`  Further Reading --------------- Additional documentation is available on the Community wiki: [Big Data Plugin for Java Developers](http://wiki.pentaho.com/display/BAD/Getting+Started+for+Java+Developers)  License ------- Licensed under the Apache License, Version 2.0. See LICENSE.txt for more information.
asakusafw/asakusafw	# Asakusa Framework  Asakusa is a full stack framework for distributed/parallel computing, which provides with a development platform and runtime libraries supporting various distributed/parallel computing environments such as [Hadoop](http://hadoop.apache.org), [Spark](http://spark.apache.org), [M<sup>3</sup> for Batch Processing](https://github.com/fixstars/m3bp), and so on. Users can enjoy the best performance on distributed/parallel computing transparently changing execution engines among MapReduce, SparkRDD, and C++ native based on their data size.  Other than query-based languages, Asakusa helps to develop more complicated data flow programs more easily, efficiently, and comprehensively due to following components.  * Data-flow oriented DSL    Data-flow based approach is suitable for DAG constructions which is appropriate for distributed/parallel computing. Asakusa offers Domain Specific Language based on Java with data-flow design, which is integrated with compilers.  * Compilers    A multi-tier compiler is supported. Java based source code is once compiled to inter-mediated representation and then optimized for each execution environments such that Hadoop(MapReduce), Spark(RDD), M<sup>3</sup> for Batch Processing(C++ Native), respectively.  * Data-Modeling language    Data-Model language is supported, which is comprehensive for mapping with relational models, CSVs, or other data formats.  * Test Environment    JUnit based unit testing and end-to-end testing are supported, which are portable among each execution environments. Source code, test code, and test data are fully compatible across Hadoop, Spark, M<sup>3</sup> for Batch Processing and others.  * Runtime execution driver    A transparent job execution driver is supported.  All these features have been well designed and developed with the expertise from experiences on enterprise-scale system developments over decades and promised to contribute to large scale systems on distributed/parallel environments to be more robust and stable.  ## How to build  ### Maven artifacts  ```sh ./mvnw clean install -DskipTests ```  ### Gradle plug-ins  ```sh cd gradle ./gradlew clean [build] install ```  ## How to run tests  ### Maven artifacts  ```sh export HADOOP_CMD=/path/to/bin/hadoop ./mvnw test ```  ### Gradle plug-ins  ```sh cd gradle ./gradlew [clean] check ```  ## How to import projects into Eclipse  ### Maven artifacts  ```sh ./mvnw eclipse:eclipse ```  And then import existing projects from Eclipse.  If you run tests in Eclipse, please activate `Preferences > Java > Debug > 'Only include exported classpath entries when launching'`.  ### Gradle plug-ins  ```sh cd gradle ./gradlew eclipse ```  And then import existing projects from Eclipse.  ## Sub Projects * [Asakusa Framework Language Toolset](https://github.com/asakusafw/asakusafw-compiler) * [Asakusa on Spark](https://github.com/asakusafw/asakusafw-spark) * [Asakusa on M<sup>3</sup>BP](https://github.com/asakusafw/asakusafw-m3bp) * [Asakusa Framework Documentation](https://github.com/asakusafw/asakusafw-documentation)  ## Related Projects * [Asakusa Framework Examples](https://github.com/asakusafw/asakusafw-examples) * [Asakusa Framework Legacy Modules](https://github.com/asakusafw/asakusafw-legacy) * [Jinrikisha](https://github.com/asakusafw/asakusafw-starter) * [Shafu](https://github.com/asakusafw/asakusafw-shafu)  ## Resources * [Asakusa Framework Documentation (ja)](http://docs.asakusafw.com/) * [Asakusa Framework Community Site (ja)](http://asakusafw.com)  ## Bug reports, Patch contribution * Please report any issues to [repository for issue tracking](https://github.com/asakusafw/asakusafw-issues/issues) * Please contribute with patches according to our [contribution guide (Japanese only, English version to be added)](http://docs.asakusafw.com/latest/release/ja/html/contribution.html)  ## License * [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)
uber/hoodie	# Hoodie Hoodie manages storage of large analytical datasets on [HDFS](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) and serve them out via two types of tables   * **Read Optimized Table** - Provides excellent query performance via purely columnar storage (e.g. [Parquet](https://parquet.apache.org/))  * **Near-Real time Table (WIP)** - Provides queries on real-time data, using a combination of columnar & row based storage (e.g Parquet + [Avro](http://avro.apache.org/docs/current/mr.html))  For more, head over [here](https://uber.github.io/hoodie)
klarna/HiveRunner	[![Build Status](https://travis-ci.org/klarna/HiveRunner.svg?branch=release_to_maven_central)](https://travis-ci.org/klarna/HiveRunner)  ![ScreenShot](/images/HiveRunnerSplash.png)   HiveRunner ==========  Welcome to HiveRunner - Zero installation open source unit testing of Hive applications  [Watch the HiveRunner teaser on youtube!](http://youtu.be/B7yEAHwgi2w)  Welcome to the open source project HiveRunner. HiveRunner is a unit test framework based on JUnit4 and enables TDD development of HiveQL without the need of any installed dependencies. All you need is to add HiveRunner to your pom.xml as any other library and you're good to go.  HiveRunner is under constant development. We use it extensively in all our Hive projects. Please feel free to suggest improvements both as Pull requests and as written requests.   A word from the inventors --------- HiveRunner enables you to write Hive SQL as releasable tested artifacts. It will require you to parametrize and modularize HiveQL in order to make it testable. The bits and pieces of code should then be wired together with some orchestration/workflow/build tool of your choice, to be runnable in your environment (e.g. Oozie, pentaho, Talend, maven, etc…)   So, even though your current Hive SQL probably won't run off the shelf within HiveRunner, we believe the enforced testability and enabling of a TDD workflow will do as much good to the scripting world of SQL as it has for the Java community.    Cook Book ==========  1. Include HiveRunner ----------  HiveRunner is published to [Maven Central](http://search.maven.org/). To start to use it, add a dependency to HiveRunner to your pom file.      <dependency>         <groupId>com.klarna</groupId>         <artifactId>hiverunner</artifactId>         <version>[HIVERUNNER VERSION]</version>         <scope>test</scope>     </dependency>  Alternatively, if you want to build from source, clone this repo and build with       mvn install  Then add the dependency as mentioned above.  Also explicitly add the surefire plugin and configure forkMode=always to avoid OutOfMemory when building big test suites.      <plugin>         <groupId>org.apache.maven.plugins</groupId>         <artifactId>maven-surefire-plugin</artifactId>         <version>2.17</version>         <configuration>             <forkMode>always</forkMode>         </configuration>     </plugin>  as an alternative if this does not solve the OOM issues, try increase the -Xmx and -XX:MaxPermSize settings. For example:      <plugin>         <groupId>org.apache.maven.plugins</groupId>         <artifactId>maven-surefire-plugin</artifactId>         <version>2.17</version>         <configuration>             <forkCount>1</forkCount>             <reuseForks>false</reuseForks>             <argLine>-Xmx2048m -XX:MaxPermSize=512m</argLine>         </configuration>     </plugin>  (please note that the forkMode option is deprecated and you should use forkCount and reuseForks instead)  With forkCount and reuseForks there is a possibility to reduce the test execution time drastically, depending on your hardware. A plugin configuration which are using one fork per CPU core and reuse threads would look like:      <plugin>         <groupId>org.apache.maven.plugins</groupId>         <artifactId>maven-surefire-plugin</artifactId>         <version>2.17</version>         <configuration>             <forkCount>1C</forkCount>             <reuseForks>true</reuseForks>             <argLine>-Xmx2048m -XX:MaxPermSize=512m</argLine>         </configuration>     </plugin>  By default, HiveRunner uses mapreduce (mr) as the execution engine for hive. If you wish to run using tez, set the  System property hiveconf_hive.execution.engine to 'tez'.   (Any hive conf property may be overridden by prefixing it with 'hiveconf_')                  <plugin>             <groupId>org.apache.maven.plugins</groupId>             <artifactId>maven-surefire-plugin</artifactId>             <version>2.17</version>             <configuration>                 <systemProperties>                     <hiveconf_hive.execution.engine>tez</hiveconf_hive.execution.engine>                     <hiveconf_hive.exec.counters.pull.interval>1000</hiveconf_hive.exec.counters.pull.interval>                 </systemProperties>             </configuration>         </plugin>  Timeout - It's possible to configure HiveRunner to make tests time out after some time and retry those tests a couple of times.. This is to cover for the bug https://issues.apache.org/jira/browse/TEZ-2475 that at times causes test cases to not terminate due to a lost DAG reference. The timeout feature can be configured via the 'enableTimeout', 'timeoutSeconds' and 'timeoutRetries' properties. A configuration which enables timeouts after 30 seconds and allows 2 retries would look like      <plugin>         <groupId>org.apache.maven.plugins</groupId>         <artifactId>maven-surefire-plugin</artifactId>         <version>2.17</version>         <configuration>             <systemProperties>                 <enableTimeout>true</enableTimeout>                 <timeoutSeconds>30</timeoutSeconds>                 <timeoutRetries>2</timeoutRetries>             </systemProperties>         </configuration>     </plugin>   ### Logging src/main/resources/log4j.properties configures the log levels. Log level is default set to WARN. Some traces remain due to the fact that Hive logs to stdout. All result sets are logged. Enable by setting ```log4j.logger.com.klarna.hiverunner.HiveServerContainer=DEBUG``` in log4j.properties.   2. Look at the examples ---------- Look at the [com.klarna.hiverunner.examples.HelloHiveRunner](/src/test/java/com/klarna/hiverunner/examples/HelloHiveRunner.java) reference test case to get a feeling for how a typical test case looks like. If you're put off by the verbosity of the annotations, there's always the possibility to use HiveShell in a more interactive mode.  The [com.klarna.hiverunner.SerdeTest](/src/test/java/com/klarna/hiverunner/SerdeTest.java) adds a resources (test data) interactively with HiveShell instead of using annotations.  Annotations and interactive mode can be mixed and matched, however you'll always need to include the [com.klarna.hiverunner.annotations.HiveSQL](/src/main/java/com/klarna/hiverunner/annotations/HiveSQL.java) annotation e.g:           @HiveSQL(files = {"serdeTest/create_table.sql", "serdeTest/hql_custom_serde.sql"}, autoStart = false)          public HiveShell hiveShell;  Note that the *autostart = false* is needed for the interactive mode. It can be left out when running with only annotations.  ### Sequence files If you work with __sequence files__ (Or anything else than regular text files) make sure to take a look at [ResourceOutputStreamTest](/src/test/java/com/klarna/hiverunner/ResourceOutputStreamTest.java)  for an example of how to use the new method [HiveShell](src/main/java/com/klarna/hiverunner/HiveShell.java)\#getResourceOutputStream to manage test input data.   ### Programatically create test input data  Test data can be programmatically inserted into any Hive table using `HiveShell.insertInto(...)`. This seamlessly handles different storage formats and partitioning types allowing you to focus on the data required by your test scenarios:      hiveShell.execute("create database test_db");     hiveShell.execute("create table test_db.test_table ("         + "c1 string,"         + "c2 string,"         + "c3 string"         + ")"         + "partitioned by (p1 string)"         + "stored as orc");      hiveShell.insertInto("test_db", "test_table")         .withColumns("c1", "p1").addRow("v1", "p1")       // add { "v1", null, null, "p1" }         .withAllColumns().addRow("v1", "v2", "v3", "p1")  // add { "v1", "v2", "v3", "p1" }         .copyRow().set("c1", "v4")                        // add { "v4", "v2", "v3", "p1" }         .addRowsFromTsv(file)                             // parses TSV data out of a file resource         .addRowsFrom(file, fileParser)                    // parses custom data out of a file resource         .commit();  See [com.klarna.hiverunner.examples.InsertTestData](/src/test/java/com/klarna/hiverunner/examples/InsertTestData.java) for working examples.  3. Understand a little bit of the order of execution ---------- HiveRunner will in default mode setup and start the HiveShell before the test method is invoked. If autostart is set to false, the [HiveShell](/src/main/java/com/klarna/hiverunner/HiveShell.java) must be started manually from within the test method. Either way, HiveRunner will do the following steps when start is invoked.  1. Merge any [@HiveProperties](/src/main/java/com/klarna/hiverunner/annotations/HiveProperties.java) from the test case with the hive conf 2. Start the HiveServer with the merged conf 3. Copy all [@HiveResource](/src/main/java/com/klarna/hiverunner/annotations/HiveResource.java) data into the temp file area for the test 4. Execute all fields annotated with [@HiveSetupScript](/src/main/java/com/klarna/hiverunner/annotations/HiveSetupScript.java) 5. Execute the script files given in the [@HiveSQL](/src/main/java/com/klarna/hiverunner/annotations/HiveSQL.java) annotation  The [HiveShell](/src/main/java/com/klarna/hiverunner/HiveShell.java) field annotated with [@HiveSQL](/src/main/java/com/klarna/hiverunner/annotations/HiveSQL.java) will always be injected before the test method is invoked.   Hive version compatibility ============ - This version of HiveRunner is built for hive 14. - Command shell emulations are provided to closely match the behaviour of both the Hive CLI and Beeline interactive shells. The desired emulation can be specified in your `pom.xml` file like so:           <plugin>             <groupId>org.apache.maven.plugins</groupId>             <artifactId>maven-surefire-plugin</artifactId>             <version>2.17</version>             <configuration>                 <systemProperties>                     <!-- Defaults to HIVE_CLI -->                     <commandShellEmulation>BEELINE</commandShellEmulation>                 </systemProperties>             </configuration>         </plugin>    Or provided on the command line using a system property:        mvn -DcommandShellEmulation=BEELINE test  Future work and Limitations ============ * HiveRunner does not allow the add jar statement. It is considered bad practice to keep environment specific code together with the business logic that targets HiveRunner. Keep environment specific stuff in separate files and use your build/orchestration/workflow tool to run the right files in the right order in the right environment. When running HiveRunner, all SerDes available on the classpath of the IDE/maven will be available.  * HiveRunner runs Hive and Hive runs on top of hadoop, and hadoop has limited support for windows machines. Installing [Cygwin](http://www.cygwin.com/ "Cygwin") might help out.  * Some of the HiveRunner annotations should probably be rebuilt to be more test method specific. E.g. Resources may be described on a test method basis instead as for a whole test case. Feedback is always welcome!  * The interactive API of HiveShell should support as much as the annotated DSL. Currently some features are not implemented in the HiveShell but only available via annotations.      __DONE:__ _HiveShell now includes all the features of the annotations and then some!_   * Clean up the logs           __DONE:__ _There's now a working log4j.properties here: src/main/resources/log4j.properties_  * HiveRunner currently uses in-memory Derby as metastore. It seems to be a real performance bottleneck. We are looking to replace it with hsqldb in the near future.         __DONE:__ _This version of HiveRunner uses an in-memory hsqldb instead of derby. It's faster and more reliant!_    * Currently the HiveServer spins up and tears down for every test method. As a performance option it should be possible to clean the HiveServer and metastore between each test method invocation. The choice should probably be exposed to the test writer. By switching between different strategies, side effects/leakage can be ruled out during test case debugging.  * Redirect derby.log. It currently ends up in the build root dir.      __DONE:__ _Derby is gone -> derby.log is gone!_    Change Log (From version 2.2.0 and onwards) ==============  ### __3.2.1__ * The way of setting writable permissions on JUnit temporary folder changed to make it compatible with Windows.  ### __3.2.0__ * Added functionality for headers in TSV parser. This way you can dynamically add TSV files declaring a subset of columns using insertInto.  ### __3.1.1__ * Added debug logging of result set. Enable by setting ```log4j.logger.com.klarna.hiverunner.HiveServerContainer=DEBUG``` in log4j.properties.  ### __3.1.0__ * Added methods to the shell that allow statements contained in files to be executed and their results gathered. These are particularly useful for HQL scripts that generate no table based data and instead write results to STDOUT. In practice we've seen these scripts used in data processing job orchestration scripts (e.g `bash`) to check for new data, calculate processing boundaries, etc. These values are then used to appropriately configure and launch some downstream job. * Support abstract base class (Issue #48).  ### __3.0.0__  * Upgraded to Hive 1.2.1 (Note: new major release with backwards incompatibility issues). As of Hive 1.2 there are a number of new reserved keywords, see [DDL manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Keywords,Non-reservedKeywordsandReservedKeywords) for more information.  If you happen to have one of these as an identifier, you could either backtick quote them (e.g. \`date\`, \`timestamp\` or \`update\`) or set hive.support.sql11.reserved.keywords=false.                                             * Removed the custom HiveConf hive.vs. Use hadoop.tmp.dir instead. * Users of Hive version 0.14 or older are recommended to use HiveRunner version 2.6.0.  ### __2.6.0__  * Introduced command shell emulations to replicate different handling of full line comments in `hive` and `beeline` shells. Now strips full line comments for executed scripts to match the behaviour of the `hive -f` file option.   * Option to use files as input for com.klarna.hiverunner.HiveShell.execute(...)  ### __2.5.1__  Fixed  deadlock in ThrowOnTimeout.java that occured when running with long running test case and disabled timeout.  ### __2.5.0__  Added support with `HiveShell.insertInto` for fluently generating test data in a table storage format agnostic manner.  ### __2.4.0__  Enabled any hiveconf variables to be set as System properties by using the naming convention hiveconf_[HiveConf property name]. E.g: hiveconf_hive.execution.engine  Fixed bug: Results sets bigger than 100 rows only returned the first 100 rows.   ### __2.3.0__  Merged tez and mr context into the same context again. Now, the same test suite may alter between execution engines by doing  E.g:        hive> set hive.execution.engine=tez;      hive> [some query]      hive> set hive.execution.engine=mr;      hive> [some query]   ### __2.2.0__ * Added support for setting hivevar:s via HiveShell      Known Issues =====================  ### UnknownHostException I've had issues with UnknownHostException on OS X after upgrading my system or running docker.  Usually a restart of my machine solved it, but last time I got some corporate  stuff installed the restarts stopped working and I kept getting UnknownHostExceptions.  Following this simple guide solved my problem: http://crunchify.com/getting-java-net-unknownhostexception-nodename-nor-servname-provided-or-not-known-error-on-mac-os-x-update-your-privateetchosts-file/   ### IOException in Hive 0.14.0 Described in this issue: https://github.com/klarna/HiveRunner/issues/3  This is a known bug in hive. Try setting hive.exec.counters.pull.interval to 1000 millis. It has worked for some projects. You can do this in the surefire plugin:             <plugin>               <groupId>org.apache.maven.plugins</groupId>               <artifactId>maven-surefire-plugin</artifactId>               <version>2.17</version>               <configuration>                   <systemProperties>                       <hiveconf_hive.exec.counters.pull.interval>1000</hiveconf_hive.exec.counters.pull.interval>                   </systemProperties>               </configuration>           </plugin>   Also you can try to use the retry functionality in Surefire: https://maven.apache.org/surefire/maven-surefire-plugin/examples/rerun-failing-tests.html    ### Tez queries do not terminate Tez will at times forget the process id of a random DAG. This will cause the query to never terminate. To get around this there is  a timeout and retry functionality implemented in HiveRunner:            <plugin>              <groupId>org.apache.maven.plugins</groupId>              <artifactId>maven-surefire-plugin</artifactId>              <version>2.17</version>              <configuration>                  <systemProperties>                      <enableTimeout>true</enableTimeout>                      <timeoutSeconds>30</timeoutSeconds>                      <timeoutRetries>2</timeoutRetries>                      </systemProperties>              </configuration>          </plugin>           Make sure to set the timeoutSeconds to that of your slowest test in the test suite and then add some padding.  TAGS ========= Hive Hadoop HiveRunner HDFS Unit test JUnit SQL HiveSQL HiveQL   Releasing hiverunner to maven central =====================================  Deployment to Sonatype OSSRH using maven and travis-ci ------------------------------------------------------  HiveRunner has been setup to build continuously on a travis-ci.org build server as well as prepared to be manually released from a travis-ci.org buildserver to maven central. The following steps were involved.  * A Sonatype OSSRH (Open Source Software Repository Hosting) account has been created for user "klarna.odin". * The OSSRH username and password were encrypted according to http://docs.travis-ci.com/user/encryption-keys/ * A special maven settings.xml file was created that uses the encrypted environment variables in the ossrh server definition. * A Gnu PGP keypair was created locally according to http://central.sonatype.org/pages/working-with-pgp-signatures.html * The pubring.gpg and secring.gpg were tar'ed into secrets.tar * The secrets.tar was encrypted according to http://docs.travis-ci.com/user/encrypting-files/ * The GPG_PASSPHRASE variable was encrypted for travis usage. * The pom has had sections added to it according to instructions from http://central.sonatype.org/pages/apache-maven.html * A .travis.yml build file has been added  The above steps are enough for deploying to sonatype/maven central. Depending on the version number in the pom, the build artifact will be deployed to either the snapshots repository or the staging-repository.  Note ---- The gpg key used for signing expires 2017-10-17, after which a new one needs to be created and added as described above. Don't forget that the GPG_PASSPHRASE also needs to be updated if another passphrase is used when creating the gpg keypair.   Playbook for making a release ----------------------------- Basically follow this guide: http://central.sonatype.org/pages/apache-maven.html#performing-a-release-deployment  * Change the version number to the release version you want. Should not include -SNAPSHOT in the name. * Update change log for new version. * Commit, tag with release number, push  ```       git commit -a -m "Setting version number before releasing"       git tag -a v2.5.0 -m "HiveRunner-2.5.0"       git push origin --tags ```  * Travis builds and deploys. Make sure to check the status of the build in Travis (https://travis-ci.org/klarna/HiveRunner). * Follow the http://central.sonatype.org/pages/releasing-the-deployment.html guide to promote the staged release to maven central. * Change version number to next snapshot version * Commit and push  ```      git commit -m "Setting version to next development version"      git push origin ```
pranab/avenir	## Introduction Set of predictive and exploratory machine learning tools. Runs on Hadoop, Spark and Storm  ## Philosophy * Simple to use * Input output in CSV format * Metadata defined in simple JSON file * Extremely configurable with tons of configuration knobs  ## Solution * Exploratry analytic including correlation, feature subset selection * Naive Bayes * Discrimininant analysis * Nearest neighbor * Decision tree and Random Forest * Association Mining * Reinforcement learning * Stochastic Optimization   ## Blogs The following blogs of mine are good source of details of avenir. These are the only source of detail documentation * http://pkghosh.wordpress.com/2014/03/12/using-mutual-information-to-find-critical-factors-in-hospital-readmission/ * http://pkghosh.wordpress.com/2014/01/09/boost-lead-generation-with-online-reinforcement-learning/ * http://pkghosh.wordpress.com/2013/11/06/retarget-campaign-for-abandoned-shopping-carts-with-decision-tree/ * http://pkghosh.wordpress.com/2013/10/06/predicting-customer-loyalty-trajectory/ * http://pkghosh.wordpress.com/2013/08/25/bandits-know-the-best-product-price/ * http://pkghosh.wordpress.com/2013/06/29/learning-but-greedy-gambler/ * http://pkghosh.wordpress.com/2013/04/15/smarter-email-marketing-with-markov-model/ * http://pkghosh.wordpress.com/2013/03/18/analytic-is-your-doctors-friend/ * http://pkghosh.wordpress.com/2013/02/19/stop-the-customer-separation-pain-bayesian-classifier/ * http://pkghosh.wordpress.com/2013/01/31/explore-with-cramer-index/ * https://pkghosh.wordpress.com/2015/07/06/customer-conversion-prediction-with-markov-chain-classifier/ * https://pkghosh.wordpress.com/2015/05/11/is-bigger-data-better-for-machine-learning/ * https://pkghosh.wordpress.com/2015/12/13/association-mining-with-improved-apriori-algorithm/ * https://pkghosh.wordpress.com/2016/03/14/is-neural-network-better-off-with-big-data/ * https://pkghosh.wordpress.com/2016/04/13/customer-churn-prediction-with-svm-using-scikit-learn/ * https://pkghosh.wordpress.com/2016/06/14/inventory-forecasting-with-markov-chain-monte-carlo/ * https://pkghosh.wordpress.com/2016/07/30/customer-segmentation-based-on-online-behavior-using-scikitlearn/ * https://pkghosh.wordpress.com/2016/10/27/supplier-fulfillment-forecasting-with-continuous-time-markov-chain-using-spark/ * https://pkghosh.wordpress.com/2017/04/30/predicting-call-hangup-in-customer-service-calls-with-decision-tree-and-random-forest/ * https://pkghosh.wordpress.com/2017/06/26/project-assignment-optimization-with-simulated-annealing-on-spark/ * https://pkghosh.wordpress.com/2017/09/18/handling-rare-events-and-class-imbalance-in-predictive-modeling-for-machine-failure/    ## Getting started Project's resource directory has various tutorial documents for the use cases described in the blogs.  ## Configuration  All configuration parameters are described in the wiki page https://github.com/pranab/avenir/wiki/Configuration  ## Build Please refer to resource/dependency.txt for build time and run time dependencies  For Hadoop 1 * mvn clean install  For Hadoop 2 (non yarn) * git checkout nuovo * mvn clean install  For Hadoop 2 (yarn) * git checkout nuovo * mvn clean install -P yarn  ## Help Please feel free to email me at pkghosh99@gmail.com  ## Contribution Contributors are welcome. Please email me at pkghosh99@gmail.com
sonalgoyal/hiho	# HIHO: Hadoop In, Hadoop Out.   > Hadoop Data Integration, deduplication, incremental update and more.    This branch is for support for HIHO on Apache Hadoop 0.21.  ## Import from a database to HDFS  **query based import**    Join multiple tables, provide where conditions, dynamically bind parameters to SQL queries to get data to Hadoop. As simple as creating a simple config and running the job.  	bin/hadoop jar hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf dbInputQueryDelimited.xml  or   	${HIHO_HOME}/scripts/hiho import  		-jdbcDriver <jdbcDriver>  		-jdbcUrl <jdbcUrl>  		-jdbcUsername <jdbcUsername>  		-jdbcPassword <jdbcPassword>  		-inputQuery <inputQuery>  		-inputBoundingQuery <inputBoundingQuery>  		-outputPath <outputPath>  		-outputStrategy <outputStrategy>  		-delimiter <delimiter>  		-numberOfMappers <numberOfMappers>  		-inputOrderBy <inputOrderBy>    **table based import**    	bin/hadoop jar hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf dbInputTableDelimited.xml  or    	${HIHO_HOME}/scripts/hiho import  		-jdbcDriver <jdbcDriver>  		-jdbcUrl <jdbcUrl>  		-jdbcUsername <jdbcUsername>  		-jdbcPassword <jdbcPassword> 		-outputPath <outputPath>  		-outputStrategy <outputStrategy>  		-delimiter <delimiter>  		-numberOfMappers <numberOfMappers>  		-inputOrderBy <inputOrderBy>  		-inputTableName <inputTableName>  		-inputFieldNames <inputFieldNames>  **incremental import** by appending to existing `HDFS` location so that all data is in one place. just specify `isAppend = true` in the configurations and import. Import will be written to existing HDFS folder.  **configurable format for data import: delimited, avro** by specifying the `mapreduce.jdbc.hiho.input.outputStrategy` as DELIMITED or AVRO.   **Note:**   1. Please specify delimiter in double qoutes, because in some cases such as semi colon ';' it breaks For example `-delimiter ";"`. If you are specifing * for `inputFieldNames` then also you put in double qoutes    ## Export to Databases  **high performance MySQL loading using LOAD DATA INFILE**  	${HIHO_HOME}/scripts/hiho export mysql 		-inputPath <inputPath>  		-url <url>  		-userName <userName>  		-password <password>  		-querySuffix  <querySuffix>  **high performance Oracle loading by creating external tables.** See [expert opinion](http://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:6611962171229)   For information on external tables, check [here](http://download.oracle.com/docs/cd/B12037_01/server.101/b10825/et_concepts.htm)   On the Oracle server  1. Make a folder  		mkdir -p ageTest  2. Create a directory through the Oracle Client (sqlplus) and grant it privileges.  		sqlplus>create or replace directory age_ext as '/home/nube/age';  3. Allow ftp to the Oracle server 	 		${HIHO_HOME}/scripts/hiho export oracle  			-inputPath <inputPath>  			-oracleFtpAddress <oracleFtpAddress>  			-oracleFtpPortNumber <oracleFtpPortNumber>  			-oracleFtpUserName <oracleFtpUserName>  			-oracleFtpPassword <oracleFtpPassword>  			-oracleExternalTableDirectory <oracleExternalTableDirectory>  			-driver <driver>  			-url <url>  			-userName <userName>  			-password <password>  			-externalTable <createExternalTableQuery>  **custom loading and export to any database** by emitting own `GenericDBWritables`. Check `DelimitedLoadMapper`  ## Export to SalesForce    **send computed map reduce results to Salesforce.**  For this, you need to have a developer account with Bulk API enabled. You can join at http://developer.force.com/join   If you get message:   >[LoginFault [ApiFault  exceptionCode='INVALID_LOGIN' exceptionMessage='Invalid username, password, security token; or user locked out.' "Invalid username, password, security token; or user locked out. Are you at a new location? When accessing Salesforce--either via a desktop client or the API--from outside of your company’s trusted networks, you must add a security token to your password to log in. To receive a new security token, log in to [Salesforce](http://www.salesforce.com) and click Setup | My Personal Information | Reset Security Token." login and get the security token.   then try  	sfUserName - Name of Salesforce account 	sfPassword - Password and security token. The Security Token can be obtained by logging in to the Salesforce.com site and clicking on Reset Security Token. 	sfObjectType - The Salesforce object to export 	sfHeaders - header describing the Salesforce object properties. For more information, refer to the Bulk API Developer's Guide.  	${HIHO_HOME}/scripts/hiho export saleforce  		-inputPath <inputPath>  		-sfUserName <sfUserName>  		-sfPassword <sfPassword>  		-sfObjectType <sfObjectType>  		-sfHeaders <sfHeaders>   ## Export results to an FTP Server.  Use the `co.nubetech.hiho.mapreduce.lib.output.FTPOutputFormat` directly in your job, just like `FileOutputFormat`. For usage, check `co.nubetech.hiho.job.ExportToFTPserver`. This job writes the output directly to an FTP server.  It can be invoked as:  	${HIHO_HOME}/scripts/hiho export ftp  		-inputPath <inputPath>  		-outputPath <outputPath>  		-ftpUserName <ftpUserName>  		-ftpAddress <ftpAddress>  		-ftpPortNumper <ftpPortNumper>  		-ftpPassword <ftpPassword>  Where:  	ftpUserName - FTP server login username 	ftpAddress - FTP server address 	ftpPortNumper - FTP port 	ftpPassword - FTP server password 	outputPath is the location on the FTP server to which the output will be written. It should be a complete directory path - /home/sgoyal/output     ## Export to Hive This is used to export data from any other database to Hive database.  Hive export can be done in two method query base and table based configuration needed are   	mapreduce.jdbc.hiho.input.loadTo - this configuration defines in which database you want to load your data from HDFS for eg:- hive 	mapreduce.jdbc.hiho.input.loadToPath - Our program also generates script for all queries , this configuration defines where to store that script on your local system   	mapreduce.jdbc.hiho.hive.driver - name of hive jdbc driver For eg:- org.apache.hadoop.hive.jdbc.HiveDriver 	mapreduce.jdbc.hiho.hive.url - hive url for jdbc connection For eg:- jdbc:hive:// (for embedded mode),jdbc:hive://localhost:10000/default for standalone mode 	mapreduce.jdbc.hiho.hive.usrName - user name for jdbc connection  	mapreduce.jdbc.hiho.hive.password - password for jdbc connection 	mapreduce.jdbc.hiho.hive.partitionBy - This configuration is when we want to create partitioned hive table. For eg :- country:string:us;name:string:jack (basic partition), country:string:us;name:string (static and one dynamic partition), country:string (dynamic partition) till now we allow only one dynamic partition 											We also allow to store data in a table for multiple partition at a time for that value is given as country:string:us,uk,aus for this we need to define three different queries or table in there respective configurations  	mapreduce.jdbc.hiho.hive.ifNotExists - set true if you want include 'if not exits' clause in your create table query 	mapreduce.jdbc.hiho.hive.tableName - write the name for the table in the hive you want to create 	mapreduce.jdbc.hiho.hive.sortedBy - this can be only used if clusteredBy configuration is defined, in this give the name of column by which u want to sort your data 	mapreduce.jdbc.hiho.hive.clusteredBy - This configuration defines name of column by which you want to cluster your data and define the number of buckets you want to create. For eg:- name:2   Execution command for table based   	bin/hadoop jar ~/workspace/hiho/build/classes/hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf  ~/workspace/hiho/conf/dbInputTableDelimitedHive.xml  or  	${HIHO_HOME}/scripts/hiho import  		-jdbcDriver <jdbcDriver>  		-jdbcUrl <jdbcUrl>  		-jdbcUsername <jdbcUsername>  		-jdbcPassword <jdbcPassword>  		-outputPath <outputPath>  		-outputStrategy <outputStrategy>  		-delimiter <delimiter>  		-numberOfMappers <numberOfMappers>  		-inputOrderBy <inputOrderBy>  		-inputTableName <inputTableName>  		-inputFieldNames <inputFieldNames>  		-inputLoadTo hive  		-inputLoadToPath <inputLoadToPath>  		-hiveDriver <hiveDriver>   		-hiveUrl <hiveUrl>  		-hiveUsername <hiveUsername>  		-hivePassword <hivePassword>  		-hivePartitionBy <hivePartitionBy>  		-hiveIfNotExists <hiveIfNotExists>  		-hiveTableName <hiveTableName>  		-hiveSortedBy <hiveSortedBy>  		-hiveClusteredBy <hiveClusteredBy>    For query based  	bin/hadoop jar ~/workspace/hiho/build/classes/hiho.jar co.nubetech.hiho.job.DBQueryInputJob -conf  ~/workspace/hiho/conf/dbInputQueryDelimitedHive.xml or  	${HIHO_HOME}/scripts/hiho import  		-jdbcDriver <jdbcDriver>  		-jdbcUrl <jdbcUrl>  		-jdbcUsername <jdbcUsername>  		-jdbcPassword <jdbcPassword>  		-outputPath <outputPath>  		-outputStrategy <outputStrategy>  		-delimiter <delimiter>  		-numberOfMappers <numberOfMappers>  		-inputOrderBy <inputOrderBy>  		-inputLoadTo hive  		-inputLoadToPath <inputLoadToPath>  		-hiveDriver <hiveDriver>   		-hiveUrl <hiveUrl>  		-hiveUsername <hiveUsername>  		-hivePassword <hivePassword>  		-hivePartitionBy <hivePartitionBy>  		-hiveIfNotExists <hiveIfNotExists>  		-hiveTableName <hiveTableName>  		-hiveSortedBy <hiveSortedBy>  		-hiveClusteredBy <hiveClusteredBy>  **Notes:**    1. Hive table name is mandatory when you are quering more than one query or table that is the case of multiple partition 2. Please note that sorted feature will not work untill clustered feature is defined   ## Dedup details  	bin/hadoop jar ~/workspace/HIHO/deploy/hiho.jar co.nubetech.hiho.dedup.DedupJob -inputFormat <inputFormat> -dedupBy <"key" or "value"> -inputKeyClassName <inputKeyClassName> -inputValueClassName <inputValueClassName> -inputPath <inputPath> -outputPath <outputPath> -delimeter <delimeter> -column <column> -outputFormat <outputFormat>  Alternatively Dedup can also be executed as:- Running HadoopTransform script present in `$HIHO_HOME/scripts/`  	${HIHO_HOME}/scripts/hiho dedup  		-inputFormat <inputFormat>  		-dedupBy <"key" or "value">  		-inputKeyClassName <inputKeyClassName>  		-inputValueClassName <inputValueClassName>  		-inputPath <inputPath>  		-outputPath <outputPath>  		-delimeter <delimeter> -column <column>  **Example For Deduplication with key:**    For Sequence Files:   	${HIHO_HOME}/scripts/hiho dedup  		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat  		-inputKeyClassName org.apache.hadoop.io.IntWritable  		-inputValueClassName org.apache.hadoop.io.Text  		-inputPath testData/dedup/inputForSeqTest  		-outputPath output -dedupBy key  For Delimited Text Files:   	${HIHO_HOME}/scripts/hiho dedup  		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat  		-inputKeyClassName org.apache.hadoop.io.Text  		-inputValueClassName org.apache.hadoop.io.Text  		-inputPath testData/dedup/textFilesForTest  		-outputPath output -delimeter ,  		-column 1  		-dedupBy key  **Example For Deduplication with value:**    For Sequence Files:   	${HIHO_HOME}/scripts/hiho dedup  		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat  		-inputKeyClassName org.apache.hadoop.io.IntWritable  		-inputValueClassName org.apache.hadoop.io.Text  		-inputPath testData/dedup/inputForSeqTest  		-outputPath output -dedupBy value  For Delimited Text Files:   	${HIHO_HOME}/scripts/hiho dedup  		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat  		-inputKeyClassName org.apache.hadoop.io.Text  		-inputValueClassName org.apache.hadoop.io.Text  		-inputPath testData/dedup/textFilesForTest  		-outputPath output  		-dedupBy value  ## Merge details:   	${HIHO_HOME}/scripts/hiho merge  		-newPath <newPath>  		-oldPath <oldPath>  		-mergeBy <"key" or "value">  		-outputPath <outputPath>  		-inputFormat <inputFormat>  		-inputKeyClassName <inputKeyClassName>  		-inputValueClassName <inputValueClassName>  		-outputFormat <outputFormat>  **Example For Merge with key:**    For Sequence Files:  	${HIHO_HOME}/scripts/hiho merge  		-newPath testData/merge/inputNew/input1.seq  		-oldPath testData/merge/inputOld/input2.seq  		-mergeBy key -outputPath output   		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat  		-inputKeyClassName org.apache.hadoop.io.IntWritable  		-inputValueClassName org.apache.hadoop.io.Text  For Delimited Text Files:  	${HIHO_HOME}/scripts/hiho merge  		-newPath testData/merge/inputNew/fileInNewPath.txt  		-oldPath testData/merge/inputOld/fileInOldPath.txt  		-mergeBy key  		-outputPath output  		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat  		-inputKeyClassName org.apache.hadoop.io.Text  		-inputValueClassName org.apache.hadoop.io.Text  **Example For Merge with value:**    For Sequence Files:  	${HIHO_HOME}/scripts/hiho merge  		-newPath testData/merge/inputNew/input1.seq  		-oldPath testData/merge/inputOld/input2.seq  		-mergeBy value  		-outputPath output   		-inputFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat  		-inputKeyClassName org.apache.hadoop.io.IntWritable  		-inputValueClassName org.apache.hadoop.io.Text  For Delimited Text Files:  	${HIHO_HOME}/scripts/hiho merge  		-newPath testData/merge/inputNew/fileInNewPath.txt  		-oldPath testData/merge/inputOld/fileInOldPath.txt  		-mergeBy value  		-outputPath output  		-inputFormat co.nubetech.hiho.dedup.DelimitedTextInputFormat  		-inputKeyClassName org.apache.hadoop.io.Text  		-inputValueClassName org.apache.hadoop.io.Text  ## Export to DB:  	bin/hadoop jar deploy/hiho-0.4.0.jar co.nubetech.hiho.job.ExportToDB   		-jdbcDriver <jdbcDriverName>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUserName>   		-jdbcPassword <jdbcPassword>  		-delimiter <delimiter>  		-numberOfMappers <numberOfMappers>  		-tableName <tableName>  		-columnNames <columnNames>  		-inputPath <inputPath>  or  	${HIHO_HOME}/scripts/hiho export db  		-jdbcDriver <jdbcDriverName>   		-jdbcUrl <jdbcUrl>   		-jdbcUsername <jdbcUserName>   		-jdbcPassword <jdbcPassword>  		-delimiter <delimiter>  		-numberOfMappers <numberOfMappers>  		-tableName <tableName>  		-columnNames <columnNames>  		-inputPath <inputPath>   ## New Features in this release - incremental import and introduction of AppendFileInputFormat - Oracle export - FTP Server integration - Salesforce - Support for Apache Hadoop 0.20 - Support for Apache Hadoop 0.21 - Generic dedup and merge  ## Other improvements - Ivy based build and dependency management - Junit and mockito based test cases   **Note:**   1. To run `TestExportToMySQLDB` we need to add `hiho-0.4.0.jar`, all jars of hadoop and hadoop lib with also `mysql-connector-java.jar` 		in the `classpath`.
pranab/beymani	## Introduction Beymani consists of set of Hadoop and Storm based tools for outlier and anamoly  detection, which can be used for fraud detection, intrusion detection. All the  implementations will be ported to Spark.  ## Philosophy * Simple to use * Input output in CSV format * Metadata defined in simple JSON file * Extremely configurable with tons of configuration knobs  ## Blogs The following blogs of mine are good source of details of beymani * http://pkghosh.wordpress.com/2012/01/02/fraudsters-outliers-and-big-data-2/ * http://pkghosh.wordpress.com/2012/02/18/fraudsters-are-not-model-citizens/ * http://pkghosh.wordpress.com/2012/06/18/its-a-lonely-life-for-outliers/ * http://pkghosh.wordpress.com/2012/10/18/relative-density-and-outliers/ * http://pkghosh.wordpress.com/2013/10/21/real-time-fraud-detection-with-sequence-mining/  ## Algorithms * Multi variate instance distribution model * Multi variate sequence or multi gram distribution model * Average instance Distance * Relative instance Density * Markov chain with sequence data * Instance clustering * Sequence clustering  ## Getting started Project's resource directory has various tutorial documents for the use cases described in the blogs.  ## Build For Hadoop 1 * mvn clean install  For Hadoop 2 (non yarn) * git checkout nuovo * mvn clean install  For Hadoop 2 (yarn) * git checkout nuovo * mvn clean install -P yarn  ## Help Please feel free to email me at pkghosh99@gmail.com  ## Contribution Contributors are welcome. Please email me at pkghosh99@gmail.com
mozilla-metrics/akela	# Akela #  Version: 0.5    #### Mozilla's utility library for Hadoop, HBase, Pig, etc. ####  ### Version Compatability ### This code is built with the following assumptions.  You may get mixed results if you deviate from these versions.  * [Hadoop](http://hadoop.apache.org) 0.20.2+ * [HBase](http://hbase.apache.org) 0.90+ * [Pig](http://pig.apache.org) 0.9+ * [Hive](https://github.com/xstevens/hive) 0.7 with [automatic promotion of certain types](https://github.com/xstevens/hive/commit/566ca633546e5231cf5ea20d554c1f61784f39e4) * [Jackson](https://github.com/FasterXML) 2.x (for all things JSON) 	* We don't use anything fancy but fasterxml switch broke from 1.x packaging. You can probably fork and compile fairly easily if you want to use Jackson 1.x since that's what ships with Hadoop projects.  ### Building ### To make a jar you can do:    `mvn package`  To make a Hadoop MapReduce job jar with no defined main class in the manifest:    `mvn assembly:assembly`   ### License ### All aspects of this software written in Java are distributed under Apache Software License 2.0.  ### Contributors ###  * Xavier Stevens ([@xstevens](http://twitter.com/xstevens)) * Daniel Einspanjer ([@deinspanjer](http://twitter/deinspanjer)) * Mark Reid ([@reid_write](http://twitter.com/reid_write))
awslabs/emr-dynamodb-connector	# emr-dynamodb-connector **Access data stored in Amazon DynamoDB with Apache Hadoop, Apache Hive, and Apache Spark**  ## Introduction You can use this connector to access data in Amazon DynamoDB using Apache Hadoop, Apache Hive, and Apache Spark in Amazon EMR. You can process data directly in DynamoDB using these frameworks, or join data in DynamoDB with data in Amazon S3, Amazon RDS, or other storage layers that can be accessed by Amazon EMR.  - [Using Apache Hive in Amazon EMR with Amazon DynamoDB][emr-dynamodb-hive-docs] - [Accessing data in Amazon DynamoDB with Apache Spark][dynamodb-spark-blog-post]  For more information about supported data types in DynamoDB, see [Data Types for Hive and DynamoDB] [hive-dynamodb-data-types] in the *[Amazon EMR Release Guide][emr-release-guide]*.  ### Hive StorageHandler Implementation For more information, see [Hive Commands Examples for Exporting, Importing, and Querying Data in DynamoDB][hive-commands-emr-dev-guide] in the *[Amazon DynamoDB Developer Guide] [dynamodb-dev-guide]*.  ### Hadoop InputFormat and OutputFormat Implementation An implementation of [Apache Hadoop InputFormat interface][input-format-javadoc] and [OutputFormat][output-format-javadoc] are included, which allows [DynamoDB AttributeValues] [dynamodb-attributevalues] to be directly ingested by MapReduce jobs. For an example of how to use these classes, see [Set Up a Hive Table to Run Hive Commands][set-up-hive-table] in the *[Amazon EMR Release Guide][emr-release-guide]*, as well as their usage in the Import/Export tool classes in [DynamoDBExport.java] [export-tool-source] and [DynamoDBImport.java][import-tool-source].  ### Import/Export Tool This simple tool that makes use of the InputFormat and OutputFormat implementations provides an easy  way to import to and export data from DynamoDB.  ### Supported Versions Currently the project builds against Hive 2.3.0, 1.2.1, and 1.0.0. Set this by using the `hive1.version`, `hive1.2.version and `hive2.version` properties in the root Maven `pom.xml`, respectively.  ## How to Build After cloning, run `mvn clean install`.  ## Example: Hive StorageHandler Syntax to create a table using the DynamoDBStorageHandler class: ``` CREATE EXTERNAL TABLE hive_tablename (     hive_column1_name column1_datatype,     hive_column2_name column2_datatype ) STORED BY 'org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler' TBLPROPERTIES (     "dynamodb.table.name" = "dynamodb_tablename",     "dynamodb.column.mapping" =         "hive_column1_name:dynamodb_attribute1_name,hive_column2_name:dynamodb_attribute2_name" ); ```  ## Example: Input/Output Formats with Spark Using the DynamoDBInputFormat and DynamoDBOutputFormat classes with `spark-shell`: ``` $ spark-shell --jars /usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar ... import org.apache.hadoop.io.Text; import org.apache.hadoop.dynamodb.DynamoDBItemWritable import org.apache.hadoop.dynamodb.read.DynamoDBInputFormat import org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat import org.apache.hadoop.mapred.JobConf import org.apache.hadoop.io.LongWritable  var jobConf = new JobConf(sc.hadoopConfiguration) jobConf.set("dynamodb.input.tableName", "myDynamoDBTable")  jobConf.set("mapred.output.format.class", "org.apache.hadoop.dynamodb.write.DynamoDBOutputFormat") jobConf.set("mapred.input.format.class", "org.apache.hadoop.dynamodb.read.DynamoDBInputFormat")  var orders = sc.hadoopRDD(jobConf, classOf[DynamoDBInputFormat], classOf[Text], classOf[DynamoDBItemWritable])  orders.count() ```  ## Example: Import/Export Tool ##### Export usage ``` java -cp target/emr-dynamodb-tools-4.2.0-SNAPSHOT.jar org.apache.hadoop.dynamodb.tools.DynamoDBExport /where/output/should/go my-dynamo-table-name ``` ##### Import usage ``` java -cp target/emr-dynamodb-tools-4.2.0-SNAPSHOT.jar org.apache.hadoop.dynamodb.tools.DynamoDBImport /where/input/data/is my-dynamo-table-name ```  #### Additional options ``` export <path> <table-name> [<read-ratio>] [<total-segment-count>]  read-ratio: maximum percent of the specified DynamoDB table's read capacity to use for export  total-segments: number of desired MapReduce splits to use for the export ```  ``` import <path> <table-name> [<write-ratio>]  write-ratio: maximum percent of the specified DynamoDB table's write capacity to use for import ```  ## Maven Dependency To depend on the specific components in your projects, add one (or both) of the following to your `pom.xml`.  #### Hadoop InputFormat/OutputFormats & DynamoDBItemWritable ``` <dependency>   <groupId>com.amazon.emr</groupId>   <artifactId>emr-dynamodb-hadoop</artifactId>   <version>4.2.0</version> </dependency> ``` #### Hive SerDes & StorageHandler ``` <dependency>   <groupId>com.amazon.emr</groupId>   <artifactId>emr-dynamodb-hive</artifactId>   <version>4.2.0</version> </dependency> ```  ## Contributing * **If you find a bug or would like to see an improvement, open an issue.**      Check first to make sure there isn't one already open. We'll do our best to respond to issues     and review pull-requests  * **Want to fix it yourself? Open a pull request!**      If adding new functionality, include new, passing unit tests, as well as documentation. Also     include a snippet in your pull request showing that all current unit tests pass. Tests are ran     by default when invoking any goal for maven that results in the `package` goal being executed     (`mvn clean install` will run them and produce output showing such).  * **Follow the [Google Java Style Guide][google-style-guide]**      Style is enforced at build time using the [Apache Maven Checkstyle Plugin][maven-checkstyle-plugin].  [emr-release-guide]: http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/emr-release-components.html [dynamodb-dev-guide]: http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html [hive-commands-emr-dev-guide]: http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EMR_Hive_Commands.html [set-up-hive-table]: http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/EMR_Interactive_Hive.html [hive-dynamodb-data-types]: http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/EMR_Interactive_Hive.html#EMR_Hive_Properties [dynamodb-spark-blog-post]: https://blogs.aws.amazon.com/bigdata/post/Tx1G4SQRV049UL0/Analyze-Your-Data-on-Amazon-DynamoDB-with-Apache-Spark [emr-dynamodb-hive-docs]: http://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/EMRforDynamoDB.html [input-format-javadoc]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html [output-format-javadoc]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/OutputFormat.html [dynamodb-attributevalues]: http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_AttributeValue.html [export-tool-source]: emr-dynamodb-tools/src/main/java/org/apache/hadoop/dynamodb/tools/DynamoDBExport.java [import-tool-source]: emr-dynamodb-tools/src/main/java/org/apache/hadoop/dynamodb/tools/DynamoDBImport.java [google-style-guide]: https://google.github.io/styleguide/javaguide.html [maven-checkstyle-plugin]: https://maven.apache.org/plugins/maven-checkstyle-plugin/index.html
lintool/Ivory	Ivory =====  A Hadoop toolkit for web-scale information retrieval research: http://ivory.cc
spotify/hdfs2cass	# hdfs2cass  hdfs2cass is a wrapper around BulkOutputFormat(s) of Apache Cassandra (C\*). It is written using Apache Crunch's API in attempt to make moving data from Hadoop's HDFS into C\* easy.  ## Quickstart  Here's a quick walkthrough of what needs to be done to successfully run hdfs2cass.  ### Set up a C\* cluster  To start with, let's assume we have a C\* cluster running somewhere and one host in that cluster having a hostname of:      cassandra-host.example.net  In that cluster, we create the following schema:      CREATE KEYSPACE example WITH replication = {       'class': 'SimpleStrategy', 'replication_factor': '1'};     CREATE TABLE example.songstreams (       user_id text,       timestamp bigint,       song_id text,       PRIMARY KEY (user_id));   ### Get some Avro files      Next, we'll need some Avro files. Check out [this tutorial](http://avro.apache.org/docs/1.7.7/gettingstartedjava.html) to see how to get started with Avro. We will assume the Avro files have this schema:      {"namespace": "example.hdfs2cass",      "type": "record",      "name": "SongStream",      "fields": [          {"name": "user_id", "type": "string"},          {"name": "timestamp", "type": "int"},          {"name": "song_id", "type": "int"}      ]     }  We will place files of this schema on our (imaginary) Hadoop file system (HDFS) to a location      hdfs:///example/path/songstreams   ### Run hdfs2cass  Things should™ work out of the box by doing:      $ git clone this-repository && cd this-repository     $ mvn package     $ JAR=target/spotify-hdfs2cass-2.0-SNAPSHOT-jar-with-dependencies.jar     $ CLASS=com.spotify.hdfs2cass.Hdfs2Cass     $ INPUT=/example/path/songstreams     $ OUTPUT=cql://cassandra-host.example.net/example/songstreams?reducers=5     $ hadoop jar $JAR $CLASS --input $INPUT --output $OUTPUT  This should run a hdfs2cass export with 5 reducers.   ### Check data in C\*  If we're lucky, we should eventually see our data in C\*:      $ cqlsh $(cassandra-host.example.net) -e "SELECT * from example.songstreams limit 1;"            user_id |  timestamp |   song_id     ----------+------------+----------     rincewind |   12345678 | 43e0-e12s  ## Additional Arguments  [hdfs2cass](src/main/java/com/spotify/hdfs2cass/Hdfs2Cass.java) supports additional arguments: * `--rowkey` to determine which field from the input records to use as row key, defaults to the first field in the record * `--timestamp` to specify the timestamp of values in C\*, defaults to now * `--ttl` to specify the TTL of values in C\*, defaults to 0 * `--ignore` to omit fields from source records, can be repeated to specify multiple fields  ## Output URI Format  The format of the output URI is:      (cql|thrift)://cassandra-host[:port]/keyspace/table?args...  The protocols in the output URI can be either `cql` or `thrift`. They are used to determine what type of C\* column family the data is imported into. The `port` is the binary protocol port C\* listens to client connections on.  The `params...` are all optional. They can be:    * `columnnames=N1,N2` - Relevant for CQL. Used to override inferred order of columns in the prepared insert statement. See [this](src/main/java/com/spotify/hdfs2cass/crunch/cql/CQLRecord.java) for more info.    * `compressionclass=S` - What compression to use when building SSTables. Defaults to whichever the table was created with.    * `copiers=N` - The default number of parallel transfers run by reduce during the copy (shuffle) phase. Defaults to 5.    * `distributerandomly` - Used in the shuffle phase. By default, data is grouped on reducers by C\*'s partitioner. This option disables that.    * `mappers=N` - How many mappers should the job run with. By default this number is determined by magic.    * `reducers=N` - How many reducers should the job run with. Having too few reducers for a lot of data will cause the job to fail.    * `streamthrottlembits=N` - Maximum throughput allowed when streaming the SSTables. Defaults to C\*'s default.    * `rpcport=N` - Port used to stream the SSTables. Defaults to the port C\* uses for streaming internally.  ## More info  For more examples and information, please go ahead and [check how hdfs2cass works](src/main/java/com/spotify/hdfs2cass). You'll find examples of Apache Crunch jobs that can serve as a source of inspiration.
GoogleCloudPlatform/bigdata-interop	# bigdata-interop  Libraries and tools for interoperability between Hadoop-related open-source software and Google Cloud Platform.  ## Google Cloud Storage connector for Hadoop  The Google Cloud Storage connector for Hadoop enables running MapReduce jobs directly on data in Google Cloud Storage by implementing the Hadoop FileSystem interface. For details, see the README in the `/gcs/` folder.  ### Building  The Google Cloud Storage (GCS) connector is built with Maven 3 (as of 2014-05-23, version 3.2.1 has been tested). To build the connector for Hadoop 1, run the following commands from the main directory:      mvn -P hadoop1 package  To build the connector with support for Hadoop 2 & YARN, run the following commands from the main directory:      mvn -P hadoop2 package  In both cases the GCS connector JAR can be found in gcs/target/.
ctripcorp/dataworks-zeus	# dataworks-zeus Ctrip Hadoop Job Scheduling System derived from https://github.com/alibaba/zeus  #Commiter  + 杨晓青([@vinceyang](https://github.com/vinceyang))  + 陈永城([@cyc821211](https://github.com/cyc821211))  + 龙科宇([@new12](https://github.com/new12))   #Contributor  + 宋荣([@mine](https://github.com/mine))
amient/kafka-hadoop-loader	# kafka-hadoop-loader  This is a hadoop job for incremental loading of kafka topics into hdfs. It works by creating a split for each partition, whether physical ( pre kafka v.0.8) or logical( kafka v0.8 or higher). It differs from the most comprehensive solution (Kafka Connect) mainly in that it works for simple, schema-less cases directly off kafka brokers without  having to integrate with any other components.  # Offset-tracking  It uses hdfs for check-pointing by defauit (a bit more work needs to be done to guarantee exactly-once delivery) but offers also a switch to use kafka's consumer group mechanism via zookeeper store. Each split is assigned  to a task and when that task completes loading all the up-to-date messages in the given partition, the output file for the task is   committed using the standard hadoop output committer mechanism.   # Output Partitioning  The output paths are configurable via formatter which uses placeholders for topic name `{T}` and partition id `{P}` and the default format is:      '{T}/{P}'      This default format basically follows kafka partitioning model which translates the output file paths as follows:      <job-output-directory>/<topic-name>/<partition-id>/<unique-filename>  job-output-directory is fixed and unique-filename is a combination of topic partition and start offset where the incremental load started.   Optionally, a TimestampExtractor may be provided by configuration which enables time based partitioning format, for example:      '{T}/d='dddd-MM-yy   would result in the following file paths:      <job-output-directory>/<topic-name>/d=<date>/<unique-filename>      From Kafka 0.10 each message has a default timestamp metadata which will be available automatically on the 0.10 and higher versions of  the hadoop loader.  # Schema-less model  It is schema-less and when used with the built-in MutliOutputFormat it simply writes out each message payload byte-by-byte on a new line. This way it can be used for simple csv/tsv/json encodings.  Hadoop Loader is capable of transformations within the mapper, be it schema-based or purpose-formatted but for these use cases,  Kafka Connect is much more suitable framework.   # OUT-OF-THE-BOX LAUNCH CONFIGURATIONS  The default program can be packaged with `mvn package` which produces a jar that has a limited functionality via command line arguments. To see how the job can be configured and extended programatically see system tests under src/test.  ## TO RUN FROM AN IDE     add run configuration arguments: -r [-t <coma_separated_topic_list>] [-z <zookeeper>] [target_hdfs_path]   ## TO RUN REMOTELY      $ mvn package     $ java -jar kafka-hadoop-loader.jar -r [-t <coma_separated_topic_list>] [-z <zookeeper>] [target_hdfs_path]     TODO -r check if jar exists otherwise use addJarByClass   ## TO RUN AS HADOOP JAR     $ mvn package     $ hadoop jar kafka-hadoop-loader.jar [-z <zookeeper>] [-t <topic>] [target_hdfs_path]    # ANATOMY      HadoopJob         -> KafkaInputFormat             -> zkUtils.getBrokerPartitionLeaders             -> FOR EACH ( logical partition ) CREATE KafkaInputSplit         -> FOR EACH ( KafkaInputSplit ) CREATE MapTask:             -> KafkaInputRecordReader( KafkaInputSplit[i] )                 -> checkpoint manager getLastConsumedOffset                 -> intialize simple kafka consumer                 -> reset watermark if given as option                 -> WHILE nextKeyValue()                     -> KafkaInputContext.getNext() -> (offset,message):newOffset                     -> KafkaInputRecordReader advance currentOffset+=newOffset and numProcessedMessages++                     -> HadoopJobMapper(offset,message) -> (date, message)                         -> KafkaOutputFormat.RecordWriter.write(date, message)                             -> recordWriters[date].write( date,message )                                 -> LineRecordWriter.write( message ) gz compressed or not                 -> close KafkaInputContext                 -> zkUtils.commitLastConsumedOffset
alexholmes/hiped2	Source code for Hadoop in Practice, Second Edition ==================================================  This project contains the source code that accompanies the book [Hadoop in Practice, Second Edition](http://www.manning.com/holmes2/).  ## License  Apache version 2.0 (for more details look at the [license](LICENSE)).  ## Usage  ### Tarball  The easiest way to start working with the examples is to download a tarball distribution of this project. Doing so will mean that running your first example is just three steps away:  1. Go to the [releases](https://github.com/alexholmes/hiped2/releases) and download the most recent tarball. 2. Extract the contents ot the tarball.          $ tar -xzvf hip-<version>-package.tar.gz  3. The examples in the book all use the `hip` script located in `bin/hip` to execute the examples. While it's not required, it's recommended that you add `hip-<version>/bin` to your path so that you can simply execute `hip` and execute the examples in the book by directly copy-pasting the commands.  4. Run the "hello world" example, which is  ```bash $ cd hip-<version>  # create two input files in HDFS $ hadoop fs -mkdir -p hip1/input $ echo "cat sat mat" | hadoop fs -put - hip1/input/1.txt $ echo "dog lay mat" | hadoop fs -put - hip1/input/2.txt  # run the inverted index example $ ./hip hip.ch1.InvertedIndexJob --input hip1/input --output hip1/output  # examine the results in HDFS $ hadoop fs -cat hip1/output/part* ```  Done! The tarball also includes the sources and JavaDocs.  ### Building your own distribution  Here you're going to checkout the trunk and then use Maven to run a build.  1. Checkout the code.          $ git clone git@github.com:alexholmes/hiped2.git  2. Build the code and distribution tarball.  ```bash $ cd hiped2 $ mvn clean $ mvn validate $ mvn package ```  The JAR's and tarball will be under the `target` directory. Now you can follow the instructions in the "Tarball" section above to explode the tarball and run an example.  ## What's next?  At this point check out the book for more examples and how you can execute them. Or if you find any issues then please go to the [issues](https://github.com/alexholmes/hiped2/issues) and open a new issue.
spotify/spydra	# Spydra (Beta)  [![License](https://img.shields.io/github/license/spotify/spydra.svg)](LICENSE)  Ephemeral Hadoop clusters using Google Compute Platform  ## Description `Spydra` is "Hadoop Cluster as a Service" implemented as a library utilizing [Google Cloud Dataproc](https://cloud.google.com/dataproc/)  and [Google Cloud Storage](https://cloud.google.com/storage/). The intention of `Spydra` is to enable the use of ephemeral Hadoop clusters while hiding the complexity of cluster lifecycle management and keeping troubleshooting simple. `Spydra` is designed to be integrated as a `hadoop jar` replacement.  `Spydra` is part of Spotify's effort to migrate its data infrastructure to Google Compute Platform and is being used in production. The principles and the design of `Spydra` are based on our experiences in scaling and maintaining our Hadoop cluster to over 2500 nodes and over 100 PBs of capacity running about 20,000 independent jobs per day.   `Spydra` supports submitting data processing jobs to Dataproc as well as to existing on-premise Hadoop infrastructure  and is designed to ease the migration to and/or dual use of Google Cloud Platform and on-premise infrastructure.  `Spydra` is designed to be very configurable and allows the usage of all job types and configurations supported by the  [gcloud dataproc clusters create](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/) and [gcloud dataproc jobs submit](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/) commands.  ### Development Status `Spydra` is the rewrite of a concept that has been developed at Spotify for more than a year. The current version of `Spydra` is in beta, used in production at Spotify, and actively developed and supported by our data infrastructure team.  `Spydra` is in beta and things might change but we are aiming at not breaking the currently exposed APIs and configuration.  ### Spydra at Spotify At Spotify, `Spydra` is being used for our on-going migration to Google Cloud Platform. It handles the  submission of on-premise Hadoop jobs as well as Dataproc jobs, simplifying the switch from on-premise Hadoop to Dataproc.  `Spydra` is packaged in a [docker](https://www.docker.com/) image that is used to deploy data pipelines. This docker image includes Hadoop tools and configurations to be able to submit to our on-premise Hadoop cluster as well as an installation of [gcloud](https://cloud.google.com/sdk/gcloud/) and other basic dependencies required to execute Hadoop jobs in our environment. Pipelines are then scheduled using [Styx](https://github.com/spotify/styx) and orchestrated by [Luigi](https://github.com/spotify/luigi) which then invokes `Spydra` instead of `hadoop jar`.  ### Design  `Spydra` is built as a wrapper around Google Cloud Dataproc and designed not to have any central component. It exposes all functionality supported by Dataproc via its own configuration while adding some defaults. `Spydra` manages clusters and submits jobs invoking the `gcloud dataproc` command. `Spydra` ensures that clusters are eventually deleted by updating a heartbeat marker in the cluster's metadata and utilizes [initialization-actions](https://cloud.google.com/dataproc/docs/concepts/init-actions) to set up a self-deletion script on the cluster to handle the deletion of the cluster in the event of client failures.  For submitting jobs to an existing on-premise Hadoop infrastructure, `Spydra` utilizes the `hadoop jar` command which is required to be installed and configured in the environment.   For Dataproc as well as on-premise submissions, `Spydra` will act similar to hadoop jar and print out driver output.  #### Credentials `Spydra` is designed to ease the usage of Google Compute Platform credentials by utilizing  [service accounts](https://cloud.google.com/compute/docs/access/service-accounts). The same credential that is  used locally by `Spydra` to manage the cluster and submit jobs, is also by default forwarded to the Hadoop cluster when calling Dataproc. This means that access rights to resources need only be given to a single set of credentials.  #### Storing Execution Data and Logs To make job execution data available after an ephemeral cluster was shut down, and to provide similar functionality to the Hadoop MapReduce History Server, `Spydra` stores execution data and logs on Google Cloud Storage, grouping it by  a user-defined client id. Typically client id is unique per job. The execution data and logs are then made available via  `Spydra` commands. These allow spinning up a local MapReduce History Server to access execution data and logs as well as dumping them.  #### Autoscaler `Spydra` has an **experimental** autoscaler which can be executed on the cluster. It monitors the current resource utilization on the cluster and scales the cluster according to a user defined utilization factor and maximum worker count by adding [preemptible VMs](https://cloud.google.com/dataproc/docs/concepts/preemptible-vms). Note that the use of  preemptible VMs might negatively impact performance as nodes might be shut down any time.  The autoscaler is being installed on the cluster using a Dataproc [initialization-action](https://cloud.google.com/dataproc/docs/concepts/init-actions).  #### Cluster Pooling `Spydra` has **experimental** support for cluster pooling withing a single Google Compute Platform project. Cluster pooling can be used to limit the resources used by the job submissions, and also limit the cluster initialization overhead. The maximum number of clusters to be used can be defined as well as their maximum lifetime. Upon job submission, a random cluster is chosen to submit the job into. When reaching their maximum lifetime, pooled clusters are being deleted by the self-deletion mechanism.  ## Usage ### Installation There's a pre-built [Spydra on maven central](https://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.spotify.data.spydra%22%20a%3A%22spydra%22). This is built using the parameters from `.travis.yml`, the bucket `spydra-init-actions` is provided for by Spotify.  ### Prerequisites To be able to use Dataproc and on-premise Hadoop, a few things need to be set up before using `Spydra`.  * Java 8 * A [Google Cloud Platform project](https://cloud.google.com/resource-manager/docs/creating-managing-projects)   with the right (Google Cloud Dataproc API) [APIs enabled](https://support.google.com/cloud/answer/6158841?hl=en) * A [service account](https://cloud.google.com/compute/docs/access/service-accounts) with   [project editor](https://cloud.google.com/compute/docs/access/iam) rights in your project * JSON key for the service account * [gcloud](https://cloud.google.com/sdk/gcloud/) needs to be installed * `gcloud` needs to be [authenticated using the service account](https://cloud.google.com/sdk/gcloud/reference/auth/) * The environment variable [GOOGLE_APPLICATION_CREDENTIALS](https://developers.google.com/identity/protocols/application-default-credentials)   needs to point to the location of the service account JSON key. This cannot be a user credential. * [hadoop jar](https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/CommandsManual.html#jar)   needs to be installed and configured to submit to your cluster  ### Spydra CLI  `Spydra` CLI supports multiple sub-commands:  * [`submit`](#submission) - submitting jobs to on-premise Hadoop and GCP Dataproc * [`run-jhs`](#running-an-embedded-jobhistoryserver) - embedded history server * [`dump-logs`](#retrieving-logs) - viewing logs * [`dump-history`](#retrieving-history-data) - viewing history  #### Submission  ``` $ java -jar spydra/target/spydra-VERSION-jar-with-dependencies.jar submit --help  usage: submit [options] [jobArgs]     --clientid <arg>     client id, used as identifier in job history output     --spydra-json <arg>  path to the spydra configuration json     --jar <arg>          main jar path, overwrites the configured one if                          set     --jars <arg>         jar files to be shipped with the job, can occur                          multiple times, overwrites the configured ones if                          set     --job-name <arg>     job name, used as dataproc job id  -n,--dry-run            Do a dry run without executing anything ```  Only a few basic things can be supplied on the command line; a client-id (an arbitrary identifier of the client running `Spydra`), the main and additional JAR files for the job, and arguments for the job. For any use-case requiring more details, the user needs to create a JSON file and supply the path to that as a parameter. All the command-line options will override the corresponding options in the JSON config. Apart from all the command-line options and some general settings, it can also transparently pass along parameters to the `gcloud` command for [cluster creation](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create) or [job submission](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/hadoop).  A job name can also be supplied. This will be sanitized and have a unique identifier attached to it, which will then be used as the Dataproc job ID. This is useful in finding the job in the Google Cloud Console.  ##### The spydra-json argument All properties that cannot be controlled via the few arguments of the *submit* command, can be set in the configuration file supplied with the --spydra-json parameter. The configuration file follows the structure of the  `cloud dataproc clusters create` and `cloud dataproc jubs submit` commands and allows to set all the possible arguments for these commands. The basic structure looks as follows:  ```json {   "client_id": "spydra-test",                 # Spydra client id. Usually left out as set by the frameworks during runtime.   "cluster_type": "dataproc",                 # Where to execute. Either dataproc or onpremise. Defaults to onpremise.   "job_type": "hadoop",                       # Defaults to hadoop. For supported types see gcloud dataproc jobs submit --help   "log_bucket": "spydra-test-logs",           # The bucket where Hadoop logs and history information are stored.   "region": "europe-west1",                   # The region in which the cluster is spun up   "cluster": {                                # All cluster related configuration     "options": {                              # Map supporting all options from the gcloud dataproc clusters create command       "project": "spydra-test",       "num-workers": "13",       "worker-machine-type": "n1-standard-2", # The default machine type used by Dataproc is n1-standard-8.       "master-machine-type": "n1-standard-4"     }   },   "submit": {                                 # All configuration related to job submission     "job_args": [                             # Job arguments. Usually left out as set by the frameworks during runtime.       "pi",       "2",       "2"     ],     "options": {                              # Map supporting all options from the gcloud dataproc jobs submit [hadoop,spark,hive...] command       "jar": "/path/my.jar"                   # Path of the job jar file. Usually left out as set by the frameworks during runtime.     }   } } ```  For details on the format of the JSON file see [this schema](/spydra/src/main/resources/spydra_config_schema.json) and [these examples](spydra/src/main/resources/config_examples/).  ##### Minimal Submission Example  Using only the command-line: ``` $ java -jar spydra/target/spydra-VERSION-jar-with-dependencies.jar submit --client-id simple-spydra-test --jar hadoop-mapreduce-examples.jar pi 8 100 ```  JSON config: ``` $ cat examples.json {   "client_id": "simple-spydra-test",   "cluster_type": "dataproc",   "log_bucket": "spydra-test-logs",   "region": "europe-west1",   "cluster": {     "options": {       "project": "spydra-test"     }   },   "submit": {     "job_args": [       "pi",       "8",       "100"     ],     "options": {       "jar": "hadoop-mapreduce-examples.jar"     }   } } $ spydra submit --spydra-json example.json ```  ##### Cluster Autoscaling (Experimental) The `Spydra` autoscaler provides automatic sizing for `Spydra` clusters by adding enough preemptible worker nodes until a user supplied percentage of containers is running in parallel on the cluster. It enables cluster sizes to automatically adjust to growing resource needs over time and removes the need to come up with a good size when scheduling a job executed on `Spydra`. The autoscaler has two modes, upscale only and downscale.  Downscale will remove nodes when the cluster is not fully utilized. After choosing to downscale, it will wait for the `downscale_timeout` to allow active jobs to complete before terminating nodes. Note that though nodes may not have active YARN containers running, active jobs may be storing intermediate "shuffle" data on them. See [Dataproc Graceful Downscale](https://cloud.google.com/dataproc/docs/concepts/scaling-clusters#using_graceful_decommissioning) for more information.  To enable autoscaling, add an autoscaler section similar to the one below to your `Spydra` configuration.  ```json {   "cluster:" {...},   "submit:" {...},   "auto_scaler": {     "interval": "2",        # Execution interval of the autoscaler in minutes     "max": "20",            # Maximum number of workers     "factor": "0.3",        # Percentage of YARN containers that should be running at any point in time 0.0 to 1.0.     "downscale": "false",    # Whether or not to downscale.     # If downscale is enabled, how long in minutes to wait for active jobs to finish     # before terminating nodes and potentially interrupting those jobs.     # Note that the autoscaler will not be able to add nodes during this interval.     "downscale_timeout": "10"   } } ```  ##### Static Cluster Submission If you prefer to manage your Dataproc clusters manually you still can use Spydra for job submission and just skip dynamic cluster creation part. The only change that is needed to be done to Spydra configurations is that you need to specify the name of the cluster you want to submit the job to. Here is an example:  ```json {   "client_id": "simple-spydra-test",   "cluster_type": "dataproc",   "log_bucket": "spydra-test-logs",   "submit": {     "options": {         "project": "spydra-test",         "cluster": "NAME_OF_YOUR_CLUSTER"     }     "job_args": [       "pi",       "8",       "100"     ],     "options": {       "jar": "hadoop-mapreduce-examples.jar"     }   } } ``` Also notice that `project` parameter is specified in `submit/options` section instead of `cluster/options` section.  ##### Cluster Pooling (Experimental) Disclaimer: The usage of the pooling is experimental!  The `Spydra` cluster pooling provides automatic pooling for `Spydra` clusters by selecting an existing cluster according to certain conditions.  To enable cluster pooling add a pooling section similar to the one below to your `Spydra` configuration.  ```json {   "cluster:" {...},   "submit:" {...},   "pooling": {     "limit": 2,     # limit of concurrent clusters     "max_age": "P1D"# A java.time.Duration for the maximum age of a cluster   } } ```  ##### Submission Gotchas    * You can use `--` if you need to pass a parameter starting with dashes to your job,      e.g. `submit --jar=jar ... -- -myParam`    * Don't forget to specify `=` for arguments like `--jar=$jar`, otherwise the CLI parsing      will break.    * If the specified jar contains a Main-Class entry in it's manifest, specifying --mainclass      will often lead to undesired behaviour, as the value of main-class will be passed as      an argument to the application instead of invoking this class.    * Not setting the default fs to GCS using the `fs.defaultFS` property can lead to crashes      and undesired behavior as a lot of the frameworks use the default filesystem implementation      instead of getting the correct filesystem for a given URI. It can also lead to the Crunch      output committer working very slowly while copying all files from HDFS to GCS in a      last non-distributed step.  #### Running an Embedded JobHistoryServer The *run-jhs* is designed for an interactive exploration of the job execution. This command spawns an embedded  JobHistoryServer that can display all jobs executed using the client id associated with your job submission. Familiarity with the use of JobHistoryServer from on-premise Hadoop is assumed. The JHS is accessible on default port 19888.  The client id used when executing the job, and the log bucket is required for running *run-jhs* command.  ```java -jar spydra/target/spydra-VERSION-jar-with-dependencies.jar run-jhs --clientid=JOB_CLIENT_ID --log-bucket=LOG_BUCKET```  #### Retrieving Logs The *dump-logs* command will dump logs for an application to stdout. Currently only full logs of the YARN application can be dumped - similarly to YARN logs when no specific container is specified. This is useful for processing/exploration with further tools in the shell.  The client id used when executing the job, the Hadoop application id, and the log bucket is required for running *dump-logs* command.  ```java -jar spydra/target/spydra-VERSION-jar-with-dependencies.jar dump-logs --clientid=MY_CLIENT_ID --username=HADOOP_USER_NAME --log-bucket=LOG_BUCKET --application=APPLICATION_ID```  #### Retrieving History Data The history files can be dumped as in regular Hadoop using the *dump-history* command.  The client id used when executing the job, the Hadoop application id, and the log bucket is required for running *dump-history* command.  ```java -jar spydra/target/spydra-VERSION-jar-with-dependencies.jar dump-history --clientid=MY_CLIENT_ID --log-bucket=LOG_BUCKET --application=APPLICATION_ID```  ## Accessing Hadoop Web Interfaces for Ephemeral Clusters [Dataprocxy](https://github.com/spotify/dataprocxy) can be used to open the web interfaces of the Hadoop daemons of an ephemeral cluster as long as the cluster is running.   ## Building  ### Prerequisites * Java JDK 8 * Maven 3.2.2 * A Google Compute Platform project with Dataproc enabled * A Google Cloud Storage bucket for uploading init-actions. Ensure that this bucket is readable with all credentials used with `Spydra`. * A Google Cloud Storage bucket for storing integration test logs * JSON key for a [service account](https://cloud.google.com/compute/docs/access/service-accounts)   with editor access to the project and bucket * The environment variable `GOOGLE_APPLICATION_CREDENTIALS` pointing at the location of the service   account JSON key * [gcloud](https://cloud.google.com/sdk/gcloud/) authenticated with the service account * [gsutil](https://cloud.google.com/storage/docs/gsutil) authenticated with the service account  ### Integration Test Configuration  In order to run integration tests, basic configuration needs to be provided during the build process. Create a file with name *integration-test-config.json* similar to the one below and reference it during the maven invocation.  ```json {   "log_bucket": "YOUR_GCS_LOG_BUCKET",   "region": "europe-west1" } ```  Replace the YOUR_GCS_LOG_BUCKET with a bucket you have in your GCP project for storing the logs.  The project will be taken from the service account credentials, you do not need to specify the *project* parameter in *integration-test-config.json* (or elsewhere).  Notice that the file name must be exactly *integration-test-config.json* as that is what the integration test will search for when it is run on the maven verify phase.  ### Build, Test and Package  In the following command, replace YOUR_INIT_ACTION_BUCKET with the bucket you created when setting up the prerequisites and YOUR_TEST_CONFIG_DIR with a directory name containing the file *integration-test-config.json* you created in the previous step. YOUR_TEST_CONFIG_DIR cannot be the same as the package root, so create a separate directory for this purpose. Then execute the maven command:  ``` mvn clean install -Dinit-action-uri=gs://YOUR_INIT_ACTION_BUCKET/spydra -Dtest-configuration-dir=YOUR_TEST_CONFIG_DIR ```  Executing the maven command above will run the integration tests, and create a spydra-VERSION-jar-with-dependencies.jar under spydra/target that packages `Spydra`, which can be executed with `java -jar`. Using `package` instead of `install` can be used  to run just unit-tests and package Spydra.  If you want to copy the init-scripts into the defined init-action bucket, activate profile `install-init-scripts`:  ``` mvn clean install -Pinstall-init-scripts -Dinit-action-uri=gs://YOUR_INIT_ACTION_BUCKET/spydra -Dtest-configuration-dir=YOUR_TEST_CONFIG_DIR ```  Do not run Maven `deploy` step, as it will try to upload created packages into the Spotify owned repositories, which will fail unless you have Spotify specific credentials.  ## Communications  If you use Spydra and experience any issues, please create an issue under this Github project in [here](https://github.com/spotify/spydra/issues/new).  You can also ask for help and talk to us on Spydra related issues in [Spotify FOSS Slack](https://spotify-foss.slack.com) on channel [#spydra](https://spotify-foss.slack.com/messages/C58L8GVS8/).  ## Contributing  This project adheres to the [Open Code of Conduct][code-of-conduct]. By participating, you are expected to honor this code.  [code-of-conduct]: https://github.com/spotify/code-of-conduct/blob/master/code-of-conduct.md
Conductor/kangaroo	Intro ============  Kangaroo is Conductor's collection of open source Hadoop Map/Reduce utilities.  Currently, Kangaroo includes:  1. A scalable [Kafka input format](#kafka) 2. Several [FileInputFormats optimized for S3 input data](#s3). 3. A input format for [distributed task execution](#wrtiablevalue) 4. A [compression codec](#codec) for the [framing format](https://github.com/google/snappy/blob/master/framing_format.txt) of Snappy  # Setting up Kangaroo  You can build Kangaroo with:  ```mvn clean package```  ## <a name="kafka"></a>Using the Kafka Input Format For more details, check out our [blog post about the Kafka input format](http://www.conductor.com/nightlight/data-stream-processing-bulk-kafka-hadoop/ "Data Stream Processing: A Scalable Bridge from Kafka to Hadoop").  For a Kafka 0.8.1-compatible version of this code, see [this branch](https://github.com/Conductor/kangaroo/tree/kafka-8). (It compiles, is unit tested, but completely untested in the wild - please help us get it up to snuff!)  ### Create a Mapper ```java public static class MyMapper extends Mapper<LongWritable, BytesWritable, KEY_OUT, VALUE_OUT> {      @Override     protected void map(final LongWritable key, final BytesWritable value, final Context context) throws IOException, InterruptedException {         // implementation     } } ```  * The `BytesWritable` value is the raw bytes of a single Kafka message. * The `LongWritable` key is the Kafka offset of the message.  ### Single topic  ```java // Create a new job final Job job = Job.getInstance(getConf(), "my_job");  // Set the InputFormat job.setInputFormatClass(KafkaInputFormat.class);  // Set your Zookeeper connection string KafkaInputFormat.setZkConnect(job, "zookeeper-1.xyz.com:2181");  // Set the topic you want to consume KafkaInputFormat.setTopic(job, "my_topic");  // Set the consumer group associated with this job KafkaInputFormat.setConsumerGroup(job, "my_consumer_group");  // Set the mapper that will consume the data job.setMapperClass(MyMapper.class);  // (Optional) Only commit offsets if the job is successful if (job.waitForCompletion(true)) {     final ZkUtils zk = new ZkUtils(job.getConfiguration());     zk.commit("my_consumer_group", "my_topic");     zk.close(); } ```  ### Multiple topics ```java // Create a new job final Job job = Job.getInstance(getConf(), "my_job");  // Set the InputFormat job.setInputFormatClass(MultipleKafkaInputFormat.class);  // Set your Zookeeper connection string KafkaInputFormat.setZkConnect(job, "zookeeper-1.xyz.com:2181");  // Add as many queue inputs as you'd like MultipleKafkaInputFormat.addTopic(job, "my_first_topic", "my_consumer_group", MyMapper.class); MultipleKafkaInputFormat.addTopic(job, "my_second_topic", "my_consumer_group", MyMapper.class); // ...  // (Optional) Only commit offsets if the job is successful if (job.waitForCompletion(true)) {     final ZkUtils zk = new ZkUtils(job.getConfiguration());     // commit the offsets for each topic     zk.commit("my_consumer_group", "my_first_topic");     zk.commit("my_consumer_group", "my_second_topic");     // ...     zk.close(); } ```  ### Customize Your Job Our Kafka input format allows you to limit the number of splits consumed in a single job: * By consuming data created approximately on or after a timestamp. ```java // Consume Kafka partition files with were last modified on or after October 13th, 2014 KafkaInputFormat.setIncludeOffsetsAfterTimestamp(job, 1413172800000); ``` * By consuming a maximum number of Kafka partition files (splits), per Kafka partition. ```java // Consume the oldest five unconsumed Kafka files per partition KafkaInputFormat.setMaxSplitsPerPartition(job, 5); ```  ### Static Access to InputSplits Our Kafka input format exposes static access to a hypothetical job's `KafkaInputSplits`.  We've found this information useful when estimating the number of reducers for certain jobs. This calculation is pretty fast; for a topic with 30 partitions on a 10-node Kafka cluster, this calculation took about 1 second. ```java final Configuration conf = new Configuration(); conf.set("kafka.zk.connect", "zookeeper-1.xyz.com:2181");  // Get all splits for "my_topic" final List<InputSplit> allTopicSplits = KafkaInputFormat.getAllSplits(conf, "my_topic"); // Get all of "my_consumer_group"'s splits for "my_topic" final List<InputSplit> consumerSplits = KafkaInputFormat.getSplits(conf, "my_topic", "my_consumer_group");  // Do some interesting calculations... long totalInputBytesOfJob = 0; for (final InputSplit split : consumerSplits) {     totalInputBytesOfJob += split.getLength(); } ```  ## <a name="s3"></a>Using the S3 Input Formats  The job setup of these `FileInputFormat`s are optimized for S3. Namely, each one:  1. Uses the `AmazonS3` client instead of the `S3FileSystem`. 2. Uses `AmazonS3.listObjects` to efficiently discover input files recursively. 3. Trims out all of the `FileSystem` operations that are irrelevant to S3.  Th overall performance boost varies based on the number of input directories (S3 prefixes in this case). With 10 or more input directories, you can expect 2-3x faster split discovery.  If your input directories share a common S3 prefix, **only add the common prefix to your job**.  This will give you the biggest performance boost because the input format takes advantage of `AmazonS3.listObjects`.  In one test of 7000 input files that shared a common prefix, our input format discovered splits in 10 seconds, whereas the Hadoop `FileInputFormat` took 730 seconds.  ### Job setup  You use these input formats *exactly* the way you normally use `SequenceFileInputFormat` or `TextFileInputFormat`, except you specify our S3 input format on the job settings:  ```java // put your AWS credentials in the Configuration final Configuration conf = new Configuration(); conf.set("fs.s3n.awsAccessKeyId", "YOUR_AWS_KEY"); conf.set("fs.s3n.awsSecretAccessKey", "YOUR_AWS_SECRET");  // create a job final Job job = Job.getInstance(getConf(), "my_job");  // This is the only difference! All other settings are exactly the same. job.setInputFormatClass(S3SequenceFileInputFormat.class);  // add your input paths - if your input paths share a common prefix, just add the parent prefix!! SequenceFileInputFormat.addInputPath(job, new Path("s3n://my-bucket/input/path")); SequenceFileInputFormat.addInputPath(job, new Path("s3n://my-bucket/other/path"));  // other FileInputFormat or SequenceFileInputFormat settings... other job settings... ```  ### Available Input Formats  | S3 Input Format | Corresponding Hadoop Input Format | | --------------- | --------------------------------- | | `S3SequenceFileInputFormat` | `org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat` | | `S3TextInputFormat` | `org.apache.hadoop.mapreduce.lib.input.TextInputFormat` | | `S3SequenceFileInputFormatMRV1` | `org.apache.hadoop.mapred.TextInputFormat` | | `S3TextInputFormatMRV1` | `org.apache.hadoop.mapred.SequenceFileInputFormat` |  We've included MRV1 versions of these input formats, which we use for S3-backed Hive tables.  ## <a name="wrtiablevalue"></a>Distributed task execution using WritableValueInputFormat  When multiple threads in a single JVM won't suffice, Kangaroo comes to the Rescue.  The `WritableValueInputFormat` allows you to distribute computational work across a configurable number of map tasks in Map/Reduce.  ### Create a Mapper  Your mapper will take a `NullWritable` key, and a value that must implement `Writable`.  ```java public static class MyComputationalMapper extends Mapper<NullWritable, UnitOfWork, KEY_OUT, VALUE_OUT> {      @Override     protected void map(final NullWritable key, final UnitOfWork value, final Context context) throws IOException, InterruptedException {         // process UnitOfWork, and output the result(s) if you want to reduce it     } }  ``` ### Job setup  To setup a `Job`, calculate the units of work and specify exactly how many inputs each map task gets.  ```java // compute the work to be done final List<UnitOfWork> workToBeDone = ...;  // Create the job and setup your input, specifying 50 units of work per mapper. final Job job = Job.getInstance(getConf(), "my_job"); job.setInputFormatClass(WritableValueInputFormat.class); WritableValueInputFormat.setupInput(workToBeDone, UnitOfWork.class, 50, job);  // If you want to add EVEN MORE concurrency to your job, use the MultithreadedMapper! job.setMapperClass(MultithreadedMapper.class); MultithreadedMapper.setMapperClass(job, MyComputationalMapper.class); // your actual mapper MultithreadedMapper.setNumberOfThreads(job, 10); // 10 threads per mapper  ```  ## <a name="codec"></a>Snappy Framing Compression Codec `com.conductor.hadoop.compress.SnappyFramedCodec` will allow your Map/Reduce jobs to read and write files compressed in  the Snappy framing format. Firstly, make sure that you set the following hadoop configuration properties accordingly (property names may vary by distribution, [the below are for CDH4/YARN](http://www.cloudera.com/content/cloudera/en/documentation/cdh4/latest/CDH4-Installation-Guide/cdh4ig_topic_23_3.html)):  | Property Name | Meaning | Optional? | Value | | ------------- | ------- | --------- | ----- | | `io.compression.codecs` | Registry of available compression codecs | no | `...,com.conductor.hadoop.compress.SnappyFramedCodec,...` | | `mapred.map.output.compression.codec` | Compression codec for intermediate (map) output | yes, except for map-only jobs | `com.conductor.hadoop.compress.SnappyFramedCodec` | | `mapred.output.compression.codec` | Compression codec for final (reduce) output | no | `com.conductor.hadoop.compress.SnappyFramedCodec` |
datasalt/pangool	Pangool =======  Pangool is a framework on top of Hadoop that implements Tuple MapReduce.  More info in:  http://pangool.net http://www.datasalt.com/2012/02/tuple-mapreduce-beyond-the-classic-mapreduce/
jaibeermalik/searchanalytics-bigdata	Analyzing Search Clicks Data Using Flume, Hadoop, Hive, Pig, Oozie, ElasticSearch, Akka, Spring Data, Spark streaming, Hbase.  ===================    Repository contains unit/integration test cases to generate analytics based on clicks events related to the product search on any e-commerce website.      Getting Started  ---------------    The project is maven project and can be build with Eclipse. Check pom dependencies for relevant version of earch application. It uses cloudera hadoop distribution version 2.3.0-cdh5.0.0.     Functionality  -----  The scenario covered in the application for the search analytics using big data is as follow,    Events based:  <pre>  ->Customer(Session containing customer information)  ->Product Search (out of products)  ->Search Session(SearchCriteria)  ->On each Search Click(SearchQueryInstruction)  ->Flume Embedded Agent(each one for multiple app servers)  ->Flume Source(Combine information from all agents, if multiple)  ->Sink Filtering for Multiple Sinks(Hadoop Sink and ElasticSearch Sink)  ->Hadoop Sink to store all clicks data (store all data for later analysis and reporting purpose)  ->ElasticSearch Sink to store recently viewed items(can be used to show recently viewed for each customer)  </pre>    Job Based:  <pre>  ->Hive partition for Hadoop data(based on year/month/day/hour basis)  ->Oozie Coordinator job to automatically create hive partition once data directoy available    ->Hive Customer top queries in last one month(based on query string)  ->Hive External tablet to load topqueries data to ElasticSearch  ->Oozie bundle job to load hive data for top queries and accordingly update ES index data.   </pre>    Hadoop  ---------------  The application uses mini hdfs and mini mr cluster for test cases.  If you want to use the same for external hdfs location, please change relevant configurations and use accordingly.    Flume  ---------------  FlumeAgentService to control map search events to both hdfs and ES bases on multiplexing selector approach.  The application uses inbuilt rolling file sink for the EmbeddedAgent. You can also setup and start external flume agent and point the embedded agent to the same.     JSONSerDe:  To map the json data to hive queries, custom SerDe is used. Create jar and add to your own hive environment to query data if you use external flume source as configured above.    To create json SerDe jar,  $ jar cf jaihivejsonserde-1.0.jar org/jai/hive/serde/JSONSerDe.class    ElasticSearch  ---------------  ElasticSearchJsonBodyEventSerializer:  Customer ES serializer is used to put data from hadoop to ElasticSearch using hive.  To create ES jsons erializer jar,  $ cd target/classes  $ jar cf jaiflumeesjsonserializer-1.0.jar org/jai/flume/sinks/elasticsearch/serializer/ElasticSearchJsonBodyEventSerializer.class    Product Search Functionality    -----  ElasicSearch is used to index products data and to be able to filter on the products.  SearchCriteria store different user selection information which can be specific query string, sorting information, pagination information, different facet/filter selection etc.  SearchQueryInstruction to generate json data for customer clicks,    <pre>  {"eventid":"629e9b5f-ff4a-4168-8664-6c8df8214aa7-1399386809805-24","hostedmachinename":"192.168.182.1330","pageurl":"http://blahblah:/5","customerid":24,"sessionid":"648a011d-570e-48ef-bccc-84129c9fa400","querystring":null,"sortorder":"desc","pagenumber":3,"totalhits":28,"hitsshown":7,"createdtimestampinmillis":1399386809805,"clickeddocid":"41","favourite":null,"eventidsuffix":"629e9b5f-ff4a-4168-8664-6c8df8214aa7","filters":[{"code":"searchfacettype_color_level_2","value":"Blue"},{"code":"searchfacettype_age_level_2","value":"12-18 years"}]}  {"eventid":"648b5cf7-7ca9-4664-915d-23b0d45facc4-1399386809782-298","hostedmachinename":"192.168.182.1333","pageurl":"http://blahblah:/4","customerid":298,"sessionid":"7bf042ea-526a-4633-84cd-55e0984ea2cb","querystring":"queryString48","sortorder":"desc","pagenumber":0,"totalhits":29,"hitsshown":19,"createdtimestampinmillis":1399386809782,"clickeddocid":"9","favourite":null,"eventidsuffix":"648b5cf7-7ca9-4664-915d-23b0d45facc4","filters":[{"code":"searchfacettype_color_level_2","value":"Green"}]}   {"eventid":"74bb7cfe-5f8c-4996-9700-0c387249a134-1399386809799-440","hostedmachinename":"192.168.182.1330","pageurl":"http://blahblah:/1","customerid":440,"sessionid":"940c9a0f-a9b2-4f1d-b114-511ac11bf2bb","querystring":"queryString16","sortorder":"asc","pagenumber":3,"totalhits":5,"hitsshown":32,"createdtimestampinmillis":1399386809799,"clickeddocid":null,"favourite":null,"eventidsuffix":"74bb7cfe-5f8c-4996-9700-0c387249a134","filters":[{"code":"searchfacettype_brand_level_2","value":"Apple"}]}    {"eventid":"9da05913-84b1-4a74-89ed-5b6ec6389cce-1399386809828-143","hostedmachinename":"192.168.182.1332","pageurl":"http://blahblah:/1","customerid":143,"sessionid":"08a4a36f-2535-4b0e-b86a-cf180202829b","querystring":null,"sortorder":"desc","pagenumber":0,"totalhits":21,"hitsshown":34,"createdtimestampinmillis":1399386809828,"clickeddocid":"38","favourite":true,"eventidsuffix":"9da05913-84b1-4a74-89ed-5b6ec6389cce","filters":[{"code":"searchfacettype_color_level_2","value":"Blue"},{"code":"product_price_range","value":"10.0 - 20.0"}]}   </pre>    Hadoop File storage based on Year/Month/Day/Hour  -----    <pre>  Check:hdfs://localhost.localdomain:54321/searchevents/2014/05/06/16/searchevents.1399386809864  body is:{"eventid":"e8470a00-c869-4a90-89f2-f550522f8f52-1399386809212-72","hostedmachinename":"192.168.182.1334","pageurl":"http://blahblah:/0","customerid":72,"sessionid":"7871a55c-a950-4394-bf5f-d2179a553575","querystring":null,"sortorder":"desc","pagenumber":0,"totalhits":8,"hitsshown":44,"createdtimestampinmillis":1399386809212,"clickeddocid":"23","favourite":null,"eventidsuffix":"e8470a00-c869-4a90-89f2-f550522f8f52","filters":[{"code":"searchfacettype_brand_level_2","value":"Apple"},{"code":"searchfacettype_color_level_2","value":"Blue"}]}  body is:{"eventid":"2a4c1e1b-d2c9-4fe2-b38d-9b7d32feb4e0-1399386809743-61","hostedmachinename":"192.168.182.1330","pageurl":"http://blahblah:/0","customerid":61,"sessionid":"78286f6d-cc1e-489c-85ce-a7de8419d628","querystring":"queryString59","sortorder":"asc","pagenumber":3,"totalhits":32,"hitsshown":9,"createdtimestampinmillis":1399386809743,"clickeddocid":null,"favourite":null,"eventidsuffix":"2a4c1e1b-d2c9-4fe2-b38d-9b7d32feb4e0","filters":[{"code":"searchfacettype_age_level_2","value":"0-12 years"}]}  </pre>    ElasticSearch Recently Viewed items by customers  -----    <pre>  {timestamp=1399386809743, body={pageurl=http://blahblah:/0, querystring=queryString59, pagenumber=3, hitsshown=9, hostedmachinename=192.168.182.1330, createdtimestampinmillis=1399386809743, sessionid=78286f6d-cc1e-489c-85ce-a7de8419d628, eventid=2a4c1e1b-d2c9-4fe2-b38d-9b7d32feb4e0-1399386809743-61, totalhits=32, clickeddocid=null, customerid=61, sortorder=asc, favourite=null, eventidsuffix=2a4c1e1b-d2c9-4fe2-b38d-9b7d32feb4e0, filters=[{value=0-12 years, code=searchfacettype_age_level_2}]}, eventId=2a4c1e1b-d2c9-4fe2-b38d-9b7d32feb4e0}  {timestamp=1399386809757, body={pageurl=http://blahblah:/1, querystring=null, pagenumber=1, hitsshown=34, hostedmachinename=192.168.182.1330, createdtimestampinmillis=1399386809757, sessionid=e6a3fd51-fe07-4e21-8574-ce5ab8bfbd68, eventid=fe5279b7-0bce-4e2b-ad15-8b94107aa792-1399386809757-134, totalhits=9, clickeddocid=22, customerid=134, sortorder=desc, favourite=null, eventidsuffix=fe5279b7-0bce-4e2b-ad15-8b94107aa792, filters=[{value=Blue, code=searchfacettype_color_level_2}]}, State=VIEWED, eventId=fe5279b7-0bce-4e2b-ad15-8b94107aa792}  {timestamp=1399386809765, body={pageurl=http://blahblah:/0, querystring=null, pagenumber=4, hitsshown=2, hostedmachinename=192.168.182.1331, createdtimestampinmillis=1399386809765, sessionid=29864de8-5708-40ab-a78b-4fae55698b01, eventid=886e9a28-4c8c-4e8c-a866-e86f685ecc54-1399386809765-317, totalhits=2, clickeddocid=null, customerid=317, sortorder=asc, favourite=null, eventidsuffix=886e9a28-4c8c-4e8c-a866-e86f685ecc54, filters=[{value=0-12 years, code=searchfacettype_age_level_2}, {value=0.0 - 10.0, code=product_price_range}]}, eventId=886e9a28-4c8c-4e8c-a866-e86f685ecc54}  {timestamp=1399386809782, body={pageurl=http://blahblah:/4, querystring=queryString48, pagenumber=0, hitsshown=19, hostedmachinename=192.168.182.1333, createdtimestampinmillis=1399386809782, sessionid=7bf042ea-526a-4633-84cd-55e0984ea2cb, eventid=648b5cf7-7ca9-4664-915d-23b0d45facc4-1399386809782-298, totalhits=29, clickeddocid=9, customerid=298, sortorder=desc, favourite=null, eventidsuffix=648b5cf7-7ca9-4664-915d-23b0d45facc4, filters=[{value=Green, code=searchfacettype_color_level_2}]}, State=VIEWED, eventId=648b5cf7-7ca9-4664-915d-23b0d45facc4}  {timestamp=1399386809805, body={pageurl=http://blahblah:/5, querystring=null, pagenumber=3, hitsshown=7, hostedmachinename=192.168.182.1330, createdtimestampinmillis=1399386809805, sessionid=648a011d-570e-48ef-bccc-84129c9fa400, eventid=629e9b5f-ff4a-4168-8664-6c8df8214aa7-1399386809805-24, totalhits=28, clickeddocid=41, customerid=24, sortorder=desc, favourite=null, eventidsuffix=629e9b5f-ff4a-4168-8664-6c8df8214aa7, filters=[{value=Blue, code=searchfacettype_color_level_2}, {value=12-18 years, code=searchfacettype_age_level_2}]}, State=VIEWED, eventId=629e9b5f-ff4a-4168-8664-6c8df8214aa7}  </pre>    Hive Parition information  -----  External table search_clicks pointing to above hdfs data location.     <pre>  par: search_clicks  par: 1399386825  par: hdfs://localhost.localdomain:54321/searchevents/2014/05/06/16  par: 4  par: [2014, 05, 06, 16]  </pre>    ElasticSearch Customer Top queries information  -----    <pre>  {id=61_queryString59, querystring=queryString59, querycount=1, customerid=61}  {id=298_queryString48, querystring=queryString48, querycount=1, customerid=298}  {id=440_queryString16, querystring=queryString16, querycount=1, customerid=440}  {id=47_queryString85, querystring=queryString85, querycount=1, customerid=47}  </pre>    Oozie  -----  Coordinator jobs runs hourly to create hive partitions based on hadoop data.  Bundle job to query top query strings and index to elasticsearch on daily basis.  LocalOozie is used to start oozier server for testing purpose.    Spring Data Hadoop  -----  Spring data is used for hive server management. The bean and context loading support to manage dependent start/shutdown of different servers/services.    Spark Streaming  -----  Spark streaming integrated with Flume events to deliver top search queries in last an hour or top viewed products in last an hour.  <pre>  Top 10 entries are: 321  (68,queryString81)  (67,queryString33)  (66,queryString17)  (64,queryString78)  (62,queryString65)  (61,queryString9)  (60,queryString37)  (59,queryString12)  (59,queryString25)  (59,queryString85)  </pre>    Hbase  -----  MiniHbaseCluster setup to store data. Spring data to use hbase client. Integration wih Flume agent to directly store data in hbase using HbaseSink. HbaseJsonSerializer to serialize the JSON data.  Schema Design,  <pre>  {  "client:eventid" => "24-1399386809805-629e9b5f-ff4a-4168-8664-6c8df8214aa7",  "client:eventidsuffix" => "629e9b5f-ff4a-4168-8664-6c8df8214aa7",  "client:hostedmachinename" => "192.168.182.1330",  "client:pageurl" => "http://blahblah:/5",  "client:createdtimestampinmillis" => 1399386809805,  "client:cutomerid" => 24,  "client:sessionid" => "648a011d-570e-48ef-bccc-84129c9fa400",   "search:querystring" => null,  "search:sortorder" => desc,  "search:pagenumber" => 3,  "search:totalhits" => 28,  "search:hitsshown" => 7,  "search:clickeddocid" => "41",  "search:favourite" => null,  "filters:searchfacettype_color_level_2" => "Blue",  "filters:searchfacettype_age_level_2" => "12-18 years"  }  </pre>  Hbase functionality,  <pre>  ->Get total calls in last an hour from client column family  ->Get top 10 search query string in last an hour from search column family  ->Get top 10 clicked facet filters in last an hour from filters column family  ->Get recent search query string by a customer in last 30 days from search column family  ->Dynamic column generation for facet filters to accommodate new filters.   </pre>      Blog Posts  -----  Check below blog posts for details how each functionality is used,  - [Customer product search clicks analytics using big data](http://jaibeermalik.wordpress.com/2014/05/14/customer-product-search-clicks-analytics-using-big-data/)  - [Flume: Gathering customer product search clicks data using Apache Flume](http://jaibeermalik.wordpress.com/2014/05/19/flume-gathering-customer-product-search-clicks-data-using-apache-flume/)  - [Hive: Query customer top search query and product views count using Apache Hive](http://jaibeermalik.wordpress.com/2014/05/20/hive-query-customer-top-search-query-and-product-views-count-using-apache-hive/)  - [ElasticSearch-Hadoop: Indexing product views count and customer top search query from Hadoop to ElasticSearch](http://jaibeermalik.wordpress.com/2014/05/22/elasticsearch-hadoop-indexing-product-views-count-and-customer-top-search-query-from-hadoop-to-elasticsearch/)  - [Oozie: Scheduling Coordinator/Bundle jobs for Hive partitioning and ElasticSearch indexing](http://jaibeermalik.wordpress.com/2014/05/28/oozie-scheduling-coordinatorbundle-jobs-for-hive-partitioning-and-elasticsearch-indexing/)    [Jaibeer Malik](http://jaibeermalik.wordpress.com)
hopshadoop/hops	# Hops Hadoop Distribution  [![Join the chat at https://gitter.im/hopshadoop/hops](https://badges.gitter.im/hopshadoop/services.png)](https://gitter.im/hopshadoop/hops) [![Google Group](https://img.shields.io/badge/google-group-blue.svg)](https://groups.google.com/forum/#!forum/hopshadoop)  <a href=""><img src="http://www.hops.io/sites/default/files/hops-50x50.png" align="left" hspace="10" vspace="6"></a>  **Hops** (<b>H</b>adoop <b>O</b>pen <b>P</b>latform-as-a-<b>S</b>ervice) is a next generation distribution of [Apache Hadoop](http://hadoop.apache.org/core/) with scalable, highly available, customizable metadata. Hops consists internally of two main sub projects, HopsFs and HopsYarn. <b>HopsFS</b> is a new implementation of the the Hadoop Filesystem (HDFS), that supports multiple stateless NameNodes, where the metadata is stored in [MySQL Cluster](https://www.mysql.com/products/cluster/), an in-memory distributed database. HopsFS enables more scalable clusters than Apache HDFS (up to ten times larger clusters), and enables NameNode metadata to be both customized and analyzed, because it can now be easily accessed via a SQL API. <b>HopsYARN</b> introduces a distributed stateless Resource Manager, whose state is migrated to MySQL Cluster. This enables our YARN architecture to have no down-time, with failover of a ResourceManager happening in a few seconds. Together, HopsFS and HopsYARN enable Hadoop clusters to scale to larger volumes and higher throughput.   # Online Documentation You can find the latest Hops documentation, including a programming guide, on the project [web page](http://www.hops.io). This README file only contains basic setup and compilation instructions.   # How to Build   #### Software Required For compiling the Hops Hadoop Distribution you will need the following software. - Java 1.7 or higher - Maven - cmake for compiling the native code  - [Google Protocol Buffer](https://github.com/google/protobuf) Version [2.5](https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz) - [MySQL Cluster NDB](https://dev.mysql.com/downloads/cluster/) native client library   We combine Apache and GPL licensed code, from Hops and MySQL Cluster, respectively, by providing a DAL API (similar to JDBC). We dynamically link our DAL implementation for MySQL Cluster with the Hops code. Both binaries are distributed separately.  Perform the following steps in the following order to compile the Hops Hadoop Distribution.  #### Database Abstraction Layer ```sh git clone https://github.com/hopshadoop/hops-metadata-dal ``` The `master` branch represents most stable version of Hops Hadoop and the `develop` branch contains all the newly developed features and bug fixes. If you choose to use `master` branch then also checkout the `master` branch in the other Hops Projects and vice versa.     ```sh git checkout master mvn clean install -DskipTests ```  #### Database Abstraction Layer Implementation ```sh git clone https://github.com/hopshadoop/hops-metadata-dal-impl-ndb git checkout master mvn clean install -DskipTests ``` This project also contains c++ code that requires NDB `libndbclient.so` library. Download the [MySQL Cluster Distribution](https://dev.mysql.com/downloads/cluster/) and extract the `libndbclient.so` library. Alternatively you can download a custom MySQL Cluster library from our servers. Our custom library supports binding I/O threads to CPU cores for better performance.		 ```sh cd tmp wget http://kompics.sics.se/maven/repository/com/mysql/ndb/clusterj-native/7.5.4/clusterj-native-7.5.4-natives-linux.jar unzip clusterj-native-7.5.4-natives-linux.jar cp libndbclient.so /usr/lib ```  See this [section](#connecting-the-driver-to-the-database) for specifying the database `URI` and `username/password`.     #### Building Hops Hadoop  ```sh git clone https://github.com/hopshadoop/hops git checkout master ``` ##### Building a Distribution ```sh mvn package -Pdist,native,docs -DskipTests -Dtar ```  ##### Compiling and Running Unit Tests Locally ```sh mvn clean generate-sources install -Pndb -DskipTests mvn test-compile ``` For running tests use the `ndb` profile to add the the [database access layer](https://github.com/hopshadoop/hops-metadata-dal-impl-ndb) driver to the class path. The driver is loaded at run time. For example,  ```sh mvn test -Dtest=TestFileCreation -Pndb ```  Set the `ndb` profile in the IDE if you are running the unit test in an IDE. For example, in IntelliJ you can do this by  ``` View --> Tool Windows --> Maven Projects Expand the "Profiles" Check the "ndb" profile ```  #### Connecting the Driver to the Database There are two way to configure the NDB data access layer driver  - **Hard Coding The Database Configuration Parameters: ** While compiling the database access layer all the required configuration parameters can be written to the ```./hops-metadata-dal-impl-ndb/src/main/resources/ndb-config.properties``` file. When the diver is loaded it will try to connect to the database specified in the configuration file.   - **hdfs-site.xml: ** Add `dfs.storage.driver.configfile` parameter to hdfs-site.xml to read the configuration file from a sepcified path. For example, to read the configuration file in the current directory add the following the hdfs-site.xml ```xml <property>       <name>dfs.storage.driver.configfile</name>       <value>hops-ndb-config.properties</value> </property>   ```  # Hops Auto Installer The Hops stack includes a number of services also requires a number of third-party distributed services: <ul> <li>Java 1.7 (OpenJDK or Oracle JRE/JDK)</li> <li>NDB 7.4+ (MySQL Cluster)</li> <li>J2EE7 web application server (default: Glassfish)</li> <li>ElasticSearch 1.7+</li> </ul>  Due to the complexity of installing and configuring all Hops’ services, we recommend installing Hops using the automated installer [Karamel/Chef](http://www.karamel.io). Detailed documentation on the steps for installing and configuring all services in Hops is not discussed here. Instead, Chef cookbooks contain all the installation and configuration steps needed to install and configure Hops. The Chef cookbooks are available at: https://github.com/hopshadoop.  #### Installing on Cloud Platforms (AWS, GCE, OPenStack) 1. Download and install Karamel (http://www.karamel.io). 2. Run Karamel. 3. Click on the “Load Cluster Definition” menu item in Karamel. You are now prompted to select a cluster definition YAML file. Go to the examples/stable directory, and select a cluster definition file for your target cloud platform for one of the following cluster types:     * Amazon Web Services EC2 (AWS)     * Google Compute Engine (GCE)     * OpenStack     * On-premises (bare metal)  For more information on how to configure cloud-based installations,  go to help documentation at http://www.karamel.io. For on-premises installations, we provide some additional installation details and tips later in this section.  #### On-Premises (baremetal) Installation For on-premises (bare-metal) installations, you will need to prepare for installation by:  1. Identifying a master host, from which you will run Karamel;     * the master must have a display for Karamel’s user interface;     * the master must be able to ping (and connect using ssh) to all of the target hosts.    2. Identifying a set of target hosts, on which the Hops software and 3rd party services will be installed.   * the target nodes should have http access to the open Internet to be able to download software during the installation process. (Cookbooks can be configured to download software from within the private network, but this requires a good bit of configuration work for Chef attributes, changing all download URLs).  The master must be able to connect using SSH to all the target nodes, on which the software will be installed. If you have not already copied the master’s public key to the .ssh/authorized_keys file of all target hosts, you can do so by preparing the machines as follows:  1. Create an openssh public/private key pair on the master host for your user account. On Linux, you can use the ssh-keygen utility program to generate the keys, which will by default be stored in the $HOME/.ssh/id_rsa and $HOME/.ssh/id_rsa.pub files. If you decided to enter a password for the ssh keypair, you will need to enter it again in Karamel when you reach the ssh dialog, part of Karamel’s Launch step. We recommend no password (passwordless) for the ssh keypair. 2. Create a user account USER on the all the target machines with full sudo privileges (root privileges) and the same password on all target machines. 3. Copy the $HOME/.ssh/id_rsa.pub file on the master to the /tmp folder of all the target hosts. A good way to do this is to use pscp utility along with a file ( hosts.txt ) containing the line-separated hostnames (or IP addresss) for all the target machines. You may need to install the pssh utility programs ( pssh ), first.  ``` sudo apt-get install pssh or yum install pssh vim hosts.txt  # Enter the row-separated IP addresses of all target nodes in hosts.txt  128.112.152.122 18.31.0.190 128.232.103.201 .....  pscp -h hosts.txt -P PASSWORD -i USER ~/.ssh/id_rsa.pub /tmp pssh -h hosts.txt -i USER -P PASSWORD mkdir -p /home/USER/.ssh pssh -h hosts.txt -i USER -P PASSWORD cat /tmp/id_rsa.pub >> /home/USER/.ssh/authorized_keys ```  Update your Karamel cluster definition file to include the IP addresses of the target machines and the USER account name. After you have clicked on the launch menu item, you will come to a ssh dialog. On the ssh dialog, you need to open the advanced section. Here, you will need to enter the password for the USER account on the target machines ( sudo password text input box). If your ssh keypair is password protected, you will also need to enter it again here in the keypair password text input box.   #### Vagrant (Virtualbox) You can install Hops on your laptop/desktop with Vagrant. You will need to have the following software packages installed:  * chef-dk, version >0.5+ (but not >0.8+) * git * vagrant * vagrant omnibus plugin * virtualbox  You can now run vagrant, using:  ``` sudo apt-get install virtualbox vagrant vagrant plugin install vagrant-omnibus git clone https://github.com/hopshadoop/hopsworks-chef.git cd hopsworks-chef berks vendor cookbooks vagrant up ```  # Export Control  This distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software. BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted. See <http://www.wassenaar.org/> for more information.   The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms. The form and manner of this Apache Software Foundation distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.  The following provides more details on the included cryptographic software: Hadoop Core uses the SSL libraries from the Jetty project written by mortbay.org.  #Contact   <ul> <li><a href="https://twitter.com/hopshadoop">Follow our Twitter account.</a></li> <li><a href="https://gitter.im/hopshadoop/hops">Chat with Hops developers in Gitter.</a></li> <li><a href="https://groups.google.com/forum/#!forum/hopshadoop">Join our developer mailing list.</a></li> <li><a href="https://cloud17.sics.se/jenkins/view/develop/">Checkout the current build status.</a></li> </ul>  # License  Hops is released under an [Apache 2.0 license](LICENSE.txt).
HadoopGenomics/Hadoop-BAM	Hadoop-BAM ==============  Hadoop-BAM: a library for the manipulation of files in common bioinformatics formats using the Hadoop MapReduce framework.  Build status ------------ [![Build Status](https://travis-ci.org/HadoopGenomics/Hadoop-BAM.svg?branch=master)](https://travis-ci.org/HadoopGenomics/Hadoop-BAM) [![Coverage Status](https://coveralls.io/repos/github/HadoopGenomics/Hadoop-BAM/badge.svg?branch=master)](https://coveralls.io/github/HadoopGenomics/Hadoop-BAM?branch=master)   The [file formats](http://samtools.github.io/hts-specs/) currently supported are:     - BAM (Binary Alignment/Map)    - SAM (Sequence Alignment/Map)    - CRAM    - FASTQ    - FASTA (input only)    - QSEQ    - VCF (Variant Call Format)    - BCF (Binary VCF) (output is always BGZF-compressed)  For a longer high-level description of Hadoop-BAM, refer to the article "Hadoop-BAM: directly manipulating next generation sequencing data in the cloud" in Bioinformatics Volume 28 Issue 6 pp. 876-877, also available online at: http://dx.doi.org/10.1093/bioinformatics/bts054  If you are interested in using Apache Pig (http://pig.apache.org/) with Hadoop-BAM, refer to SeqPig at: http://seqpig.sourceforge.net/  Note that the library part of Hadoop-BAM is primarily intended for developers with experience in using Hadoop. SeqPig is a more versatile and higher-level interface to the file formats supported by Hadoop-BAM. In addition, [ADAM](http://bdgenomics.org/) and [GATK version 4](https://github.com/broadinstitute/gatk) both use Hadoop-BAM and offer  high-level command-line bioinformatics tools that run on Spark clusters.   For examples of how to use Hadoop-BAM as a library to read data files in Hadoop see the `examples/` directory.  ------------ Dependencies ------------  Hadoop. Tested with 2.2.0 and later. Version 4.2.0 of Cloudera's distribution, CDH, has also been tested. Use other versions at your own risk. You can change the version of Hadoop linked against by modifying the corresponding parameter in the pom.xml build file.  HTSJDK (formerly Picard SAM-JDK) Version 2.3.0 is required. Later versions may also work but have not been tested.  Availability:     - Hadoop              http://hadoop.apache.org/    - HTSJDK/Picard       https://github.com/samtools/htsjdk  Note that starting from version 7.4.0 Hadoop-BAM requires Java 8.  ------------ Installation ------------  If you're using Hadoop 2.2.0, a precompiled "hadoop-bam-X.Y.Z.jar" is available that you can use.  Otherwise, you'll have to build Hadoop-BAM yourself by by using Maven (version 3.0.4 at least) and following the instructions below.  ### Hadoop version  You must set the <hadoop.version> tag in pom.xml appropriately for the version of hadoop you're using.  Run "hadoop version" and copy the string from the output:      [luca@vm Hadoop-BAM]# hadoop version 2>/dev/null | grep Hadoop     Hadoop 2.0.0-cdh4.6.0  In the output above, the version string is "2.0.0-cdh4.6.0".  ### Build  Build Hadoop-BAM with the following command:      mvn clean package -DskipTests     It will create two files:      target/hadoop-bam-X.Y.Z-SNAPSHOT.jar     target/hadoop-bam-X.Y.Z-SNAPSHOT-jar-with-dependencies.jar  The former contains only Hadoop-BAM whereas the latter one also contains all dependencies and can be run directly via      hadoop jar target/hadoop-bam-X.Y.Z-SNAPSHOT-jar-with-dependencies.jar  Javadoc documentation is generated automatically and can then be found in the `target/apidocs` subdirectory.  Finally, unit tests can be run via:      mvn test  ------------- Library usage -------------  Hadoop-BAM provides the standard set of Hadoop file format classes for the file formats it supports: a FileInputFormat and one or more RecordReaders for input, and a FileOutputFormat and one or more RecordWriters for output. These are summarized  in the table below.  |File Format|InputFormat|OutputFormat| |-----------|-----------|------------| |BAM|`BAMInputFormat`|`KeyIgnoringBAMOutputFormat`| |SAM|`SAMInputFormat`|`KeyIgnoringAnySAMOutputFormat`| |CRAM|`CRAMInputFormat`|`KeyIgnoringCRAMOutputFormat`| |BAM, SAM, or CRAM|`AnySAMInputFormat`|`KeyIgnoringAnySAMOutputFormat`| |FASTQ|`FastqInputFormat`|`FastqOutputFormat`| |FASTA|`FastaInputFormat`|N/A| |QSEQ|`QseqInputFormat`|`QseqOutputFormat`| |VCF or BCF|`VCFInputFormat`|`KeyIgnoringVCFOutputFormat`|  `AnySAMInputFormat` detects the format (BAM, SAM, or CRAM) by file extension, then by looking at the first few bytes of the file (if file extension detection is disabled or  is inconclusive). `VCFInputFormat` works in a similar way for VCF and BCF files.   The output formats all discard the key and only use the value field when writing the  output file. Some of the output formats indicate this explictly through the  `KeyIgnoring` prefix in their name, but `FastqOutputFormat` and `QseqOutputFormat`  actually ignore the key too.  The abstract base classes `BAMOutputFormat`, `CRAMOutputFormat`, `AnySAMOutputFormat`,  and `VCFOutputFormat` cannot be used directly, but can be subclassed in order to add  custom logic (to provide `ValueIgnoring` versions, for example).   When using `KeyIgnoringAnySAMOutputFormat`, the format of the files written (BAM, SAM, or CRAM)  must be specified by setting the property `hadoopbam.anysam.output-format`. Similarly, set the property `hadoopbam.vcf.output-format` in order to specify which file format `KeyIgnoringVCFOutputFormat` will use (VCF or BCF).  The properties that can be set on input and output formats are summarized in the table  below:  |Format|Property|Default|Description| |------|--------|-------|-----------| |`AnySAMInputFormat`|`hadoopbam.anysam.trust-exts`|`true`|Whether to detect the file format (BAM, SAM, or CRAM) by file extension. If `false`, use the file contents to detect the format.| |`KeyIgnoringAnySAMOutputFormat`|`hadoopbam.anysam.output-format`| |(Required.) The file format to use when writing BAM, SAM, or CRAM files. Should be one of `BAM`, `SAM`, or `CRAM`.| | |`hadoopbam.anysam.write-header`|`true`|Whether to write the SAM header in each output file part. If `true`, call `setSAMHeader()` or `readSAMHeaderFrom()` to set the desired header.| |`BAMInputFormat`|`hadoopbam.bam.bounded-traversal`|`false`|If `true`, only include reads that match the intervals specified in `hadoopbam.bam.intervals`, and unplaced unmapped reads, if `hadoopbam.bam.traverse-unplaced-unmapped` is set to `true`.| | |`hadoopbam.bam.intervals`| |Only include reads that match the specified intervals. BAM files must be indexed in this case. Intervals are comma-separated and follow the same syntax as the `-L` option in SAMtools. E.g. `chr1:1-20000,chr2:12000-20000`. When using this option `hadoopbam.bam.bounded-traversal` should be set to `true`.| | |`hadoopbam.bam.traverse-unplaced-unmapped`|`false`|If `true`, include unplaced unmapped reads (that is, unmapped reads with no position) When using this option `hadoopbam.bam.bounded-traversal` should be set to `true`.| |`KeyIgnoringBAMOutputFormat`|`hadoopbam.bam.write-splitting-bai`|`false`|If `true`, write _.splitting-bai_ files for every BAM file.| |`CRAMInputFormat`|`hadoopbam.cram.reference-source-path`| |(Required.) The path to the reference. May be an `hdfs://` path.| |`FastqInputFormat`|`hbam.fastq-input.base-quality-encoding`|`sanger`|The encoding used for base qualities. One of `sanger` or `illumina`.| | |`hbam.fastq-input.filter-failed-qc`|`false`|If `true`, filter out reads that didn't pass quality checks.| |`QseqInputFormat`|`hbam.qseq-input.base-quality-encoding`|`illumina`|The encoding used for base qualities. One of `sanger` or `illumina`.| | |`hbam.qseq-input.filter-failed-qc`|`false`|If `true`, filter out reads that didn't pass quality checks.| |`VCFInputFormat`|`hadoopbam.vcf.trust-exts`|`false`|Whether to detect the file format (VCF or BCF) by file extension. If `false`, use the file contents to detect the format.| |`KeyIgnoringVCFOutputFormat`|`hadoopbam.vcf.output-format`| |(Required.) The file format to use when writing VCF or BCF files. Should be one of `VCF` or `BCF`.| | |`hadoopbam.vcf.write-header`|`true`|Whether to write the VCF header in each output file part. If `true`, call `setHeader()` or `readHeaderFrom()` to set the desired header.|  Note that Hadoop-BAM is based around the newer Hadoop API introduced in the 0.20 Hadoop releases instead of the older, deprecated API.  For examples of how to link to Hadoop-BAM in your own Maven project see the `examples/` folder. There are examples for reading and writing BAM as well as VCF files.  When using Hadoop-BAM as a library in your program, remember to have hadoop-bam-X.Y.Z.jar as well as the HTSJDK .jars (including the Commons JEXL .jar) in your CLASSPATH and HADOOP_CLASSPATH; alternatively, use the *-jar-with-dependencies.jar which contains already all dependencies.  Linking against Hadoop-BAM --------------------------  If your Maven project relies on Hadoop-BAM the easiest way to link against it is by relying on the OSS Sonatype repository:  ~~~~~~~ <project> ...     <dependencies>         <dependency>             <groupId>org.seqdoop</groupId>             <artifactId>hadoop-bam</artifactId>             <version>7.9.0</version>         </dependency>         ...     </dependencies>     ... </project> ~~~~~~~  ------------------ Running under Hadoop --------------------  To use Hadoop-BAM under Hadoop, the easiest method is to use the jar that comes packaged with all dependencies via      hadoop jar hadoop-bam-X.Y.Z-jar-with-dependencies.jar  Alternatively, you can use the "-libjars" command line argument when running Hadoop-BAM to provide different versions of dependencies as follows:      hadoop jar hadoop-bam-X.Y.Z.jar \       -libjars htsjdk-2.3.0.jar,commons-jexl-2.1.1.jar  Finally, all jar files can also be added to HADOOP_CLASSPATH in the Hadoop configuration's hadoop-env.sh.  File paths under Hadoop -----------------------  When running under Hadoop, keep in mind that file paths refer to the distributed file system, HDFS. To explicitly access a local file, instead of using the plain path such as "/foo/bar", you must use a file: URI, such as "file:/foo/bar". Note that paths in file: URIs must be absolute.
impetus-opensource/jumbune	jumbune [![Build Status](https://travis-ci.org/impetus-opensource/jumbune.svg?branch=master)](https://travis-ci.org/impetus-opensource/jumbune) =======  ## Synopsis  Jumbune is an open-source product built for analyzing Hadoop cluster and MapReduce jobs. It provides development & administrative insights of Hadoop based analytical solutions. It enables user to Debug, Profile, Monitor & Validate analytical solutions hosted on decoupled clusters.  ## Website http://jumbune.org  ## Issue Tracker http://jumbune.org/jira/browse/JUM  ## Building  Jumbune can be build with maven  - To build for a Yarn Hadoop cluster   `mvn clean install -P yarn`  - To build for a non Yarn Hadoop cluster   `mvn clean install`  ## Installation  _Detailed installation guide can be found at http://bit.ly/1kBG4Qo_  _Deployment Planning guide at http://bit.ly/1oiXGk2_  - Deploying Jumbune  `java -jar <location of the distribution build jar>`  - Deploying Jumbune (in verbose mode)  `java -jar <location of the distribution build jar> -verbose`  - Running Jumbune Agent  `java -jar <jumbune agent jar> <port> <|verbose>`  - Running Jumbune  `./startWeb`  or  `./runCli`  ## Docker Image (Jumbune + Apache YARN )  - Building from root of the checked out repository `$ sudo docker build –t = "jumbune/pseudo-distributed:1.5.0" .`  - Building from the github.com repository `$ sudo docker build –t = "jumbune/pseudo-distributed:1.5.0" github.com/impetus-opensource/jumbune/`  - Getting the automated build from docker registry `https://registry.hub.docker.com/u/jumbune/jumbune/`  - Running the built image `$ docker run -d --name="jumbune" -h "jumbune-docker" -p 8080:8080 -p 5555 jumbune/pseudo-distributed:1.5.0`  ## Code Examples  Code examples are packages inside the distribution,  - For Flow analyzer: BankDefaulters, ClickStreamAnalysis, USRegionPortouts - For Profilng: MovieRating - For Data Validation - script  ## Documentation - Quick Start Guide: http://bit.ly/1mY9qWe - Installation Guide: http://bit.ly/1kBG4Qo - Release Notes: http://bit.ly/TkWSj8 - Architecture Guide: http://bit.ly/UgJayB - Deployment Planning Guide: http://bit.ly/1oiXGk2 - Administration Guide: http://bit.ly/1uq5LVV - Troubleshooting Guide: http://bit.ly/1pXEYOd - Getting Involved Guide: http://bit.ly/1i7XjDn  ## License  Jumbune is licensed under LGPLv3 license
jpatanooga/KnittingBoar	KnittingBoar ============  Parallel Iterative Algorithm (SGD) on Hadoop's YARN framework  * Built on top of BSP-style computation framework "Iterative Reduce" (Hadoop / YARN) * Uses Mahout's implementation of Stochastic Gradient Descent (SGD) as basis for worker process  Slides From Hadoop World 2012:  http://www.cloudera.com/content/cloudera/en/resources/library/hadoopworld/strata-hadoop-world-2012-knitting-boar_slide_deck.html  Knitting Boar is an experimental machine learning application which parallelizes Mahout's Stochastic Gradient Descent on top of a new YARN based framework for Hadoop called [Iterative Reduce](https://github.com/emsixteeen/IterativeReduce)  * [Intro to Knitting Boar](https://github.com/jpatanooga/KnittingBoar/wiki/Intro-to-Knitting-Boar) * [Quick Start] (https://github.com/jpatanooga/KnittingBoar/wiki/Quick-Start) * [Frequently Asked Questions] (https://github.com/jpatanooga/KnittingBoar/wiki/FAQ) * [Command Line Usage] (https://github.com/jpatanooga/KnittingBoar/wiki/Command-Line-Usage) * [Knitting Boar Internals] (https://github.com/jpatanooga/KnittingBoar/wiki/Knitting-Boar-Internals) * [Iterative Reduce] (https://github.com/jpatanooga/KnittingBoar/wiki/Iterative-Reduce) * [Outstanding Issues] (https://github.com/jpatanooga/KnittingBoar/wiki/Outstanding-Issues) * [Parallel SGD Resources] (https://github.com/jpatanooga/KnittingBoar/wiki/Parallel-SGD-Resources)
confluentinc/camus	# Intro Camus is LinkedIn's [Kafka](http://kafka.apache.org "Kafka")->HDFS pipeline. It is a mapreduce job that does distributed data loads out of Kafka. It includes the following features:  * Automatic discovery of topics * Avro schema management / In progress * Date partitioning  It is used at LinkedIn where it processes tens of billions of messages per day. You can get a basic overview from this paper: [Building LinkedIn’s Real-time Activity Data Pipeline](http://sites.computer.org/debull/A12june/pipeline.pdf "Building LinkedIn’s Real-time Activity Data Pipeline").  There is a [Google Groups mailing list](https://groups.google.com/forum/#!forum/camus_etl "Google Groups mailing list") that you can email or search if you have any questions.  For a more detailed documentation on the main Camus components, please see [Camus InputFormat and OutputFormat Behavior](https://github.com/linkedin/camus/wiki/Camus-InputFormat-and-OutputFormat-Behavior "Camus InputFormat and OutputFormat Behavior") # Brief Overview All work is done within a single Hadoop job divided into three stages:  1. Setup stage fetches available topics and partitions from Zookeeper and the latest offsets from the Kafka Nodes.  1. Hadoop job stage allocates topic pulls among a set number of tasks.  Each task does the following:     *  Fetch events from Kafka server and collect count statistics.     *  Move data files to corresponding directories based on timestamps of events.     *  Produce count  events and write to HDFS.  * TODO: Determine status of open sourcing Kafka Audit.     *  Store updated offsets in HDFS.  1. Cleanup stage reads counts from all tasks, aggregates the values, and submits the results to Kafka for consumption by Kafka Audit.   ## Setup Stage   1. Setup stage fetches from Zookeeper Kafka broker urls and topics (in /brokers/id and /brokers/topics).  This data is transient and will be gone once Kafka server is down.  1. Topic offsets stored in HDFS.  Camus maintains its own status by storing offset for each topic in HDFS. This data is persistent.  1. Setup stage allocates all topics and partitions among a fixed number of tasks.  ## Hadoop Stage   ### 1. Pulling the Data   Each hadoop task uses a list of topic partitions with offsets generated by setup stage as input. It uses them to initialize Kafka requests and fetch events from Kafka brokers. Each task generates four types of outputs (by using a custom MultipleOutputFormat): Avro data files, Count statistics files, Updated offset files, and Error files.   * Note, each task generates an error file even if no errors were encountered.  If no errors occurred, the file is empty.  ### 2. Committing the data   Once a task has successfully completed, all topics pulled are committed to their final output directories. If a task doesn't complete successfully, then none of the output is committed.  This allows the hadoop job to use speculative execution.  Speculative execution happens when a task appears to be running slowly.  In that case the job tracker then schedules the task on a different node and runs both the main task and the speculative task in parallel.  Once one of the tasks completes, the other task is killed.  This prevents a single overloaded hadoop node from slowing down the entire ETL.  ### 3. Producing Audit Counts   Successful tasks also write audit counts to HDFS.   ### 4. Storing the Offsets   Final offsets are written to HDFS and consumed by the subsequent job.  ## Job Cleanup   Once the hadoop job has completed, the main client reads all the written audit counts and aggregates them.  The aggregated results are then submitted to Kafka.  ## Setting up Camus  ### Building the project  You can build Camus with: ``` mvn clean package ```  Note that there are two jars that are not currently in a public Maven repo. These jars (kafka-0.7.2 and avro-schema-repository-1.74-SNAPSHOT) are supplied in the lib directory, and maven will automatically install them into your local Maven cache (usually ~/.m2).  ### First, Create a Custom Kafka Message to Avro Record Decoder  We hope to eventually create a more out of the box solution, but until we get there you will need to create a custom decoder for handling Kafka messages.  You can do this by implementing the abstract class com.linkedin.batch.etl.kafka.coders.KafkaMessageDecoder.  Internally, we use a schema registry that enables obtaining an Avro schema using an identifier included in the Kafka byte payload. For more information on other options, you can email camus_etl@googlegroups.com.  Once you have created a decoder, you will need to specify that decoder in the properties as described below.  You can also start by taking a look at the existing Decoders to see if they will work for you, or as examples if you need to implement a new one.    ### Decoding JSON messages  Camus can also process JSON messages. Set "camus.message.decoder.class=com.linkedin.camus.etl.kafka.coders.JsonStringMessageDecoder" in camus.properties. Additionally, there are two more options "camus.message.timestamp.format" (default value: "[dd/MMM/yyyy:HH:mm:ss Z]") and "camus.message.timestamp.field" (default value: "timestamp").  ### Writing to different formats  By default Camus writes Avro data.  But you can also write to different formats by implementing and RecordWriterProvider.  For examples see https://github.com/linkedin/camus/blob/master/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/AvroRecordWriterProvider.java and https://github.com/linkedin/camus/blob/master/camus-etl-kafka/src/main/java/com/linkedin/camus/etl/kafka/common/StringRecordWriterProvider.java.  You can specify which writer to use with "etl.record.writer.provider.class".  ### Configuration  Camus can be run from the command line as Java App.  You will need to set some properties either by specifying a properties file on the classpath using -p (filename), or an external properties file using -P (filepath to local file system, or to `hdfs:`), or from the command line itself using -D property=value. If the same property is set using more than one of the previously mentioned methods, the order of precedence is command-line, external file, classpath file.  Here is an abbreviated list of commonly used parameters.  An example properties file is also located https://github.com/linkedin/camus/blob/master/camus-example/src/main/resources/camus.properties.  * Top-level data output directory, sub-directories will be dynamically created for each topic pulled     * `etl.destination.path=` * HDFS location where you want to keep execution files, i.e. offsets, error logs, and count files     * `etl.execution.base.path=` * Where completed Camus job output directories are kept, usually a sub-dir in the base.path     * `etl.execution.history.path=` * Filesystem for the above folders. This can be a hdfs:// or s3:// address.     * `fs.default.name=` * List of at least one Kafka broker for Camus to pull metadata from     * `kafka.brokers=`     * All files in this dir will be added to the distributed cache and placed on the classpath for Hadoop tasks     * `hdfs.default.classpath.dir=` * Max hadoop tasks to use, each task can pull multiple topic partitions     * `mapred.map.tasks=30` * Max historical time that will be pulled from each partition based on event timestamp     * `kafka.max.pull.hrs=1` * Events with a timestamp older than this will be discarded.      * `kafka.max.historical.days=3` * Max minutes for each mapper to pull messages     * `kafka.max.pull.minutes.per.task=-1` * Decoder class for Kafka Messages to Avro Records     * `camus.message.decoder.class=` * If whitelist has values, only whitelisted topic are pulled.  Nothing on the blacklist is pulled     * `kafka.blacklist.topics=`     * `kafka.whitelist.topics=` * Class for writing records to HDFS/S3     * `etl.record.writer.provider.class=` * Delimiter for writing string records (default is "\n")     * `etl.output.record.delimiter=`  ### Running Camus  Camus can be run from the command line using hadoop jar.  Here is the usage: ``` usage: hadoop jar camus-example-<version>-SNAPSHOT.jar com.linkedin.camus.etl.kafka.CamusJob  <br/>  -D <property=value>   use value for given property<br/>  -P <arg>              external properties filename<br/>  -p <arg>              properties filename from the classpath<br/> ```
Knewton/KassandraMRHelper	#KassandraMRHelper  [![Build Status](https://travis-ci.org/Knewton/KassandraMRHelper.svg)](https://travis-ci.org/Knewton/KassandraMRHelper) [![Coverage Status](https://coveralls.io/repos/Knewton/KassandraMRHelper/badge.svg?branch=master)](https://coveralls.io/r/Knewton/KassandraMRHelper?branch=master) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.knewton.mapreduce/KassandraMRHelper/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.knewton.mapreduce/KassandraMRHelper/)  ##Short Summary The KassandraMRHelper library provides necessary Record Readers, InputFormats  and Mapper classes to help you with the process of reading data directly from  Cassandra SSTables. By using this library you will avoid the sstable2json step.  This library does not require a live Cassandra cluster.  KassandraMRHelper is compatible with Cassandra versions 2.x.  Relevant [blog post](http://www.knewton.com/tech/blog/2013/11/cassandra-and-hadoop-introducing-the-kassandramrhelper)  ##Building the Project To build the example you can just run  `mvn clean package`  However there's three other maven profiles that may be of interest to you.  The default one is set for EMR but there's a HadoopMapReduce (for running the  example in a regular non EMR Hadoop cluster) and a local option for running it  in eclipse or on a local Hadoop cluster.  Choosing a profile will write a property to knewton-site.xml named  "com.knewton.mapreduce.environment" and you can then access it from the  configuration and using the MREnvironment enum type.  To select a profile you can do:  `mvn clean package -P HadoopMapReduce`  ##Usage You can use com.knewton.mapreduce.cassandra.WriteSampleSSTable in the test  source packages to generate a sample SSTable with student events to use as  input. To run it from the command line you can use:  ```bash java -cp ./KassandraMRHelper-0.1.jar:./KassandraMRHelper-0.1-tests.jar \ 	com.knewton.mapreduce.cassandra.WriteSampleSSTable ```  ```bash usage: WriteSampleSSTable [OPTIONS] <output_dir>  -e,--studentEvents <arg>   The number of student events per student to be                             generated. Default value is 10  -h,--help                  Prints this help message.  -s,--students <arg>        The number of students (rows) to be generated.                             Default value is 100. ``` ##Things To Watch Out For 1.	There's a property in knewton-site.xml named com.knewton.cassandra.backup.compression.  	You should set this to true only if you are reading from a backup SSTable 	location that has extra snappy compression on top of any cassandra compression 	scheme. If you're using Priam, for example, and have enabled compression in  	Cassandra then your tables are probably double compressed. You DO NOT need to set 	this property to true if you are using only the Cassandra compression since the  	library will auto detect that by the presence of the CompressionInfo.db file.  ##Contributing Contributions are always welcome and encouraged!  If you would like to contribute to this project, please contact the current project maintainer, or use the Github pull request feature.  The project maintainer is [Giannis Neokleous](https://github.com/gneokleo)  You can find the style files [here](https://github.com/Knewton/KnewtonStyles)  ###Future Work 1.  Add SSTable RecordWriters and OutputFormats. This will directly write the     SSTables without the need of having a live cluster.     * Create cluster partitioners for partitioning keys based on the         Cassandra ring topology.  ##Author Giannis Neokleous		  www.giann.is  ## License Licensed under the [Apache License 2.0](http://www.apache.org/licenses/LICENSE-2.0.html). See the LICENSE file for more details.
blackberry/KaBoom	# KaBoom - A High Performance Consumer Client for Kafka KaBoom uses Krackle to consume from partitions of topics in Kafka and write them to boom files in HDFS.    ## Features * Uses the [Curator Framework](http://curator.apache.org/) for  [Apache Zookeeper](zookeeper.apache.org) to distribute work amongst multiple servers * Supports writing to secured Hadoop clusters via Kerberos based secure impersonation (conveniently pulled from [Flume](http://flume.apache.org/)) * Recovers from Kafka server failures (even when newly elected leaders weren't in-sync when elected) * Supports consuming with either GZIP or Snappy compression * Configurable: Each topic can be configured with a unique HDFS path template with date/time variable substitution * Supports flagging timestamp template HDFS directories as 'Ready' when all a topic's partition's messages have been written for a given hour  ## Author(s) * [Dave Ariens](<mailto:dariens@blackberry.com>) ([Github](https://github.com/ariens)) * [Matthew Bruce](<mailto:mbruce@blackberry.com>) ([Github](https://github.com/MatthewRBruce))  ## Building Performing a Maven install produces:   * An RPM package that currently installs RPM based Linux distributions * A Debian package for dpkg based Linux distributions  ## Major Changes in 0.8.x This release contains the most significant updates to KaBoom we have introduced in a single version bump.  The most significant changes include the migration of all running configuration parameters and topic configurations to ZooKeeper and the introduction of worker sprints.  The remaining confiugration continues to be read in via a property file and is now referred to as startup configuration.    ## Worker Sprints  Workers are now aligned to sprints lasting `workerSprintDurationSeconds` seconds.  A sprint is associated to a partition ID (topic-partition), and starting Kafka offset.  A KaBoom worker will keep track of the partition IDs latest offest and largest parsed message timestamp for the duration of it's sprint.  Following `fileCloseGraceTimeAfterExpiredMs` milliseconds after the end of a sprint, KaBoom will then close off any open boom files and record the sprint's latest offset and largest observed timestamp to ZooKeeper.    ## Startup versus Running Configurations Startup configuration changes require a KaBoom service restart to be loaded, whereas the running configuration is reloaded by KaBoom as changes are made in ZooKeeper.  Updated running configuration values are then used as they are accessed by KaBoom.  For example you can change the number of HDFS replicas to store for boom files in Hadoop however it will not affect any open or previously closed files only files that are created after the new configuration has been loaded (as replicas are specified only when creating files).  ## Topic Configurations Unlike running configurations which are reloaded instantly topic configuration updates trigger all workers assigned to a partition of the topic to be gracefully shutdown (boom files closed, offsets, and offset timestamps  stored in ZK).  The KaBoom client will then detect and restart any gracefully shutdown workers.  Workers load their topic configuration when they are launched.  ## Example Topic Configuration The topic configurations are stored at `zk://<root>/kaboom/topics/<id>`, as:  ``` {         version: 1,         id: "devtest-test3",         hdfsRootDir: "/service/82/devtest/logs/%y%M%d/%H/devtest-test3",         proxyUser: "dariens",         defaultDirectory: "data",         filterSet: [ ] } ```  Note: The empty filterSet array is reserved for future to-be-implemented  use-cases.  ## Startup Configuration Example startup configuration (property file based):  ``` ###################### # KaBoom Configuration ######################  kaboom.id=1001 hadooop.fs.uri=hdfs://hadoop.company.com #kaboom.weighting=<number> - the default is number of cores kerberos.keytab=/opt/kaboom/config/kaboom.keytab kerberos.principal=kaboom@COMPANY.COM #kaboom.hostname=<name> - the default is the system's hostname zookeeper.connection.string=r3k1.kafka.company.com:2181,r3k2.kafka.company.com:2181,r3k3.kafka.company.com:2181/KaBoomDev kafka.zookeeper.connection.string=r3k1.kafka.company.com:2181,r3k2.kafka.company.com:2181,r3k3.kafka.company.com:2181 #kaboom.load.balancer.type=even - this is the default #kaboom.runningConfig.zkPath=/kaboom/config - this is the default  ######################## # Consumer Configuration  ########################  metadata.broker.list=r3k1.kafka.company.com:9092,r3k2.kafka.company.com:9092,r3k3.kafka.company.com:9092 fetch.message.max.bytes=10485760 fetch.wait.max.ms=5000 #fetch.min.bytes=1 - this is the default socket.receive.buffer.bytes=10485760 auto.offset.reset=smallest #socket.timeout.seconds=30000 - this is the default ```  ## Running Configuration Here is an example running configuration stored at `zk://<root>/kaboom/config`:  ``` {         version: 8,         allowOffsetOverrides: true,         sinkToHighWatermark: true,         useTempOpenFileDirectory: false,         useNativeCompression: false,         readyFlagPrevHoursCheck: 24,         leaderSleepDurationMs: 600001,         compressionLevel: 6,         boomFileBufferSize: 16384,         boomFileReplicas: 3,         boomFileBlocksize: 268435456,         boomFileTmpPrefix: "_tmp_",         periodicHdfsFlushInterval: 30000,         kaboomServerSleepDurationMs: 10000,         fileCloseGraceTimeAfterExpiredMs: 30000,         forcedZkOffsetTsUpdateMs: 600000,         kafkaReadyFlagFilename: "_READY",         maxOpenBoomFilesPerPartition: 5,         workerSprintDurationSeconds: 3600,         propagateReadyFlags: true,         propagateReadyFlagFrequency: 600000,         propateReadyFlagDelayBetweenPathsMs: 10 } ```  ## Example Configuration File: /opt/kaboom/config/kaboom-env.sh Here is an exmaple environemnt configuration file (defines runtime configuration and JVM properties):  ``` JAVA=`which java` BASEDIR=/opt/kaboom BINDIR="$BASEDIR/bin" LIBDIR="$BASEDIR/lib" LOGDIR="/var/log/kaboom" CONFIGDIR="$BASEDIR/config" JMXPORT=9580 LOG4JPROPERTIES=$CONFIGDIR/log4j2.xml PIDBASE=/var/run/kaboom KABOOM_USER=kafka  JAVA_OPTS="" JAVA_OPTS="$JAVA_OPTS -server" JAVA_OPTS="$JAVA_OPTS -Xms6G -Xmx6G" JAVA_OPTS="$JAVA_OPTS -XX:+UseParNewGC -XX:+UseConcMarkSweepGC" JAVA_OPTS="$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly -XX:+CMSConcurrentMTEnabled -XX:+CMSScavengeBeforeRemark" JAVA_OPTS="$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=30"  JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution" JAVA_OPTS="$JAVA_OPTS -Xloggc:$LOGDIR/gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M"  JAVA_OPTS="$JAVA_OPTS -Djava.awt.headless=true" JAVA_OPTS="$JAVA_OPTS -Dcom.sun.management.jmxremote" JAVA_OPTS="$JAVA_OPTS -Dcom.sun.management.jmxremote.authenticate=false" JAVA_OPTS="$JAVA_OPTS -Dcom.sun.management.jmxremote.ssl=false" JAVA_OPTS="$JAVA_OPTS -Dcom.sun.management.jmxremote.port=$JMXPORT"  JAVA_OPTS="$JAVA_OPTS -Dlog4j.configurationFile=file:$LOG4JPROPERTIES"  JAVA_OPTS="$JAVA_OPTS -Dkaboom.logs.dir=$LOGDIR"  CLASSPATH=$CONFIGDIR:/etc/hadoop/conf:$LIBDIR/* ```  ## Example Configuration File: /opt/kaboom/config/log4j2.xml (logging) ``` <?xml version="1.0" encoding="UTF-8"?> <!-- This status="$LEVEL" on the next line  is for the logging of log4j2 as it configured tself, don't adjust it for  your application logging --> <configuration status="WARN" monitorInterval="30">    <appenders>       <Console name="console" target="SYSTEM_OUT">          <PatternLayout pattern="[%d] %p %m (%c)%n" />       </Console>       <RollingFile name="primary" fileName="/var/log/kaboom/server.log" filePattern="/var/log/kaboom/server.log.%d{yyyy-MM-dd-k}.log">          <PatternLayout>             <Pattern>[%d] %p %m (%c)%n</Pattern>          </PatternLayout>          <Policies>             <TimeBasedTriggeringPolicy interval="1" modulate="true" />          </Policies>       </RollingFile>    </appenders>    <Loggers>       <Logger name="stdout" level="info" additivity="false">          <AppenderRef ref="console" />       </Logger>       <Root level="INFO">          <AppenderRef ref="primary" />       </Root>    </Loggers> </configuration> ```  ## Running After configuration simply start the kaboom service 'service kabom start'.  ## Monitoring Exposed via [Dropwizard Metric's](https://github.com/dropwizard/metrics) are metrics for monitoring message count, size, and lag (measure of how far behind KaBoom is compared to most recent message in Kafka--both in offset count and seconds):  New monitoring metrics in 0.7.1:  * Meter: boom writes (The number of boom file writes)  Kaboom (Aggregate metrics--for the KaBoom cluster):  * Gauge: max message lab sec  * Gauge: sum message lag sec  * Gauge: avg message lag sec  * Gauge: max message lag  * Gauge: sum message lag * Gauge: avg message lag  * Gauge: avg messages written per sec * Gauge: total messages written per sec  Kaboom (Instance metrics -- for a KaBoom worker assigned to a topic and partition):  * Gauge: offset lag * Gauge: seconds lag * Gauge: messages written per second * Gauge: early offsets received (when compression is enabled and messages are included from earlier than requested offset) * Meter: boom writes  Krackle:  * Meter: MessageRequests * Meter: MessageRequestsTotal * Meter: MessagesReturned * Meter: MessagesReturnedTotal * Meter: BytesReturned * Meter: BytesReturnedTotal * Meter: MessageRequestsNoData * Meter: MessageRequestsNoDataTotal * Meter: BrokerReadAttempts * Meter: BrokerReadAttemptsTotal * Meter: BrokerReadSuccess * Meter: BrokerReadSuccessTotal * Meter: BrokerReadFailure * Meter: BrokerReadFailureTotal  ## Boom Files This section contains portions from the [hadoop-logdriver](https://github.com/blackberry/hadoop-logdriver) project's description of Boom files.  A Boom file is a place where we store logs in HDFS.  The goals of Boom are:  * Be splittable by Hadoop, so that we can efficiently run MapReduce jobs against it. * Be compressed to save storage. * Be able to determine order of lines, even if they are processed out of order.  ## File extention The .bm file extension is used for Boom files.  ## Boom File Format A Boom file is a specific type of Avro [Object Container File](http://avro.apache.org/docs/1.6.3/spec.html#Object+Container+Files).  Familiarize yourself with those docs before you keep going.  Specifically, we always use a compression codec of 'deflate' and we always use the following Schema:      {       "type": "record",       "name": "logBlock",       "fields": [         { "name": "second",      "type": "long" },         { "name": "createTime",  "type": "long" },         { "name": "blockNumber", "type": "long" },         { "name": "logLines", "type": {           "type": "array",             "items": {               "type": "record",               "name": "messageWithMillis",               "fields": [                  { "name": "ms",      "type": "long" },                 { "name": "message", "type": "string" }               ]             }         }}       ]     }  ### Basic Structure The file contains any number of "logBlock" records.  Each logBlock contains data for multiple log lines, but all of the log lines in the record are timestamped in the same second.  Log lines in the same logBlock can have difference millisecond timestamps.  ### Fields in logBlock * second : the number of seconds since Jan 1, 1970 UTC.  All log lines in this record are timestamped with a time that occurs within this second. * createTime : the time (in milliseconds) that this logBlock was created.  This is used for sorting logBlocks. * blockNumber : a number indicating the sequence in which the logBlocks were written by whatever wrote the file.  This is used for sorting logBlocks. * logLines : an array of "messageWithMillis" records, one per log line.  ### Fields in messageWithMillis * ms : the milliseconds part of the timestamp for this log line.  To get the complete timestamp, use second * 1000 + ms. * eventId : an event identifier, reserved for future use.  Use 0 for raw log lines. * message : the contents of the log line, excluding the timestamp and one space after the timestamp.  ## Boom suggested defaults Although no limitations should be assumed on the file beyond what has already been stated, these are sensible defaults that should be followed.  * The logLines field should contain no more that 1000 messageWithMillis entries.  If there are more than 1000 log lines within a second, then use multiple logBlock's with the same second value. * The Avro Object Container File defines a "sync interval".  A good value for this seems to be 2MB (2147483648). * While we are required to use the deflate codec, the compression level is configurable.  If you don't have a specific need, then level 6 is a good default.  ## Sorting log lines If the order of log lines is important, then the fields can be sorted by comparing fields in this order  * timestamp : first timestamp is first (after adding seconds and milliseconds) * createTime : logBlocks that were written first go first. * blockNumber : If two logBlocks were written in the same millisecond, then use them in the order they were written. * index within logLines : If the log lines are the same timestamp, written in the same block, then the order is determined by where they are within the logLines array.  This is the default sorting for LogLineData objects.  ## Contributing To contribute code to this repository you must be [signed up as an official contributor](http://blackberry.github.com/howToContribute.html).  ## Disclaimer THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
pranab/chombo	## Introduction Hadoop based ETL and various utility classes for Hadoop and Storm  ## Philosophy * Simple to use * Input output in CSV format * Metadata defined in simple JSON file * Extremely configurable with many configuration knobs  ## Solution * Various relational algebra operation, including Projection, Join etc * Data extraction ETL to extract structured record from unstructured data * Data extraction ETL to extract structured record from JSON data * Data validation ETL with configurable rules and statistical parameters  * Data profiling ETL with various techniques * Data transformation ETL with configurable transformation rules * Various statistical data exploration solutions * Data normalization * Seasonal data analysis * Various statistical parameter  calculation  * Various long term statistical parameter calculation with incremental data   * Bulk inset, update and delete of Hadoop data * Bases classes for Storm Spout and Bolt * Utility classes for string, configuration * Utility classes for Storm and Redis  ## Blogs The following blogs of mine are good source of details. These are the only source of detail documentation. Map reduce jobs in this projec are used in other projects including sifarish, avenir etc. Blogs related to thos projects are also relevant.  * http://pkghosh.wordpress.com/2015/04/26/bulk-insert-update-and-delete-in-hadoop-data-lake/ * https://pkghosh.wordpress.com/2015/06/08/data-quality-control-with-outlier-detection/ * https://pkghosh.wordpress.com/2015/07/28/validating-big-data/ * https://pkghosh.wordpress.com/2015/09/22/profiling-big-data/ * https://pkghosh.wordpress.com/2015/08/25/anomaly-detection-with-robust-zscore/ * https://pkghosh.wordpress.com/2015/10/22/operational-analytics-with-seasonal-data/ * https://pkghosh.wordpress.com/2015/11/17/transforming-big-data/ * https://pkghosh.wordpress.com/2016/10/04/simple-sanity-checks-for-data-correctness-with-spark/ * https://pkghosh.wordpress.com/2016/12/06/json-to-relational-mapping-with-spark/ * https://pkghosh.wordpress.com/2017/02/19/mobile-phone-usage-data-analytics-for-effective-marketing-campaign/ * https://pkghosh.wordpress.com/2017/05/30/mining-seasonal-products-from-sales-data/ * https://pkghosh.wordpress.com/2017/07/26/processing-missing-values-with-hadoop/ * https://pkghosh.wordpress.com/2017/08/15/measuring-campaign-effectiveness-for-an-online-service-on-spark/   ## Build For Hadoop 1 * mvn clean install  For Hadoop 2 (non yarn) * git checkout nuovo * mvn clean install  For Hadoop 2 (yarn) * git checkout nuovo * mvn clean install -P yarn  For spark * Build chombo first in master branch with  	* mvn clean install   	* sbt publishLocal * Build chombo-spark in  chombo/spark directory 	* sbt clean package  ## Need help? Please feel free to email me at pkghosh99@gmail.com  ## Contribution Contributors are welcome. Please email me at pkghosh99@gmail.com
pranab/visitante	## Introduction The goal of visitante is to calculate various web analytic metric as defined by  Avinash Kaushik (http://www.kaushik.net/avinash/) on the Hadoop and Storm platform. However, it has evolved into a general purpose log analytic and mining solution, beyond  web server logs.   It also includes customer or marketing  analytic solution. Since customer behavior data  is mostly captured in logs, there is a close relationship between customer analytics and  log analytics.  ## Philosophy * Simple and easy to use batch and real time web analytic * Highly configurable  ## Blogs The following blogs of mine are good source of details of visitante  * http://pkghosh.wordpress.com/2012/06/05/big-web-analytic/ * http://pkghosh.wordpress.com/2012/08/10/big-web-checkout-abandonment/ * https://pkghosh.wordpress.com/2014/10/05/tracking-web-site-bounce-rate-in-real-time/ * https://pkghosh.wordpress.com/2016/01/19/detecting-incidents-with-context-from-log-data/ * https://pkghosh.wordpress.com/2016/02/23/customer-lifetime-value-present-and-future/  ## Solutions * Hadoop based batch analytic for      * Num of pages visited     * Total time spent      * Last page visited     * Flow status (e.g., whether checkout flow was entered, entered but not completed or completed)     * Incident detection 	* Pattern based event detection with context     * Customer life time value  * Storm based real time analytic for     * Bounce rate     * Visit depth distribution
msgpack/msgpack-hadoop	MessagePack-Hadoop Integration ========================================  This package contains the bridge layer between MessagePack (http://msgpack.org) and Hadoop (http://hadoop.apache.org/) families.  This enables you to run MR jobs on the MessagePack-formatted data, and also enables you to issue Hive query language over it.  MessagePack-Hive adapter enables SQL-based adhoc-query, which takes *nested* *unstructured* data as input (like JSON, but binary-encoded). Of course, query is executed with MapReduce framework!  Here is the sample MessagePack-Hive query, which counts unique user per URL.  > CREATE EXTERNAL TABLE IF NOT EXISTS mpbin (v string) \   ROW FORMAT DELIMITED FIELDS TERMINATED BY '@'  LINES TERMINATED BY '\n' \   LOCATION '/path/to/hdfs/';  > SELECT url, COUNT(1) \   FROM mpbin LATERAL VIEW msgpack_map(v, 'user', 'url') m AS user, url   GROUP BY txt;  Required Setup ========================================  Please setup Hadoop + Hive system. Either Local, Pseudo-Distributed, or Distributed environment is OK.  Hive Getting Started ========================================  1. locate jars  Put these jars to $HIVE_HOME/lib/ directory.  * msgpack-hadoop-$version.jar * msgpack-$version.jar * javassist-$version.jar  2. exec hive shell  Please execute the following command.  $ hive --auxpath $HIVE_HOME/lib/msgpack-hadoop-$version.jar,$HIVE_HOME/lib/msgpack-$version.jar,$HIVE_HOME/lib/javassist-$version.jar  You can skip --auxpath option once modify your hive-site.xml.  <property>   <name>hive.aux.jars.path</name>   <value>$HIVE_HOME/lib/msgpack-hadoop-$version.jar,$HIVE_HOME/lib/msgpack-$version.jar,$HIVE_HOME/lib/javassist-$version.jar</value> </property>  3. add jar and load custom UDTF function  This step is required for every Hive query.  hive> add $HIVE_HOME/lib/msgpack-hadoop-$version.jar hive> add $HIVE_HOME/lib/msgpack-$version.jar hive> add $HIVE_HOME/lib/javassist-$version.jar hive> CREATE TEMPORARY FUNCTION msgpack_map AS 'org.msgpack.hadoop.hive.udf.GenericUDTFMessagePackMap';  4. create external table  Create external table, which points the data directory.  hive> CREATE EXTERNAL TABLE IF NOT EXISTS mp_table (v string) \       ROW FORMAT DELIMITED FIELDS TERMINATED BY '@'  LINES TERMINATED BY '\n' \       LOCATION '/path/to/hdfs/';  5. execute the query  Finally, execute the SELECT query over input data.  Input msgpack data is unstructured, nested data. Therefore, you need to "map" MessagePack structure to Hive field name. Actually, you can map the field by using msgpack_map() UDTF function, and name the fields by "AS" clause.  hive> SELECT url, COUNT(1) \       FROM mp_table LATERAL VIEW msgpack_map(v, 'user', 'url') m AS user, url       GROUP BY txt;  Caveats ========================================  Currently, MessagePackInputFormat is now unsplittable. Therefore, you need to manually *shred* the data into small pieces.
sriksun/Ivory	Ivory Overview  Ivory is a feed processing and feed management system aimed at making it easier for end consumers to onboard their feed processing and feed management on hadoop clusters.  Why?  * Dependencies across various data processing pipelines are not easy to   establish. Gaps here typically leads to either incorrect/partial   processing or expensive reprocessing. Repeated duplicate definition of   a single feed multiple times can lead to inconsistencies / issues.  * Input data may not arrive always on time and it is required to kick off   the processing without waiting for all data to arrive and accommodate   late data separately  * Feed management services such as feed retention, replications across   clusters, archival etc are tasks that are burdensome on individual   pipeline owners and better offered as a service for all customers.  * It should be easy to onboard new workflows/pipelines  * Smoother integration with metastore/catalog  * Provide notification to end customer based on availability of feed   groups (logical group of related feeds, which are likely to be used   together)  Usage  a. Setup cluster definition    $IVORY_HOME/bin/ivory entity -submit -type cluster -file /cluster/definition.xml -url http://ivory-server:ivory-port  b. Setup feed definition    $IVORY_HOME/bin/ivory entity -submit -type feed -file /feed1/definition.xml -url http://ivory-server:ivory-port    $IVORY_HOME/bin/ivory entity -submit -type feed -file /feed2/definition.xml -url http://ivory-server:ivory-port  c. Setup process definition    $IVORY_HOME/bin/ivory entity -submit -type process -file /process/definition.xml -url http://ivory-server:ivory-port  d. Once submitted, entity definition, status and dependency can be queried.    $IVORY_HOME/bin/ivory entity -type [cluster|feed|process] -name <<name>> [-definition|-status|-dependency] -url http://ivory-server:ivory-port     or entities for a particular type can be listed through    $IVORY_HOME/bin/ivory entity -type [cluster|feed|process] -list  e. Schedule process    $IVORY_HOME/bin/ivory entity  -type process -name process -schedule -url http://ivory-server:ivory-port  f. Once scheduled entities can be suspended, resumed or deleted (post submit)    $IVORY_HOME/bin/ivory entity  -type [cluster|feed|process] -name <<name>> [-suspend|-delete|-resume] -url http://ivory-server:ivory-port  g. Once scheduled process instances can be managed through irovy CLI    $IVORY_HOME/bin/ivory instance -processName <<name>> [-kill|-suspend|-resume|-re-run] -start "yyyy-MM-dd'T'HH:mm'Z'" -url http://ivory-server:ivory-port  Example configurations  Cluster:  <?xml version="1.0"?> <!--     Production cluster configuration   -->  <cluster colo="ua2" description="" name="staging-red" xmlns="uri:ivory:cluster:0.1" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">      <interfaces>         <interface type="readonly" endpoint="hftp://gsgw1001.red.ua2.inmobi.com:50070"                    version="0.20.2-cdh3u0" />          <interface type="write" endpoint="hdfs://gsgw1001.red.ua2.inmobi.com:54310"                    version="0.20.2-cdh3u0" />          <interface type="execute" endpoint="gsgw1001.red.ua2.inmobi.com:54311" version="0.20.2-cdh3u0" />          <interface type="workflow" endpoint="http://gs1134.blue.ua2.inmobi.com:11000/oozie/"                    version="3.1.4" />          <interface type="messaging" endpoint="tcp://gs1134.blue.ua2.inmobi.com:61616?daemon=true"                    version="5.1.6" />     </interfaces>      <locations>         <location name="staging" path="/projects/ivory/staging" />         <location name="temp" path="/tmp" />         <location name="working" path="/projects/ivory/working" />     </locations>      <properties/>  </cluster>  Feed:  <?xml version="1.0" encoding="UTF-8"?> <!--     Hourly ad carrier summary. Generated by hourly processing of rr logs   -->  <feed description="RRHourlyAdCarrierSummary" name="RRHourlyAdCarrierSummary" xmlns="uri:ivory:feed:0.1" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">     <partitions/>      <groups>rmchourly</groups>      <frequency>hours</frequency>     <periodicity>1</periodicity>      <late-arrival cut-off="hours(6)" />      <clusters>         <cluster name="staging-red" type="source">             <validity start="2009-01-01T00:00Z" end="2099-12-31T00:00Z" timezone="UTC" />             <retention limit="months(24)" action="delete" />         </cluster>     </clusters>      <locations>         <location type="data" path="/projects/bi/rmc/rr/${YEAR}-${MONTH}-${DAY}-${HOUR}.concat/HourlyAdCarrierSummary" />         <location type="stats" path="/none" />         <location type="meta" path="/none" />     </locations>      <ACL owner="rmcuser" group="users" permission="0755" />      <schema location="/none" provider="none" />      <properties/>  </feed>   Process:  <?xml version="1.0" encoding="UTF-8"?> <!--     RMC Daily process, produces 34 new feeds  --> <process name="rmc-daily"> 	<cluster name="staging-red" />  	<frequency>days(1)</frequency>  	<validity start="2012-04-03T06:00Z" end="2022-12-30T00:00Z" timezone="UTC" />  	<inputs> 		<input name="WapAd" feed="WapAd" start="today(0,0)" end="today(0,0)" /> 	</inputs>  	<outputs> 	        <output name="TrafficDailyAdSiteSummary" feed="TrafficDailyAdSiteSummary" instance="yesterday(0,0)" /> 	</outputs>  	<properties> 		<property name="lastday" value="${formatTime(yesterday(0,0), 'yyyy-MM-dd')}" /> 	</properties>  	<workflow engine="oozie" path="/projects/bi/rmc/pipelines/workflow/rmcdaily" />  	<retry policy="backoff" delay="5" delayUnit="minutes" attempts="3" /> </process>
laserson/avro2parquet	avro2parquet ============  Hadoop MapReduce program to convert [Avro][2] data files to [Parquet format][2].  Installation ------------      git clone https://github.com/laserson/avro2parquet.git     cd avro2parquet     mvn clean package  This will generate the `jar` files in the `target/` directory.  Usage -----  This tool will work on Avro container files (which I believe is just the standard Avro data file format).  It contains the Avro `GenericRecord` objects as the key and a `NullWritable` as the value.  The tool is currently hardcoded to output [Snappy-compressed][3] Parquet.  It is simply a MapReduce job using the `Tool` interface.  The command is like so:      hadoop jar <avro2parquet jar file> \     com.cloudera.science.avro2parquet.Avro2Parquet \     <and generic options to the JVM> \     hdfs:///path/to/avro/schema.avsc \     hdfs:///path/to/avro/data \     hdfs:///output/path  so for example:      hadoop jar avro2parquet-0.1.0-jar-with-dependencies.jar \     com.cloudera.science.avro2parquet.Avro2Parquet \     -D mapred.child.java.opts=-Xmx1024M \     hdfs:///user/lasersou/schemas/data.avsc \     hdfs:///user/lasersou/data \     hdfs:///user/lasersou/output  [1]: http://avro.apache.org/ [2]: http://parquet.io/ [3]: https://code.google.com/p/snappy/
alexholmes/hadoop-utils	Hadoop Utils ============  A set of Hadoop utilities to make working with Hadoop a little easier. The utilities can be called from your Hadoop application code, and in addition some can be called directly from the command-line.  ## License  Apache version 2.0 (for more details look at [LICENSE](https://github.com/alexholmes/hadoop-utils/blob/master/LICENSE)).  ## Usage  ### For integrating into your application code  To get started, simply:  1. Download, and run `mvn package`. 2. Use the generated JAR `target/hadoop-utils-<version>.jar` in your application. 3. Understand the API's by reading the generated JavaDocs in `target/hadoop-utils-<version>-javadoc.jar`.  ### To run the bundled utilities  Look at file [CLI.md](https://github.com/alexholmes/hadoop-utils/blob/master/CLI.md) for more details.
miniway/kafka-hadoop-consumer	kafka-hadoop-consumer =====================  Another kafka-hadoop-consumer  ## Quick Start     $ mvn start     $ java -cp target/hadoop_consumer-1.0-SNAPSHOT.jar:`hadoop classpath` kafka.consumer.HadoopConsumer -z <zookeeper> -t <topic> target_hdfs_path
NetApp/NetApp-Hadoop-NFS-Connector	NetApp-Hadoop-NFS-Connector =========================== This site is obsolete. Please use NetApp FAS NFSConnector product from [now.netapp.com](http://mysupport.netapp.com/tools/info/ECMLP2570720I.html?platformID=61127&productID=62124)   Overview -------------------------------------  The Hadoop NFS Connector allows Apache Hadoop (2.2+) and Apache Spark (1.2+) to use a NFSv3 storage server as a storage endpoint. The NFS Connector supports two modes: (1) secondary filesystem - where Hadoop/Spark runs using HDFS as its primary storage and can use NFS as a second storage endpoint, and (2) primary filesystem - where Hadoop/Spark runs entirely on a NFSv3 storage server.   The code is written in a way such that existing applications do not have to change. All one has to do is to copy the connector jar into the lib/ directory of Hadoop/Spark. Then, modify core-site.xml to provide the necessary details.  **NOTE: The code is in beta. We would love for you to try it out and give us feedback.**  This is the first release and it does the following: * Connects to a NFSv3 storage server supporting AUTH_NONE or AUTH_SYS authentication method. * Works with Apache Hadoop (vanilla) 2.2 or newer, Hortonworks HDP 2.2 or newer  * Supports all operations defined by the Hadoop FileSystem interface. * Pipelines the READ/WRITE requests to utilize the underlying network (works fine with 1GbE and 10GbE networks)  We are planning to add these in the near future: * Ability to connect to multiple NFS endpoints (multiple IP addresses). This allows for even more bandwidth. * Integrate with Hadoop user authentication  How to use -------------------------------------  Once the NFS connector is configured, you can easily invoke it from the command-line using the Hadoop shell. ```   console> bin/hadoop fs -ls nfs://<nfs-server-hostname>:2049/ (if using as secondary filesystem)   console> bin/hadoop fs -ls / (if using as default/primary filesystem) ```  When new jobs are submitted, you can simply provide it as an input or output path or both: ```   (assuming NFS is used as a secondary filesystem)   console> bin/hadoop jar <path-to-examples> jar terasort nfs://<nfs-server-hostname>:2049/tera/in /tera/out   console> bin/hadoop jar <path-to-examples> jar terasort /tera/in nfs://<nfs-server-hostname>:2049/tera/out   console> bin/hadoop jar <path-to-examples> jar terasort nfs://<nfs-server-hostname>:2049/tera/in nfs://<nfs-server-hostname>:2049/tera/out ```     Configuration ------------------------------------- <ol> <li>Compile the project ``` console> mvn clean package ``` </li> <li>Copy the jar file to the shared common library directory based on your Hadoop installation. For example, for hadoop-2.4.1: ``` console> cp target/hadoop-connector-nfsv3-1.0.jar $HADOOP_HOME/share/hadoop/common/lib/ ``` </li> <li>Add parameters of NFSv3 connector into core-site.xml located in Hadoop configuration directory (e.g., for hadoop-2.4.1: $HADOOP_HOME/conf) ```   <!-- If NFS should be the primary/default filesystem -->   <property>       <name>fs.defaultFS</name>       <value>nfs://<nfsserver>:2049</value>   </property>   <property>       <name>fs.nfs.configuration</name>       <value><path-to-json-configuration-file>/nfs-mapping.json</value>   </property>   <property>   <name>fs.nfs.impl</name>       <value>org.apache.hadoop.fs.nfs.NFSv3FileSystem</value>   </property>       <property>       <name>fs.AbstractFileSystem.nfs.impl</name>       <value>org.apache.hadoop.fs.nfs.NFSv3AbstractFilesystem</value>   </property> ``` </li> <li>Start Hadoop. NFS can now be used inside Hadoop. </li> <ol>
P7h/storm-camel-example	# Real-time analysis and Viz with Storm-Camel-Highcharts ----------  #### You might also be interested in checking out extension of this repo for Twitter sentiment of states of US using D3.js Choropleth Map and Highcharts Columncharts on [StormTweetsSentimentD3Viz](https://github.com/P7h/StormTweetsSentimentD3Viz) and also a similar project for UK Twitter Sentiment on [StormTweetsSentimentD3UKViz](https://github.com/P7h/StormTweetsSentimentD3UKViz).  ## Introduction This repository contains an application for demonstrating `Apache Storm` distributed framework by counting the names of the companies fed randomly by the code [in the Spout] in real-time.<br>This project also visualizes the output in real-time using Queues, Websockets and Highcharts.<br>  [Apache Storm](http://storm.apache.org) is an open source distributed real-time computation system, developed at BackType by Nathan Marz and team. It has been open sourced by Twitter [post BackType acquisition] in August, 2011. And became a top level project in Apache on 29<sup>th</sup> September, 2014.<br> This application has been developed and tested with Storm v0.8.2 on Windows 7 in local mode; and was eventually updated and tested with Storm v0.9.3 on 04<sup>th</sup> January, 2015. Application may or may not work with earlier or later versions than Storm v0.9.3.<br>  The initial version of this application was actually forked and later updated further from Robin van Breukelen's [storm-camel-example](https://github.com/robinvanb/storm-camel-example) project.  ## Features * Application receives a random company name from a Spout every 200 milliseconds.<br> * Bolt counts frequency of the each company name passed on from the Spout.<br> * This frequency is written to ActiveMQ using a JMSBolt.<br> * Camel is used to read these tuples from ActiveMQ and also write to a WebSocket.<br> * A pure HTML5 front-end reads from the above Websocket and visualizes the output in a Bar chart using Highcharts and updates the chart in real-time to reflect the latest count provided by the Bolt.<br> * This project has also been made compatible with both Eclipse IDE and IntelliJ IDEA. 	* Import the project in your favorite IDE and you can quickly follow the code. * As of today, this codebase has almost no or very less comments.<br>  ## Demo ![GIF of visualization](Storm-Camel-Websockets__Demo.gif)  ![Screenshot of visualization](Storm-Camel-Websockets__Demo.png)  ## Dependencies * Storm v0.9.3 * Camel v2.14.1 * ActiveMQ v5.10.0 * Spring v4.1.3.RELEASE * xbean v4.1  Also, please check [`pom.xml`](pom.xml) for complete information on the various dependencies of the project.<br>  ## Requirements You need the following on your machine:  * Oracle JDK >= 1.8.x * Apache Maven >= 3.2.3 * Clone this repo and import as an existing Maven project to either Eclipse IDE or IntelliJ IDEA. 	* Please enable / download Maven Plugin in the IDE.  ## Usage To build and run this topology, you must use Java 1.8.<br>  ### Local Mode: Local mode can also be run on Windows environment using either IntelliJ IDEA or Eclipse IDE. <br>*Note*: Please be sure to clean your temp folder as it adds lot of temporary files in every run.  * Launch your favorite IDE and execute [`Runner.java`](runner/src/main/java/nl/java/runner/Runner.java).<br> * All the required frameworks and libraries are downloaded by Maven as required in the above import process in the IDE. 	* If not, please execute `mvn clean compile` command once at the root of this repo. * Launch a browser [preferably Google Chrome] and open [`index.html`](runner/src/main/resources/index.html)  	* Click on "Start Viz" button to trigger the initialization.<br> 	* You can stop the visualization at any time by clicking on "Stop Viz" button.<br> * This chart updates every second and displays real-time visualization of the words processed by the Bolts.<br>  ### Remote Mode: TBA.  ## Problems If you find any issues, please report them either raising an [issue](https://github.com/P7h/storm-camel-example/issues) here on Github or alert me on my Twitter handle [@P7h](http://twitter.com/P7h). Or even better, please send a [pull request](https://github.com/P7h/storm-camel-example/pulls).<br> Appreciate your help. Thanks!  ## License Copyright &copy; 2013-2015 Robin van Breukelen and Prashanth Babu.<br> Licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).
pentaho/pentaho-hadoop-shims	# pentaho-hadoop-shims # Hadoop Configurations, also known and shims and the Pentaho Big Data Adaptive layer, are collections of Hadoop libraries required to communicate with a specific version of Hadoop (and related tools: Hive, HBase, Sqoop, Pig, etc.). They are designed to be easily configured.  How to build --------------  pentaho-hadoop-shims uses the maven framework.    #### Pre-requisites for building the project: * Maven, version 3+ * Java JDK 1.8 * This [settings.xml](https://raw.githubusercontent.com/pentaho/maven-parent-poms/master/maven-support-files/settings.xml) in your <user-home>/.m2 directory  #### Building it  This is a maven project, and to build it use the following command  ``` $ mvn clean install ``` Optionally you can specify -Drelease to trigger obfuscation and/or uglification (as needed)  Optionally you can specify -Dmaven.test.skip=true to skip the tests (even though you shouldn't as you know)  The build result will be a Pentaho package located in ```target```.  #### Running the tests  __Unit tests__  This will run all unit tests in the project (and sub-modules). To run integration tests as well, see Integration Tests below.  ``` $ mvn test ```  If you want to remote debug a single java unit test (default port is 5005):  ``` $ cd core $ mvn test -Dtest=<<YourTest>> -Dmaven.surefire.debug ```  __Running tests on Windows__  Running tests on Window requires additional environment set up because of existing problems running Hadoop on Windows (please see https://wiki.apache.org/hadoop/WindowsProblems).  Exactly it needs to have **hadoop.home.dir** variable pointed to dir with ` \bin\winutils.exe`.  __Steps to set up environment:__    - Download *winutils.exe*  binary. E.g. from: https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-winutils  - Create any dir (e.g. d:\TEMP_DIR) and **bin** sub dir in it. Then put **winutils.exe** in **bin**:  `d:\TEMP_DIR\bin\winutils.exe`    - Set system property `hadoop.home.dir="d:\TEMP_DIR"` or just run the Java process with this property: ``` $ mvn test -Dhadoop.home.dir="d:\TEMP_DIR" ``` or ``` $ mvn clean install -Dhadoop.home.dir="d:\TEMP_DIR" ```  __Integration tests__  In addition to the unit tests, there are integration tests that test cross-module operation. This will run the integration tests.  ``` $ mvn verify -DrunITs ```  To run a single integration test:  ``` $ mvn verify -DrunITs -Dit.test=<<YourIT>> ```  To run a single integration test in debug mode (for remote debugging in an IDE) on the default port of 5005:  ``` $ mvn verify -DrunITs -Dit.test=<<YourIT>> -Dmaven.failsafe.debug ``` PentahoMapReduceIT.java  To skip test  ``` $ mvn clean install -DskipTests ```  To get log as text file  ``` $ mvn clean install test >log.txt ```   __IntelliJ__  * Don't use IntelliJ's built-in maven. Make it use the same one you use from the commandline.   * Project Preferences -> Build, Execution, Deployment -> Build Tools -> Maven ==> Maven home directory  ````
P7h/StormTweetsSentimentD3Viz	# StormTweetsSentimentD3Viz ----------  ### You might also be interested in checking out my other project, an extension of this repo for Twitter sentiment of counties / regions of UK using D3.js Choropleth Map on [StormTweetsSentimentD3UKViz](https://github.com/P7h/StormTweetsSentimentD3UKViz).  ## Introduction This repository contains an application which is built to demonstrate as an example of Apache Storm distributed framework by performing sentiment analysis of tweets originating from U.S. in real-time. This Topology retrieves tweets originating from US and computes and visualizes the sentiment scores of each of the state of United States [based on tweets] in a Choropleth Map using [D3.js](http://d3js.org) continuously for 10 minutes [in local mode]. User can also explicitly kill the topology by pressing `Ctrl+C` for exiting the application. Also, there is a column chart visualization of each State and its sentiment value using [Highcharts](http://www.highcharts.com).  [Apache Storm](http://storm.apache.org) is an open source distributed real-time computation system, developed at BackType by Nathan Marz and team. It has been open sourced by Twitter [post BackType acquisition] in August, 2011. And Storm became a [top level project in Apache](https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces64) on 29<sup>th</sup> September, 2014.<br> This application has been developed and tested with Storm v0.8.2 on Windows 7 in local mode; and was eventually updated and tested with Storm v0.9.3 on 05<sup>th</sup> January, 2014. Application may or may not work with earlier or later versions than Storm v0.9.3.<br>  This application has been tested in:<br>  + Local mode on a CentOS virtual machine and even on Microsoft Windows 7 machine. + Cluster mode on a private cluster and also on Amazon EC2 environment of 4 machines and 5 machines respectively; with all the machines in private cluster running Ubuntu while EC2 environment machines were powered by CentOS. 	+ Recent update to Apache Storm v0.9.3 has not been tested in a Cluster mode.  ## Features * Application retrieves tweets using Twitter Streaming API (using [Twitter4J](http://twitter4j.org)).<br> * It analyses sentiments of all the tweets originating from US. * There are three different objects within a tweet that we can use to determine it’s origin. This application tries to find the location using all the three options and prioritizes location received in the following order [high to low]: 	* The coordinates object. 	* The place object. 	* The user object. * For reverse geocoding, this application uses Bing Maps API.  	* For more information and sign up, please check [Getting Started with Bing Maps](http://msdn.microsoft.com/en-us/library/ff428643.aspx). 	* Please note that you would need Windows Live account for signing up for [Bing Maps API key](https://www.bingmapsportal.com/). 	* Also, please consider opting for Basic Plan for Bing Maps API, as that is better for our usage. As of 18th June, 2013, limit is 50k requests for 24 hours in Basic Plan. 	* I chose Bing Maps and not Google Maps since Google Maps is too restrictive for our usage, as it has a limit of only 2500 requests per day. * This application uses [AFINN](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) which contains a list of pre-computed sentiment scores. 	* These words are used to determine sentiment of the each tweet which is retrieved using Streaming API. * By understanding sentiment values, we can get the most happiest state of US and most unhappiest state as well. * For visualization, I am using D3 to display the sentiment value of each state in real-time by conveying it in a color, appropriate to the sentiment value. Color of the State moves from red to green, as its corresponding sentiment value improves.  * There is another visualization context using Highcharts, which is a column chart of each State and its corresponding sentiment value. This chart also updates in real-time based on the sentiment value of each of the state. * This codebase has been updated with decent comments, wherever required. * Also this project has been made compatible with both Eclipse IDE and IntelliJ IDEA. Import the project in your favorite IDE [which has Maven plugin installed] and you can quickly follow the code.  ## Demo ### D3 Choropleth Visualization #### GIF of D3 Choropleth Visualization ![GIF of D3 Visualization](D3_Viz.gif) #### Screenshot of D3 Choropleth Visualization ![Screenshot of D3 Visualization](D3_Viz.png)  ### Highcharts Visualization #### GIF of Highcharts Visualization ![GIF of Highcharts Visualization](HC_Viz.gif) #### Screenshot of Highcharts Visualization ![Screenshot of Highcharts Visualization](HC_Viz.png)  ## Configuration * Please check the [`config.properties`](src/main/resources/config.properties#L3-6) and add your own values and complete the integration of Twitter API to your application by looking at your values from [Twitter Developer Page](https://dev.twitter.com/apps).<br> 	* If you did not create a Twitter App before, then please create a new Twitter App where you will get all the required values of `config.properties` afresh and then populate them here without any mistake.<br> * Also please add the value of Bing Maps API Key to [`config.properties`](src/main/resources/config.properties#L10), as that will be used for getting the reverse geocode location using Latitude and Longitude.<br> 	* If you do not have Bing Maps API Key, please check [Getting Started with Bing Maps](http://msdn.microsoft.com/en-us/library/ff428643.aspx) for signup and other information.<br> * And finally please check [but _do not modify_] the [`AFINN-111.txt`](src/main/resources/AFINN-111.txt) file to see the pre-computed sentiment scores of ~2500 words / phrases. 	* For more info on AFINN, please check its [`AFINN-README.txt`](src/main/resources/AFINN-README.txt) and also check his [paper](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010).  ## Dependencies * Storm v0.9.3 * Jackson v1.9.13 * Spring v4.0.3 * Camel v2.13.0 * ActiveMQ Camel v5.9.0 * Twitter4J v4.0.2 * Google Guava v18.0 * Logback v1.1.2  Also, please check [`pom.xml`](pom.xml) for more information on the various other dependencies of the project.<br>  ## Requirements This project uses Maven to build and run the topology.<br> You need the following on your machine:  * Oracle JDK >= 1.8.x * Apache Maven >= 3.2.3 * Python v2.7.x installed on the machine for triggering the visualization server-side hosting. * Clone this repo and import as an existing Maven project to either Eclipse IDE or IntelliJ IDEA. * This application uses [Google Guava](https://code.google.com/p/guava-libraries) for making life simple while using Collections and other generic stuff. * This application also uses [Jackson](http://jackson.codehaus.org) for unmarshalling the JSON response got from Bing Maps API. * Requires ZooKeeper, etc installed and configured in case of executing this project in distributed mode i.e. Storm Cluster.<br> 	- Follow the steps on [Storm Wiki](http://storm.apache.org/documentation/Setting-up-a-Storm-cluster.html) for more details on setting up a Storm Cluster.<br>  Rest of the required frameworks and libraries are downloaded by Maven as required in the build process, the first time the Maven build is invoked.  ## Usage To build and run this topology, you must use Java 1.8.  ### Local Mode: * All the required frameworks and libraries are downloaded by Maven as required.<br> * Local mode can also be run on Windows environment without installing any specific software or framework as such.<br> *Note*: Please be sure to clear your temp folder as it adds lot of temporary files in every run.<br> * In local mode, this application can be run from command line by invoking:  Either      mvn clean compile exec:java -Dexec.classpathScope=compile -Dexec.mainClass=org.p7h.storm.sentimentanalysis.topology.SentimentAnalysisTopology  or      mvn clean compile package && java -jar target/storm-sentiment-viz-0.1-jar-with-dependencies.jar  * Start Python SimpleHTTPServer in the [`web`](web) folder of this code repo.   Command:      python -m SimpleHTTPServer  * For D3 Choropleth Map visualization, launch a browser [preferably Google Chrome] and point to [`index.html`](http://localhost:8000) hosted on the above Python server.<br> 	* Click on "Start Viz" button to trigger the D3 Choropleth Map visualization.<br> 	* You can stop the visualization any time by clicking on "Stop Viz" button.<br> 	* This Map updates as and when a tweet is analyzed by Storm and displays in real-time, visualization of the sentiment value of each of the State of United States of America.<br> * For Highcharts visualization, launch a browser [preferably Google Chrome] and point to [`US__HighchartsViz.html`](http://localhost:8000/US__HighchartsViz.html)  	* Click on "Start Viz" button to trigger the Highcharts visualization.<br> 	* You can stop the visualization as well by clicking on "Stop Viz" button.<br> 	* This chart updates every second and displays in real-time, visualization of the sentiment value of each of the State of United States of America.<br>  ### Distributed [or Cluster / Production] Mode: Distributed mode requires a complete and proper Storm Cluster setup. Please check [wiki on Apache Storm website](http://storm.apache.org/documentation/Setting-up-a-Storm-cluster.html) for setting up a Storm Cluster.<br> In distributed mode, after starting Nimbus and Supervisors on individual machines, this application can be executed on the master [or Nimbus] machine by invoking the following on the command line:  Command:      storm jar target/storm-sentiment-viz-0.1.jar org.p7h.storm.sentimentanalysis.topology.SentimentAnalysisTopology SentimentAnalysis  ## Problems If you find any issues, please report them either raising an [issue](https://github.com/P7h/StormTweetsSentimentD3Viz/issues) here on GitHub or alert me on my Twitter handle [@P7h](http://twitter.com/P7h). Or even better, please send a [pull request](https://github.com/P7h/StormTweetsSentimentD3Viz/pulls). Appreciate your help. Thanks!  ## License Copyright &copy; 2013-2015 Prashanth Babu.<br> Licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).
Azure/azure-documentdb-hadoop	# Microsoft Azure DocumentDB Hadoop Connector  ![](https://img.shields.io/github/release/azure/azure-documentdb-hadoop.svg) ![](https://img.shields.io/maven-central/v/com.microsoft.azure/azure-documentdb-hadoop.svg) ![](https://img.shields.io/github/issues/azure/azure-documentdb-hadoop.svg)  This project provides a client library in Java that allows Microsoft Azure DocumentDB to act as an input source or output sink for MapReduce, Hive and Pig jobs.  ## Download ### Option 1: Via Github  To get the binaries of this library as distributed by Microsoft, ready for use within your project, you can use [GitHub releases](https://github.com/Azure/azure-documentdb-hadoop/releases).  ### Option 2: Source Via Git  To get the source code of the connector via git just type:      git clone git://github.com/Azure/azure-documentdb-hadoop.git  ### Option 3: Source Zip  To download a copy of the source code, click "Download ZIP" on the right side of the page or click [here](https://github.com/Azure/azure-documentdb-hadoop/archive/master.zip).   ### Option 4: Via Maven  To get the binaries of this library as distributed by Microsoft, ready for use within your project, you can use Maven.  ```xml <dependency> 	<groupId>com.microsoft.azure</groupId> 	<artifactId>azure-documentdb-hadoop</artifactId> 	<version>1.2.0</version> </dependency> ``` ### Option 5: HDInsight  Install the DocumentDB Hadoop Connector onto HDInsight clusters through custom action scripts. Full instructions can be found [here](https://azure.microsoft.com/documentation/articles/documentdb-run-hadoop-with-hdinsight/).   ## Requirements * Java Development Kit 7  ## Supported Versions * Apache Hadoop & YARN 2.4.0     * Apache Pig 0.12.1     * Apache Hive & HCatalog 0.13.1 * Apache Hadoop & YARN 2.6.0     * Apache Pig 0.14.0     * Apache Hive $ HCatalog 0.14.0 * HDI 3.1 ([Getting started with HDInsight](https://azure.microsoft.com/documentation/articles/documentdb-run-hadoop-with-hdinsight/)) * HDI 3.2  ## Dependencies * Microsoft Azure DocumentDB Java SDK 1.6.0 (com.microsoft.azure / azure-documentdb / 1.6.0)  When using Hive: * OpenX Technologies JsonSerde 1.3.1-SNAPSHOT (org.openx.data / json-serde-parent / 1.3.1-SNAPSHOT)     GitHub repo can be found [here](https://github.com/rcongiu/Hive-JSON-Serde)  Please download the jars and add them to your build path.   ## Usage  To use this client library with Azure DocumentDB, you need to first [create an account](http://azure.microsoft.com/en-us/documentation/articles/documentdb-create-account/).  ### MapReduce  ##### Configuring input and output from DocumentDB Example ```Java     // Import Hadoop Connector Classes     import com.microsoft.azure.documentdb.Document;     import com.microsoft.azure.documentdb.hadoop.ConfigurationUtil;     import com.microsoft.azure.documentdb.hadoop.DocumentDBInputFormat;     import com.microsoft.azure.documentdb.hadoop.DocumentDBOutputFormat;     import com.microsoft.azure.documentdb.hadoop.DocumentDBWritable;      // Set Configurations     Configuration conf = new Configuration();     final String host = "Your DocumentDB Endpoint";     final String key = "Your DocumentDB Primary Key";     final String dbName = "Your DocumentDB Database Name";     final String inputCollNames = "Your DocumentDB Input Collection Name[s]";     final String outputCollNames = "Your DocumentDB Output Collection Name[s]";     final String query = "[Optional] Your DocumentDB Query";     final String outputStringPrecision = "[Optional] Number of bytes to use for String indexes"     final String offerType = "[Optional] Your performance level for Output Collection Creations";     final String upsert = "[Optional] Bool to disable or enable document upsert";      conf.set(ConfigurationUtil.DB_HOST, host);     conf.set(ConfigurationUtil.DB_KEY, key);     conf.set(ConfigurationUtil.DB_NAME, dbName);     conf.set(ConfigurationUtil.INPUT_COLLECTION_NAMES, inputCollNames);     conf.set(ConfigurationUtil.OUTPUT_COLLECTION_NAMES, outputCollNames);     conf.set(ConfigurationUtil.QUERY, query);     conf.set(ConfigurationUtil.OUTPUT_STRING_PRECISION, outputStringPrecision);     conf.set(ConfigurationUtil.OUTPUT_COLLECTIONS_OFFER, offerType);     conf.set(ConfigurationUtil.UPSERT, upsert); ```  Full MapReduce sample can be found [here](https://github.com/Azure/azure-documentdb-hadoop/blob/master/samples/MapReduceTutorial.java).  ### Hive ##### Loading data from DocumentDB Example ```Java     CREATE EXTERNAL TABLE DocumentDB_Hive_Table( COLUMNS )     STORED BY 'com.microsoft.azure.documentdb.hive.DocumentDBStorageHandler'     tblproperties (         'DocumentDB.endpoint' = 'Your DocumentDB Endpoint',         'DocumentDB.key' = 'Your DocumentDB Primary Key',         'DocumentDB.db' = 'Your DocumentDB Database Name',         'DocumentDB.inputCollections' = 'Your DocumentDB Input Collection Name[s]',         'DocumentDB.query' = '[Optional] Your DocumentDB Query' ); ```  ##### Storing data to DocumentDB Example ```Java     CREATE EXTERNAL TABLE Hive_DocumentDB_Table( COLUMNS )     STORED BY 'com.microsoft.azure.documentdb.hive.DocumentDBStorageHandler'      tblproperties (          'DocumentDB.endpoint' = 'Your DocumentDB Endpoint',          'DocumentDB.key' = 'Your DocumentDB Primary Key',          'DocumentDB.db' = 'Your DocumentDB Database Name',          'DocumentDB.outputCollections' = 'Your DocumentDB Output Collection Name[s]',         '[Optional] DocumentDB.outputStringPrecision' = '[Optional] Number of bytes to use for String indexes',         '[Optional] DocumentDB.outputCollectionsOffer' = '[Optional] Your performance level for Output Collection Creations',         '[Optional] DocumentDB.upsert' = '[Optional] Bool to disable or enable document upsert');     INSERT INTO TABLE Hive_DocumentDB_Table ``` Full Hive sample can be found [here](https://github.com/Azure/azure-documentdb-hadoop/blob/master/samples/Hive_Tutorial.hql).  ### Pig ##### Loading data from DocumentDB Example ```Java     LOAD 'Your DocumentDB Endpoint'      USING com.microsoft.azure.documentdb.hadoop.pig.DocumentDBLoader(          'Your DocumentDB Primary Key',          'Your DocumentDB Database Name',         'Your DocumentDB Input Collection Name[s]',         '[Optional] Your DocumentDB SQL Query' ); ```  ##### Storing data to DocumentDB Example ```Java     STORE data  INTO 'DocumentDB Endpoint'      USING com.microsoft.azure.documentdb.hadoop.pig.DocumentDBStorage(          'DocumentDB Primary Key',         'DocumentDB Database Name',         'DocumentDB Output Collection Name[s]',         '[Optional] Your performance level for Output Collection Creations',         '[Optional] Number of bytes to use for String indexes',         '[Optional] Bool to disable or enable document upsert'); ``` Full Pig sample can be found [here](https://github.com/Azure/azure-documentdb-hadoop/blob/master/samples/Pig_Tutorial.pig).  ## Remarks * When outputting to DocumentDB, your output collection will require capacity for an [additional stored procedure](http://azure.microsoft.com/en-us/documentation/articles/documentdb-limits/). The stored procedure will remain in your collection for reuse. * The Hadoop Connector automatically sets your indexes to range indexes with max precision on strings and numbers. More information can be found [here](http://azure.microsoft.com/en-us/documentation/articles/documentdb-indexing-policies/). * Connector supports configurable *upsert* option. *Upsert* configuration is automatically set to *true* and will overwrite documents within the same collection with the same *id*.  * Reads and writes to DocumentDB will be counted against your provisioned throughput for each collection. * Output to DocumentDB collections is done in batch round robin. * Connector supports configurable *offer* option. *Offer* configuration allows users to set the [performance tier](http://azure.microsoft.com/en-us/documentation/articles/documentdb-performance-levels/) of their newly creation collections (this does not apply when outputting to an already existing collection). * Connector supports output to partitioned collections. Hadoop Connector **will not** automatically create partitioned collections for Hadoop job outputs.  ## Need Help?  Be sure to check out the Microsoft Azure [Developer Forums on MSDN](https://social.msdn.microsoft.com/forums/azure/en-US/home?forum=AzureDocumentDB) or the [Developer Forums on Stack Overflow](http://stackoverflow.com/questions/tagged/azure-documentdb) if you have trouble with the provided code. Also, check out our [tutorial](http://azure.microsoft.com/en-us/documentation/articles/documentdb-run-hadoop-with-hdinsight/) for more information.  ## Contribute Code or Provide Feedback  If you would like to become an active contributor to this project please follow the instructions provided in [Azure Projects Contribution Guidelines](http://azure.github.io/guidelines.html).  If you encounter any bugs with the library please file an issue in the [Issues](https://github.com/Azure/azure-documentdb-hadoop/issues) section of the project.  ## Learn More * [DocumentDB with HDInsight Tutorial](https://azure.microsoft.com/documentation/articles/documentdb-run-hadoop-with-hdinsight/) * [Official Hadoop Documentation](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html) * [Azure Developer Center](http://azure.microsoft.com/en-us/develop/java/) * [Azure DocumentDB Service](http://azure.microsoft.com/en-us/documentation/services/documentdb/) * [Azure DocumentDB Team Blog](http://blogs.msdn.com/b/documentdb/)
lintool/clueweb	ClueWeb Tools =============  Hadoop tools for manipulating ClueWeb collections, the most recent of which is [ClueWeb12 collection](http://lemurproject.org/clueweb12/).  Sign up to the mailing list at [the clueweb-list@cwi.nl mailman page](https://lists.cwi.nl/mailman/listinfo/clueweb-list).  Getting Stated --------------  You can clone the repo with the following command:  ``` $ git clone git://github.com/lintool/clueweb.git ```   Once you've cloned the repository, build the package with Maven:  ``` $ mvn clean package appassembler:assemble ```  Two notes:  + `appassembler:assemble` automatically generates a few launch scripts for you. + in addition to the normal jar (`clueweb-tools-0.X-SNAPSHOT.jar`), this package uses the [Maven Shade plugin](http://maven.apache.org/plugins/maven-shade-plugin/) to create a "fat jar" (`clueweb-tools-0.X-SNAPSHOT-fatjar.jar`) that includes all dependencies except for Hadoop, so that the jar can be directly submitted via `hadoop jar ...`.  To automatically generate project files for Eclipse:  ``` $ mvn eclipse:clean $ mvn eclipse:eclipse ```  You can then use Eclipse's Import "Existing Projects into Workspace" functionality to import the project.  Counting Records ----------------  For sanity checking and as a "template" for other Hadoop jobs, the package provides a simple program to count WARC records in ClueWeb12:  ``` hadoop jar target/clueweb-tools-0.X-SNAPSHOT-fatjar.jar \  org.clueweb.clueweb12.app.CountClueWarcRecords -input /path/to/warc/files/ ```  Examples of `/path/to/warc/files/` are:  + `/data/private/clueweb12/Disk1/ClueWeb12_00/*/*.warc.gz`: for a single ClueWeb12 segment + `/data/private/clueweb12/Disk1/ClueWeb12_*/*/*.warc.gz`: for an entire ClueWeb12 disk + `/data/private/clueweb12/Disk[1234]/ClueWeb12_*/*/*.warc.gz`: for all of ClueWeb12  Building a Dictionary ---------------------  The next step is to build a dictionary that provides three capabilities:  + a bidirectional mapping from terms (strings) to termids (integers) + lookup of document frequency (*df*) by term or termid + lookup of collection frequency (*cf*) by term or termid  To build the dictionary, we must first compute the term statistics:  ``` hadoop jar target/clueweb-tools-0.X-SNAPSHOT-fatjar.jar \  org.clueweb.clueweb12.app.ComputeTermStatistics \  -input /data/private/clueweb12/Disk1/ClueWeb12_00/*/*.warc.gz \  -output term-stats/segment00 ```  By default, the program throws away all terms with *df* less than 100, but this parameter can be set on the command line. The above command compute term statistics for a segment of ClueWeb12. It's easier to compute term statistics segment by segment to generate smaller and more manageable Hadoop jobs.  Compute term statistics for all the other segments in the same manner.  Next, merge all the segment statistics together:  ``` hadoop jar target/clueweb-tools-0.X-SNAPSHOT-fatjar.jar \  org.clueweb.clueweb12.app.MergeTermStatistics \  -input term-stats/segment* -output term-stats-all ```  Finally, build the dictionary:  ``` hadoop jar target/clueweb-tools-0.X-SNAPSHOT-fatjar.jar \  org.clueweb.clueweb12.app.BuildDictionary \  -input term-stats-all -output dictionary -count 7160086 ```  You need to provide the number of terms in the dictionary via the `-count` option. That value is simply the number of output reducers from `MergeTermStatistics`.  To explore the contents of the dictionary, use this little interactive program:  ``` hadoop jar target/clueweb-tools-0.X-SNAPSHOT-fatjar.jar \  org.clueweb.clueweb12.dictionary.DefaultFrequencySortedDictionary dictionary ```  On ClueWeb12, following the above instructions will create a dictionary with 7,160,086 terms.   **Implementation details:** Tokenization is performed by first using Jsoup throw away all markup information and then passing the resulting text through Lucene's `StandardAnalyzer`.  The dictionary has two components: the terms are stored as a front-coded list (which necessarily means that the terms must be sorted); a monotone minimal perfect hash function is used to hash terms (strings) into the lexicographic position. Term to termid lookup is accomplished by the hashing function (to avoid binary searching through the front-coded data structure, which is expensive). Termid to term lookup is accomplished by direct accesses into the front-coded list. An additional mapping table is used to convert the lexicographic position into the (*df*-sorted) termid.   Building Document Vectors -------------------------  With the dictionary, we can now convert the entire collection into a sequence of document vectors, where each document vector is represented by a sequence of termids; the termids map to the sequence of terms that comprise the document. These document vectors are much more compact and much faster to scan for processing purposes.  The document vector is represented by the interface `org.clueweb.data.DocVector`. Currently, there are two concrete implementations:  + `VByteDocVector`, which uses Hadoop's built-in utilities for writing variable-length integers (what Hadoop calls VInt). + `PForDocVector`, which uses PFor compression from Daniel Lemire's [JavaFastPFOR](https://github.com/lemire/JavaFastPFOR/) package.  To build document vectors, use either `BuildVByteDocVectors` or `BuildPForDocVectors`:  ``` hadoop jar target/clueweb-tools-0.X-SNAPSHOT-fatjar.jar \  org.clueweb.clueweb12.app.Build{VByte,PFor}DocVectors \  -input /data/private/clueweb12/Disk1/ClueWeb12_00/*/*.warc.gz \  -output /data/private/clueweb12/derived/docvectors/segment00 \  -dictionary /data/private/clueweb12/derived/dictionary \  -reducers 100 ```  Once again, it's advisable to run on a segment at a time in order to keep the Hadoop job sizes manageable. Note that the program run identity reducers to repartition the document vectors into 100 parts (to avoid the small files problem).  The output directory will contain `SequenceFile`s, with a `Text` containing the WARC-TREC-ID as the key. For VByte, the value will be a `BytesWritable` object; for PFor, the value will be an `IntArrayWritable` object.  To process these document vectors, either use `ProcessVByteDocVectors` or `ProcessPForDocVectors` in the `org.clueweb.clueweb12.app` package, which provides sample code for consuming these document vectors and converting the termids back into terms.  Size comparisons, on the entire ClueWeb12 collection:  + 5.54 TB: original compressed WARC files + 1.08 TB: repackaged as `VByteDocVector`s + 0.86 TB: repackaged as `PForDocVector`s + ~1.6 TB: uncompressed termids (collection size is ~400 billion terms)  License -------  Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0
pranab/sifarish	## Introduction Sifarish is a suite of solutions for recommendation personalization implementaed on  Hadoop and Storm. Various  algorithms, including  feature similarity based recommendation  and collaborative filtering based recommendation using social rating data are available  ## Philosophy * Providing complete business solutions, not just bunch of machine learning algorithms * Simple to use * Input output in CSV format * Metadata defined in simple JSON file * Extremely configurable with tons of configuration knobs  ## Getting Started Please read ../resource/GentleIntroductionToSifarish.docx for a high level introduction and overview. The various tutorial documents in the resource directory are useful for running different example use cases.  ## Blogs The following blogs of mine are good source of details of sifarish. These are the only source of detail documentation  * http://pkghosh.wordpress.com/2011/10/26/similarity-based-recommendation-basics/ * http://pkghosh.wordpress.com/2011/11/28/similarity-based-recommendation-hadoop-way/ * http://pkghosh.wordpress.com/2011/12/15/similarity-based-recommendation-text-analytic/ * http://pkghosh.wordpress.com/2012/04/21/socially-accepted-recommendation/ * http://pkghosh.wordpress.com/2010/10/19/recommendation-engine-powered-by-hadoop-part-1/ * http://pkghosh.wordpress.com/2010/10/31/recommendation-engine-powered-by-hadoop-part-2/ * http://pkghosh.wordpress.com/2012/12/31/get-social-with-pearson-correlation/ * http://pkghosh.wordpress.com/2012/09/03/from-item-correlation-to-rating-prediction/ * http://pkghosh.wordpress.com/2014/02/10/from-explicit-user-engagement-to-implicit-product-rating/ * http://pkghosh.wordpress.com/2014/04/14/making-recommendations-in-real-time/ * http://pkghosh.wordpress.com/2014/05/26/popularity-shaken/ * http://pkghosh.wordpress.com/2014/06/23/novelty-in-personalization/ * http://pkghosh.wordpress.com/2014/09/10/realtime-trending-analysis-with-approximate-algorithms/ * http://pkghosh.wordpress.com/2014/12/22/positive-feedback-driven-recommendation-rank-reordering/ * https://pkghosh.wordpress.com/2015/01/20/diversity-in-personalization-with-attribute-diffusion/ * https://pkghosh.wordpress.com/2015/03/22/customer-service-and-recommendation-system/  ## Content Similarity Based Recommendation In the absence of social rating data, the only options is a feature similarity  based recommendation. Similarity is calculated based on distance between entities  in a multi dimensional feature space. Some examples are - recommending jobs based  on user's resume - recommending products based on user profile. These solutions are known as content based recommendation, because it's based innate  features of some entity.  There are two different solutions as follows 1. Similarity between entities of different types (e.g. user profile and product) 2. Similarity between entities of same type (e.g. product)  Attribute meta data is defined in a json file. Both entities need not have the  same set of attributes. Mapping between attributes values from one entity to  the other can be defined in the config file.  The data type supported are numerical (integer), categorical, text, geo location, time. The  distance algorithms  can be chosed to be euclidian, manhattan or minkowski. The default algorithm  is euclidian.   The distancs between different atrributes of different types are combined to find distance between  two entity instances. Different weights can be assigned to the attributes to control the relative  importance of different attributes.  The tutorial ../resource/product_similarity_tutorial.txt is a good starting point. The relevant blogs are useful to understand the inner workings.   ## Social Interaction Data Based Recommendation These solutions are based user behavior data with respect to some product  or service. these algorithms are also known as collaborative filtering.    User behavior data is defined in terms of some explicit rating by user  or it's derived from user  behavior in the site. The essential  input to all these algorithms  is a matrix of user and items. The value for a cell could be the ratingas an integer. It could  also be boolean,  if the user's interest in an item is expressed as a boolean  The tutorial ../resource/tutorial.txt is a good starting point. The relevant blogs are useful  to understand the inner workings.   ## Cold Starting Recommenders These solutions are used when enough social data is not avaialable.   1. If data contains text attributes, use TextAnalyzer MR to convert text to token stream     using lucene 2. Find similar items based on user profile. Use DiffTypeSimilarity MR 3. Use TopMatches MR to find top n matches for a profile   ## Warm Starting Recommenders When limited amount of user behavior data is available, these solutuions are appropriate  1. If data contains text attributes, use TextAnalyzer MR to convert text to token stream     using lucene 2. Find similar items by pairing items with one another using SameTypeSimilarity MR 3. Use TopMatches MR to find top n matches for a product   ## Recommenders with Fully engaged Users When significant of user behavior data is available, these soltions can be used. In  the order of  complexity, the choices are as follows. They are all based on social data  There two phases for collaborative filetering based recommendation using social data 1. Find correlation between items 2. Predict rating based on items alreadyv rated and  result of 1  The process involved running multiple map reduce jobs. Some of them are optional. Please refer to the  tutorial document tutorial.txt in the resource directory   ## Real Time Recommendation Recommendations can be made real time based on user's current behavior in a pre defined time window. The solution is based on Storm, although Hadoop gets used to compute item correlation matrix from historical user behavior data.  ## Text Attribute For content based recommendation being able to find match between text field is an important factor. Text attributes are stemmed or normalized with Apache Lucene. Various languages, in addition to default of english are supported. They are german, french, italian, spanish, polish and brazilian portuguese. Text matching algorithms supported are cosine, jaccard and semantic. For semantic matching, RDF semantic graph is used   ## Complex Attributes For content based recommendation, There is  support for structured fields e.g., Location, Time Window,  Event, Categorized Item etc. Many of these  provide contextual dimensions to recommendation. They  are particularly relevant for recommendation in the mobile space  ## Novelty  Novelty for an item can be computed at individual user level or the whole user community as a whole. Novelty is blended into the final recommendation list by taking weighted average of predicted rating and novelty    ## Diversilty  Based on recent work in the academic world, I am working on implementing some  algorithms to introduce   diversity in recommendation. Unlike novelty, diversity is group wise property. Diversity can be defined either in terms item dissimilarity in a collaborative filtering sense or structural and content  sense  ## Facted Match For content based recommendation, faceted match is supported as faceted search in Solr. Faceted fields are specified through a configuration parameter  ## Dithering Dithering effectively handles the problem users usually not browsing the first few items in a list. The dithering process shuffles the list little bit, every time recommended items  are presented to the user.   ## Getting started Please use the tutorial.txt file in the resource directory for batch mode recommendation  processing. For real time recommendation please use the tutorial document there is a separate tutorial document realtime\_recommendation\_tutorial.txt  ## Integration with other recommndation systems If you use Apache mahout or some thing else for recommendation, you can bring your basic recommendation output (userID, itemID, predictedRating) to sifarish for additional postprocessing to improve the quality of the output. They are listed in the next section.  ## Post processing plugins Just accuracy from the CF algorithm is not enough for a good recommender. There are various post processing plugins are essential. They improve the quality of results.  Here is the list. Sifarsh supports most them. Some are under development.  * Business goal injection  * Adding novelty  * Adding diversity  * Rank reordering for explicit positive feedback  * Rank reordering for implicit negative feedback  * Dithering  ## Configuration Please refer to the wiki page for a detailed list of all configuration parameters https://github.com/pranab/sifarish/wiki/Configuration. Going through the tutorial documents in the resource directory, you can find sample configuration for various use cases.  ## Build Please read jar\_dependency.txt in the resource directory for build and run time dependency  For Hadoop 1 * mvn clean install  For Hadoop 2 (non yarn), use the branch nuovo * git checkout nuovo * mvn clean install  For Hadoop 2 (yarn), use the branch nuovo * git checkout nuovo * mvn clean install -P yarn  ## Help Please feel free to email me at pkghosh99@gmail.com  ## Contribution Contributors are welcome. Please email me at pkghosh99@gmail.com
Teradata/kylo	[![alt Kylo](https://cloud.githubusercontent.com/assets/5693584/22863033/4976d7d2-f0ee-11e6-95ec-3a30e2162a3c.png)](http://kylo.io/)  Kylo is an enterprise-ready data lake management software platform for Hadoop and Spark integrating best practices around metadata management, governance, and security learned from over Think Big's 150+ successful big data projects.  Visit [http://kylo.io](http://kylo.io) and unlock the power of Kylo today!  ## Quick Start  You can download a pre-configured sandbox and get started with Kylo in no time. To get started visit the [Quick Start](http://kylo.io/quickstart.html) page.  ## Documentation  Please visit [Kylo documentation](http://kylo.readthedocs.io/) to learn more.    ## Issues and Support  To raise issues with Kylo, please register and visit the [Jira instance](https://kylo-io.atlassian.net/projects/KYLO).  For support, questions can be asked on the [Google Groups group](https://groups.google.com/forum/#!forum/kylo-community).  ## Code Structure  The layout of code in the following subtrees follows particular reasoning described below:   | Subfolder        | Description           | | ------------- |-------------| | [commons](commons) |  Utility or common functionality | [core](core) | API frameworks that can generally used by developers to extend the capabilities of Kylo | [docs](docs) | Documentation that should be distributed as part of release | [integrations](integrations) | Pure integration projects with 3rd party software such as NiFi and Spark.  | [metadata](metadata) | The metadata server is a top-level project for providing a metadata repository | [plugins](plugins) | Alternative and often optional implementations for operating with different distributions or technology branches | [samples](samples) | Sample plugins, feeds, and templates, | [security](security) | Support for application security for both authentication and authorization | [services](services) | Provides REST endpoints and core server-side processing | [ui](ui) | User Interface module for Kylo
lintool/Cloud9	Cloud9 ======  A Hadoop toolkit for working with big data: http://cloud9lib.org/  As of December 2015, this library is no longer being actively developed or maintained. Please see http://bespin.io/ for its replacement. Why? The Cloud9 codebase dates back to 2007 and has accumulated a lot of cruft; it's time to start over with a blank slate.
Esri/spatial-framework-for-hadoop	[![Build Status](https://travis-ci.org/Esri/spatial-framework-for-hadoop.png?branch=master)](https://travis-ci.org/Esri/spatial-framework-for-hadoop) # spatial-framework-for-hadoop  The __Spatial Framework for Hadoop__ allows developers and data scientists to use the Hadoop data processing system  for spatial data analysis.  For tools, [samples](https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples), and [tutorials](https://github.com/Esri/gis-tools-for-hadoop/wiki) that use this framework, head over  to [GIS Tools for Hadoop](https://github.com/Esri/gis-tools-for-hadoop).  ## What's New  * GeoJSON support: Hadoop InputFormat and Hive SerDe for GeoJSON as well as for Esri GeoServices REST JSON (in v1.2.0 release). * ST_Geometry works under SparkSQL as well as under Hive itself (the new part is the JSON SerDe classes working under SparkSQL - pretty much everything else had already worked).  ## Features  * **[JSON Utilities](https://github.com/Esri/spatial-framework-for-hadoop/wiki/JSON-Utilities)** - Utilities  for interacting with JSON exported from ArcGIS  * [Javadoc](http://esri.github.com/spatial-framework-for-hadoop/json/) * **[Hive Spatial](https://github.com/Esri/spatial-framework-for-hadoop/wiki/Hive-Spatial)** - User-Defined  Functions and SerDes for spatial analysis in Hive  * [UDF Documentation](https://github.com/Esri/spatial-framework-for-hadoop/wiki/UDF-Documentation)  * [JSON SerDe](https://github.com/Esri/spatial-framework-for-hadoop/wiki/Hive-JSON-SerDe)  ## Getting Started  ### Maven  Build as you would any other Mavenized repository.  All dependencies are pulled automatically.   ### Ant  Ant build files are also available  At the root level of this repository, you can build a single jar with everything in the framework  using [Apache Ant](http://ant.apache.org/).  Alternatively, you can build a jar at the root level of each  framework component (i.e., `hive/build.xml`).  The build files use [Maven Ant Tasks](http://maven.apache.org/ant-tasks/download.html) for dependency  management. You will need the jar in a place Ant can find it (i.e., `~/.ant/lib/maven-ant-tasks-2.1.3.jar`).   ## Dependencies  * [Esri Geometry API for Java](https://github.com/Esri/geometry-api-java) - Java geometry library for spatial data  processing.  ## Requirements  * Hive 0.9.0 and above (see [Hive Compatibility issues](https://github.com/Esri/spatial-framework-for-hadoop/wiki/ST_Geometry-for-Hive-Compatibility-with-Hive-Versions)) * Workflows calling MapReduce jobs require the location of the custom job to be run. * Custom MapReduce jobs that use the Esri Geometry API require that the developer has authored the job,  (referencing the com.esri.geometry.\* classes), and deployed the job Jar file to the Hadoop system, prior to the  ArcGIS user submitting the workflow file.   ## Resources  * [GeoData Blog on the ArcGIS Blogs](http://blogs.esri.com/esri/arcgis/author/jonmurphy/) * [Big Data Place on GeoNet](https://geonet.esri.com/groups/big-data) * [ArcGIS Geodata Resource Center]( http://resources.arcgis.com/en/communities/geodata/) * [ArcGIS Blog](http://blogs.esri.com/esri/arcgis/) * [twitter@esri](http://twitter.com/esri)  ## Issues  Find a bug or want to request a new feature?  Please let us know by submitting an issue.  ## Contributing  Esri welcomes contributions from anyone and everyone. Please see our [guidelines for contributing](https://github.com/esri/contributing)  ## Licensing Copyright 2013-2017 Esri  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.  A copy of the license is available in the  repository's [license.txt](https://raw.github.com/Esri/spatial-framework-for-hadoop/master/license.txt) file.
alexholmes/hadoop-book	Source code for book "Hadoop in Practice", Manning Publishing =============================================================  ## Overview  This repo contains the code, scripts and data files that are referenced from the book [Hadoop in Practice](http://www.manning.com/holmes/), published by Manning.  ##  Issues  If you hit any compilation or execution problems please create an issue and I'll look into it as soon as I can.  ## Hadoop Version  All the code has been exercised against CDH3u2, which for the purposes of the code is the same has Hadoop 0.20.x.  There are a couple of places where I utilize some features in Pig 0.9.1, which won't work with CDH3u1 which uses 0.8.1.  I've recently run some basic MapReduce jobs against CDH4, and I also updated the examples so that they would run against Hadoop 2. Please let me know [on the Manning forum](http://www.manning-sandbox.com/forum.jspa?forumID=800) or in a [GitHub ticket](https://github.com/alexholmes/hadoop-book/issues) if you encounter any issues.   ## Building and running  ####  Download from github  <pre><code>git clone git://github.com/alexholmes/hadoop-book.git </code></pre>  ####  Build  <pre><code>cd hadoop-book mvn package </code></pre>  #### Runtime Dependencies  Many of the examples use Snappy and LZOP compression.  Therefore you may get runtime errors if you don't have them installed and configured in your cluster.  Snappy can be installed on CDH by following the instructions at https://ccp.cloudera.com/display/CDHDOC/Snappy+Installation.  To install LZOP follow the instructions at https://github.com/kevinweil/hadoop-lzo.  ####  Run an example  <pre><code># copy the input files into HDFS hadoop fs -mkdir /tmp hadoop fs -put test-data/ch1/* /tmp/  # replace the path below with the location of your Hadoop installation # this isn't required if you are running CDH3 export HADOOP_HOME=/usr/local/hadoop  # run the map-reduce job bin/run.sh com.manning.hip.ch1.InvertedIndexMapReduce /tmp/file1.txt /tmp/file2.txt output </code></pre>
ogrisel/pignlproc	# pignlproc  Apache Pig utilities to build training corpora for machine learning / NLP out of public Wikipedia and DBpedia dumps.  ## Project status  This project is alpha / experimental code. Features are implemented when needed.  Some preliminary results are available in this blog post:    * [Mining Wikipedia with Hadoop and Pig for Natural Language Processing](http://www.nuxeo.com/blog/mining-wikipedia-with-hadoop-and-pig-for-natural-language-processing/)  ## Building from source  Install maven (tested with 2.2.1) and java jdk 6, then:      $ mvn assembly:assembly  This should download the dependencies, build a jar in the target/ subfolder and run the tests.  ## Usage  The following introduces some sample scripts to demo the User Defined Functions provided by pignlproc for some practical Wikipedia mining tasks.  Those examples demo how to use pig on your local machine on sample files. In production (with complete dumps) you might want to startup a real Hadoop cluster, upload the dumps into HDFS, adjust the above paths to match your setup and remove the '-x local' command line parameter to tell pig to use your Hadoop cluster.  The [pignlproc wiki](https://github.com/ogrisel/pignlproc/wiki) provides comprehensive documentation on where to download the dumps from and how to setup a Hadoop cluster on EC2 using [Apache Whirr]( http://incubator.apache.org/whirr).  ### Extracting links from a raw Wikipedia XML dump  You can take example on the extract-links.pig example script:      $ pig -x local \       -p PIGNLPROC_JAR=target/pignlproc-0.1.0-SNAPSHOT.jar \       -p LANG=fr \       -p INPUT=src/test/resources/frwiki-20101103-pages-articles-sample.xml \       -p OUTPUT=/tmp/output \       examples/extract_links.pig  ### Building a NER training / evaluation corpus from Wikipedia and DBpedia  The goal of those samples scripts is to extract a pre-formatted corpus suitable for the training of sequence labeling algorithms such as MaxEnt or CRF models with [OpenNLP](http://incubator.apache.org/opennlp), [Mallet](http://mallet.cs.umass.edu/) or [crfsuite](http://www.chokkan.org/software/crfsuite/).  To achieve this you can run time following scripts (splitted into somewhat independant parts that store intermediate results to avoid recomputing everything from scratch when you can the source files or some parameters.  The first script parses a wikipedia dump and extract occurrences of sentences with outgoing links along with some ordering and positioning information:      $ pig -x local \       -p PIGNLPROC_JAR=target/pignlproc-0.1.0-SNAPSHOT.jar \       -p LANG=en \       -p INPUT=src/test/resources/enwiki-20090902-pages-articles-sample.xml \       -p OUTPUT=workspace \       examples/ner-corpus/01_extract_sentences_with_links.pig  The parser has been measured to run at a processing of 1MB/s on in local mode on a MacBook Pro of 2009.  The second script parses dbpedia dumps assumed to be in the folder /home/ogrisel/data/dbpedia:      $ pig -x local \       -p PIGNLPROC_JAR=target/pignlproc-0.1.0-SNAPSHOT.jar \       -p LANG=en \       -p INPUT=/home/ogrisel/data/dbpedia \       -p OUTPUT=workspace \       examples/ner-corpus/02_dbpedia_article_types.pig  This step should complete in a couple of minutes in local mode.  This script could be adapted / replaced to use other typed entities knowledge bases linked to Wikipedia with downloadable dumps in NT or TSV formats; for instance: [freebase](http://freebase.com) or [Uberblic](http://uberblic.org).  The third script merges the partial results of the first two scripts and order back the results by grouping the sentences of the same article together to be able to build annotated sentences suitable for OpenNLP for instance:      $ pig -x local \       -p PIGNLPROC_JAR=target/pignlproc-0.1.0-SNAPSHOT.jar \       -p INPUT=workspace \       -p OUTPUT=workspace \       -p LANG=en \       -p TYPE_URI=http://dbpedia.org/ontology/Person \       -p TYPE_NAME=person \       examples/ner-corpus/03bis_filter_join_by_type_and_convert.pig      $ head -3 workspace/opennlp_person/part-r-00000     The Table Talk of <START:person> Martin Luther <END> contains the story of a 12-year-old boy who may have been severely autistic .     The New Latin word autismus ( English translation autism ) was coined by the Swiss psychiatrist <START:person> Eugen Bleuler <END> in 1910 as he was defining symptoms of schizophrenia .     Noted autistic <START:person> Temple Grandin <END> described her inability to understand the social communication of neurotypicals , or people with normal neural development , as leaving her feeling "like an anthropologist on Mars " .   ### Building a document classification corpus  TODO: Explain howto extract bag of words or ngrams and document frequency features suitable for document classification using a SGD model from [Mahout](http://mahout.apache.org) for instance.   ## License  Copyright 2010 Nuxeo and contributors:    Licensed under the Apache License, Version 2.0 (the "License");   you may not use this file except in compliance with the License.   You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software   distributed under the License is distributed on an "AS IS" BASIS,   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   See the License for the specific language governing permissions and   limitations under the License.
claudiomartella/dbpedia4neo	A set of hacks to setup a dbpedia endpoint through neo4j
dbpedia/links	DBpedia-Links ============= A repo that contains links and alternative classifications for DBpedia. Other database owners can contribute links into the links folder. The link framework is run each day and validates all link contributions. An overiew and current errors can be seen at the [LinkViz](http://dbpedia.github.io/links/tools/linkviz/).    # About  Links are the key enabler for retrieval of related information on the Web of Data and DBpedia is one of the central interlinking hubs in the Linked Open Data (LOD) cloud. The DBpedia-Links repository maintains linksets between DBpedia and other LOD datasets. System for maintenance, update and quality assurance of the linksets are in place and can be explored further.  In this README, we will include descriptions on how to download and use the links, run all available tools as well as pointers to the most important documentation. if questions remain please use the [GitHub Issue tracker](http://github.com/dbpedia/dbpedia-links/issues). If you want to give us more feedback, feel free to use the [DBpedia Discussion mailinglist](http://lists.sourceforge.net/lists/listinfo/dbpedia-discussion).  ## Why upload your links? All links you are contributing will be loaded (after a quality check) into the main DBpedia datasets and therefore will link to your data. Users of DBpedia can then better find your data. Also we will be able to tell you, which other external databases link to your data.   # Repository license All data in the repository links folder is provided as CC-0. All software is provided under Apache 2.0 License.  Please cite our [paper](http://ceur-ws.org/Vol-1695/paper21.pdf) : ``` @inproceedings{DojchinovskiDBpediaLinks,   author = {Dojchinovski, Milan and Kontokostas, Dimitris and R{\"o}ßling, Robert and Knuth, Magnus and Hellmann, Sebastian},   booktitle = {Proceedings of the SEMANTiCS 2016 Conference (SEMANTiCS 2016)},   title = {DBpedia Links: The Hub of Links for the Web of Data},   year = 2016 } ```  # How to contribute links to DBpedia? If you're interested in contributing links and to learn more about the project, please visit the [how to wiki page](https://github.com/dbpedia/links/wiki/How-To-Contribute-Links-to-DBpedia) for more detailed informations.   # How to create/update links for one dataset? If you want to update links for one dataset, either create a new folder or update/patch an existing linkset and send a new pull request. Please follow the [how to](https://github.com/dbpedia/links/wiki/How-To-Contribute-Links-to-DBpedia) to learn more about how to create a patch for the dataset which will be applied automatically on the next release.  To make sure that your dataset is following proper conventions as mentioned in the [how to](https://github.com/dbpedia/links/wiki/How-To-Contribute-Links-to-DBpedia). Below are instructions to run the framework and validation for your contributed folder before sending the pull request.    # How to download the monthly link release If you want to download the current, or older, releases of the given links, please go [here](http://downloads.dbpedia.org/links/) and click at the corresponding month.  # How to download the daily link snapshot  The publishing process is automated via a cronjob which will run all given downloads, scripts, LIMES/SILK configurations, patches, etc. to generate the linksets. It is executed daily on our own server and published (http://downloads.dbpedia.org/links/snapshot)  Please check out the [how to](https://github.com/dbpedia/links/wiki/How-To-Contribute-Links-to-DBpedia#automated-process) for more informations regarding the automated process, how to set it up, run it and customize it.    # Overview of current linksets An overiew and current errors can be seen at the [LinkViz](http://dbpedia.github.io/links/tools/linkviz/).   # How to run the link extraction framework  ## Install ``` mvn clean install  # tests are deactivated by default, tests will test the links, so activating tests will make a full run of the software  mvn clean install -DskipTests=true ```  ## Running ``` # create a snapshot mvn exec:java -Dexec.mainClass="org.dbpedia.links.CLI" -Dexec.args="--generate"  # create a snapshot, also running scripts (increase runtime, immensely) mvn exec:java -Dexec.mainClass="org.dbpedia.links.CLI" -Dexec.args="--generate --scripts true"  # run everything for one folder, e.g. your contributed link folder: mvn exec:java -Dexec.mainClass="org.dbpedia.links.CLI" -Dexec.args="--generate --scripts true --basedir links/dbpedia.org/YOUR_PROJECT" ```  # description of further tools in the repo and how to access/execute them  * `backlinks.py` This script can be executed via python 3 (please note that [rdflib](https://github.com/RDFLib/rdflib) needs to be installed). On start it will prompt for a full folder path, please insert the full path to the linkset destinated main folder. All n-triple files will be read in there and compared to every other linkset within the `links` folder and check if certain subjects within the given linkset are contained in other linksets. All the triples will be stored within a `backlinks.nt` file   # Contributors (alphabetically)  - Sarven Capadisli, AKSW, Uni Leipzig (Csarven) - Pascal Christoph, (dr0i) - Christopher Gutteridge (cgutteridge) - Amy Guy, BBC / Uni Edinburgh (rhiaro) - Sebastian Hellmann, AKSW, Uni Leipzig (kurzum) - Anja Jentzsch, HPI Potsdam (ajeve) - Barry Norton (BarryNorton) - Heiko Paulheim, Uni Mannheim (HeikoPaulheim) - Petar Ristoski, Uni Mannheim (petarR) - Robert Rößling, AKSW, Uni Leipzig (rpod) - Søren Roug, (sorenroug)
skate056/spring-security-oauth2-google	# Spring Security OAuth2 Google Connector  --- ## What do I do?  I am simple Spring MVC application which secured by Spring Security. Instead of using simple form based security, I am secured by Spring Security OAuth2 and the OAuth provider is Google.   ## Why am I required? Developing a Spring MVC web application with Security enabled and integrated with Google is hard to implement. So this is a sample implementation of Spring MVC + Spring Security OAuth2 + Google Provider.  ## Get it up and runnning The project is built with Gradle so you can run the build using the following gradle command `gradle clean build`  To start up the application using the following command (Since it is a Spring boot application) `gradle bootRun`  The application.properties (in src/main/resources) contains the details of the Google application which it uses to authenticate details. Change the values of the following attributes to the values for your application google.client.id google.client.secret  ###To register a Google App perform the following steps * Go to https://console.developers.google.com and login with your Google account (this will be the developer account and the email of  this account will be published when someone tries to authenticate with the Google application) * If you don't have a project create a new one and then click into the project. * In the menu on the left side select "APIs & auth" --> "Credentials" --> "Create a new client ID" * In the popup select the following   * Application Type = Web Application   * Authorized Javascript Origins = <YOUR DOMAIN>,    * Authorized Redirect URI = <THE CALL BACK HANDLER>, the URI for our application is /googleLogin so for local testing you should enter    http://localhost:9000/googleLogin and http://localhost:9000/googleLogin/ on different lines.   * Copy the client ID and client Secret and update the application.properties  * Make sure you update the mandatory values on the "APIs & auth" --> "Consent screen" page as the application will not work without it.   When you have a the Google App configured and the Spring boot application and you navigate to http://localhost:9000. It will redirect  you to a Google login page. Upong login it will ask you to authorize your application for access to your account to get email and profile  data. On successful login it will render the basic HTML page which means the authentication was sucessful.   If you are interested in fetching the Google profile data in type in the following URL http://localhost:9000/find?term=hello in the   authenticated browser session and the profile data will be logged in the console of the Spring application.   ## Technical nitty gritties  It is important to use the correct filters from Spring Security OAuth2 to get this right and it is also very important to get their   order right.    The filters oauth2ClientContextFilter and oAuth2AuthenticationProcessingFilter are very important the their position in the chain is   equally important. oauth2ClientContextFilter must exist before oAuth2AuthenticationProcessingFilter. Also both these must be before the   FILTER_SECURITY_INTERCEPTOR so that the authentication with google is complete before we check for the security permissions for access to  the application.   Also the authentication entry point of the secured application is clientAuthenticationEntryPoint which essentially redirects the user to   the configured url /googleLogin when the user is not authenticated. When the url /googleLogin is hit the filters listed above lead to a   redirect to the Google URL. Upon authentication, Google redirects back to us and the filter populates the Authentication object.   After all the authentication is complete, the request is redirected back to http://localhost:9000. This request will hit the   FILTER_SECURITY_INTERCEPTOR but the session is already authenticated and the Security Context is populated so access to the application   is granted by the interceptor and the session is authentication.   If you want to see the profile data, in an authenticated session (since the resource uses the OAuth2RestOperations) a GET to   http://localhost:9000/find?term=hello will display the profile data on the web application console. See the   src/main/java/com/rst/oauth2/google/api/UserResource.java for implementation details.
neel4software/SpringSecurityOAuth2	SpringSecurityOAuth2 ====================  Securing Restful Web Services with Spring Security and OAuth2  <br/><br/><br/> The flow of application will go something like this:<br/><br/> 1) User sends a GET request to server with five parameters: grant_type,  username, password, client_id, client_secret;  something like this <br/> <a href="http://localhost:8080/SpringRestSecurityOauth/oauth/token?grant_type=password&client_id=restapp&client_secret=restapp&username=beingjavaguys&password=spring@java">http://localhost:8080/SpringRestSecurityOauth/oauth/token?grant_type=password&client_id=restapp&client_secret=restapp&username=beingjavaguys&password=spring@java</a>  <br/><br/>  2) Server validates the user with help of spring security, and if the user is authenticated, OAuth generates a access token and send sends back to user in following format.<br/> {<br/> "access_token": "22cb0d50-5bb9-463d-8c4a-8ddd680f553f",<br/> "token_type": "bearer",<br/> "refresh_token": "7ac7940a-d29d-4a4c-9a47-25a2167c8c49",<br/> "expires_in": 119<br/> }<br/><br/>  Here we got access_token for further communication with server or to get some protected resourses(API’s), it mentioned a expires_in time that indicates the validation time of the token and a refresh_token that is being used to get a new token when token is expired.<br/><br/> 3) We access protected resources by passing this access token as a parameter, the request goes something like this:<br/><br/> <a href="http://localhost:8080/SpringRestSecurityOauth/api/users/?access_token=8c191a0f-ebe8-42cb-bc18-8e80f2c4238e">http://localhost:8080/SpringRestSecurityOauth/api/users/?access_token=8c191a0f-ebe8-42cb-bc18-8e80f2c4238e</a><br /> Here http://localhost:8080/SpringRestSecurityOauth is the server path, and  /api/users/ Is an API  URL that returns a list of users and is being protected to be accessed. <br/><br/> 4) If the token is not expired and is a valid token, the requested resources will be returned.<br/><br/> 5) In case the token is expired, user needs to get a new token using its refreshing token that was accepted in step(2). A new access token request after expiration looks something like this:<br/> <a href="http://localhost:8080/SpringRestSecurityOauth/oauth/token?grant_type=refresh_token&client_id=restapp&client_secret=restapp&refresh_token=7ac7940a-d29d-4a4c-9a47-25a2167c8c49">http://localhost:8080/SpringRestSecurityOauth/oauth/token?grant_type=refresh_token&client_id=restapp&client_secret=restapp&refresh_token=7ac7940a-d29d-4a4c-9a47-25a2167c8c49</a><br/><br/> And you will get a new access token along with a new refresh token.
NashLegend/SourceWall	#果壳的壳  ###应用直接下载地址: [点击下载最新版](http://app.mi.com/download/87827) ###或者去小米应用商店下载：[进入小米商店查看](http://app.mi.com/details?id=net.nashlegend.sourcewall)  目前功能最强大的果壳第三方客户端，目的是为了让壳友更方便的使用手机刷果壳。现提供果壳网三大版块（科学人、小组、问答）的全面功能以及果壳网通知。  当前版本功能点如下：  - 科学人：查看文章、评论文章、收藏文章、推荐文章，回复别人的评论，分享文章。 - 小组：发表新贴，查看我的小组、热门回贴、按小组查看贴子、倒序查看回贴、回复贴子、推荐贴子、收藏贴子、分享贴子、评论别人的回复。加载自己关注的小组，调整小组顺序。 - 果壳问答：查看问题、查看答案、回答问题、反对支持感谢问题、评论问题、评论答案、查看热门、精彩回答、分享问题、根据标签查看问答列表、管理我关注的标签。 - 通知功能：查看果壳网通知，包括被推荐、回复、点赞关注等操作。 - 保存草稿：对于未发布的贴子和评论，都会保存为草稿，但是保存草稿是本地行为，并没有使用果壳的草稿系统。 - 夜间模式：在被窝里刷果壳不会再亮瞎双眼。 - 无图模式：可以选择仅在wifi下加载图片、始终加载图片、始终不加载图片。 - 自定义尾巴：在发贴、回复贴子、回复科学人文章、回答问题的时候，默认会带一条小尾巴——来自SourceWall，你也可以设置为来自你的手机型号，也可以设置为任意自定义尾巴或者不使用尾巴。 - 搜索果壳网内容 - 查看所有的收藏 - 查看我自己发的贴子 - 查看我自己提的问题 - 查看我自己的回答 - 举报 - 匿名回贴
fedefernandez/MyAppList	My App List  ========================    This GitHub repository hosts the code for the Android app My App List    Download from: https://play.google.com/store/apps/details?id=com.projectsexception.myapplist    Building  ------------    1.Add the next sign properties to your gradle distribution (for example in ~/.gradle/gradle.properties)        storeFile=/home/user/keystore      storePassword=keystorepassword      keyAlias=keyalias      keyPassword=keypassword    2.Build the desired distribution    2.1. Play Store: `gradle clean assemblePlayRelease`    2.2. Open source: `gradle clean assembleOpenRelease`    Contributing  ------------    If you want to contribute fork the repository, code and tell me about it    Help me to translate: http://crowdin.net/project/my-app-list/invite    License  -------        Copyright 2012 Projects Exception        Licensed under the Apache License, Version 2.0 (the "License");      you may not use this file except in compliance with the License.      You may obtain a copy of the License at           http://www.apache.org/licenses/LICENSE-2.0        Unless required by applicable law or agreed to in writing, software      distributed under the License is distributed on an "AS IS" BASIS,      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.      See the License for the specific language governing permissions and      limitations under the License.
Protocoder/Protocoder	Protocoder ========== [![Build Status](https://travis-ci.org/Protocoder/Protocoder.svg?branch=develop)](https://travis-ci.org/Protocoder/Protocoder)  Protocoder is a self-contained coding environment + framework in Javascript for quick prototyping on Android devices.  Just install the app in your Android device and access the web IDE from your computer using the minicloud created in your phone. Code in javascript using the protocoder framework. No needs to write dozends of lines to access sensors or write an UI, simple to use, fast to code.  ``` //how to get sensor data sensors.accelerometer.onChange(function(x, y, z)) {  	console.log(x + " " + " " + y + " " + z);  }  //send and sms android.sendSMS(number, "text");  //play a video ui.addVideoView("fileName", 0, 0, 500, 200); ```  It uses a webserver and a websockets server inside the project to do some of the magic.  It has support of most android hardware functionality, networking using OSC and websockets, audio synthesis and processing using Pure Data though libPd, OSMmaps, IOIO support and muuuuuuuch more.  How to compile  -------------- Clone the project, import it in Android Studio, make sure the API version you are using to compile is 21 or greater.
viritin/viritin	[![Build Status](https://travis-ci.org/viritin/viritin.svg?branch=master)](https://travis-ci.org/viritin/viritin) # Viritin - The "commons" library  for Vaadin  **Note, Attention, Alert for existing users**, Maddon was recently renamed to Viritin. New Maven coordinates: org.vaadin:viritin:1.22 (or whatever the latest version). After dependency change fixing imports in broken classes should do the thing. In addition to package rename, there is one potentially breaking change. EagerValidation is now default in AbstractForm.  #### [Documentation](https://github.com/viritin/viritin/wiki) project is going on in wiki  ## Background  All large and stable libraries have their inconveniences and missing pieces. This is true for JDK libraries and just as true for Vaadin. Advanced Java users use various apache-commons libraries or Guava to fix these inconveniences in JDK libraries. This library tries to do the same for Vaadin.  The idea is to concentrate on server side Java to improvements to existing APIs, so that throwing this in to an applications or to another add-ons should be as easy as possible. Hopefully one can avoid some of stupid boilerplate code by using this library and make you more productive than ever with Vaadin.  So far improvements have been focused on three main categories:   * Core component extensions that fix wrong defaults and provide more expressive API  * Data binding improvements, better "forms" for both end users and developers  * Essential new components build with server side composition (e.g. DisclosurePanel and fields to handle collections)   Some examples of improvements:  E.g. this very common Vaadin code...  ```java // Bind fields to entity properties by naming convention BeanFieldGroup<Contact> binder = new BeanFieldGroup<Contact>(Contact.class); binder.setBuffered(false); binder.bindMemberFields(view); binder.setItemDataSource(contact); ```  ...becomes like this:  ```java BeanBinder.bind(contact, view) ```  And using basic layouts without this add-on...  ```java VerticalLayout verticalLayout = new VerticalLayout(); verticalLayout.setMargin(true); verticalLayout.setSpacing(true); verticalLayout.setHeight("100%"); HorizontalLayout horizontalLayout = new HorizontalLayout(c, d); horizontalLayout.setWidth("100%"); horizontalLayout.setMargin(false); horizontalLayout.setSpacing(true); verticalLayout.addComponents(a, b, horizontalLayout); setContent(verticalLayout); ```  ... may start to feel rather mad, after using them with sane defaults and "fluent api":  ```java setContent(new MVerticalLayout().withFullHeight().with(a, b,         new MHorizontalLayout(c, d).withFullWidth().withMargin(false))); ```  Or when using the "intelligent" expand method:  ```java         VerticalLayout wrapper = new VerticalLayout();         wrapper.setMargin(true);         wrapper.setSpacing(true);         wrapper.setHeight("100%");         HorizontalLayout toolbar = new HorizontalLayout(c, d);         toolbar.setWidth("100%");         toolbar.setMargin(false);         toolbar.setSpacing(true);         HorizontalLayout wrapper2 = new HorizontalLayout();         wrapper2.addComponent(menu);         wrapper2.addComponent(mainContent);         wrapper2.setSizeFull();         mainContent.setSizeFull();         wrapper2.setExpandRatio(mainContent, 1);         wrapper.addComponents(toolbar, wrapper2);         wrapper.setExpandRatio(wrapper2, 1);         setContent(wrapper); ```  Becomes...  ```java         setContent(new MVerticalLayout(new MHorizontalLayout(c, d).withFullWidth())                 .expand(new MHorizontalLayout(menu).expand(mainContent))); ```  Note that in the above the expand call does setExpandRation(component, 1), where 1 the value that people use 99% of the time, but it takes care of adding component and setting sane size values for the layout and the added component.   The example is optimized for the number of lines, but most often you want to uses some line breaks and indentation for better readability. In anyways, the improved API lets you express the layout so that it is much easier to understand the hierarchy of component tree from the code, without adding lots of methods or classes.   Using Table to select a row for editor may simplify from ...  ```java Table table = new Table(); BeanItemContainer<Entity> beanItemContainer = new BeanItemContainer<Entity>(Entity.class); beanItemContainer.addAll(findBeans()); table.setContainerDataSource(beanItemContainer); table.setVisibleColumns("property", "another"); table.setColumnHeaders("Property 1", "Second"); table.setSelectable(true); table.setImmediate(true); table.addValueChangeListener(new Property.ValueChangeListener() {     @Override     public void valueChange(Property.ValueChangeEvent event) {         Entity entity = (Entity) event.getProperty().getValue();         editEntity(entity);     } }); ```  to...  ```java MTable<Entity> t = new MTable(findBeans())         .withProperties("property", "another")         .withColumnHeaders("Property 1", "Second"); t.addMValueChangeListener(new MValueChangeListener<Entity>() {     @Override     public void valueChange(MValueChangeEvent<Entity> event) {         editEntity(event.getValue());     } }); ``` Note, that in the above, the event actually provides you the value you are interested directly and in with the correct type. Especially for new Vaadin users this is very helpful.  Using Label extensions can make your life both simpler and safer:  ```java layout.addComponent(new Header("Tempertures during past week:")); layout.addComponent(new RichText().withMarkDown("#How di hou [We have a link](https://vaadin.com/)!")); layout.addComponent(new RichText().withMarkDown(getClass().getResourceAsStream("/readme.md"))); layout.addComponent(new RichText().withSafeHtml(getClass().getResourceAsStream("/readme.html"))); ```  Hopefully all of these enhancements will end up in the core Vaadin library, in a one form or another.   If you have a re-usable server side Vaadin "api-hack" that you continuously use in you project to reduce your madness, please contribute it. I promise to accept pull requests sooner and with lighter process than to the core Vaadin.   Also feel free to suggest dependencies to new or possibly already existing add-ons that are similar in nature (pure server side, simple and smallish). This way you'll only need to add one helper dependency to get what you most often want to. If your dependency or addition is not small or simple, we can still consider if it is otherwise high quality (well tested, industry proven). Current dependencies:   * ConfirmDialog (used by ConfirmButton in Viritin)  If a helper extends core Vaadin class, the convention is to simply prefix it with "M" letter. Otherwise, lets just use good naming conventions and reasonable sub packages.  ## List of helpers   Currently project contains following helpers (might be slightly outdated):  * BeanBinder, [Vaadin Ticket](http://dev.vaadin.com/ticket/13068) 	* Reduces the most common usage of BeanFieldGroup into oneliner * layouts 	* Basic layouts have bad defaults (margin,spacing), corrected in these [Vaadin  Ticket](http://dev.vaadin.com/ticket/12966)  	* "fluent api" for selected methods like, margin and spacing (withMargin(false))  	* current implementations    		* MVerticalLayout    	 	* MHorizontalLayout    	 	* MGridLayout * MTextField 	* smart immediate handling (like what is coming in 7.2) 	* empty sting as null representation ([Vaadin Ticket](http://dev.vaadin.com/ticket/13221))  * Typed selects 	* Most developers use basic pojos as options and as field value. The cores Container-Item-Property with some legacy makes code look horrible in some cases. Thus these selects have 		* Generics for value/options 		* Has API for fluent usages with java.util.Collections 		* on TODO separate multi select implementations  	* Current implementations (TODO lots of essential stuff needed here!!) 		* TypedSelect 			* A single select to another entity type 			* By default NativeSelect, but implementation is configurable 		* EnumSelect 			* a zero-conf select for enums (when used with FieldGroup), based on NativeSelect    		* MTable extends Table (note that Table is often also the best real life listselect)  			* single select only but better typed for generic bean usage        	  	* in addition to typing and related methods          		* smart selection mode and immediate (based on value change listener, [Vaadin Ticket](http://dev.vaadin.com/ticket/8029) )  	     	   	* withProperties(String...), withColumnHeaders 		* MultiSelectTable 			* A Field suitable for editing varios collections of other entities (e.g. List<MyPojo> pojos), selected from a list given by developer. 			* Suitable for editing e.g. JPA ManyToMany mapped field 			* The field modifies the existing set/list, making if efficent and more compatible with your backend. The standard "multiselect mode" in core Vaadin selects always uses new Set and the value is always Set :-( 			* Works with both Set and List * Element collection fields 	* Two implemenatations: ElementCollectionField (based on gridlayout) and ElementCollectionTable (based on Table) 	* A Vaadin field that edits a collection of embedded objects. E.g. OneToMany or ElementCollection annotated collection fields in JPA apps. 	* Example: Edits a List of Address objects in a Person object in an addressbook app * Label extensions  * Header to make headers without html hacking  * RichText component for XSS safe html inclusion. Also supports Markdown syntax. * Button extension  * PrimaryButton (stylename + enter shortcut)  * ConfirmButton (Customizeble confirm dialog before the actual listener is called) * ListContainer (used by e.g. MTable, but can be used separately as well)  * A lightweight replacement for BeanItemContainer that uses apache commons-beanutils for introspection. The performance (both memory and CPU) is also superior to the core Vaadin counterpart.  * Note, that beans are kept in orinal list that developer used to pass beans. Wrap in e.g. ArrayList if you wish to keep original List unmodified. * LazyList and SortableLazyList  * Together with ListContainer makes memory and query efficient wiring from UI components like Table into your backend dead simple.  * MTable supports the wiring directly with constructors, LazyComboBox has custom version that supports filtering done by ComboBox. * DowndloadButton  * Simplifies creating downloads for dynamically created resources, like PDF reports. Inverts the core Vaadin api to provide output stream for developer into where the content should be written. Also connects download extension to button automatically. * AbstractForm  * An abstract super class for simple entity editor. Provides save/cancel buttons and interfaces for hooking into them. Does binding with naming convention using BeanBinder when setEntity(T) is called. It is suggested to use the "setEagerValidation(true)" mode as it will make your forms much more UX friendly by default. This makes fields validating while you edit the form, instead of postponing it until the save click. Since 1.22 this is the default, but can be disabled if eager validation is for some reason not wanted.  * DisclosurePanel is a simple panel whose content is hidden by default. Easy method to hide fields that are most often irrelevant for the end user.   ## Online demo/usage examples  Online demo for this add-on hopefully makes no sense. But there are though many demo apps that contain helpful usage examples. Some of them listed here:   * Add-ons own test sources, both [integration tests](https://github.com/viritin/viritin/tree/master/src/test/java/org/vaadin/viritin/it) and [unit tests](https://github.com/mstahv/maddon/tree/master/src/test/java/org/vaadin/viritin) contains some usage examples.  * [Vaadin Java EE application example](https://hub.jazz.net/project/vaadin/vaadin-jpa-app/overview)  * [Spring Data + Vaadin UI example](https://github.com/mstahv/spring-data-vaadin-crud)  * Another [Java EE app example](https://github.com/mstahv/jpa-addressbook) - contains JPA usage examples for ManyToMany (MultiSelectTable) and ElementCollection (InlineEditableCollection).  * Invoicing example [Java EE app](https://github.com/mstahv/jpa-invoicer) - contains JPA usage examples ElementCollection (InlineEditableCollection), LazyComboBox, DownloadButton.  ### Usage examples  Run the following mvn command to start a small application (http://localhost:9998) with all usage examples under `org.vaadin.viritin.it`.  ``` mvn -e exec:java -Dexec.mainClass="org.vaadin.viritin.it.UiRunner" -Dexec.classpathScope=test ```  ## Download a release  Official releases of this add-on are available at Vaadin Directory. For Maven instructions, download and reviews, go to http://vaadin.com/addon/viritin  ## Maddon is now called Viritin  My colleagues teased me enough about the add-on name Maddon. M can mean Matti's add-on, but it also had to dual nature meaning I always made helpers there when APIs in core Vaadin (possibly even written by me) made me mad when using it :-)  Anyways, Viritin is a Finnish word, meaning tuner, and I think it describes the add-on pretty well too. It tunes Vaadin to be on the same frequency with you. English people might pronounce it almost like "wire it in" which also suits for the add-on, as lots of its latest helpers are related to binding your domain model to UI, wiring it in for the actual end user.  The add-on itself has been quite popular already. And there has bee lots of active contributors, thank you! There is now also a viritin organization in the github and I hope to get even more contributions for it in the future.  You can also expect some cool other Vaadin related things to pop up to under the governance of "Viritin organization". That yet to be published helper might change the way you use Vaadin even more than the Viritin add-on itself.
techery/presenta	## What's this for? Mortar and flow together provide a good way to follow MVP pattern and get rid of [lol-cycle with Fragments uglyness](https://corner.squareup.com/2014/10/advocating-against-android-fragments.html). Nevertheless it adds some boilerplate to create a single Screen:  1. Create `Path` class; 2. Create inner-class `@module` (for Dagger 1) or `@component` + `@module` (for Dagger 2) to provide view with presenter; 3. Inject `Presenter` into the `View`.  `Mortar` already gives a way to provide view with presenter via scoped context, and step 2 is odd in most cases – that's what `Presenta` for. `Presenta` uses basic `mortar + flow` example with extra extra annotation to skip `Dagger` in the middle of `preseter-view` injection.  ## Getting started *Workflow is identical to mortar-sample:*  1. Add root scope for Mortar, optionally link it with your Dagger main component; 2. Add Flow support to main activity; 3. Create Path screen with presenter and view refs.  Presenta provides `InjectablePresenter` as a base class for presenters which want to benefit from Dagger and Mortar, so you have Dagger injections available inside presenter with no hassle.  #### 1. Declare your `Path` using `@WithPresenter` ```java @Layout(R.layout.chat_list_view) @WithPresenter(ChatListScreen.Presenter.class) public class ChatListScreen extends Path {   ... } ``` #### 2. Add Presenter ```java public static class Presenter extends InjectablePresenter<ChatListView> {      @Inject Chats chats;     List<Chat> chatList;      public Presenter(PresenterInjector injector) {       super(injector); // Dagger injection will be held there       this.chatList = chats.getAll();     }        } ``` #### 3. Use Mortar service to get presenter from View ```java public class ChatListView extends ListView {   Presenter presenter;    public ChatListView(Context context, AttributeSet attrs) {     super(context, attrs);     presenter = PresenterService.getPresenter(context);   }   ... ``` ## Arguments for presenter Most of the time `Path` would have arguments for presenter, which identifies data on screen to be loaded. It's easy with inner-class like presenter and still safe – as presenter is already linked to flow path and will be destroyed even before path is. Note `messageId` and `chatId` in next sample: ```java @Layout(R.layout.message_view) @WithPresenter(MessageScreen.Presenter.class) public class MessageScreen extends Path {   private final int chatId;   private final int messageId;    public MessageScreen(int chatId, int messageId) {     this.chatId = chatId;     this.messageId = messageId;   }    public class Presenter extends InjectablePresenter<MessageView> {     private final Observable<Message> messageSource;     private Message message;     @Inject Chats service;      public Presenter(PresenterInjector injector) {       super(injector);       this.messageSource = service.getChat(chatId).getMessage(messageId);     }     ...   } } ``` ## Dagger's Component support  Is still here. It's recommended to use @Scoped injection for singletons per path context. Presenta comes with `AppScope` and `ScreenScope` for this purpose.  ```java @Layout(R.layout.friend_view) @WithComponent(FriendScreen.Component.class) public class FriendScreen extends Path implements HasParent {   private final int index;    public FriendScreen(int index) {     this.index = index;   }    @Override public FriendListScreen getParent() {     return new FriendListScreen();   }    @ScreenScope(FriendScreen.class)   @dagger.Component(dependencies = MortarDemoActivity.Component.class, modules = Module.class)   public static interface Component{     void inject(FriendView view);   }    @dagger.Module   public class Module {     @Provides User provideFriend(Chats chats) {       return chats.getFriend(index);     }   }    @ScreenScope(FriendScreen.class)   public static class Presenter extends ViewPresenter<FriendView> {     private final User friend;      @Inject     public Presenter(User friend) {       this.friend = friend;     }      @Override public void onLoad(Bundle savedInstanceState) {       super.onLoad(savedInstanceState);       if (!hasView()) return;       getView().setFriend(friend.name);     }   } } ``` ## Additions Mortar-flow sample has useful PathContainers to show up working example of it's philisophy. Those containers and view helpers are reused in `library-additions` ## Dev. status Experimental, trying to use in prod. build. ## Installation Use jitpack.io ```groovy buildscript {     repositories {         jcenter()     }     dependencies {         classpath 'com.android.tools.build:gradle:1.1.3'         classpath 'com.github.dcendents:android-maven-plugin:1.2'     } } ... apply plugin: 'com.android.application' apply plugin: 'com.neenbedankt.android-apt' ... repositories {     jcenter()     maven { url "https://oss.sonatype.org/content/repositories/snapshots/" }     maven { url "https://jitpack.io" } }  dependencies {     compile 'com.github.techery.presenta:library:{version}'     compile 'com.github.techery.presenta:library-additions:{version}'     ...     compile 'com.google.dagger:dagger:2.0-SNAPSHOT'     apt 'com.google.dagger:dagger-compiler:2.0-SNAPSHOT'     provided 'org.glassfish:javax.annotation:10.0-b28'     ...     compile 'com.google.code.gson:gson:2.3.1' } ``` [![Analytics](https://ga-beacon.appspot.com/UA-60536876-1/presenta/readme?pixel)](https://github.com/igrigorik/ga-beacon)   [![Bitdeli Badge](https://d2weczhvl823v0.cloudfront.net/techery/presenta/trend.png)](https://bitdeli.com/free "Bitdeli Badge")
Diolor/ADP	# Android Distribution Platform  Android Distribution Platform (ADP) is a fullstack framework for storing, distributing and installing versionized apk files based (mostly for internal team or roll-back purposes) on each CI git commit.  ![](client/mobile/src/main/res/drawable-xxxhdpi/ic_launcher.png)  The developer can access easily from all devices a full precompiled list of the application's versions per git commit. This saves valuable compilation and installation time both in examining old application behaviour or distributing directly a version via a git commit.   ## Technology behind  The logic is straightforward: - A CI tool (Travis / Jenkins) compiles the desired version(s) and uploads the file(s) with a PUT command such as: ```bash $ curl -X PUT https://myadp.rhcloud.com/upload -H "Api-Token: aaa"  -F "f=@/app-debug.apk" -F "c=13c422d" -F "v=1.0" -F "b=productionRelease" ``` - The server saves the file in Amazon S3 (also for manual access) - The app client lists the availiable apks or installs the desired apk using a token handshake  The backend is a [Python Flask](http://flask.pocoo.org) microframework able to run on a WSGI which supports [persistence objects](https://docs.python.org/2/library/shelve.html) (e.g. Openshift). Heroku is not supporting it. An [Amazon S3 bucket](http://aws.amazon.com/s3/) is needed to store the apks.    ## Instalation  Estimated installation time is <20 minutes.  It has 4 steps an requires basic knowledge to setup a server & CI tools.  #### 1. Amazon S3  First you need a S3 bucket, a AWS_KEY and the AWS_SECRET to use it remotely. Have the name and the tokens in hand to use them later.   To see how to create a bucket see [here](http://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html). To see how to obtain the Key and the Secret see [here](http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSGettingStartedGuide/AWSCredentials.html).  #### 2. Backend  Download the backend code locally: ```bash $ svn export https://github.com/Diolor/ADP/trunk/backend ```  In **config.cfg** file change the **AWS_KEY**, **AWS_SECRET**, **AWS_BUCKET** and **AWS_REGION** (if required) with the desired ones you have acquired above. Also change the **API_TOKEN** which is required to authenticate with the client app.  For S3 region codes see [here](http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region).  The code is ready to work with OpenShift's servers. You can use the awesome free gears that [RedHat's OpenShift servers](https://www.openshift.com) provide with ssl wrapping (create a simple python-2.7 gear). If you use OpenShift your server ip will look like: `https://myadp-user.rhcloud.com`.  Alternatively use your own python server/host.   #### 3. CI  First you need to expose the verion of your application in your CI tool. A simple way is the following snippet in gradle:  ```gradle apply plugin: 'com.android.application'  String VERSION_NAME = "1.0.0"  android {     defaultConfig {         versionName VERSION_NAME     } }  task version << {     println VERSION_NAME } ``` Running the above you can expose the version in the bash like: `./gradlew -q version`   An **example** of a Travis CI file is the following. Adjust the values as needed for your project. This will compile you and upload you 2 build variant apk files to the server:  ```yml language: android android:   components:     - build-tools-21.1.2     - android-21  env:   global:     - ADP_TOKEN=my_very_complex_token1234  install:   - export JAVA7_HOME="/usr/lib/jvm/java-7-oracle"   - ./gradlew assembleDebug   - ./gradlew assembleRelease   - export APP_VERSION=`./gradlew -q version`  script:   # maybe run some tests here     # upload to ADP   - COMMIT_SHORT=${TRAVIS_COMMIT:0:7}   - COMMIT_MESSAGE=$(git show -s --format=%B $TRAVIS_COMMIT | tr -d '\n')    - >     curl -X PUT https://myadp-user.rhcloud.com/upload     -H "Api-Token: $ADP_TOKEN"     -F f="@$TRAVIS_BUILD_DIR/app/app-debug.apk"     -F v="$APP_VERSION"     -F c="$COMMIT_SHORT"     -F n="$COMMIT_MESSAGE"     -F b=debug    - >     curl -X PUT https://myadp-user.rhcloud.com/upload     -H "Api-Token: $ADP_TOKEN"     -F f="@$TRAVIS_BUILD_DIR/app/app-release.apk"     -F v="$APP_VERSION"     -F c="$COMMIT_SHORT"     -F n="$COMMIT_MESSAGE"     -F b=release ```  The files should be uploaded to `http(s)://MY_SERVER_IP/upload`. The parameters are the following: ``` -F f="@pathto/file.apk" -F v="the_version" -F c="the_commit_id" -F n="the_commit_message"   # optional field -F b="the_relese_flavor" ```    #### 4. Client  You can install the latest client application from [Google Play](https://play.google.com/store/apps/details?id=com.lorentzos.adp).  By default the application asks for the base api endpoint of your server and the token you choose. Upon saving the preferences you will simply see the list of the availiable apks that you have stored. Simply click one and it will be downloaded automatically.  ![](img/token.png) ![](img/list.png)    ## Costs / alternatives  Assuming that your apk is 20MB and you upload two flavors it will cost you $1/1€ for 830 commits per month. Alternative open source solution: [HockeyKit](https://github.com/bitstadium/HockeyKit)    ## Developed by  * Dionysis Lorentzos - <diolorentzos@gmail.com>    ## Licence  ``` The MIT License (MIT)  Copyright (c) 2015 Dionysis Lorentzos  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```  [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-ADP-brightgreen.svg?style=flat)](http://android-arsenal.com/details/1/1646)
mangstadt/ez-vcard	# ez-vcard  |     |     | | --- | --- | | Continuous Integration: | [![](https://travis-ci.org/mangstadt/ez-vcard.svg?branch=master)](https://travis-ci.org/mangstadt/ez-vcard) | | Code Coverage: | [![codecov.io](http://codecov.io/github/mangstadt/ez-vcard/coverage.svg?branch=master)](http://codecov.io/github/mangstadt/ez-vcard?branch=master) | | Maven Central: | [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.googlecode.ez-vcard/ez-vcard/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.googlecode.ez-vcard/ez-vcard) | | Chat Room: | [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/mangstadt/ez-vcard?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) | | Q & A: | [![Codewake](https://www.codewake.com/badges/ask_question.svg)](https://www.codewake.com/p/ez-vcard) | | License: | [![FreeBSD License](https://img.shields.io/badge/License-FreeBSD-red.svg)](https://github.com/mangstadt/ez-vcard/blob/master/LICENSE) |  ez-vcard is a vCard library written in Java.  It can read and write vCards in many different formats.  The "ez" stands for "easy" because the goal is to create a library that's easy to use.  <p align="center"><strong><a href="https://github.com/mangstadt/ez-vcard/wiki/Downloads">Downloads</a> | <a href="http://mangstadt.github.io/ez-vcard/javadocs/latest/index.html">Javadocs</a> | <a href="#mavengradle">Maven/Gradle</a> | <a href="https://github.com/mangstadt/ez-vcard/wiki">Documentation</a></strong></p>  ```java String str = "BEGIN:VCARD\r\n" + "VERSION:4.0\r\n" + "N:Doe;Jonathan;;Mr;\r\n" + "FN:John Doe\r\n" + "END:VCARD\r\n";  VCard vcard = Ezvcard.parse(str).first(); String fullName = vcard.getFormattedName().getValue(); String lastName = vcard.getStructuredName().getFamily(); ```  ```java VCard vcard = new VCard();  StructuredName n = new StructuredName(); n.setFamily("Doe"); n.setGiven("Jonathan"); n.getPrefixes().add("Mr"); vcard.setStructuredName(n);  vcard.setFormattedName("John Doe");  String str = Ezvcard.write(vcard).version(VCardVersion.V4_0).go(); ```  # Features   * Simple, intuitive API (see [Examples](https://github.com/mangstadt/ez-vcard/wiki/Examples)).  * Streaming API.  * Android compatibility (see [ez-vcard-android](https://github.com/mangstadt/ez-vcard-android) project).  * Full compliance with 2.1, 3.0, and 4.0 specifications (see [Supported Specifications](https://github.com/mangstadt/ez-vcard/wiki/Supported-Specifications)).  * Supports XML, HTML, and JSON encoded vCards (see [Supported Specifications](https://github.com/mangstadt/ez-vcard/wiki/Supported-Specifications)).  * Extensive unit test coverage.  * Low Java version requirement (1.5 or above, 1.6 for jCards).  * Few dependencies on external libraries.  Dependencies can be selectively excluded based on the functionality that is needed (see [Dependencies](https://github.com/mangstadt/ez-vcard/wiki/Dependencies)).  # News  **August 12, 2017**  [Version 0.10.3](https://github.com/mangstadt/ez-vcard/wiki/Downloads) released.  This release adds a few improvements and bug fixes.  Please see the [changelog](https://github.com/mangstadt/ez-vcard/wiki/Changelog) for details.  **February 19, 2017**  [Version 0.10.2](https://github.com/mangstadt/ez-vcard/wiki/Downloads) released.  This release adds a number of improvements and bug fixes.  Please see the [changelog](https://github.com/mangstadt/ez-vcard/wiki/Changelog) for details.  **December 31, 2016**  [Version 0.10.1](https://github.com/mangstadt/ez-vcard/wiki/Downloads) released.  This release adds a number of improvements and bug fixes.  Please see the [changelog](https://github.com/mangstadt/ez-vcard/wiki/Changelog) for details.  **October 8, 2016**  [Version 0.10.0](https://github.com/mangstadt/ez-vcard/wiki/Downloads) released.  This release contains mostly refactorings to support a newly integrated library.  Please see the [changelog](https://github.com/mangstadt/ez-vcard/wiki/Changelog) for details.  [Old News](https://github.com/mangstadt/ez-vcard/wiki/Old-News)  # Maven/Gradle  **Maven**  ```xml <dependency>    <groupId>com.googlecode.ez-vcard</groupId>    <artifactId>ez-vcard</artifactId>    <version>0.10.3</version> </dependency> ```  **Gradle**  ``` compile 'com.googlecode.ez-vcard:ez-vcard:0.10.3' ```  # Build Instructions  ez-vcard uses [Maven](http://maven.apache.org/) as its build tool, and adheres to its convensions.  To build the project: `mvn compile`   To run the unit tests: `mvn test`   To build a JAR: `mvn package`  **Eclipse users:** Due to a quirk in the build process, before running the `eclipse:eclipse` goal, you must tweak some of the `<resource>` definitions in the POM file.  See the comments in the POM file for details.  # Questions / Feedback  You have some options:   * [Issue tracker](https://github.com/mangstadt/ez-vcard/issues)  * [Gitter chat room](https://gitter.im/mangstadt/ez-vcard)  * [codewake Q&A forum](https://www.codewake.com/p/ez-vcard)  * [Post a question to StackOverflow](http://stackoverflow.com/questions/ask) with `vcard` as a tag.  * Email me directly: [mike.angstadt@gmail.com](mailto:mike.angstadt@gmail.com)  Please submit bug reports and feature requests to the [issue tracker](https://github.com/mangstadt/ez-vcard/issues).  Contributors are listed in the project credits.  # Credits  **Lead Developer**   Michael Angstadt  **Documentation**   Michael Angstadt  **Architecture Ideas**   George El-Haddad ([CardMe Project](https://sourceforge.net/projects/cardme/))  **Maven Central Reviewer**   Joel Orlina  **Project Hosting**   [Github](https://github.com)   [Google Code](https://code.google.com)  **Contributors**   amarnathr ([hCard template bug](https://github.com/mangstadt/ez-vcard/issues/16))   [bgorven](https://github.com/bgorven) (jackson-databind integration, jackson pretty printing, [round-trip testing](https://github.com/mangstadt/ez-vcard/pull/49))   Moritz Bechler (Geo URI bug fix)   Kiran Kumar Bhushan (quoted-printable bug)   [Sean Boylan](https://github.com/seanboylan) ([XXE vulnerability](https://github.com/mangstadt/ez-vcard/issues/55))   Florian Brunner ([OSGi metadata](https://github.com/mangstadt/ez-vcard/issues/11))   Pratyush Chandra ([ez-vcard-android](http://github.com/mangstadt/ez-vcard-android))   Lívio Cipriano ([Issue 35](https://github.com/mangstadt/ez-vcard/issues/35))   [cmargenau](https://github.com/cmargenau) ([XML 1.1 support](https://github.com/mangstadt/ez-vcard/issues/29))   [DerBlade](https://github.com/DerBlade) ([missing parameter method](https://github.com/mangstadt/ez-vcard/issues/52))   Ed Developer (wiki fix)   Juliane Dombrowski ([quoted-printable line folding](https://github.com/mangstadt/ez-vcard/issues/9))   F. Gaffron ([quoted-printable charsets](https://github.com/mangstadt/ez-vcard/issues/12))   [isindir](https://github.com/isindir) ([Javadoc fix](https://github.com/mangstadt/ez-vcard/pull/53))   knutolav ([Issue 1](https://github.com/mangstadt/ez-vcard/issues/1), [Issue 2](https://github.com/mangstadt/ez-vcard/issues/2))   [Nico Lehmann](https://github.com/ekorn) ([Windows 10 Contacts compatibility issue](https://github.com/mangstadt/ez-vcard/issues/56))   [Alexander Myltsev](https://github.com/alexander-myltsev) ([Issue with folding quoted printable values](https://github.com/mangstadt/ez-vcard/issues/80))   David Nault ([Issue 3](https://github.com/mangstadt/ez-vcard/issues/3), [Issue 7](https://github.com/mangstadt/ez-vcard/issues/7))   [NeverWise](https://github.com/NeverWise) ([chaining parser & custom scribes issue](https://github.com/mangstadt/ez-vcard/issues/81))   [rfc2822](https://github.com/rfc2822) ([folding line issue](https://github.com/mangstadt/ez-vcard/issues/30), [IMPP issue](https://github.com/mangstadt/ez-vcard/issues/32), [trailing semicolons issue](https://github.com/mangstadt/ez-vcard/issues/57))   [Steven Ruppert](https://github.com/blendmaster) ([folding surrogate character pairs](https://github.com/mangstadt/ez-vcard/pull/36), [parsing tel URIs](https://github.com/mangstadt/ez-vcard/pull/38))   Melchin Sapir (README suggestion)   Matt Siegel ([base64 property value bug](https://github.com/mangstadt/ez-vcard/issues/21), [unit test bug](https://github.com/mangstadt/ez-vcard/issues/22))   David Spieler ([hCard template bug](https://github.com/mangstadt/ez-vcard/issues/19))   stonio ([unit test fix](https://github.com/mangstadt/ez-vcard/pull/63), [readme fix](https://github.com/mangstadt/ez-vcard/pull/68))   Tom Vogel ([quoted-printable charsets](https://github.com/mangstadt/ez-vcard/issues/10))   Eike Weyl (Wiki fix, Javadoc fix)   沈健 (plain-text vCard formatting issue)  **Donators**   [DAVDroid](https://davdroid.bitfire.at/)  **Caffeine Suppliers**   'feine   Starbucks   Volo Coffeehouse  _No animals were harmed in the making of this library._  [![](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=8CEN7MPKRBKU6&lc=US&item_name=Michael%20Angstadt&item_number=ez%2dvcard&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted)
stardog-union/pellet	Pellet: An Open Source OWL DL reasoner for Java  -----------------------------------------------    [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/complexible/pellet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Pellet is the OWL 2 DL reasoner:      * [open source](https://github.com/complexible/pellet/blob/master/LICENSE.txt) (AGPL) or commercial license  * pure Java  * developed and [commercially supported](http://complexible.com/) by Complexible Inc.     Pellet can be used with Jena or OWL-API libraries. Pellet provides functionality to check consistency of ontologies, compute the classification hierarchy,   explain inferences, and answer SPARQL queries.    _Pellet 3.0, a closed source, next-gen version of Pellet, is embedded and available in [Stardog](http://stardog.com/), the RDF database._    Feel free to fork this repository and submit pull requests if you want to  see changes, new features, etc. in Pellet.    Documentation about how to use Pellet is in the doc/ directory and there are some   code samples in the examples/ directory.                                        Commercial support for Pellet is [available](http://complexible.com/). The [Pellet FAQ](http://clarkparsia.com/pellet/faq) answers some frequently asked questions.    There is a [pellet-users mailing list](https://groups.google.com/forum/?fromgroups#!forum/pellet-users) for questions and feedback. You can search [pellet-users archives](http://news.gmane.org/gmane.comp.web.pellet.user).   Bug reports and enhancement requests should be sent to the mailing list. Issues are on [Github](http://github.com/complexible/pellet/issues).    Thanks for using Pellet.
HubSpot/Rosetta	# Rosetta [![Build Status](https://travis-ci.org/HubSpot/Rosetta.svg?branch=master)](https://travis-ci.org/HubSpot/Rosetta)  ## Overview  Rosetta is a Java library that leverages [Jackson](https://github.com/FasterXML/jackson) to take the pain out of mapping objects to/from the DB, designed to integrate seamlessly with [jDBI](https://github.com/jdbi/jdbi). Jackson is extremely fast, endlessly configurable, and already used by many Java webapps.   Rosetta isn't an ORM. It doesn't silently read and write to your database, validate input, or manage connections. It does two things:  1. Binds Java objects to query parameters using a jDBI `BindingAnnotation` 2. Maps query results back to Java objects using a jDBI `ResultSetMapperFactory`  ## Usage  To use with jDBI on Maven-based projects, add the following dependency:  ```xml <dependency>   <groupId>com.hubspot.rosetta</groupId>   <artifactId>RosettaJdbi</artifactId>   <version>3.11.2</version> </dependency> ```  ## Binding  You can [bind JDBI arguments](http://www.jdbi.org/sql_object_api_argument_binding) in your DAO using `@BindWithRosetta`.  ```java public interface MyDAO {   @SqlUpdate("UPDATE my_table SET name = :name, type = :type WHERE id = :id")   void update(@BindWithRosetta MyRow obj); } ```  `@BindWithRosetta` behaves like jDBI's `@BindBean`, but it converts the object to a tree using Jackson which lets you use all the Jackson annotations you know and love to customize the representation. It's also generally more robust - it supports the not-quite-standard naming conventions, enums, fluent setters, nested objects (with dot-notation), getters without fields, etc.  ## Mapping  To register Rosetta globally for mapping, you can add it to your `DBI` like so: ```java dbi.registerMapper(new RosettaMapperFactory()); ```  Or to test it out on a single DAO you would do: ```java @RegisterMapperFactory(RosettaMapperFactory.class) public interface MyDAO { /* ... */ } ```  Or to use in combination with a `Handle`: (same idea to register on a `Query`) ```java handle.registerMapper(new RosettaMapperFactory()); ```  ## Advanced Features  For a list of advanced features, see [here](FEATURES.md)
matsjj/thundernetwork	# thundernetwork  Active development of this project has been moved to [blockchain](https://github.com/blockchain/thunder "blockchain")
caelum/restfulie-java	h1. Website  Check out "Restfulie's website":http://restfulie.caelumobjects.com if you still did not.  h1. Restfulie: quit pretending  CRUD through HTTP is a good step forward to using resources and becoming RESTful, another step further into it is to make use of hypermedia based services and this gem allows you to do it really fast.  You can read the "article on using the web for real":http://guilhermesilveira.wordpress.com/2009/11/03/quit-pretending-use-the-web-for-real-restfulie/ which gives an introduction to hypermedia/aware resources.  h2. Why would I use restfulie?  1. Easy --> writing hypermedia aware resource based clients 2. Easy --> hypermedia aware resource based services 3. Small -> it's not a bloated solution with a huge list of APIs 4. HATEOAS --> clients you are unaware of will not bother if you change your URIs 5. HATEOAS --> services that you consume will not affect your software whenever they change part of their flow or URIs  h2. Could you compare it with other REST related APIs?  Restfulie was the first API trying to somehow implement "Jim Webber":http://jim.webber.name/ point of view on how RESTFul systems use hypermedia as the way to lead your client's path through a business process.  You can see a "3rd party comparison":http://code.google.com/p/implementing-rest/wiki/ByLanguage between all REST frameworks.  Therefore Restfulie is unique in its feature set when compared to other (JAX-RS) based implementations: looking for simple code and favoring conventions over manual configurations when creating hypermedia aware system. Restfulie also handle content negotiation and its client implements cache and other set of features unsupported so far in other frameworks.  According to Richardson Maturity Model , systems are only to be called RESTFul if they support this kind of state flow transition through hypermedia content contained within resources representations:  <pre> <order> 	<product>basic rails course</product> 	<product>RESTful training</product> 	<atom:link rel="payment" href="http://www.caelum.com.br/orders/1/pay" xmlns:atom="http://www.w3.org/2005/Atom"/> 	<atom:link rel="cancel" href="http://www.caelum.com.br/orders/1" xmlns:atom="http://www.w3.org/2005/Atom"/> </order> </pre>  If you are to implement a 3rd level (restful) service, Restfulie is the way to go.   h2. More examples  There is a "Restfulie guide being built":http://github.com/caelum/restfulie-guide but still in beta version. You can also "download a sample application":http://code.google.com/p/restfulie , both client and server code. FInally, do not forget to ask your questions at our "mailing list":http://groups.google.com/group/restfulie-java?lnk=srg .  h2. Java or Ruby  Restfulie comes many different flavors, "java":http://github.com/caelum/restfulie-java and "ruby":http://github.com/caelum/restfulie.  h1. One minute examples  h2. Client side  The client side code allows you to hide http-protocol specifics if required, while allowing you to re-configure it when needed. Example on accessing a resource and its services through the restfulie API:  <pre> Order order = new Order();  // place the order order = service("http://www.caelum.com.br/order").post(order);  // cancels it resource(order).getRelation("cancel").execute(); </pre>  h2. Server side  This is a simple example how to make your state changes available to your resource consumers:  <pre> public class Order implements HypermediaResource {  	public List<Relation> getRelations(Restfulie control) { 		if (status.equals("unpaid")) { 			control.relation("latest").uses(OrderingController.class).get(this); 			control.relation("cancel").uses(OrderingController.class).cancel(this); 		} 		return control.getRelations(); 	}  } </pre>  h1. Installation  h2. Download everything  Start "downloading all data":http://code.google.com/p/restfulie : the client jars, "vraptor":http://www.vraptor.org jars and both server side and client side application.  You can download a sample client and server side application on the same link, those will be helpful for you too understand how to use Restfulie.  h2. Client side installation  In order to use Restfulie in your client side app, simply add "all required jars":http://code.google.com/p/restfulie/ to your classpath.  h2. Server side installation  Download "vraptor's blank project":http://www.vraptor.org and configure your web.xml file. You are ready to go.  h1. Client side usage  The entry point for *Restfulie's* api is the *Restfulie* class. It's basic usage is through the *resource* method which, given an URI, will allow you to retrieve a resource or post to a resource controller:  <pre>   Order order = Restfulie.resource("http://www.caelum.com.br/orders/1").get();      Client client = new Client();   Restfulie.resource("http://www.caelum.com.br/clients").post(client); </pre>  Due to the nature of the entry point and the java bytecode, Restfulie is still unable to allow the user to make the http verb even more transparent.  As seen earlier, as soon as you have acquired an object through the use of the restfulie api, you can invoke its transitions:  <pre> Order order = Restfulie.resource("http://www.caelum.com.br/orders/1").get(); resource(order).getRelation("cancel").access(); </pre>  The *resource* method can be statically imported from the *Restfulie* class.  h2. Serialization configuration  Restfulie uses XStream behind the scenes, therefore all XStream related annotations are supported by default when using it. The following example shows how to alias a type:  <pre> @XStreamAlias("order") public class Order { } </pre>  More info on how to configure XStream through the use of annotations can be "found in its website":"http://xstream.codehaus.org".  By default, Restfulie serializes all primitive, String and enum types. In order to serialize child elements, one has pre-configure Restfulie. This is the typical usage-pattern applications will face while using restfulie:  <pre> Resources resources = Restfulie.resources(); resources.configure(Order.class).include("items");  // the configuration step is completed, so lets use it now: resources.entryAt("http://www.caelum.com.br/clients").post(new Client()); </pre>  The entire serialization process can be configured either through the *Resources* interface's methods or using *XStream*'s explicit configuration.  h2. Caching  Most REST frameworks will not help the developer providing etags, last modified and other cache related headers.  Meanwhile, in the server side, Restfulie might add extra headers to handle last modified, etag and max age situations that will improve response time and avoid useless bandwidth consumption. In order to benefit from such cache characteristics, simply implement the RestfulEntity interface.  h2. Accessing all possible transitions  One can access all possible transitions for an object by invoking a resource's *getRelations* method:  <pre> 	List<Relation> relations = resource(order).getRelations(); </pre>  While typical level 2 frameworks will only provide a statically, compilation time checked, relation/transition invocation, Restfulie allows clients/bots to adapt to REST results, giving your clients even less coupling to your services protocol.  h2. HTTP verbs  By default, restfulie uses a well known table of defaults for http verb detection according to the rel element:  * destroy, cancel and delete send a DELETE request * update sends a PUT request * refresh, reload, show, latest sends a GET request * other methods sends a POST request  If you want to use a custom http verb in order to send your request, you can do it:  <pre>  payment = resource(order).getRelation("payment").method(HttpMethod.PUT).accessAndRetrieve(payment);  </pre>  h2. Sending some parameters  If you need to send some information to the server, this can be done by passing an argument to the execute method, which will be serialized and sent as the request body's content:  <pre>  payment = resource(order).getRelation("payment").method(HttpMethod.PUT).accessAndRetrieve(payment);  </pre>  h2. More info  Once you have found the entry point you want to use (retrieving a resource or creating one), the javadoc api is a resourcefull place for more info.   h1. Server side usage  The default way to use Restfulie is to define the getRelations method in your resource. The method receives a *Restfulie* instance (server side version) which allows you to dsl-like create transitions. In order to do that, given a *Restfulie* object, invoke the transition method with your *rel* name and the relative *controller action*:  <pre> 	public List<Relation> getRelations(Restfulie control) { 		control.relation("delete").uses(OrderingController.class).cancel(this); 		return control.getRelations(); 	} </pre>  Note that both the *OrderingController* class with its *cancel* method are web methods made available through the use of vraptor:  <pre> @Resource public OrderingController {  	@Delete 	@Path("/order/{order.id}") 	@Transition 	public void cancel(Order order) { 		order = database.getOrder(order.getId()); 		order.cancel(); 		status.ok(); 	} } </pre>  Now you need to set up your application package in web.xml. This is the only configuration required:  <pre> 	<context-param>         <param-name>br.com.caelum.vraptor.packages</param-name>         <param-value>br.com.caelum.vraptor.restfulie,com.your.app.package.without.leading.whitespace</param-value>     </context-param> </pre>  h2. Relation/Transition invocation  By using the *@Transition* to annotate your method, Restfulie will automatically load the order from the database and check for either 404 (object not found), 405 (method not allowed), 409 (conflict: transition is not allowed for this resource's state) and 406 (content negotiation failed).  This is one of the advantages of using Restfulie over other level 2 Rest frameworks. By supporting hypermedia content and handling transitions out of the box, Restfulie creates a new layer capable of helping the server to deal with unexpected states.   h2. Typical example  1. Create your model (i.e. Order)  <pre> @XStreamAlias("order") public class Order {  	private String id; 	private Location location; 	private List<Item> items;  	private transient String status; 	private Payment payment;  	public enum Location { 		takeAway, drinkIn 	}; 	 	// ...  }  @XStreamAlias("item") public class Item { 	enum Coffee {LATTE, CAPPUCINO, ESPRESSO}; 	enum Milk {SKIM, SEMI, WHOLE}; 	enum Size {SMALL, MEDIUM, LARGE};  	private Coffee drink; 	private int quantity; 	private  Milk milk; 	private Size size;  	// ...  } </pre>  2. Usually the *getRelations* method would check the resource state in order to coordinate which transitions can be executed: So add the *getRelations* method returning an array of possible transitions/relations:  <pre> public class Order implements HypermediaResource {   	public List<Relation> getRelations(Restfulie control) { 		if (status.equals("unpaid")) { 			control.relation("latest").uses(OrderingController.class).get(this); 			control.relation("cancel").uses(OrderingController.class).cancel(this); 			control.relation("payment").uses(OrderingController.class).pay(this,null); 		} 		return control.getRelations(); 	}  }</pre>   3. Create your *retrieval* method:  <pre> 	@Get 	@Path("/order/{order.id}") 	public void get(Order order) { 		order = database.getOrder(order.getId()); 		result.use(xml()).from(order).include("items").serialize(); 	} </pre>  You are ready to go. Create a new order and access it through your /order/id path. The best way to start is to download the sample application and go through the *OrderingController* and *Order* classes.  h3. Content negotiation  While most REST frameworks only support rendering xml out of the box, Restfulie already provides (through VRaptor and Spring) xml, xhtml and json representations of your resource. You can add new serializers as required. In order to take content negotiation into play, simply use VRaptor's representation() renderer:  <pre> 	@Get 	@Path("/order/{order.id}") 	public void get(Order order) { 		order = database.getOrder(order.getId()); 		result.use(representation()).from(order).include("items").serialize(); 	} </pre>  h3. Creating relations to other servers  <pre> public class Order implements HypermediaResource {   	public List<Relation> getRelations(Restfulie control) { 		control.relation("up").at('http://caelumobjects.com'); 		return control.getRelations(); 	}  }</pre>  Note that you can either create relations or transitions. We suggest clients to only use relations, but for clear semantics in some servers, you might want to invoke control.transition.  h2. Accepting more than one argument  While typical JAX-RS services will deserialize your request body into your method argument and require you to retrieve extra URI information through the requested URI, Restfulie accepts one core parameter (based on its alias) and extra parameters to be extracted through the URI itself:  <pre> 	@Post 	@Path("/order/{order.id}/pay") 	@Consumes 	@Transition 	public void pay(Order order, Payment payment) { 		order = database.getOrder(order.getId()); 		order.pay(payment); 		status.ok(); 	} </pre>  Parameter support is provided through VRaptor, so Iogi and Paranamer support is already built-in.   h1. Asynchronous Request  To make an asynchronous request, you can use getAsync, postAsync, putAsync or deleteAsync methods. For that, you must provide a RequestCallback instance with a callback method implementation (the code to be executed when the response comes), like this:  <pre> RequestCallback requestCallback = new RequestCallback() {     @Override     public void callback(Response response) {         // code to be executed when the response comes     } };  Future<Response> future1 = Restfulie.at("http://www.caelum.com.br/clients").getAsync(requestCallback); </pre>  If you prefer, you can abbreviate it this way:  <pre> Future<Response> future1 = Restfulie.at("http://www.caelum.com.br/clients").getAsync(new RequestCallback() {     @Override     public void callback(Response response) {         // code to be executed when the response comes     } }); </pre>  h2. Using Log4j to log exceptions from Asynchronous Request  If you want Log4j to log the exceptions occurred when something goes wrong with the asynchronous request, you can create a log4j.xml configuration file on your classpath like this:  <pre> <?xml version="1.0" encoding="UTF-8" ?> <!DOCTYPE log4j:configuration SYSTEM "log4j.dtd"> <log4j:configuration xmlns:log4j="http://jakarta.apache.org/log4j/">                  <appender name="stdout" class="org.apache.log4j.ConsoleAppender">                 <layout class="org.apache.log4j.PatternLayout">                         <param name="ConversionPattern"                                  value="%d{HH:mm:ss,SSS} %5p [%-20c{1}] %m%n"/>                 </layout>         </appender>          <category name="br.com.caelum.restfulie">                 <priority value="ERROR" />                 <appender-ref ref="stdout" />         </category>  </log4j:configuration> </pre>  h2. Getting help and mailing lists  If you are looking for or want to help, let us know at the mailing list:  "http://groups.google.com/group/restfulie-java":http://groups.google.com/group/restfulie-java  "VRaptor's website":http://www.vraptor.org also contain its own mailing list which can be used to get help on implementing controller's.  h2. Team  Restfulie was created and is maintained within Caelum by  Projetct Founder * "Guilherme Silveira":http://guilhermesilveira.wordpress.com ( "email":mailto:guilherme.silveira@caelum.com.br ) - twitter:http://www.twitter.com/guilhermecaelum  Contributors * Lucas Cavalcanti ("email":mailto:lucas.cavalcanti@caelum.com.br) - twitter:http://www.twitter.com/lucascs * "Adriano Almeida":http://ahalmeida.com/ ("email":mailto:adriano.almeida@caelum.com.br) - twitter:http://www.twitter.com/adrianoalmeida7 * Samuel Portela ("email":mailto:negociosnainternet@gmail.com) - twitter:http://www.twitter.com/samuel_portela  h3. Example  You can see an "application's source code":http://github.com/caelum/restfulie-java/tree/master/example/, both client and server side were implemented using *restfulie*:  h2. Contributing  Users are encouraged to contribute with extra implementations for each layer (i.e. spring mvc implementation for the controller layer).  h2. Inner libraries  In its Java version, Restfulie uses by default:  * VRaptor:http://www.vraptor.org as the server-side controller   * XStream:"http://xstream.codehaus.org" as its serialization library * java.net api for http requests * Spring IoC for dependency injection  XStream is the most famous java serialization tool around with support both to json and xml while VRaptor (as Rails) supplies a reverse URI lookup system upon its controller which provides a way to identify URI's from well defined transitions.  h2. License  Check the "license file":LICENSE  <script type="text/javascript"> var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www."); document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E")); </script> <script type="text/javascript"> try { var pageTracker = _gat._getTracker("UA-11770776-1"); pageTracker._trackPageview(); } catch(err) {}</script>
magja/magja	# Magja [![Build Status](https://travis-ci.org/magja/magja.svg?branch=master)](https://travis-ci.org/magja/magja)  Magja is a Java Connector for Magento's SOAP API that allows easy integration with the popular shop system  and allows to exchange all data available by the Magento API.  ## Core Features * Basic support for Magento 1.x SOAP API V1 * Allows access to:   * Product   * Product Media   * Product Link   * Product Categories   * Product Attributes   * Country   * Region   * Customer   * Order   * Invoice   * Cart * Extensible (without code generation) for custom API  ## Installation and configuration   Magja artifacts are available in the Sonatype Maven repository. Pleas put the following repository definition into your `pom.xml`:      <repositories>       <repository>         <id>sonatype</id>         <name>Sonatype Repository</name>         <url>http://oss.sonatype.org/content/groups/public</url>       </repository>     </repositories>      and add the dependency to your project:      <dependency>       <groupId>com.google.code.magja</groupId>       <artifactId>magja</artifactId>       <version>1.0.3-SNAPSHOT</version>     </dependency>      to get started. In order to connect to your Magento Shop installation, you need to put `magento-api.properties` on your classpath. You can copy an example properties from samples folder. Essentially, the properties are self-explanatory:             # SOAP XML/RPC user     magento-api-username=<replace with user>           # SOAP XML/RPC api key     magento-api-password=<replace with api key>          # URL of the Magento installation     # If your shop is installed not using a prefix e.g. /magento     # please add the prefix to the path:     # http://<yourmagento-host>/magento/index.php/api/soap/     magento-api-url=http://<yourmagento-host>/index.php/api/soap/          # the ID of the default attribute set     default-attribute-set-id=4       # the ID of the default root category     default-root-category-id=2  If you want to use the client behind a HTTP proxy, you can specify the settings in `magento-api.properties`:      # HTTP proxy settings, default is false     http-proxy-enabled=true     http-proxy-host=localhost     http-proxy-port=8080          # HTTP proxy auth settings, default is false     http-proxy-auth-enabled=true     http-proxy-username=user     http-proxy-password=pass  If your shop is protected with HTTP Basic Authentication, you can pass a username and password using the following properties:      # HTTP auth settings, default is false     http-auth-enabled=true     http-username=user     http-password=pass   ## Basic usage  Here is a sample usage of the product service:      Logger log = LoggerFactory.getLogger("logger");     RemoteServiceFactory remoteServiceFactory = new RemoteServiceFactory(MagentoSoapClient.getInstance());     ProductRemoteService productService = remoteServiceFactory.getProductRemoteService();      Product product = new Product();     product.setSku("0001-QWER-12090");     product.setName("Milk");      try {       productService.add(product);              List<Product> allProducts = productService.listAll();       for (Product shopProduct : allProducts) {         log.info("{}", shopProduct);;       }            } catch (ServiceException e) {       log.error("Error manipulating products", e);     } catch (NoSuchAlgorithmException e) {       log.error("Error manipulating products", e);     }  ## More examples  ### Category  Get only the basic information of a Product Category, without its dependencies:      Category category = remoteServiceFactory.getCategoryRemoteService.getByIdClean(2);  Get Product Category with its children:      Category category = remoteServiceFactory.getCategoryRemoteService.getByIdWithChildren(2);     for (Category child : category.getChildren()) {       // do something with the child     }  Get Product Category with its parent:      Category category = remoteServiceFactory.getCategoryRemoteService.getByIdWithParent(2);     Category parent = category.getParent();      Get category with its parent and children:      Category category = remoteServiceFactory.getCategoryRemoteService.getByIdWithParentAndChildren(2)   ### Product  List all products with dependencies (slower)      List<Product> products = remoteServiceFactory.getProductRemoteService.listAll();  List all products without dependencies (faster)      List<Product> products = remoteServiceFactory.getProductRemoteService.listAllNoDep()  Create Product      Product product = new Product();     product.setSku("DUMMYPRD");     product.setName("Lovely Umbrella");     product.setShortDescription("This is a short description");     product.setDescription("This is a description for Product");     product.setPrice(250.99);     product.setCost(100.22);     product.setEnabled(true);     product.setWeight(0.500);     product.setType(ProductTypeEnum.SIMPLE.getProductType());     product.setAttributeSet(new ProductAttributeSet(4, "Default"));          // category     List<Category> categories = new ArrayList<Category>();     categories.add(new Category(2));     product.setCategories(categories);       product.setWebsites(new Integer[] { 1 });     product.setVisibility(Visibility.CATALOG_SEARCH);      // inventory     product.setQty(new Double(20));     product.setInStock(true);     product.setMetaTitle("meta title");     product.setMetaDescription("meta description");     product.setMetaKeyword("keyword");      // Optional: you can set the properties directly by-passing the setter like too:     product.set("meta_description", "one two tree")      // then, we just instantiate the service to persist the product     remoteServiceFactory.getProductRemoteService.save(product)  ### Customer  Reading the Customers      List<Customer> customers = remoteServiceFactory.customerRemoteService.list();  ## Development notes  Development snapshots will be deployed periodically from Jenkins CI provided by [Bippo](http://www.bippo.co.id/) & [Soluvas](http://www.soluvas.com/) and get deployed into the Sonatype repository: https://oss.sonatype.org/content/groups/public/ You can use the search UI from https://oss.sonatype.org/  Magja is build using Apache Maven. Please run:           mvn clean package       to build the software. There is a Maven profile included, running the entire integration-test suite against a Magento Shop. Please make sure to place your `magento-api.properties` file in `src/test/resources` and run the following command to execute the integration tests:      mvn clean install -P itest       ## References and History  Magja has been developed on Google Code, so old resources are still available at:  * Magja: http://code.google.com/p/magja/ * Installation for Use: http://code.google.com/p/magja/wiki/Installation * Setup Project for Development: http://code.google.com/p/magja/wiki/SetupProject * Wiki Pages: http://code.google.com/p/magja/w/list   ## Acknowledgments  Bandung Everlasting Beauty logo downloaded from Info Bandung (http://infobandung.codeplex.com/) CodePlex project designed by Achmad Sutarjono.
JetBrains/teamcity-nuget-support	# TeamCity NuGet support [![official JetBrains project](http://jb.gg/badges/official-plastic.svg)](https://confluence.jetbrains.com/display/ALL/JetBrains+on+GitHub) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)    This plugin provides NuGet support features for TeamCity. For more details, please see [NuGet support description](https://confluence.jetbrains.com/display/TCDL/NuGet).    # Download    The plugin is bundled from TeamCity 7.0. If you need the latest build, download and install it as an [additional TeamCity plugin](https://confluence.jetbrains.com/display/TCDL/Installing+Additional+Plugins).    | Plugin | Status | Download | TeamCity |  |--------|--------|----------|----------|  | master | <a href="https://teamcity.jetbrains.com/viewType.html?buildTypeId=TeamCityPluginsByJetBrains_NuGet_NuGetSupportTrunkFor20172x&guest=1"><img src="https://teamcity.jetbrains.com/app/rest/builds/buildType:(id:TeamCityPluginsByJetBrains_NuGet_NuGetSupportTrunkFor20172x)/statusIcon.svg" alt=""/></a> | [Download](https://teamcity.jetbrains.com/repository/download/TeamCityPluginsByJetBrains_NuGet_NuGetSupportTrunkFor20172x/.lastSuccessful/dotNetPackagesSupport.zip?guest=1)| 2017.2.x |  | 0.15 | <a href="https://teamcity.jetbrains.com/viewType.html?buildTypeId=TeamCityPluginsByJetBrains_NuGet_NuGetSupportV015for10x&guest=1"><img src="https://teamcity.jetbrains.com/app/rest/builds/buildType:(id:TeamCityPluginsByJetBrains_NuGet_NuGetSupportV015for10x)/statusIcon.svg" alt=""/></a> | [Download](https://teamcity.jetbrains.com/repository/download/TeamCityPluginsByJetBrains_NuGet_NuGetSupportV015for10x/.lastSuccessful/dotNetPackagesSupport.zip?guest=1)| 2017.1.x |  | 0.14 | <a href="https://teamcity.jetbrains.com/viewType.html?buildTypeId=TeamCityPluginsByJetBrains_NuGet_NuGetSupportV014for100x&guest=1"><img src="https://teamcity.jetbrains.com/app/rest/builds/buildType:(id:TeamCityPluginsByJetBrains_NuGet_NuGetSupportV014for100x)/statusIcon.svg" alt=""/></a> | [Download](https://teamcity.jetbrains.com/repository/download/TeamCityPluginsByJetBrains_NuGet_NuGetSupportV014for100x/.lastSuccessful/dotNetPackagesSupport.zip?guest=1)| 10.0.x |  | 0.13 | <a href="https://teamcity.jetbrains.com/viewType.html?buildTypeId=TeamCityPluginsByJetBrains_NuGet_NuGetSupportV013for91x&guest=1"><img src="https://teamcity.jetbrains.com/app/rest/builds/buildType:(id:TeamCityPluginsByJetBrains_NuGet_NuGetSupportV013for91x)/statusIcon.svg" alt=""/></a> | [Download](https://teamcity.jetbrains.com/repository/download/TeamCityPluginsByJetBrains_NuGet_NuGetSupportV013for91x/.lastSuccessful/dotNetPackagesSupport.zip?guest=1)| 9.1.x |  | 0.12 | <a href="https://teamcity.jetbrains.com/viewType.html?buildTypeId=TeamCityPluginsByJetBrains_NuGet_NuGetSupportV012for90&guest=1"><img src="https://teamcity.jetbrains.com/app/rest/builds/buildType:(id:TeamCityPluginsByJetBrains_NuGet_NuGetSupportV012for90)/statusIcon.svg" alt=""/></a> | [Download](https://teamcity.jetbrains.com/repository/download/TeamCityPluginsByJetBrains_NuGet_NuGetSupportV012for90/.lastSuccessful/dotNetPackagesSupport.zip?guest=1)| 9.0.x |  | 0.11 | <a href="https://teamcity.jetbrains.com/viewType.html?buildTypeId=TeamCityPluginsByJetBrains_NuGet_NuGetSupportV011for8&guest=1"><img src="https://teamcity.jetbrains.com/app/rest/builds/buildType:(id:TeamCityPluginsByJetBrains_NuGet_NuGetSupportV011for8)/statusIcon.svg" alt=""/></a> | [Download](https://teamcity.jetbrains.com/repository/download/TeamCityPluginsByJetBrains_NuGet_NuGetSupportV011for8/.lastSuccessful/dotNetPackagesSupport.zip?guest=1)| 8.1.x |  | 0.10 | <a href="https://teamcity.jetbrains.com/viewType.html?buildTypeId=TeamCityPluginsByJetBrains_NuGetSupportV010for80&guest=1"><img src="https://teamcity.jetbrains.com/app/rest/builds/buildType:(id:TeamCityPluginsByJetBrains_NuGetSupportV010for80)/statusIcon.svg" alt=""/></a> | [Download](https://teamcity.jetbrains.com/repository/download/TeamCityPluginsByJetBrains_NuGetSupportV010for80/.lastSuccessful/dotNetPackagesSupport.zip?guest=1)| 8.0.x |    # Building the plugin  This project uses gradle as a build system. To resolve non-public libraries, you need to execute `:nuget-server:installTeamCity` gradle task or have a local TeamCity installation and define `teamcityDir` in the [gradle properties](https://docs.gradle.org/current/userguide/build_environment.html). After that you can open it in [IntelliJ IDEA](https://www.jetbrains.com/idea/help/importing-project-from-gradle-model.html) or [Eclipse](http://gradle.org/eclipse/).    ## Gradle tasks  * `:nuget-extensions:msbuild` - build .net nuget extensions.  * `:nuget-extensions:nunit` - run .net nuget extensions tests.  * `:nuget-server:installTeamCity` - downloads TeamCity distribution.  * `:nuget-server:assemble` - assemble nuget support plugin.  * `:nuget-server:build` - build & test nuget support plugin.    ## Requirements    On Windows to build nuget extensions should be installed Microsoft Build Tools.  On Linux for that should be installed 'mono-devel' package.    # Contributions  We appreciate all kinds of feedback, so please feel free to send a PR or write [an issue](https://github.com/JetBrains/teamcity-nuget-support/issues).
hakko/subsonic	Subsonic ========  Subsonic is a web-based music streaming service, created by Sindre Mehus and released under the GPL.  This project is forked from Subsonic version 4.6, adding extensive last.fm integration through the MusicCabinet library. See https://github.com/hakko/musiccabinet for details.  There's an introduction to the project at http://dilerium.se/musiccabinet.  Pre-requisites --------------  To use this add-on, you need a PostgreSQL database running on the same host as your Subsonic server. There are pre-built binary packages available for most platforms at http://www.postgresql.org/download/. During install: * you'll be asked to create a database user. Stick to the default name, "postgres". * you'll be asked to choose a port number. Stick to the default value, "5432".  You also need Java 7. Uninstalling Java 6 is a good idea, unless you don't explicitly need it.  Building --------  The build process assumes that you have Java 7 or later and Maven 3 installed, and that PostgreSQL is running.      Clone git@github.com:hakko/musiccabinet.git to $workspace/musiccabinet     Update your PostgreSQL password in $workspace/musiccabinet/musiccabinet-server/src/main/resources/local.jdbc.properties     cd $workspace/musiccabinet/musiccabinet-server     mvn compile     mvn exec:java -Dexec.mainClass=com.github.hakko.musiccabinet.service.DatabaseAdministrationService     mvn install      Clone git@github.com:hakko/subsonic.git to $workspace/subsonic.     cd $workspace/subsonic     mvn install     cd $workspace/subsonic/subsonic-booter     mvn install      cd $workspace/subsonic/subsonic-installer-standalone     mvn package  To also build a .war file that runs on a Tomcat server, execute:      cd $workspace/subsonic/subsonic-main     mvn -P tomcat package  If a test case fails during build, please report and re-run with mvn -fn as a workaround.  Installation ------------  1. If you're already running Subsonic: stop your service, and backup your settings. (Just in case.) 2. Go to $workspace/subsonic/subsonic-installer-standalone/target. 3. Unzip subsonic-installer-standalone.zip to preferred install directory. 4. In the install directory, tweak and run the subsonic.sh script to start your new server.  Log in to Subsonic as usual (localhost:4040, as admin/admin) and click the "Configure MusicCabinet" link (the header). It should be pretty self-explanatory from there.  Please note that the initial import of data from last.fm will take a while, roughly 30 minutes per 10.000 tracks. You can follow the progress meanwhile but MusicCabinet features won't work until it's finished.
franzinc/agraph-java-client	// This header text is used by Github to form an anchor on the project page. // We link to this anchor from the AllegroGraph client download page // (https://franz.com/agraph/downloads/clients) so if you change this header // text, you must update the link on the client download page. = Java client for Franz AllegroGraph  :version: 2.0.1  This is the Java client for the http://franz.com/agraph/[Franz AllegroGraph] triple store.  It contains http://rdf4j.org/[Sesame/RDF4J-] and https://jena.apache.org/[Jena]-compatible APIs.  == Client prerequisites  Java version 8 or higher, and any operating system should work with these jars.  === Usage  ==== Maven  The recommended way of using the client API is to create a dependency on its Maven artifact. To do this in a Maven project, add the following to your dependencies:  [source,xml,subs="verbatim,attributes"] ---- <dependency>   <groupId>com.franz</groupId>   <artifactId>agraph-java-client</artifactId>   <version>{version}</version>   <scope>compile</scope> </dependency> ----  For Gradle, use this  [source,groovy,subs="verbatim,attributes"] compile group: 'com.franz', name: 'agraph-java-client', version: '{version}'  Apache Ivy syntax:  [source,xml,subs="verbatim,attributes"] <dependency org="com.franz" name="agraph-java-client" rev="{version}"/>  SBT (Scala):  [source,scala,subs="verbatim,attributes"] libraryDependencies += "com.franz" % "agraph-java-client" % "{version}"  Leiningen (Clojure):  [source,clojure,subs="verbatim,attributes"] ---- [com.franz/agraph-java-client "{version}"] ----  ==== Distribution archive  Tar distributions of the AllegroGraph Java client are available from ftp://ftp.franz.com/pub/agraph/java-client/.  A tar distribution includes jar files for agraph-java-client and jars on which it depends.  Programs that use the API should include all JAR files from the `lib/` directory of the distribution on their classpath.  To use that local JAR file through Maven install it into your local repository:  [source,sh,subs="verbatim,attributes"] mvn install:install-file -Dfile=lib/agraph-java-client-{version}.jar -DpomFile=pom.xml  Note that when using the API through Maven all dependencies are downloaded from the central repository - JARs included in the distribution are not used.  == Development  === Testing  Use this command to run the default testsuite. This assumes that AllegroGraph is listening on the local machine on the default port (10035).      mvn test  To include long running tests, do      mvn test -Dtests.include=test.TestSuites\$Prepush,test.TestSuites\$Stress  === Distribution  To build a distribution archive, do      mvn package  == Tutorials  There are three tutorials located in tutorials/ that can be compiled and run. They are:    sesame/::  Example usage of the AG Sesame/RDF4J interface   jena/:: Example usage of the AG Jena interface   attributes/:: Example usage of AG Triple Attributes  === Prerequisites  An AllegroGraph server must be up and running in order to run the tutorials.  The class for each tutorial declares a number of variables near the top of its respective class definition, which provide the information necessary to communicate with AllegroGraph. If necessary, modify these variables to match the settings for your server before compiling each tutorial.  By default, each tutorial looks for AllegroGraph on localhost at port 10035. Each will create a repository named after the respective tutorial in the "java-catalog" catalog.  In order for the tutorial to run successfully, you must ensure that the "java-catalog" catalog has been defined in your agraph.cfg prior to starting AG, or change the value of CATALOG_ID to name a catalog that exists on your server. Use the empty string ("") or null to indicate the root catalog. All other variables must be updated to correspond to your server configuration as well.   === Compiling Tutorials  Each tutorial is a separate Maven project. To compile the tutorials first install the AllegroGraph Java client into your local repository. This process is described in the 'Usage' section. Then run the following command in the directory containing the tutorial:       mvn compile  ### Running Tutorials  To run one of the tutorials, use the following command line:      mvn exec:java  in the directory containing the tutorial you wish to run.  Sesame and Jena tutorials contain multiple, numbered examples. It is possible to run just the specified examples by passing their numbers as arguments in the following way:     mvn exec:java -Dexec.args="1 2 3 5 8 13 21"  The argument 'all' indicates that all examples should be run.  == License  Copyright (c) 2008-2017 Franz Inc. All rights reserved. This program and the accompanying materials are made available under the terms of the Eclipse Public License v1.0 which accompanies this distribution, and is available at http://www.eclipse.org/legal/epl-v10.html[http://www.eclipse.org/legal/epl-v10.html]
threerings/nenya	The Nenya library =================  The Nenya library provides various facilities for making networked multiplayer games. Its various packages include:  * geom, util - basic tools for doing data structure manipulation and some   geometry math * resource - tools for bundling, deploying and managing media (images,   sounds, etc.) with a game * media - a framework for doing "active" rendering in Java * media.image - tools for loading, caching, manipulating and displaying images * media.sound - tools for loading, caching, and playing audio * media.animation, media.sprite - works in concert with the active   rendering system and provides tools for defining and manipulating   sprites (graphical entities that follow paths) and animations   (graphical entities that affect the display in other ways) * miso - a framework for defining and displaying isometrically rendered scenes * cast - a framework for defining and using recolorable, composited   characters with different poses and actions  [Javadoc documentation](http://threerings.github.com/nenya/apidocs/) is provided.  Tutorial style documentation is somewhat sparse at the moment, but inspection of the code in the `src/test/java` directory shows examples of use of many features of the library.  Building --------  The library is built using Maven, or [Ant](http://ant.apache.org/).  The Maven build uses the standard targets:      package: builds the code and creates the jars and swcs     install: builds and installs the artifacts into your local Maven repository     test: builds and runs the unit tests  The Ant build uses the following targets:      all: builds the distribution files and javadoc documentation     compile: builds only the class files (dist/classes)     javadoc: builds only the javadoc documentation (dist/docs)     dist: builds the distribution jar files (dist/*.jar)  Artifacts ---------  Nenya provides three different build artifacts, for differing purposes:  * nenya: contains the main Java library; exports dependencies only for   libraries which are required by a running client or server. * nenya-tools: contains the media precompilation portions of Nenya which one   need integrate into their build; exports dependencies for libraries needed   when running precompilation. * nenyalib: contains the main ActionScript library; exports dependencies for   libraries needed when building a SWF using nenyalib.  Nenya is published to Maven Central. To add a Nenya dependency to a Maven project, add the following to your `pom.xml`:      <dependencies>       <dependency>         <groupId>com.threerings</groupId>         <artifactId>nenya</artifactId>         <version>1.6</version>       </dependency>     </dependencies>  To add it to an Ivy, SBT, or other Maven repository using project, simply remove the vast majority of the boilerplate above.  If you prefer to download pre-built binaries, those can be had here:  * [nenya-1.6.jar](http://repo2.maven.org/maven2/com/threerings/nenya/1.6/nenya-1.6.jar) * [nenya-tools-1.6.jar](http://repo2.maven.org/maven2/com/threerings/nenya-tools/1.6/nenya-tools-1.6.jar) * [nenyalib-1.6.swc](http://repo2.maven.org/maven2/com/threerings/nenyalib/1.6/nenyalib-1.6.swc)  Distribution ------------  The Nenya library is released under the LGPL. The most recent version of the library is available at http://github.com/threerings/nenya  Contact -------  Questions, comments, and other worldly endeavors can be handled via the [Three Rings Libraries](http://groups.google.com/group/ooo-libs) Google Group.  Nenya is actively developed by the scurvy dogs at [Three Rings](http://www.threerings.net) Contributions are welcome.
danielflower/maven-gitlog-plugin	![Build Status](https://travis-ci.org/danielflower/maven-gitlog-plugin.png?branch=master)    Overview  ========    This plugin allows the creation of text and HTML changelogs based on the git log. During the Maven packaging  phase this plugin can generate plaintext and HTML reports showing all the commits (with tags) from the local git  repository.  These text files can then be sent to a web server or included during packaging.    Using as a reporting plugin (with maven 3.1+), the site generation will include the generated gitlog.  In this case, the outputDirectory parameter can not be set.    Note: when using together with the reporting plugin named *changelog*, it is advised to change  the *simpleHTMLChangeLogFilename* parameter to *gitlog.html*.    Usage instructions and Documentation  ====================================    See the **[Maven Gitlog Plugin documentation](http://danielflower.github.io/maven-gitlog-plugin/)** for usage and more information.
bristleback/bristleback	== This project is no longer maintained, if you would like to maintain it, please write an issue or create a pull request so I can add you.  == Bristleback Framework version 0.3.5 Welcome,  Bristleback Framework has got a new homepage http://bristleback.pl. Several samples and tutorials can be found there.  If you like our project, please star or fork the project!  Latest Bristleback version: 0.3.5  Download section is no longer valid on GitHub. To download JavaScript client, visit http://bristleback.pl/download.html  Check our wiki pages for tutorials.  0.3.5 is now available in central Maven repository:      <dependency>       <groupId>pl.bristleback</groupId>       <artifactId>bristleback-core</artifactId>       <version>0.3.5</version>     </dependency>  Few samples are available at https://github.com/bristleback/bristleback-samples  === You can also test Bristleback by performing just few steps:  1. Go to directory where your new Bristleback Project will be created and type:      mvn archetype:generate       -DarchetypeGroupId=pl.bristleback       -DarchetypeArtifactId=webapp-archetype       -DarchetypeVersion=0.3.5  2. Enter additional information about the project.  3. Go to newly created project directory and type mvn jetty:run - your first Bristleback Framework with Jetty as Server Engine is starting!  4. Open your browser and type localhost:8080  5. Have fun developing!
apache/shindig	Apache Shindig    What is it?   -----------    Shindig is a JavaScript container and implementations of the backend APIs   and proxy required for hosting OpenSocial applications.    Documentation   -------------    The most up-to-date documentation can be found at http://shindigapache.org/    Read BUILD-JAVA for instructions on how to build and run the Java server.    Read java/README for instructions on how to run a Java gadget server.    Read php/README for instructions on how to run a php gadget server.    Read javascript/README for instructions for using the Shindig Gadget Container    JavaScript to enable your page to render Gadgets.    Read features/README for instructions on how to use features.    Licensing   ---------    Please see the file called LICENSE in the java and php directories.    Shindig URLS   ------------    Home Page:          http://shindig.apache.org/   Downloads:          http://shindig.apache.org/download/index.html   Mailing Lists:      http://shindig.apache.org/mail-lists.html   Source Code:        http://svn.apache.org/repos/asf/shindig/   Issue Tracking:     https://issues.apache.org/jira/browse/SHINDIG   Wiki:               http://cwiki.apache.org/confluence/display/SHINDIG/   This distribution includes cryptographic software.  The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software.  BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted.  See <http://www.wassenaar.org/> for more information.  The U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms.  The form and manner of this Apache Software Foundation distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.  The following provides more details on the included cryptographic software:      Apache Shindig PHP interfaces with the mcrypt API     <http://mcrypt.sourceforge.net/> to provide encryption     of messages using the AES standard.      Apache Shindig interfaces with the Java JCE APIs to provide     encryption of messages using the AES standard.
apache/batik	A  P  A  C  H  E     B  A  T  I  K    What is it?   -----------    Batik is a Java based toolkit for applications which handle   images in the Scalable Vector Graphics (SVG) format for   various purposes, such as viewing, generation or   manipulation.     The project's ambition is to give developers a set of core   modules which can be used together or individually to   support specific SVG solutions. Examples of modules are   an SVG parser, an SVG generator and an SVG DOM   implementation. Another ambition of the Batik project is to   make it highly extensible (for example, Batik allows the   developer to handle custom SVG tags). Even though the   goal of the project is to provide a set of core modules, one   of the deliveries is a full fledged SVG Viewer   implementation which validates the various modules and   their inter-operability.     In a nutshell, Batik provides building blocks that developers   can assemble in various ways in their Java technology   applications to generate, parse, view or convert SVG   contents. For example, Batik contains a Swing component   that can add SVG viewing capability to all Java technology   applications. Batik can also be used to generate SVG on a   client or on a server, and Batik can convert SVG content   into other formats such as JPEG or PNG. Batik's goal is to   make it easy for application developers to handle SVG   content for various purposes, client-side or server-side.     Where is it?   ------------    The home page for the Apache Batik project can be found in the Apache XML    Project web site (http://xmlgraphics.apache.org/batik/). There you also find    information on how to download the latest release as well as all the other    information you might need regarding this project.     Requirements   ------------     o  A Java 1.6 or later compatible virtual machine for your operating system.    Optional Libraries   ------------------    By default, Batik includes a scripting engine for ECMAScript. It is possible   to add support for additional scripting languages (Python and TCL).      See http://xmlgraphics.apache.org/batik/install.html#optionalComponents for   details.     Installation Instructions and Documentation   -------------------------------------------    Read the Install page at http://xmlgraphics.apache.org/batik/install.html   for the installation instructions.    Look for the most updated documentation on the Apache Batik web site under   the Apache XML Graphics Project (http://xmlgraphics.apache.org/batik/).     Licensing and legal issues   --------------------------    For legal and licensing issues, please read the LICENSE and NOTICE files.    Thanks for using Apache Batik.                                             The Apache XML Graphics Project                                            http://xmlgraphics.apache.org/
plista/kornakapi	kornak-api is an easy-to-use webservice for the recommenders of [Apache Mahout](http://mahout.apache.org)  An example of setting this up for a movie recommender is available in [A recommendation webservice in 10 minutes](http://ssc.io/a-recommendation-webservice-in-10-minutes/)  For early adoptors before 2012/09/04: Path of this project changed from plistaorg/kornakapi to plista/kornakapi
scottfrederick/springdoclet	# SpringDoclet  SpringDoclet is Javadoc doclet that generates documentation on Spring artifacts in a project. The detection of Spring artifacts is based on the presence of Spring annotations on Java classes and methods.  See the [Project Page](http://scottfrederick.github.com/springdoclet) for more information.
alexnederlof/Jasper-report-maven-plugin	JasperReports-plugin =============  This maven compiles Jasper files to the target directory.   Motivation ---------- The original jasperreports-maven-plugin from org.codehaus.mojo was a bit slow. This plugin is 10x faster. I tested it with 52 reports which took 48 seconds with the original plugin and only 4.7 seconds with this plugin.  Usage ----- You can use the plugin by adding it to the plug-in section in your pom;  ```xml <build> 	<plugins> 		<plugin> 			<groupId>com.alexnederlof</groupId> 			<artifactId>jasperreports-plugin</artifactId> 			<version>2.0</version> 			<executions> 				<execution> 					<phase>process-sources</phase> 	   				<goals> 	      					<goal>jasper</goal> 	   				</goals> 	   			</execution> 			</executions> 			<configuration> 				<!-- These are the default configurations: --> 				<compiler>net.sf.jasperreports.engine.design.JRJdtCompiler</compiler> 				<sourceDirectory>src/main/jasperreports</sourceDirectory> 				<outputDirectory>${project.build.directory}/jasper</outputDirectory> 				<outputFileExt>.jasper</outputFileExt> 				<xmlValidation>true</xmlValidation> 				<verbose>false</verbose> 				<numberOfThreads>4</numberOfThreads> 				<failOnMissingSourceDirectory>true</failOnMissingSourceDirectory> 				<sourceScanner>org.codehaus.plexus.compiler.util.scan.StaleSourceScanner</sourceScanner> 			</configuration> 		</plugin> 	</plugins> </build> ```  If you want to pass any Jasper options to the compiler you can do so by adding them to the configuration like so:  ```xml <plugin> 	... 	<configuration> 		... 		<additionalProperties> 			<net.sf.jasperreports.awt.ignore.missing.font>true</net.sf.jasperreports.awt.ignore.missing.font> 			<net.sf.jasperreports.default.pdf.font.name>Courier</net.sf.jasperreports.default.pdf.font.name> 			<net.sf.jasperreports.default.pdf.encoding>UTF-8</net.sf.jasperreports.default.pdf.encoding> 			<net.sf.jasperreports.default.pdf.embedded>true</net.sf.jasperreports.default.pdf.embedded>            </additionalProperties> 	</configuration> </plugin> ```  You can also add extra elements to the classpath using  ```xml <plugin> 	... 	<configuration> 		... 		<classpathElements> 			<element>your.classpath.element</element>         </classpathElements> 	</configuration> </plugin> ```
moneymanagerex/android-money-manager-ex	# MoneyManagerEx for Android    See our homepage at [http://android.moneymanagerex.org/](http://android.moneymanagerex.org/) for more user-oriented information.    #### Basic information    Money Manager Ex for Android is an port of the PC/Desktop personal finance application, currently available for Windows, Mac OSX and LINUX. It implements the most important features of the desktop version, while adding mobility and synchronization.    #### Links    [![MoneyManagerEx for Android on PlayStore](https://developer.android.com/images/brand/en_app_rgb_wo_60.png)](http://play.google.com/store/apps/details?id=com.money.manager.ex)    #### Translate    If you want to join our translation team: [MoneyManagerEx for Android on Crowdin.net](https://crowdin.net/project/android-money-manager-ex)    #### Beta Testing    You can install the Beta version from [Google Play](https://play.google.com/store/apps/details?id=com.money.manager.ex.beta) in parallel to the stable version, and help us out testing the app before it reaches the Stable channel.    # License        Copyright 2012-2015 The Android Money Manager Ex Project Team        This program is free software; you can redistribute it and/or      modify it under the terms of the GNU General Public License      as published by the Free Software Foundation; either version 3      of the License, or (at your option) any later version.        https://www.gnu.org/licenses/gpl-2.0.html        This program is distributed in the hope that it will be useful,      but WITHOUT ANY WARRANTY; without even the implied warranty of      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the      GNU General Public License for more details.         You should have received a copy of the GNU General Public License      along with this program.  If not, see <http://www.gnu.org/licenses/>.
Omertron/api-imdb	IMDb JSON API =============  This is an java implementation of the IMDb JSON API.  For use by clients authorized in writing by IMDb. Authors and users of unauthorized clients accept full legal exposure/liability for their actions.  [![Build Status](http://jenkins.omertron.com/job/API-IMDB/badge/icon)](http://jenkins.omertron.com/job/API-IMDB)  Project Documentation --------------------- The automatically generated documentation can be found [HERE](http://omertron.github.com/api-imdb/)
martypitt/JsonViewExample	JsonViewExample  ===============    Support for custom Jackson @JsonView within Spring MVC    Allows per-method support of custom views within Spring MVC methods, by annotating with `@ResponseView`.      For example:        @RequestMapping("/books")      public @ResponseBody List<Book> getBooks()      {         return data;      }      @RequestMapping("/books/summaries")      @ResponseView(SummaryView.class)      public @ResponseBody List<Book> getBookSummaries()      {          return data;      }    In the above scenario, calls to `/books` recieve the entire `Book` model, while calls to `/books/summaries` only recieve properties annotated with Jackson's `@JsonView(SummaryView.class)` annotation.    See http://martypitt.wordpress.com/2012/11/05/custom-json-views-with-spring-mvc-and-jackson/ for details.
deegree/deegree3	deegree is open source software for spatial data infrastructures and the geospatial web. deegree includes components for geospatial data management, including data access, visualization, discovery and security. Open standards are at the heart of deegree. The software is built on the standards of the Open Geospatial Consortium (OGC) and the ISO Technical Committee 211.  # User documentation General project information and user documentation (e.g. "How to set up WMS and WFS?" or "How to get support?")  can be found on the deegree homepage:  http://www.deegree.org  # Developer documentation Developer-related information (e.g. "How to build deegree webservices?") can be found on the deegree3 project wiki on GiHub:  https://github.com/deegree/deegree3/wiki  # License  [![License](https://img.shields.io/badge/License-LGPL%20v2.1-blue.svg)](https://www.gnu.org/licenses/lgpl-2.1)  deegree is distributed under the GNU Lesser General Public License, Version 2.1 (LGPL 2.1). Generally speaking this means that you have essential freedoms such as: Run the software for any purpose, find out how it works, make changes, redistribute copies (of modified versions). Practically, with these freedoms, you do not have to pay a license fee, you can create as many installations as you need, and you're not bound to one single vendor. Instead you can contact a service provider of your choice to make necessary configurations or code adjustments. Or you may even step up and do this yourself.  More information about free software can be found at the free software foundation. A good starting point is [www.gnu.org](http://www.gnu.org).
ashokgelal/Tagsnap	##About Tagsnap  This is the official project repository for [Writing a Real Android App from Scratch][1] tutorial series. By the end of the series we write a real Android app - Tagsnap.   <img src="https://dl.dropbox.com/u/83257/Tagsnap_Screenshots/screenshot.png" width="50%" />  This app uses, among others, LocationManager, SQLiteDatabase, Map, and Camera etc. It allows a user to get his/ her current location, capture a picture of the location (could be a park, museum, historical landmark, or anything), add some details providing a description/ title, and selecting a category from a pre-populated list of categories. The user can either use a built-in camera, or use the Gallery app to take a picture. Later, the user will be able to see a list of all these geotagged snaps. Each item can be edited, or deleted from the list. It also has a Map view that displays markers for each of these geotagged snaps. The user can select any of these markers to see more information. These snaps will be persistent – saving all the information in a local database.  Developed By ============  * Ashok Gelal - <ashokgelal@gmail.com>    License =======      Copyright 2013 Ashok Gelal      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.            [1]: http://www.ashokgelal.com/tag/tagsnap_tutorial/
apache/cayenne	<!-- 	Licensed to the Apache Software Foundation (ASF) under one 	or more contributor license agreements.  See the NOTICE file 	distributed with this work for additional information 	regarding copyright ownership.  The ASF licenses this file 	to you under the Apache License, Version 2.0 (the 	"License"); you may not use this file except in compliance 	with the License.  You may obtain a copy of the License at 	 	http://www.apache.org/licenses/LICENSE-2.0 	 	Unless required by applicable law or agreed to in writing, 	software distributed under the License is distributed on an 	"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY 	KIND, either express or implied.  See the License for the 	specific language governing permissions and limitations 	under the License.    --> [![Build Status](https://travis-ci.org/apache/cayenne.svg)](https://travis-ci.org/apache/cayenne) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.cayenne/cayenne-server/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.apache.cayenne/cayenne-server/)  Apache Cayenne ==============  Apache Cayenne is an open source persistence framework licensed under the Apache License, providing object-relational mapping (ORM) and remoting services. With a wealth of unique and powerful features, Cayenne can address a wide range of persistence needs. Cayenne seamlessly binds one or more database schemas directly to Java objects, managing atomic commit and rollbacks, SQL generation, joins, sequences, and more. With Cayenne's Remote Object Persistence, those Java objects can even be persisted out to clients via Web Services.  Cayenne  is designed to be easy to use, without sacrificing flexibility or design. To that end, Cayenne supports database reverse engineering and generation, as well as a Velocity-based class generation engine. All of these functions can be controlled directly through the CayenneModeler, a fully functional GUI tool. No cryptic XML or annotation based configuration is required! An entire database schema can be mapped directly to Java objects within minutes, all from the comfort of the GUI-based CayenneModeler.  Cayenne supports numerous other features, including caching, a complete object query syntax, relationship pre-fetching, on-demand object and relationship faulting, object inheritance, database auto-detection, and generic persisted objects. Most importantly, Cayenne can scale up or down to virtually any project size. With a mature, 100% open source framework, an energetic user community, and a track record of solid performance in high-volume environments, Cayenne is an exceptional choice for persistence services.  License ---------  Cayenne is available as free and open source under the Apache License, Version 2.0.  Documentation --------------  Full documentation in both pdf and html formats, tutorials, mailing lists and much more are available from our website at http://cayenne.apache.org/ . E.g. here is 3.1 [Cayenne Guide](http://cayenne.apache.org/docs/3.1/cayenne-guide/index.html).
skprasadu/spring-security-examples	Spring Security ===============  [Spring Test MVC](https://github.com/SpringSource/spring-test-mvc) is a good framework for testing Spring MVC application.  In this sample, we demonstrated a simple Calendar application, where a regular user can create a Event and can see others event but cannot modify them. Admin user can modify other user's event as well.  We will demonstrate the Spring Security capability for Inmemory authorization provider, JDBC authorization provider, LDAP authorization provider, CAS Single sign on authorization provider.  Refer to this [blog](http://krishnasblog.com/2013/02/10/spring-test-mvc-junit-testing-spring-security-layer-with-inmemorydaoimpl-2/) for more details.
dianping/wormhole	wormhole ========  Wormhole is dianping massive data transfer tool, it currently support data source and destination like hdfs, hive, hbase, mysql, greenplum, sqlserver, mongodb, sftp, salesforce  Project contact: yukang chen (yukang.chen@dianping.com)   Copyright and license ---------------------  Copyright 2013 DianPing, Inc.  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this work except in compliance with the License. You may obtain a copy of the License in the LICENSE file, or at:  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
drtshock/Essentials	![](https://i.imgur.com/CP4SZpB.png)  [![Downloads](https://i.imgur.com/MMc0PJY.png)](https://ci.drtshock.net/job/EssentialsX)  This is a fork of Essentials called EssentialsX.  If you are using this, do **NOT** ask Essentials for support.  The official upstream repository is at https://github.com/Essentials/Essentials  Why you should use it --------  EssentialsX provides several performance enhancements and fixes that are currently not available in Essentials and Spigot-Essentials, most notably mob spawner support for 1.8+ servers and buy/trade sign support for 1.9+ servers. [See the wiki for details.](https://github.com/drtshock/Essentials/wiki)  EssentialsX is almost a completely drop-in replacement for Essentials. However, it has different requirements:  * **EssentialsX requires [Vault](http://dev.bukkit.org/bukkit-plugins/vault/) to enable chat prefix/suffixes and group support if you have a supported permissions plugin.**  * **If you have an unsupported permissions plugin but still wish to use wildcards, enable `use-bukkit-permissions` in the configuration. Otherwise, the plugin will fall back to config-based permissions.**  * **EssentialsX requires Java 7 or higher.**  * **1.7.10 is no longer supported.**  Building --------  Because EssentialsX builds against the Spigot/CraftBukkit server software for legacy support, you need to run Spigot's BuildTools for several versions in order to get it to compile.  ``` java -jar BuildTools.jar --rev 1.8 java -jar BuildTools.jar --rev 1.8.3 java -jar BuildTools.jar --rev 1.9 java -jar BuildTools.jar --rev 1.9.4 ```  Then, to build with Maven, use the command ``` mvn clean install ```  Jar files can then be found in the /target folder for each module.   Commit Guidelines -----------------  Commits should fall into one of 3 areas:  - `[Feature]`: Commits which are features should start with `[Feature]` and followed by a quick summary on the top line, followed by some extra details in the commit body.  - `[Fix]`: Commits which fix bugs, or minor improvements to existing features should start with `[Fix]` and followed by a quick summary on the top line, followed by some extra details in the commit body.  - Commits which fix bugs caused by previous commits (since last release), or otherwise make no functionality changes, should have no prefix.  These will not be added to the project change log.   Other Info -----------------  This is an unofficial fork of Essentials. It will be consistently updated and maintained with the latest Minecraft and Spigot versions.  Support ----------------- [Issue Tracker](https://github.com/drtshock/Essentials/issues)  [Live Support](http://webchat.esper.net/?channels=essentialsx&prompt=1) available business hours (GMT -5)
kalixia/Grapi	# Grapi: Generated REST API  Grapi is a Java source code generator based on APT (Javac plugin).  Grapi analyzes your JAX-RS source code and generate some code in order to expose your JAX-RS resources through [Netty](http://netty.io) 4.   ## Introduction  Basically Grapi generates some optimized Java code for ease of use of JAX-RS resources through Netty.  There is no runtime dependency on any JAX-RS provider (*actually only a small one on Jersey UriTemplate class which will be removed later on*) and Grapi is not a JAX-RS provider.  Grapi avoids the introspection *crap* and generates Java source code which will work according to the JAX-RS resources. When some JAX-RS features aren't used, some Java code is dropped from the generated source code in order to both reduce complexity of the generated source code and improve the performance and maintenance of code.  Additionally, Grapi can generate a [Dagger](https://github.com/square/dagger) module in order to simplify even more the use of the generated code and also use [Metrics](http://metrics.codahale.com) in order to measure various timings about your JAX-RS resources.   ## Current Status  The project is in early stages. It's being successfully used and tested on other projects.  ### Build Status  [![Build Status](https://travis-ci.org/kalixia/Grapi.svg?branch=master)](https://travis-ci.org/kalixia/Grapi)  ### Working Features  | Feature                       | Description |-------------------------------|---------------------------------------------------------------------------------------------------- | @Path                         | Full working with all HTTP verbs, URI templates and path precedence rules | @QueryParam, @FormParam, @HeaderParam, @CookieParam | Proper extraction from HTTP requests | @Produces                     | Correctly expose your resources with the proper content-type | Data conversion               | Uses Jackson in order to convert objects to the appropriate data format (XML, JSon, etc.) -- both for incoming data and outgoing data | JAX-RS Response               | Can partially cope with JAX-RS ``` Response ``` results. | WebApplicationException       | Basic support. | CORS                          | Netty CorsHandler can be properly used. | Dagger                        | Generate a Dagger module simplifying even more the use of the generated code | Metrics                       | Expose your JAX-RS resource calls as [Metrics](http://metrics.codahale.com) | JSR-349 (Bean Validation 1.1) | If your JAX-RS resources are annotated with Bean Validation annotations, the handlers to check the parameters and result   ### Current Limitations  They are though some few limitations worth knowing. These limitations will be taken care of and are only missing features:  * the WS stack is highly experimental at the moment * no support for UriInfo injection and UriBuilder * no support for @Consumes * no support for @MatrixParam * no support for processing Groovy sources (probably through AST transformations)   ## Setup  The project is available from a [Bintray](https://bintray.com/kalixia/Grapi) repository: ```xml <repository>     <id>grapi</id>     <name>Grapi</name>     <url>http://dl.bintray.com/kalixia/Grapi</url>     <releases><enabled>true</enabled></releases>     <snapshots><enabled>false</enabled></snapshots> </repository> ```  Then you simply need to add two dependencies to your project/module (where your JAX-RS source code is located): ```xml <dependency>     <groupId>com.kalixia.grapi</groupId>     <artifactId>netty-codecs</artifactId>     <version>0.4.5</version> </dependency>  <dependency>     <groupId>com.kalixia.grapi</groupId>     <artifactId>netty-compiler</artifactId>     <version>0.4.5</version> </dependency> ```  ## Configuration  There are currently three options available:  | Option  | Description |---------|------------ | dagger  | Additionally generate a Dagger module, named ``` GeneratedJaxRsDaggerModule ``` | metrics | Additionally use [Metrics](http://metrics.codahale.com) for tracing JAX-RS resource method calls | shiro   | Additionally analyze [Shiro](http://shiro.apache.org) annotations in JAX-RS resources and ensure they are met  In order to configure a Maven project, you can use something like this: ```xml <plugin>     <artifactId>maven-compiler-plugin</artifactId>     <version>3.1</version>     <configuration>         <source>1.7</source>         <target>1.7</target>         <encoding>UTF-8</encoding>         <compilerArgs>             <compilerArg>-Adagger=true</compilerArg>             <compilerArg>-Ametrics=true</compilerArg>             <compilerArg>-Ashiro=true</compilerArg>         </compilerArgs>     </configuration> </plugin> ```  When you run ``` mvn compile ``` you'll find the generated source code in ``` target/generated-sources/annotations ```. Each JAX-RS resource's method generates a Java class. There is also one ``` GeneratedJaxRsModuleHandler ``` created which references all the generated classes.   ## Usage  ### Netty pipeline  Netty uses a ``` ChannelInitializer ``` in order to setup the *pipeline*.  You can create your own one based on the following code. This will allow you JAX-RS resources to be reached both by HTTP requests and via WebSockets.  ```java public class ApiServerChannelInitializer extends ChannelInitializer<SocketChannel> {     private final ObjectMapper objectMapper;     private final ChannelHandler apiProtocolSwitcher;     private final ShiroHandler shiroHandler;     private final GeneratedJaxRsModuleHandler jaxRsHandlers;     private static final ChannelHandler apiRequestLogger = new LoggingHandler(RESTCodec.class, LogLevel.DEBUG);      @Inject     public ApiServerChannelInitializer(ObjectMapper objectMapper,                                        ApiProtocolSwitcher apiProtocolSwitcher,                                        SecurityManager securityManager,                                        GeneratedJaxRsModuleHandler jaxRsModuleHandler) {         this.apiProtocolSwitcher = apiProtocolSwitcher;         this.jaxRsHandlers =  jaxRsModuleHandler;         this.shiroHandler = new ShiroHandler(securityManager);         SimpleModule nettyModule = new SimpleModule("Netty", PackageVersion.VERSION);         nettyModule.addSerializer(new ByteBufSerializer());         objectMapper.registerModule(nettyModule);     }      @Override     public void initChannel(SocketChannel ch) {         ChannelPipeline pipeline = ch.pipeline();          pipeline.addLast("http-request-decoder", new HttpRequestDecoder());         pipeline.addLast("http-response-encoder", new HttpResponseEncoder());         pipeline.addLast("http-object-aggregator", new HttpObjectAggregator(1048576));         pipeline.addLast("shiro", shiroHandler);          // Alters the pipeline depending on either REST or WebSockets requests         pipeline.addLast("api-protocol-switcher", apiProtocolSwitcher);          // Logging handlers for API requests         pipeline.addLast("api-request-logger", apiRequestLogger);          // JAX-RS handlers         pipeline.addLast("jax-rs-handlers", jaxRsHandlers);     } } ```  ### Dagger  Grapi will generate a class named ```GeneratedJaxRsDaggerModule ```. You have two different ways to use this module:  1. reference the generated Dagger module from another module of your own, 2. create the ``` ObjectGraph ``` with this module (and eventually other ones too).  For option 1, you can use the ``` @Module ``` annotation like so: ```java @Module(     injects = Main.class,     includes = { GeneratedJaxRsDaggerModule.class } ) ```  For option 2, you can create your Dagger graph like so: ```java ObjectGraph objectGraph = ObjectGraph.create(new GeneratedJaxRsDaggerModule()); ```  ### Shiro  When enabled, Grapi will extract OAuth2 bearer tokens and authenticate the user via Shiro and Grapi's classes.
Athou/commafeed-newsplus	CommaFeed extension for News+  =============================    How to install on your device  -----------------------------    - Download the extension from the Play Store: https://play.google.com/store/apps/details?id=com.commafeed.newsplus  - Launch the App. You will be prompted to install News+. Install it.  - Launch News+ and click "All items/Google News" in the top-left corner.   - Select CommaFeed.        How to build the extension from sources  ---------------------------------------    - `mvn clean package`  - apk is generated at `target/commafeed-newsplus.apk`
Xorlev/gatekeeper	# Gatekeeper: NGINX co-process for SOA-style infrastructures  Gatekeeper turns NGINX into your edge service. Gatekeeper watches your [Zookeeper](http://zookeeper.apache.org/) service discovery paths and dynamically builds NGINX upstreams for them. Load balancers (ELBs especially) can be SPOFs and are the first steps to madness. Cut out the middle man and let NGINX manage your upstream load balancing.   Gatekeeper uses [Mustache](http://mustache.github.io) templates to build upstreams and locations using your existing `nginx.conf`.      events {         worker_connections  4096;     }      http {         {{#clusters}}             upstream {{clusterName}} {             {{#servers}}                 server {{host}}:{{port}};             {{/servers}}                 keepalive 24;             }         {{/clusters}}         server {             port 80;              {{#locations}}                 location {{context}} {                 {{#upstream}}                     proxy_pass              {{protocol}}://{{clusterName}};                 {{/upstream}}                 {{#attributes}}                     {{key}}                 {{value}}                 {{/attributes}}                 }             {{/locations}}         }     }      With two small blocks of customizable templating, each time a node is registered or deregistered a new `nginx.conf` is written and NGINX sent a `SIGHUP`.  ## Configuration You'll need a NGINX config template and a `gatekeeper.properties`. There are defaults included in `nginx.conf.mustache.default` and `gatekeeper.properties.default`.  **Important:** Configuration changes are automatically reloaded. This may change in the future to require a SIGHUP.  Any additional configuration (e.g. FQDN, SSL, listeners, etc.) can be done in nginx.conf as you'd normally do. You can even include a default upstream and default context directly in the template.  Your `gatekeeper.properties` and `nginx.conf.mustache` should be managed by a configuration management platform, e.g. Chef/Ansible/Puppet.  ### Clusters & Routing In your `gatekeeper.properties` you'll need to define clusters, which will become your NGINX upstreams. These clusters correspond to cluster names you've defined using your service discovery extension.  The namespace is by default null, e.g. the root path. I use `/discovery`, which ensures that Curator can't accidentally have any namespace collisions with other services.  Discovery Path is the znode under which all of your services then make a znode for themselves.      /discovery (namespace)             /services (discoveryPath)                     /service-a (cluster)                           instance-1                           instance-2                     /service-b                           instance-1                           instance-2                           instance-3  Config for the above looks like this:      zookeeper.quorum=zookeeper1.mycompany.com     zookeeper.namespace=discovery     zookeeper.discoveryPath=/services      # Upstreams to look for     clusters=service-a,service-b  Each cluster can then be assigned various contexts for routing. Future work will push this config into ZooKeeper.      # Proxy_pass locations, comma-separated lists     cluster.service-a.context=/a     cluster.service-b.context=/,/b   ## Running  Build the agent as you would normally      mvn package  A jar will be built in `gatekeeper-agent/target/gatekeeper-agent-1.0.1.jar`. Take this jar and put it on your NGINX servers.  You can run the agent as below:      java -jar gatekeeper.jar -c /path/to/gatekeeper.properties      or for the default `gatekeeper.properties`      java -jar gatekeeper.jar  ### SupervisorD  I prefer to run Gatekeeper under [Supervisord](http://supervisord.org/). A config for Gatekeeper looks like this:      [program:gatekeeper]     directory=/opt/gatekeeper     command=java -Xmx128m -Xss256k -jar gatekeeper-agent.jar -c gatekeeper.properties     redirect_stderr=true     stdout_logfile=/var/log/gatekeeper.log      ### Config management  It's also quite helpful to have centralized configuration management. Changes to the NGINX template can be made via Chef cookbook, or Ansible Playbook. Gatekeeper does not come with any configuration management utilities.  ### Forcing a rewrite  Send a SIGHUP to the Gatekeeper process.      kill -HUP <pid of gatekeeper process>  You should see a message in the log.  ## Maven You'll generally want to compile this yourself, but core is available on Clojars for extension building:      <dependency>         <groupId>com.xorlev.gatekeeper</groupId>         <artifactId>gatekeeper-core</artifactId>         <version>1.0.1</version>     </dependency>  ## Service Discovery Implementations  Currently only supports Netflix Curator Service Discovery Extensions. Other implementations are trivial and can be  implemented by including `gatekeeper-core`, and including your dependency in the resulting gatekeeper-agent build.   You can build gatekeeper with a different module like so:      mvn package -discovery.package=gatekeeper-some-discovery-module  ## FAQ  ### Why Java? I'm not good at C, I believe I'd make a leaky C-module. I am confident in my ability to write a stable Java co-process. Additionally, Netflix's Curator library is also top-notch, better than any stock Zookeeper library around.  ## License Copyright 2014 Michael Rose  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this work except in compliance with the  License. You may obtain a copy of the License in the LICENSE file, or at:  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific  language governing permissions and limitations under the License.
ops4j/peaberry	peaberry ======== The peaberry project is an extension library for Google-Guice that supports dependency injection of dynamic services. It provides OSGi integration out of the box, and has plug-in support for other registry-based service frameworks. It ships as an OSGi bundle that can be dropped into any R4 framework, such as Apache Felix or Eclipse/Equinox. You can also use peaberry outside of OSGi.  Documenation ------------ [Read our Wiki](https://github.com/ops4j/peaberry/wiki/_pages)
allengeorge/libraft	# libraft [![Build Status](https://travis-ci.org/allengeorge/libraft.png?branch=master)](https://travis-ci.org/allengeorge/libraft) [![Coverage Status](https://coveralls.io/repos/allengeorge/libraft/badge.png?branch=master)](https://coveralls.io/r/allengeorge/libraft?branch=master)  libraft is a Java library that implements the [Raft distributed consensus protocol] (https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf "In Search of an Understandable Consensus Algorithm").  ## Release History  * **0.1.0**: Initial release. (11 Dec, 2013) * **0.1.1**: Bug-fix release. (25 Jan, 2014)  For more information check the detailed [release history] (https://github.com/allengeorge/libraft/wiki/Release-History "libraft release history").  ## Features  libraft **completely** implements all checked features below. Unchecked features are planned but do not have an implementation timeline.  - [x] Leader election - [x] Log replication - [o] Log compaction (snapshots) - [o] Online cluster reconfiguration  ## Overview  libraft consists of 2 components:  * libraft-core * libraft-agent  Correctness, safety, and understandability were major factors during design and implementation. Performance was an explicit non-goal.  ### libraft-core  libraft-core contains an implementation of the Raft algorithm. It also defines interfaces for the components (such as timers, packet senders, log and metadata storage, *etc*.) it needs to operate. These interfaces make very few demands on the underlying implementation and should be easy to integrate into an existing stack.  ### libraft-agent  libraft-agent:  1. Provides reference implementations for the interfaces defined in libraft-core. 2. Provides a facade over libraft-core that can be instantiated to provide consensus services.  It uses JDBC for persistence, TCP connections for networking, and json for the wire format. It wires these implementations up to the algorithm classes from libraft-core. libraft-agent also defines a simple [json configuration format] (#configuring-libraft "Configuring libraft") with which users can configure their cluster. Finally, it provides a single class - `RaftAgent` - with which applications can control the lifecycle of a Raft server and interact with the Raft cluster.  ### Samples  libraft also comes with a fully-implemented sample application in `libraft-samples/kayvee`. *KayVee* is a distributed, consistent, key-value store that uses `RaftAgent` to durably replicate key-value pairs to a cluster. It demonstrates how libraft can be integrated into an application to provide consensus services.  ## Getting libraft  The simplest way to use libraft is through Gradle or Maven - simply add `libraft-agent` as a dependency. Replace `${libraft.version.latest}` in the code below with the latest version on Maven Central.  ### Gradle  ```groovy dependencies {     compile 'io.libraft:libraft-agent:${libraft.version.latest}' } ```  ### Maven  ```xml <dependencies>     <dependency>         <groupId>io.libraft</groupId>         <artifactId>libraft-agent</artifactId>         <version>${libraft.version.latest}</version>     </dependency> </dependencies> ```  Alternatively, if you only want the algorithm components and intend to build your own implementations for its interfaces, only `libraft-core` is necessary. Simply add `libraft-core` as a dependency, and replace `${libraft.version.latest}` in the code below with the latest version on Maven Central.  ### Gradle  ```groovy dependencies {     compile 'io.libraft:libraft-core:${libraft.version.latest}' } ```  ### Maven  ```xml <dependencies>     <dependency>         <groupId>io.libraft</groupId>         <artifactId>libraft-core</artifactId>         <version>${libraft.version.latest}</version>     </dependency> </dependencies> ```  ## Configuring libraft  `RaftAgent` can be configured in one of two ways:  1. A json configuration file. 2. A `RaftConfiguration` instance.  Both methods are equivalent and have the same properties.  ### Properties  `libraft-agent` exposes the following configuration properties. **bolded** properties are **required**.  * `minElectionTimeout`: minimum election timeout for a Raft server * `additionalElectionTimeoutRange`: maximum additional time added to `minElectionTimeout` to get the applied election timeout.                                     Election timeout is defined using the following formula: `electionTimeout = minElectionTimeout + randomInRange(0, additionalElectionTimeoutRange)`. * `rpcTimeout`: maximum time a Raft server will wait for a response to an RPC request * `heartbeatInterval`: maximum time between messages from a Raft leader * `connectTimeout`: maximum time a Raft server will wait to establish a connection to another Raft server * `minReconnectInterval`: minimum interval a Raft server will wait before reconnecting to another Raft server * `additionalReconnectIntervalRange`: maximum additional time added to `minReconnectInterval` to get the applied reconnect interval.                                       Reconnect interval is defined using the following formula: `reconnectInterval = minReconnectInterval + randomInRange(0, additionalReconnectIntervalRange)`. * **`database`**: Raft database configuration block     * **`driverClass`**: fully-qualified class name of the JDBC driver     * **`url`**: JDBC connection URL     * **`user`**: database user id     * **`password`**: database password (may be empty or omitted) * **`cluster`**: Raft cluster configuration block     * **`self`**: unique id of the *local* Raft server     * **`members`**: Raft cluster configuration. Defines *all* the members in the cluster, *including* the local server.         * **`id`**: unique id of the Raft server         * **`endpoint`**: address - in "host:port" format - at which this server can be reached  ### Sample  A sample configuration file for a 5-server Raft cluster is given below. Note that `self` is `S_00`, indicating that this is the configuration file for server `S_00` in the cluster. This file includes *all* fields, both required and optional.  ```json {     "minElectionTimeout": 180,     "additionalElectionTimeoutRange": 120,     "rpcTimeout": 30,     "heartbeatInterval": 15,     "connectTimeout": 5000,     "minReconnectInterval": 10000,     "additionalReconnectIntervalRange": 1000,      "database": {         "driverClass": "org.h2.Driver",         "url": "jdbc:h2:test_db",         "user": "test",         "password": "test"     },      "cluster": {         "self": "S_00",         "members": [             {                 "id": "S_00",                 "endpoint": "192.168.1.100:9990"             },             {                 "id": "S_01",                 "endpoint": "192.168.1.100:9991"             },             {                 "id": "S_02",                 "endpoint": "192.168.1.100:9992"             },             {                 "id": "S_03",                 "endpoint": "192.168.1.100:9993"             },             {                 "id": "S_04",                 "endpoint": "192.168.1.100:9994"             }         ]     } } ```  ## Using libraft  The simplest way to use libraft is to instantiate a `RaftAgent` within an application node (such as a server). This `RaftAgent` will allow the node to participate in a Raft cluster: it can become a leader or follower and will be notified of leadership changes as well as applied (i.e. committed) commands.  Creating a `RaftAgent` requires:  1. A valid [configuration file or object] (#configuring-libraft "Configuring libraft"). 2. A specialization of `Command` for the application commands that will be committed to the Raft cluster. 3. A specialization of `RaftListener` that will be notified of leadership changes and applied commands.  Sample json configuration file ("agent.config"):  ```json {     "database": {         "driverClass": "org.h2.Driver",         "url": "jdbc:h2:test_db",         "user": "test",         "password": "test"     },      "cluster": {         "self": "agent0",         "members": [             {                 "id": "agent2",                 "endpoint": "192.168.1.100:9990"             },             {                 "id": "agent1",                 "endpoint": "192.168.1.100:9991"             },             {                 "id": "agent0",                 "endpoint": "192.168.1.100:9992"             }         ]     } } ```  Sample java code:  ```java  // // Command to be replicated to the Raft cluster //  public class MyCommand implements Command {      enum CommandType {         GET,         SET     }      @JsonProperty     CommandType commandType;      @JsonProperty     String key;      @JsonProperty     String value;      // constructor, getters, and setters follow... }  // // Listener that will be notified of events in the Raft cluster //  public class MyRaftListener implements RaftListener {      @Override     public void onLeadershipChange(@Nullable String leader) {         //...     }      @Override     public void applyCommitted(long index, Command command) {         //...     } }  // // Application code //  // create the listener that will be tied to the RaftAgent RaftListener raftListener = new MyRaftListener();  // create the RaftAgent using the configuration in "agent.config.json" // the listener created above of leadership changes and applied commands RaftAgent raftAgent = RaftAgent.fromConfigurationFile("agent.config", raftListener);  // indicate that the application command uses Jackson annotations // RaftAgent has special support for this raftAgent.setupJacksonAnnotatedCommandSerializationAndDeserialization(MyCommand.class);  // initialize the RaftAgent // (sets up state, but does not connect to any servers, start any timers, etc.) // this allows the application to assume that it has sole access to resources // allowing it to perform verification and bootstrap tasks in single-threaded mode raftAgent.initialize();  // start the RaftAgent (this starts the internal server/client, timers, etc.) raftAgent.start();  // submit commands (if leader) if (raftListener.isLeader()) {     raftAgent.submitCommand(new MyCommand(MyCommand.CommandType.SET, "key0", "val0"));     raftAgent.submitCommand(new MyCommand(MyCommand.CommandType.SET, "key1", "val1"));     // more work... } else {     // more work... }  // stop the agent (stops the internal server/clients, timers, etc.) raftAgent.stop(); ```  `RaftAgent` supports more than what is outlined here (non-Jackson-annotated command serialization, *etc.*) For more examples, see `KayVee.java` in `libraft-samples/kayvee` and `RaftAgentTest.java` in `libraft-agent`.  ## Building libraft  libraft requires java 1.6 and gradle 1.10+ to build.  To build and install into the local maven cache, run `gradle` from the repository root.  ```shell gradle build gradle install ```  libraft also ships with a gradle wrapper, which allows the code to be built without prior installation of gradle. To use gradle wrapper run `gradlew` from the repository root.  ```shell ./gradlew build ./gradlew install ```  To clean build artifacts run `gradle clean` or `gradlew clean` from the repository root.  ## Issues  The full list of issues can be seen at [Github Issues] (https://github.com/allengeorge/libraft/issues "libraft issues").  There are no known safety issues.  ## Reporting Issues  Please submit all code or documentation issues, comments and concerns or feature requests to [Github Issues] (https://github.com/allengeorge/libraft/issues "libraft issues").  If you have other libraft or Raft questions, please post to the [raft-dev] (https://groups.google.com/forum/#!forum/raft-dev "raft-dev") mailing list.  ## Thanks!  libraft stands on the shoulders of others. This implementation would not be possible without the very detailed (and clear!) paper published by Diego Ongaro and John Ousterhout. Moreover, this library was built on a huge host of open-source software - many thanks to the teams behind them!  Just *some* of the open-source software libraft uses include:  * [guava-libraries] (https://code.google.com/p/guava-libraries/ "Google Core Libraries for Java 1.6+") * [junit] (http://www.junit.org "A programmer-oriented testing framework for Java") * [mockito] (https://code.google.com/p/mockito/ "Simpler & better mocking") * [hamcrest] (https://code.google.com/p/hamcrest/ "Hamcrest - Library of Matchers for Building Test Expressions") * [logback] (http://logback.qos.ch/ "The Generic, Reliable Fast & Flexible Logging Framework") * [jacoco] (http://www.eclemma.org/jacoco/ "JaCoCo Java Code Coverage Library") * [netty] (http://www.netty.io "netty") * [jackson] (https://github.com/FasterXML/jackson "FasterXML/Jackson") * [hibernate validator] (http://www.hibernate.org/subprojects/validator.html "Bean Validation Reference Implementation") * [h2] (http://www.h2database.com "H2 Database Engine") * [dropwizard] (http://www.dropwizard.io "Dropwizard - Production Ready out of the Box") * [jersey] (https://jersey.java.net/ "Jersey - RESTful Web Services In Java") * [jetty] (http://www.eclipse.org/jetty/ "Jetty - Servlet Engine and HTTP Server") * [jdbi] (http://www.jdbi.org "JDBI") * [sqlite-jdbc (xerial)] (https://bitbucket.org/xerial/sqlite-jdbc "SQLite JDBC Driver") * [gradle] (http://www.gradle.org "Gradle - Build Automation Evolved") * [coveralls-gradle-plugin] (https://github.com/kt3k/coveralls-gradle-plugin "Coveralls Gradle Plugin")  It's this - and much, much more! - that makes libraft possible.
davidB/metrics-influxdb	<p xmlns:dct="http://purl.org/dc/terms/">   <a rel="license"      href="http://creativecommons.org/publicdomain/zero/1.0/">     <img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" style="border-style: none;" alt="CC0" />   </a>   <br />   To the extent possible under law,   <a rel="dct:publisher"      href="https://github.com/orgs/novaquark">     <span property="dct:title">Novaquark</span></a>   has waived all copyright and related or neighboring rights to   this work. </p>  [![Build Status](https://travis-ci.org/davidB/metrics-influxdb.svg?branch=master)](https://travis-ci.org/davidB/metrics-influxdb) [![Bitdeli Badge](https://d2weczhvl823v0.cloudfront.net/davidB/metrics-influxdb/trend.png)](https://bitdeli.com/free "Bitdeli Badge") [![Download](https://api.bintray.com/packages/davidb/maven/metrics-influxdb/images/download.svg) ](https://bintray.com/davidb/maven/metrics-influxdb/_latestVersion)  # The library provide :  * a lighter client than influxdb-java to push only series to an [InfluxDB](http://influxdb.org) server. * A reporter for [metrics](http://metrics.codahale.com/) which announces measurements.  The library provide a lighter client than influxdb-java to push only metrics.  ## Dependencies :  * slf4j-api for logging. * metrics-core, to provide, if you use InfluxdbReporter.  ## Install:  ### Released ```  dependencies { 	compile 'com.github.davidb:metrics-influxdb:0.9.3'  } ```  ### Dev ```  repositories {     maven { url "https://jitpack.io" }  }  dependencies { 	compile 'com.github.davidb:metrics-influxdb:-SNAPSHOT'  } ``` ## Usage :  Using the Builder API and its defaults, it is easy to use InfluxdbReporter:      ScheduledReporter reporter = InfluxdbReporter.forRegistry(registry).build();     reporter.start(10, TimeUnit.SECONDS);  With the previous simple configuration, all defaults will be used, mainly:  - protocol: `HTTP` - server: `127.0.0.1` - port: `8086` - authentication: `none` - database name: `metrics` - rates: converted to `TimeUnit.SECONDS` - duration: converted to `TimeUnit.MILLISECONDS` - idle metrics: `do not report` - influxdb protocol: `v09` [line protocol](https://influxdb.com/docs/v0.9/write_protocols/line.html) - ...  But you are free of course to define all settings by yourself : ``` final ScheduledReporter reporter = InfluxdbReporter.forRegistry(registry)     .protocol(new HttpInfluxdbProtocol("http", "influxdb-server", 8086, "admin", "53CR3TP455W0RD", "metrics"))     .convertRatesTo(TimeUnit.SECONDS)     .convertDurationsTo(TimeUnit.MILLISECONDS)     .filter(MetricFilter.ALL)     .skipIdleMetrics(false)     .tag("cluster", "CL01")     .tag("client", "OurImportantClient")     .tag("server", serverIP)     .transformer(new CategoriesMetricMeasurementTransformer("module", "artifact"))     .build(); reporter.start(10, TimeUnit.SECONDS); ```  And if you are still using v08 influxdb  ``` final InfluxdbReporter reporter = InfluxdbReporter     .forRegistry(registry)     .protocol(new HttpInfluxdbProtocol("influxdb-server", 8086, "admin", "53CR3TP455W0RD", "metrics"))     .v08()     .build(); ... ```
kiselev-dv/gazetteer	OpenStreetMap (OSM) geocoder ======================  Main purpose of this project is easy to use geocoder/geoindexer.  Project consists of two parts: Gazetteer and GazetteerWeb  Gazetteer =========  Gazetteer used to parse *osm* data and do all dirty work with geometry.  You can use Gazetteer as standalone *osm* processor, to dump addresses from *osm*.  You can ignore GazetteerWeb and use data in your own geocoding/geosearching applications. Take an osm.bz2 dump and generate `json` with  * full geocoded buildins * full geocoded POIs * streets * cyties * administrative boundaries  Details are here https://github.com/kiselev-dv/gazetteer/tree/develop/Gazetteer  You could find data extracts here: http://data.osm.me/dumps/  GazetteerWeb ============  GazetteerWeb is a second part of the project.  You may take it as example implementation of search engine for Gazetteer generated data or use it for your own purposes.  Details are here https://github.com/kiselev-dv/gazetteer/tree/develop/GazetteerWeb  Live demo map: http://osm.me/ (covers Russia, Montenegro, Croatia, Bosnia and Hercegovina)
dsyer/spring-boot-ratpack	This project provudes autoconfiguration and a CLI extension for Spring  Boot with Ratpack. Most of the features enabled here are actually already in Ratpack, e.g. see `@EnableRatpack`. Using this project as well as `ratpack-spring-boot` means lets you write an application using Ratpack and Spring with slightly less overhead, just by adding dependencies.  ## Quick Java Example  ```java @SpringBootApplication public class Application {      @Bean     public Handler handler() {         return context -> context.render("Hello World");     }      public static void main(String[] args) throws Exception {         SpringApplication.run(Application.class, args);     }  } ```  If a single `@Bean` of type `Handler` is present it is installed and serves the default route. For more routes you have to add `@Beans` of type `Action<Chain>` and add handlers as needed to the chain inside its execute method.  ## Quick Groovy DSL Example  Ratpack has a Groovy DSL for building routes. Spring Boot CLI applications can take advantage of the nice syntax as follows.  This should run (as it is, with no imports):  ```groovy ratpack {   handlers {     get {       render "Hello World"     }   } } ```  Put it in a file called `app.groovy` and run it like this:  ``` $ spring run app.groovy ... ```  Then get the result in a browser at http://localhost:5050.  You can also use `render json(...)` or `render groovyTemplate(...)` features of Ratpack with no additional effort. Spring will pick up Groovy templates by default from `classpath:/templates`.  To use Spring effectively you will want to take advantage of the dependency injection and autoconfiguration features of Spring Boot as well. Here's a simple example with dependency injection and json:  ```groovy @Service class MyService {   String message() { "Hello World" } }  ratpack {   handlers {     get { MyService service ->       render json([msg:service.message()])     }   } } ```  The `ratpack` DSL keyword only supports `handlers` inside its top-level closure (in non-Spring apps you might see `bindings` as well).  ## Installing the Spring Boot CLI  To run an application written in the Ratpack DSL you need the ratpack extensions to the [Spring Boot CLI](http://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#getting-started-installing-the-cli).  Since the Ratpack extensions are not part of the main Spring Boot feature set you will need to clone from [this repo](https://github.com/dsyer/spring-boot-ratpack) and build it locally, e.g.  ``` $ git clone https://github.com/dsyer/spring-boot-ratpack $ cd spring-boot-ratpack $ mvn install ```  Once it is built, you can install the ratpack plugin. You either need to build Spring Boot or download a snapshot build of the CLI first, e.g.  ``` $ mkdir -p /tmp/spring && cd $_ $ wget -O spring.tgz https://repo.spring.io/libs-snapshot-local/org/springframework/boot/spring-boot-cli/1.2.0.BUILD-SNAPSHOT/spring-boot-cli-1.2.0.BUILD-SNAPSHOT-bin.tar.gz $ tar -zxf spring.tgz ```  The `spring` CLI is now installed at `/tmp/spring/spring-boot-cli-1.2.0.BUILD-SNAPSHOT`, so if you are a gvm user you can do this:  ``` $ gvm install springboot ratpack /tmp/spring/spring-boot-cli-1.2.0.BUILD-SNAPSHOT $ gvm use springboot ratpack ```  and then you can install the `spring-boot-ratpack-cli` jar:  ``` $ spring install org.springframework.boot:spring-boot-ratpack-cli:1.0.0.BUILD-SNAPSHOT ```
GluuFederation/oxAuth	oxAuth ====== <p>oxAuth is an open source OpenID Provider that implements the OpenID Connect 1.0 stack of REST services. The project also includes OpenID Connect Client code which can be used by websites to validate tokens. It currently implements all required aspects of the OpenID Connect stack, including an OAuth 2.0 authorization server, Simple Web Discovery, Dynamic Client Registration, JSON Web Tokens, JSON Web Keys, and User Info Endpoint.</p> <p>oxAuth is tightly coupled with <a href="https://github.com/GluuFederation/oxTrust">oxTrust</a>. oxAuth configuration is stored in LDAP, and oxTrust is needed to generate the proper configuration.</p> <p>Refer to <a href="https://github.com/GluuFederation/install">https://github.com/GluuFederation/install</a> for installation instructions.</p> <p>To access Gluu support, please register and open a ticket on <a href="http://support.gluu.org" target="none">Gluu Support</a>
waylau/RestDemo	RestDemo ========   RESTful Web Service DemoS  with Jersey ,Hibernate,Mysql,Spring,JQuery,AngularJS...Follow me to create a completed web app. Just go go go !    Here are my articles about how to build the project step by step, and every demo is a small project :    1.用Jersey构建RESTful服务1–HelloWorld    http://www.waylau.com/jersey-restful-helloworld/    2.用Jersey构建RESTful服务2–JAVA对象转成XML输出    http://www.waylau.com/jersey-restful-java-xml/    3.用Jersey构建RESTful服务3–JAVA对象转成JSON输出    http://www.waylau.com/jersey-restful-java-json/    4.用Jersey构建RESTful服务4–通过jersey-client客户端调用Jersey的Web服务模拟CURD     http://www.waylau.com/jersey-client-curd/   5.用Jersey构建RESTful服务5–Jersey+MySQL5.6+Hibernate4.3     http://www.waylau.com/jersey-mysql5-hibernate4/    6.用Jersey构建RESTful服务6–Jersey+SQLServer+Hibernate4.3     http://www.waylau.com/jersey-sqlserver-hibernate4/     7.用Jersey构建RESTful服务7--Jersey+SQLServer+Hibernate4.3+Spring3.2     http://www.waylau.com/jersey-sqlserver-hibernate4-spring3/    8.用Jersey构建RESTful服务8--Jersey+SQLServer+Hibernate4.3+Spring3.2+jquery    http://www.waylau.com/jersey-sqlserver-hibernate4-spring3-jquery/     9.用Jersey构建RESTful服务9--Jersey+SQLServer+Hibernate4.3+Spring3.2+AngularJS    http://www.waylau.com/jersey-sqlserver-hibernate4-spring3-angular/    ##版本：    * Jersey 2.11 * MySQL5.6 * Hibernate4.3.5 final * Spring 3.2.3.RELEASE * jQuery 1.7.1  * angularJS 1.2.3 * bootstrap 3.1.1   ---------------------------------------------------  10.用 Jersey 2 和 Spring 4 构建 RESTful web service    http://www.waylau.com/jersey-2-spring-4-rest/  ##版本：  * Jersey 2.9 * Spring 4.0.3 * Hibernate 4 * Maven 3 * Tomcat 7 * Jetty 9 * MySql 5.6
opennetworkinglab/OpenVirteX	OpenVirteX ==========   OVX is a network hypervisor that can create multiple virtual and programmable networks on top of a single physical infrastructure. Each tenant can use the full addressing space, specify their own topology, and deploy the network OS of their choice. Networks can be reconfigured at run-time, and OVX can automatically recover from physical failures.
Stratio/ingestion	[![Coverage Status](https://coveralls.io/repos/github/Stratio/Ingestion/badge.svg?branch=master)](https://coveralls.io/github/Stratio/Ingestion?branch=master)  Stratio Ingestion =================  Contents --------  * Introduction * Stratio Ingestion components * Details about Stratio Ingestion * Compile & Package * FAQ   Introduction ------------  Stratio Ingestion started as a fork of Apache Flume (1.6), where you can find:  **Custom sources and sinks, developed by Stratio**   - SNMP (v1, v2c and 3)  - redis, Kafka (0.8.1.1)  - MongoDB, JDBC, Cassandra and Druid  - Stratio Decision (Complex Event Processing engine)  - REST client, Flume agents stats   **Several bug fixes**   - Some of them really important, such as unicode support  **Several enhancements of Flume's sources & sinks**   - ElasticSearch mapper, for example  You can find more documentation about us [here](https://stratio.atlassian.net/wiki/display/PLATFORM/STRATIO+INGESTION)   Stratio Ingestion components ----------------------------  * Data transporter and collector: [Apache Flume](http://flume.apache.org/) * Data extractor and transformer: [Morphlines](http://kitesdk.org/docs/current/kite-morphlines/index.html) * Custom sources types to read data from:     - REST   com.stratio.ingestion.source.rest.RestSource     - Redis FlumeStats   com.stratio.ingestion.source.redis.RedisSource     - SNMPTraps   com.stratio.ingestion.source.snmptraps.SNMPSource     - IRC   com.stratio.ingestion.source.irc.IRCSource * Custom sinks types to write the data to:     - Cassandra   com.stratio.ingestion.sink.cassandra.CassandraSink     - MongoDB   com.stratio.ingestion.sink.mongodb.MongoSink     - [Stratio Decision](https://github.com/Stratio/Decision)     - JDBC   com.stratio.ingestion.sink.jdbc.JDBCsink     - Kafka   com.stratio.ingestion.sink.kafka.KafkaSink     - Druid   com.stratio.ingestion.sink.druid.DruidSink  Details about Stratio Ingestion -------------------------------  Stratio Ingestion is based on Apache Flume so the first question is:  **What is Apache Flume?**  Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store.  Its use is not only designed for logs, in fact you can find a myriad of sources, sinks and transformations.  In addition, a sink could be a big data storage but also another real-time system (Apache Kafka, Spark Streaming).   **Interesting facts about Stratio Ingestion**   * Flume Ingestion is Apache Flume "on steroids" :)    * We are extensively using Kite SDK (morphlines) in order to do a better T from ETL, and so we have also developed a bunch of custom transformations.    * Stratio ingestion is fully open source and we work very close to the Flume community.   Compile & Package -----------------  ``` $ mvn clean compile package -Ppackage ```  Distribution will be available at stratio-ingestion-dist/target/ folder. You will find .deb, .rpm and .tar.gz packages ready to use depending your environment. If you take a look at [documentation](https://stratio.atlassian.net/wiki/display/PLATFORM/STRATIO+INGESTION) you will find more details about how to install the product, and some useful examples to get a better understanding about Stratio Ingestion.    FAQ ---   **Can I use Stratio Ingestion for aggregating data (time-based rollups, for example)?**  *This is not a good idea from our experience, but you can use [Stratio Sparkta](https://github.com/Stratio/Sparkta) for real-time aggregation.  **Is Flume Ingestion multipersistence?**  *Yes, you can write data to JDBC sources, mongoDB, Apache Cassandra, ElasticSearch, Apache Kafka, among others.*   **Can I send data to decision-cep-engine?**  *Of course, we have developed a sink in order to send events from Flume to an existing stream in our CEP engine.  The sink will create the stream if it does not exist in the engine.*   **Where can I find more details about Stratio Ingestion?**  *You can take a look at our Documentation on [Confluence](https://stratio.atlassian.net/wiki/display/PLATFORM/STRATIO+INGESTION)   Changelog ---------  See the [changelog](CHANGELOG.md) for changes.
digitalbuddha/AndroidStarter	AndroidStarter ==============  10/31/2014 -  There is a "Square Stack" version this project that uses  Retrofit and Dagger while removing Volley https://github.com/digitalbuddha/DaggerStarter    This project is an opinionated view of how to painlessly program in android Challenges this project solves: *  Background jobs/threading - Priority Job Queue (Path) *  Networking - Volley *  Messaging between background threads and UI Thread through config changes - EventBus (Green Robot) *  Fast JSON marhshalling/unmarshalling - Gson *  Make code pretty and stuff - ButterKnife (Square) *  Testing with Roboelectric (install 'Android Unit Test' Android Studio plugin to enable tests to be run by right clicking them)  #Othere interesting elements * Gradle build * Recycler View * Animations     Any and all feedback is greatly apprecated   Maintained By Mike Nakhimovich and Patricia Estridge
SpringOne2GX-2014/reactive-geocoder	# Reactive SpringOne2GX Demo  You need MongoDB running to run this demo. It's a plain Spring Boot app, so run it with Maven:  		$ mvn spring-boot:run  This demo provides an input form into which you can input your address (or even just your city and state). Click the "Geocode" button and the app geocodes that address into a Lat/Long coordinate pair using Google Maps' API. It then saves this information to the server which uses a Spring Data Repository to store the address and coordinates in MongoDB.  Upon save, your Location is pumped into a shared `Stream<Location>` that has a filter applied to it that will only process other `Location` domain instances that fall within a given geographic radius (by default 10km) from your coordinates. As other users are simultaneously filling out their own address or home city and saving that to the server, their information is incorporated into your Stream and you will see the name and city of other users who live near your coordinates in the list. You can add a Marker to the map to see where that person lives specifically.  To access the demo, open `http://localhost:5050/` (or the equivalent CloudFoundry address if deployed publicly) in your browser.  Follow these steps to see data:  - Enter a Name and some parts of an address (doesn't have to be exact as City and State/Country is fine) - Click "Save" - See any "Nearby" entries in the table below - Reload page in browser and enter another Name + City within 10km and click "Save" - You should see the other person you entered previously - Move slider to higher distance value and click "Save" - You should see people that are farther away, assuming data exists at further distances
apache/reef	![](website/src/site/resources/ApacheREEF_logo_no_margin_small.png)  Apache REEF&trade; ========================  Apache REEF&trade; (Retainable Evaluator Execution Framework) is a  library for developing portable applications for   cluster resource managers such as Apache Hadoop YARN or Apache  Mesos. For example, [Microsoft Azure Stream Analytics](https://azure.microsoft.com/en-us/services/stream-analytics/) is built on [REEF and Hadoop.](https://blogs.technet.microsoft.com/machinelearning/2015/12/17/reef-graduates-to-a-top-level-apache-project/)    Online Documentation ==================== Detailed information on REEF can be found in the following places:    * [The project website](http://reef.apache.org/)   * [The project wiki](https://cwiki.apache.org/confluence/display/REEF/Home)  [The developer mailing list](http://reef.apache.org/mailing-list.html) is the best way to reach REEF's developers when the above aren't sufficient.   Build Status =============  | Component | OS | Status | |---|:------:|:------:| |REEF Java | Ubuntu | [![Build Status](https://travis-ci.org/apache/reef.svg?branch=master)](https://travis-ci.org/apache/reef) | |REEF.NET | Windows | [![Build status](https://ci.appveyor.com/api/projects/status/qwvl6d4d8891e09d/branch/master?svg=true)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/reef/branch/master) |   Building REEF =============  |   | Java   | .NET | |---|:------:|:----:| |Build & run unit tests| [java\BUILD.md](lang/java/BUILD.md) | [cs\BUILD.md](lang/cs/BUILD.md) |  Releases =============  [![downloads](https://img.shields.io/badge/source%20code-download-brightgreen.svg)](http://reef.apache.org/downloads.html) [![NuGet package](https://img.shields.io/badge/nuget-package-brightgreen.svg)](https://www.nuget.org/packages/Org.Apache.REEF.All/)  License =============  [![Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](LICENSE)
Netflix/ocelli	## What is Ocelli?  Ocelli is a client side reactive load balancer based on RxJava.  ```java Observable<Client> loadBalancer = ...;  loadBalancer   .concatMap((client) -> client.doSomething())   .retry(2)    .subscribe(); ```   ## Why Ocelli?  In biology Ocelli is a simple eye as opposed to a complex eye.  Ocelli the library is a simple load balancer implementation as opposed to very complex load balancers that do too many via magic or are too tightly coupled with the client transport.  Don't know how to pronounce Ocelli?  Goto http://www.merriam-webster.com/audio.php?file=ocellu02&word=ocelli  ## How is Ocelli Reactive?  Ocelli exposes a very simple API to choose a client.  A new client is emitted for each subscription to the return Observable based on changes within the load balancer and optimization based on the load balancing algorithm.  Ocelli reacts to changes in the network topology either via server membership events or server failure detection.  ## How is Ocelli Functional?  Ocelli is policy driven by delegating key strategies and metric sources via simple functions.  This approach simplifies the load balancer implementation so that it is decoupled from the speicific client SPI.  These functions are essentially the glue between the load balancer algorithm and the client SPI.  ## Key design principles  ### Metrics Ocelli doesn't attempt to track metrics for client libraries.  That role is delegated to the client libraries since most libraries already perform that task.  Ocelli simply provides hooks (via functions) for libraries to provide the metric value.  ### Retry Retry friendly, not retry magic! Code that performs operations in multiple layers is easily susceptible to retry storms where each layer is unaware of retries in the next.  Ocelli makes retries possible by tracking state so that retries are efficient but does not force any particular retry policy.  Instead Ocelli let's the caller specify retry policieis via RxJava's built in retry operations such as retry(), retryWhen() and onErrorResumeNext().    ### Composability  Ocelli is composable in that the load balancer can be combined with other functionality such as caching, retry and failover.  Ocelli delegates as much functionality to the RxJava operators to give users of the library full control.  In most cases specific policies may be specified in just one line of code.  ### Light configuration TODO  ## Key features  ### Pluggable load balancing strategy  ### Pluggable metrics gathering  ### Partitioning  Services are sometimes partitioned to distribute data, load or functionality.  Ocelli provides a partitioning mechanism whereby Servers can be members of 0 to many partitions.  The client can then choose to route traffic only to one partition and fall back to other partitions.  For example, partitioning can be used to implement a rack aware topology where all requests are first directed at the same rack as the client but can fail over to other racks.  ### Round robin vs. Weighted Round Robin  ### Pluggable weighting strategy  #### Identity  The weight provided by the function is used as the weight.  The highest value translates into a higher weight.  #### reverseMax  The weight is calculated as the difference between the max and the value.  The lowest value therefore translates into a higher   ### Testing  ### Topologies  Topologies are similar to partitions but deal more with the changing position of the client host within a larger topology.  Topologies also don't deal with data or functional aspects of a service.  For example, to limit the number of hosts to which a client will connect the entire set of servers (including the client) are arranged into a ring and the client connects only to a subset of hosts within the ring using consistent hashing to ensure even distribution.  As servers come in an out of existence Ocelli will adjust its target set of hosts to meet the new topology.  ### Rate limiter TODO
WhisperSystems/WebSocket-Resources	# WebSocket-Resources  A Dropwizard library that lets you use Jersey-style Resources over WebSockets.  Install from maven central:  ``` <dependency>   <groupId>org.whispersystems</groupId>   <artifactId>websocket-resources</artifactId>   <version>${latest_version}</version> </dependency> ```  ## The problem  In the standard HTTP world, we might use Jersey to define a set of REST APIs:  ``` @Path("/api/v1/mail") public class MailResource {      @Timed   @POST   @Path("/{destination}/")   @Consumes(MediaType.APPLICATION_JSON_TYPE)   public void sendMessage(@Auth Account sender,                            @PathParam("destination") String destination,                           @Valid Message message)    {     ...   } } ```  Using JAX-RS annotations and some Dropwizard glue, we can easily define a set of resource methods that allow an authenticated sender to POST a JSON Message object.  All of the routing, parsing, validation, and authentication are taken care of, and the resource method can focus on the business logic.  What if we want to expose a similar API over a WebSocket?  It's not pretty.  We have to define our own sub-protocol, do all of the parsing and validation ourselves, keep track of the connection state, and do our own routing.  It's basically the equivalent of writing a raw servlet, but worse.  ## The WebSocket-Resources model  WebSocket-Resources is designed to make exposing an API over a WebSocket as simple as writing a Jersey resource.  The library is based on the premise that the WebSocket client and the WebSocket server should each be modeled as both a HTTP client and server simultaneously.  That is, the WebSocket server receives HTTP-style requests and issues HTTP-style responses, but it can also issue HTTP-style requests to the client, and expects HTTP-style responses from the client. This allows us to write Jersey-style resources, while also initiating bi-directional communication from the server.  What if we wanted to make the exact same resource above available over a WebSocket using WebSocket-Resources? In your standard Dropwizard service run method, just initialize WebSocket-Resources and register a standard Jersey resource:  ```   @Override   public void run(WhisperServerConfiguration config, Environment environment)       throws Exception   {     WebSocketEnvironment webSocketEnvironment = new WebSocketEnvironment(environment, config);     webSocketEnvironment.jersey().register(new MailResource());     webSocketEnvironment.setAuthenticator(new MyWebSocketAuthenticator());      WebSocketResourceProviderFactory servlet   = new WebSocketResourceProviderFactory(webSocketEnvironment);     ServletRegistration.Dynamic      websocket = environment.servlets().addServlet("WebSocket", servlet);      websocket.addMapping("/api/v1/websocket/*");     websocket.setAsyncSupported(true);     servlet.start();          ...       } ```  It's as simple as creating a `WebSocketEnvironment` from the Dropwizard `Environment` and registering Jersey resources.  ## Making requests  In order to call the Jersey resource we just registered from a client, we need to know how to format client requests.  It's possible to either define our own subprotocol, or to use the default subprotocol packaged with WebSocket-Resources, which is based in protobuf.  A subprotocol is composed of `Request`s and `Response`s.  A `Request` has four parts:  1. An `id`. 1. A `method`. 1. A `path`. 1. An optional `body`.  A `Response` has four parts:  1. The request `id` it is in response to. 1. A `status code`. 1. A `status message`. 1. An optional `body`.  This should seem strongly reminiscent of HTTP.  By default, WebSocket-Resources will use a protobuf formatted subprotocol:  ``` message WebSocketRequestMessage {   optional string verb = 1;   optional string path = 2;   optional bytes  body = 3;   optional uint64 id   = 4; }  message WebSocketResponseMessage {   optional uint64 id      = 1;   optional uint32 status  = 2;   optional string message = 3;   optional bytes  body    = 4; }  message WebSocketMessage {   enum Type {     UNKNOWN  = 0;     REQUEST  = 1;     RESPONSE = 2;   }    optional Type                     type     = 1;   optional WebSocketRequestMessage  request  = 2;   optional WebSocketResponseMessage response = 3; } ```  To use a custom wire format, it's as simple as implementing a custom `WebSocketMessageFactory` and registering it at initialization time:  ```   @Override   public void run(WhisperServerConfiguration config, Environment environment)       throws Exception   {     WebSocketEnvironment webSocketEnvironment = new WebSocketEnvironment(environment);     webSocketEnvironment.setMessageFactory(MyMessageFactory());     ...   } ```  ## Making requests from the server  To issue requests from the server, use `WebSocketClient`.  There are two ways to get a `WebSocketClient` instance: a resource annotation or a connection listener.  Resource annotation:  ``` @Path("/api/v1/mail") public class MailResource {      @Timed   @POST   @Path("/{destination}/")   @Consumes(MediaType.APPLICATION_JSON_TYPE)   public void sendMessage(@Auth Account sender,                            @WebSocketSession WebSocketSessionContext context,                            @PathParam("destination") String destination,                           @Valid Message message)    {     WebSocketClient client = context.getClient();     ...   } }  ```  Or a connect listener:  ```   @Override   public void run(WhisperServerConfiguration config, Environment environment)       throws Exception   {     WebSocketEnvironment webSocketEnvironment = new WebSocketEnvironment(environment);     webSocketEnvironment.setConnectListener(new WebSocketConnectListener() {       @Override       public void onConnect(WebSocketSessionContext context) {         WebSocketClient client = context.getClient();         ...       }     });     ...   } ```  A WebSocketClient can then be issued to transmit requests:  ```   WebSocketClient client = context.getClient();      ListenableFuture<WebSocketResponseMessage> response = client.sendRequest("PUT", "/api/v1/message", body);      Futures.addCallback(response, new FutureCallback<WebSocketResponseMessage>() {     @Override     public void onSuccess(@Nullable WebSocketResponseMessage response) {       ...     }      @Override     public void onFailure(@Nonnull Throwable throwable) {       ...     }   }); ```  License ---------------------  Copyright 2014 Open Whisper Systems  Licensed under the AGPLv3: https://www.gnu.org/licenses/agpl-3.0.html
pmwmedia/tinylog	tinylog  =======  [![Build Status](https://travis-ci.org/pmwmedia/tinylog.svg?branch=v1.3)](https://travis-ci.org/pmwmedia/tinylog)  [![Code Coverage](https://codecov.io/gh/pmwmedia/tinylog/branch/v1.3/graph/badge.svg)](https://codecov.io/gh/pmwmedia/tinylog/branch/v1.3)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.tinylog/tinylog/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.tinylog/tinylog)  [![Javadocs](http://www.javadoc.io/badge/org.tinylog/tinylog.svg)](http://www.javadoc.io/doc/org.tinylog/tinylog)    Example  -------    ```java  import org.pmw.tinylog.Logger;    public class Application {        public static void main(String[] args) {          Logger.info("Hello World!");      }    }  ```    Projects  --------    * benchmark    * Contains a benchmark for comparing logging frameworks  * jcl-binding:    * Contains the Apache Commons Logging (JCL) binding, implementing the JCL logging API  * log4j-facade:    * Contains the log4j facade, an Apache Log4j 1.x compatible logging API replacement  * slf4j-binding:    * Contains the SLF4J binding, implementing the SLF4J logging API  * tinylog-core    * Contains shared basis for tinylog and compatible server replacements  * tinylog-jboss    * Contains tinylog API for JBoss Logging backend  * tinylog-jul    * Contains tinylog API for java.util.logging backend  * tinylog    * Contains tinylog itself    All projects can be imported as Maven projects.    Other folders  -------------  	  * configuration    * Contains configuration files for Java formatter, Checkstyle and FindBugs    Support  -------    A detailed user manual and the Javadoc documentation can be found on http://www.tinylog.org/. Bug reports and feature requests are welcome and can be created via [GitHub issues](https://github.com/pmwmedia/tinylog/issues).    Build tinylog  -------------    tinylog requires at least Maven 3.5 and JDK 9 for building. The generated JARs are compatible with Java 6 and higher.    Build command:    	mvn clean checkstyle:checkstyle findbugs:findbugs install -P release    A new folder "target" with Javadoc documentation and all JARs will be created in the root directory.    License  -------    Copyright 2012 Martin Winandy    Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
kohlschutter/junixsocket	# junixsocket  junixsocket is a Java/JNI library that allows the use of [Unix Domain Sockets](https://en.wikipedia.org/wiki/Unix_domain_socket) (AF_UNIX sockets) from Java.  ## Why it's cool  * In contrast to other implementations, *junixsocket* extends the Java Sockets API (`java.net.Socket, java.net.SocketAddress`, etc.) * Supports *RMI over AF_UNIX* * Can connect to local MySQL server via Unix domain sockets (provides a *AFUNIXDatabaseSocketFactory* for Connector/J). * Apache 2.0 licensed.  ## Licensing  junixsocket has been written by Christian Kohlschütter. It is released under the Apache 2.0 License.  Commercial support is available through [http://www.kohlschutter.com/ Kohlschütter Search Intelligence].  ## Changelog  ### Noteworthy changes    * _(2014-09-29)_ *junixsocket 2.0.1*     * **Bugfix:** Added byte array bounds checking to read/write methods.    * Fix C compiler warnings    * Remove synchronized byte[] array for single-byte reads/writes.    * _(2014-09-28)_ *junixsocket 2.0.0*    * Moved from *Google Code* to *GitHub*.    * Now uses Maven as the build system, code is distributed to the *Maven Central* repository.    * C code is built using *nar-maven-plugin*    * JNI libraries are loaded using *native-lib-loader*  See the commit log for details.  For 1.x releases, please see [https://code.google.com/p/junixsocket](https://code.google.com/p/junixsocket).  ## Documentation  For now, please refer to the [Wiki on Google Code](http://code.google.com/p/junixsocket/w/list).   Quick links:  * [Getting Started](http://code.google.com/p/junixsocket/wiki/GettingStarted)  * [Socket Demo](http://code.google.com/p/junixsocket/source/browse/#svn/trunk/junixsocket/src/demo/org/newsclub/net/unix/demo)  * [RMI Demo](http://code.google.com/p/junixsocket/source/browse/#svn/trunk/junixsocket/src/demo/org/newsclub/net/unix/demo/rmi)  * [MySQL Socket Demo](http://code.google.com/p/junixsocket/wiki/ConnectingToMySQL)  * [API Javadocs](http://junixsocket.googlecode.com/svn/trunk/junixsocket/javadoc/index.html)  ## Related Work   * [JUDS](http://code.google.com/p/juds/) (LGPL, no RMI, not using Java Sockets API)  * J-BUDS (LGPL, no RMI, not using Java Sockets API, orphaned)  * gnu.net.local (GPL with Classpath exception, no RMI, not using Java Sockets API, orphaned) -- [Archive mirror](http://web.archive.org/web/20060702213439/http://www.nfrese.net/software/gnu_net_local/overview.html).
tschaeff/google-geoEngine	# Scalable geofencing API for Google App Engine  An architecture proposal for a geofencing API on Google's [App Engine][1] using [Google Cloud Endpoints][3]. This API is able to geofence complex polygons at a large throughput using App Engine, Cloud Datastore and Memcache. It uses the [Java Topology Suite][7] to create a spatial index which is stored in Memcache for fast querying access.  You can download, build and deploy the project as is using the [Google App Engine Maven plugin][4]. For a detailed explanation of the architecture please refer to this [Google Developers Blogpost][8].  ## Endpoints  - __add__: Add a fence to a certain group. - __buildIndex__: Build the spatial index and write it to Memcache. - __getById__: Get a fence's metadata by it's id. - __list__: List all fences in a certain group. - __point__: Get all fences that contain a certain point. - __polygon__: Get all fences that aren't disjoint with a certain polygon. - __polyline__: Get all fences that intersect with a certain polyline.  ## Test & Deploy to App Engine  1. Update the value of `application` in `src/main/webapp/WEB-INF/appengine-web.xml` to the app    ID you have registered in the App Engine admin console and would    like to use to host your instance of this sample.  1. **__Optional step:__** These sub steps are not required but you need this    if you want to have auth protected methods.      2. Update the values in `src/main/java/com/google/appengine/geo/fencing/Constants.java`        to reflect the respective client IDs you have registered in the        [APIs Console][6].       2. You also need to supply the web client ID you have registered        in the [APIs Console][4] to your client of choice (web, Android,        iOS).  1. Run the application with `mvn appengine:devserver`, and ensure it's    running by visiting your local server (by    default [localhost:8080][5].)  1. **__Optional step:__** Get the client library with     `$ mvn appengine:endpoints_get_client_lib`     It will generate a client library jar file under the    `target/endpoints-client-libs/<api-name>/target` directory of your    project, as well as install the artifact into your local maven    repository.        For more information on client libraries see:        - [Generating Cleint Libraries][11]    - [Client Library for JavaScript][10]  1. Deploy your application to Google App Engine with     `$ mvn appengine:update`        *Please note that you should always first test on the development server since that creates indexes for our datastore queries. Also after the first deployment App Engine takes a while to create the necessary indexes and connections, so if you get errors, just wait for a bit.*     ## Example of using the [JavaScript Google Client Library with this API][10]  - `src/main/webapp/addFence.html` is an example of how to use the Google Maps JavaScript API [Drawing Layer][9]  to draw fences to the map and store them to your App Engines Datastore using the __add__ endpoint.  - `src/main/webapp/query.html` shows you how to query your API for points, polylines and polygons.  These examples can also be used to test your API. You should always first test on the devserver (`mvn appengine:devserver`),  since this automatically creates indexes that are needed for our Datastore queries.  [1]: https://developers.google.com/appengine [2]: http://java.com/en/ [3]: https://developers.google.com/appengine/docs/java/endpoints/ [4]: https://developers.google.com/appengine/docs/java/tools/maven [5]: http://localhost:8080/ [6]: https://console.developers.google.com/ [7]: http://www.vividsolutions.com/jts/JTSHome.htm [8]: http://googledevelopers.blogspot.com/2014/12/building-scalable-geofencing-api-on.html [9]: https://developers.google.com/maps/documentation/javascript/drawinglayer [10]: https://developers.google.com/api-client-library/javascript/start/start-js [11]: https://cloud.google.com/appengine/docs/java/endpoints/gen_clients
blad/solid-android	SOLID: Noun Project Browser [![Build Status](https://travis-ci.org/blad/solid-android.svg?branch=master)](https://travis-ci.org/blad/solid-android) ============================  This project's goal is to be a demonstration of how [SOLID principles](http://en.wikipedia.org/wiki/SOLID_%28object-oriented_design%29) can be applied to Android development.  <p align="center"> <a href="https://play.google.com/store/apps/details?id=com.btellez.solidandroid"><img src="http://developer.android.com/images/brand/en_generic_rgb_wo_60.png"></a><br> <img src="https://s3.amazonaws.com/uploads.hipchat.com/56587/750791/JJlLwUNKMeDJcOu/previews.png"> </p>  ## Contributing  Contributions are welcome to the project. The goal is to adhere to the SOLID principles.  ### Reporting Bugs & Bug Fixes  If you find a bug you can report it by using the [issues section](https://github.com/blad/solid-android/issues) for this project. Bug fixes should also be done in pull requests.  ### Features & Feature Requests  If there is some feature you want to implement, I invite you to do a pull request.  Feature request should be reported in the [issues section](https://github.com/blad/solid-android/issues) for this project.  ## Tools & Libraries  - [Android Studio - Android Development Environment](https://developer.android.com/sdk/installing/studio.html) - [ButterKnife - View Injection Library](http://jakewharton.github.io/butterknife/) - [Dagger - Dependency Injection Library](http://square.github.io/dagger/) - [Gogole Gson - A Java library to convert JSON to Java objects](https://code.google.com/p/google-gson/) - [okHttp - An HTTP & SPDY client for Android and Java applications](http://square.github.io/okhttp/) - [Picasso - A powerful image downloading and caching library for Android](http://square.github.io/picasso/) - [Otto - Event Bus Library](http://square.github.io/otto/) - [Travis CI - Continuos integration platform](https://travis-ci.org/)  ## Graphics & Icons  ### The Noun Project  [The Noun Project](http://www.thenounproject.com) is the source for some the graphics used in this application. The following users' work was used:  - [Dice](http://thenounproject.com/term/dice/20125/) created by [Derek Palladino](http://thenounproject.com/derekjp/) - [Cloud-Upload](http://thenounproject.com/term/cloud-upload/9947/) created by [Scott Lewis](http://thenounproject.com/iconify/) - [Magnifying-Glass](http://thenounproject.com/term/magnifying-glass/89626/) public domain icon  The noun project is a great resource for finding clip art for use in applications.  ### Android Asset Studio  Icon generator that allows you to quickly and easily generate icons from existing source images, clipart, or text. You can generate Launcher icons, Action bar and tab icons,  Notification icons and Generic icons. The asset studio allows you to adjust sizing, padding, and tint icons.  [Android Asset Studio](http://romannurik.github.io/AndroidAssetStudio/)  ### Material Palette  Material Pallet is a simple web app that allows you to generate a color pallet and export the corresponding xml. This allows non-designers to pick a pallet that makes sense visually, and aligns with the guidelines for Material Design.  Additionally this helps enforce the correct use of color names in Android themes.  [www.MaterialPalette.com](http://www.materialpalette.com/)  ## Running this Project  From Android Studio simply choose to import and select the `build.gradle` in the root directory of the repository. Android Studio will set everything else up automatically.  ### The Noun Project API Keys  To obtain api keys for The Noun Project visit the [Getting Started](http://api.thenounproject.com/getting_started.html) page for additional information  Once you have the API key and secret, replace the placeholder values in: `app/src/main/res/values/nounproject_api_config.xml`.    ## License  > Copyright 2014 Bladymir Tellez > > Licensed under the Apache License, Version 2.0 (the "License"); > you may not use this file except in compliance with the License. > You may obtain a copy of the License at > > http://www.apache.org/licenses/LICENSE-2.0 > > Unless required by applicable law or agreed to in writing, software > distributed under the License is distributed on an "AS IS" BASIS, > WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. > See the License for the specific language governing permissions and > limitations under the License.
iDevicesInc/SweetBlue	<b>|</b>&nbsp;<a href='#why'>Why?</a> <b>|</b>&nbsp;<a href='#features'>Features</a> <b>|</b>&nbsp;<a href='#getting-started'>Getting Started</a> <b>|</b>&nbsp;<a href='#licensing'>Licensing</a> <b>|</b>&nbsp;<a href="https://github.com/iDevicesInc/SweetBlue/wiki">Wiki</a> <b>|</b>&nbsp;<a href="https://play.google.com/store/apps/details?id=com.idevicesinc.sweetblue.toolbox">Toolbox</a> <a href="https://travis-ci.org/iDevicesInc/SweetBlue">   <img align="right" src="https://img.shields.io/badge/version-2.52.12-blue.svg" />   <img align="right" src="https://travis-ci.org/iDevicesInc/SweetBlue.svg?branch=master"/> </a> <p align="center">   <br>   <a href="https://idevicesinc.com/sweetblue">     <img src="https://github.com/iDevicesInc/SweetBlue/blob/master/scripts/assets/sweetblue_logo.png" />   </a> </p> Why? ====  Android's BLE stack has some...issues...  * https://github.com/iDevicesInc/SweetBlue/wiki/Android-BLE-Issues * https://code.google.com/p/android/issues/detail?id=58381 * http://androidcommunity.com/nike-blames-ble-for-their-shunning-of-android-20131202/ * http://stackoverflow.com/questions/17870189/android-4-3-bluetooth-low-energy-unstable  SweetBlue is a blanket abstraction that shoves all that troublesome behavior behind a clean interface and gracefully degrades when the underlying stack becomes too unstable for even it to handle.  It’s built on the hard-earned experience of several commercial BLE projects and provides so many transparent workarounds to issues both annoying and fatal that it’s frankly impossible to imagine writing an app without it. It also supports many higher-level constructs, things like atomic transactions for coordinating authentication handshakes and firmware updates, flexible scanning configurations, read polling, transparent retries for transient failure conditions, and, well, the list goes on. The API is dead simple, with usage dependence on a few plain old Java objects and link dependence on standard Android classes. It offers conveniences for debugging and analytics and error handling that will save you months of work - last mile stuff you didn't even know you had to worry about.  Features ========  *	Full-coverage API documentation: http://idevicesinc.com/sweetblue/docs/api *	Sample applications. *	Battle-tested in commercial apps. *	Plain old Java with zero API-level dependencies. *	Rich, queryable state tracking that makes UI integration a breeze. *	Automatic service discovery. *	Full support for server role including advertising. *	Easy RSSI tracking with built-in polling and caching, including distance and friendly signal strength calculations. *	Highly configurable scanning with min/max time limits, periodic bursts, advanced filtering, and more. *	Continuous scanning mode that saves battery and defers to more important operations by stopping and starting as needed under the hood. *	Atomic transactions for easily coordinating authentication handshakes, initialization, and firmware updates. * 	Automatic striping of characteristic writes greater than [MTU](http://en.wikipedia.org/wiki/Maximum_transmission_unit) size of 20 bytes. *	Undiscovery based on last time seen. *	Clean leakage of underlying native stack objects in case of emergency. *	Wraps Android API level checks that gate certain methods. *	Verbose [logging](https://github.com/iDevicesInc/SweetBlue/wiki/Logging) that outputs human-readable thread IDs, UUIDs, status codes and states instead of alphabet soup. *	Wrangles a big bowl of thread spaghetti behind a nice asynchronous API - make a call on main thread, get a callback on main thread a short time later. *	Internal priority job queue that ensures serialization of all operations so native stack doesn’t get overloaded and important stuff gets done first. *	Optimal coordination of the BLE stack when connected to multiple devices. *	Detection and correction of dozens of BLE failure conditions. *	Numerous manufacturer-specific workarounds and hacks all hidden from you. *	Built-in polling for read characteristics with optional change-tracking to simulate notifications. *	Transparent retries for transient failure conditions related to connecting, getting services, and scanning. *	Comprehensive callback system with clear enumerated reasons when something goes wrong like connection or read/write failures. *	Distills dozens of lines of boilerplate, booby-trapped, native API usages into single method calls. *	Transparently falls back to Bluetooth Classic for certain BLE failure conditions. *	On-the-fly-configurable reconnection loops started automatically when random disconnects occur, e.g. from going out of range. *	Retention and automatic reconnection of devices after BLE off->on cycle or even complete app reboot. *	One convenient method to completely unwind and reset the Bluetooth stack. *	Detection and reporting of BLE failure conditions that user should take action on, such as restarting the Bluetooth stack or even the entire phone. *	Runtime analytics for tracking average operation times, total elapsed times, and time estimates for long-running operations like firmware updates.   Getting Started =============== 1. If using **Android Studio** or **Gradle**...   1. [Download](http://idevicesinc.com/sweetblue/#tryit) the latest release to a subfolder of your project such as `MyApp/src/main/lib/`. This ZIP contains several samples, precompiled JARS, and API docs and is preferable to downloading from GitHub, which only contains the raw source.   2. Open the app module's `build.gradle` file.   3. If building with source, your gradle file should look something like this:      ```          android {         compileSdkVersion 25         buildToolsVersion '25.0.3'                  defaultConfig {             minSdkVersion 18             targetSdkVersion 25             ...         }              sourceSets {             main.java.srcDirs += 'src/main/lib/sweetblue/src'             main.res.srcDirs += 'src/main/lib/sweetblue/res'             ...         }         ...     }          ```        4. If you're building with source from github, the sourceSet path is a bit different:        ```              android {           compileSdkVersion 25           buildToolsVersion '25.0.3'                      defaultConfig {               minSdkVersion 18               targetSdkVersion 25               ...           }                  sourceSets {               main.java.srcDirs += 'src/main/lib/sweetblue/library/src/main/java'                             main.res.srcDirs += 'src/main/lib/sweetblue/library/src/main/res'               ...           }           ...       }              ```        5. Else if building with JAR, it should look something like this:      ```          android {         compileSdkVersion 25         buildToolsVersion '25.0.3'                  defaultConfig {             minSdkVersion 18             targetSdkVersion 25             ...         }              dependencies {             compile fileTree(dir: 'libs', include: '*.jar')             ...         }         ...     }          ```      2. Now add these to the root of `MyApp/AndroidManifest.xml`:       ```     <uses-sdk android:minSdkVersion="18" android:targetSdkVersion="25" />     <uses-feature android:name="android.hardware.bluetooth_le" android:required="true" />     <uses-permission android:name="android.permission.BLUETOOTH" />     <uses-permission android:name="android.permission.BLUETOOTH_ADMIN" />     <uses-permission android:name="android.permission.BLUETOOTH_PRIVILEGED" />     <uses-permission android:name="android.permission.WAKE_LOCK" />     <uses-permission android:name="android.permission.ACCESS_COARSE_LOCATION" />          <!-- NOTE: Location is a new requirement for scanning in Android M.  -->     <!--       You may use ACCESS_FINE_LOCATION also or instead.         -->     ```          3. From your `Activity` or `Service` or `Application` instance, this is all it takes to discover a device, connect to it, and read a characteristic:     ```     // A ScanFilter decides whether a BleDevice instance will be created from a     // BLE advertisement and passed to the DiscoveryListener implementation below.     final ScanFilter scanFilter = new ScanFilter()     {     	@Override public Please onEvent(ScanEvent e)     	{     		return Please.acknowledgeIf(e.name_normalized().contains("my_device"))     		             .thenStopScan();     	}     };          // New BleDevice instances are provided through this listener.     // Nested listeners then listen for connection and read results.     // Obviously you will want to structure your actual code a little better.     // The deep nesting simply demonstrates the async-callback-based nature of the API.     final DiscoveryListener discoveryListener = new DiscoveryListener()     {     	@Override public void onEvent(DiscoveryEvent e)     	{     		if( e.was(LifeCycle.DISCOVERED) )     		{ 	    		e.device().connect(new StateListener() 	    		{ 	    			@Override public void onEvent(StateEvent e) 	    			{ 	    				if( e.didEnter(BleDeviceState.INITIALIZED) ) 	    				{ 	    					e.device().read(Uuids.BATTERY_LEVEL, new ReadWriteListener() 	    					{ 	    						@Override public void onEvent(ReadWriteEvent e) 	    						{ 	    							if( e.wasSuccess() ) 	    							{ 	    								Log.i("", "Battery level is " + e.data_byte() + "%");     								}     							}     						});     					}     				}     			});     		}     	} };          // This class helps you navigate the treacherous waters of Android M Location requirements for scanning.     // First it enables bluetooth itself, then location permissions, then location services. The latter two     // are only needed in Android M. This must be called from an Activity instance.     BluetoothEnabler.start(this, new DefaultBluetoothEnablerFilter()     {     	@Override public Please onEvent(BluetoothEnablerEvent e)     	{         	if( e.isDone() )         	{         		e.bleManager().startScan(scanFilter, discoveryListener);         	}         	         	return super.onEvent(e);         }     });     ```   Licensing =========  SweetBlue is released here under the [GPLv3](http://www.gnu.org/copyleft/gpl.html). Please visit http://idevicesinc.com/sweetblue for proprietary licensing options. In a nutshell, if you're developing a for-profit commercial app you may use this library for free for evaluation purposes, but most likely your use case will require purchasing a proprietary license before you can release your app to the public. See the [FAQ](https://github.com/iDevicesInc/SweetBlue/wiki/FAQ) for more details and https://tldrlegal.com/license/gnu-general-public-license-v3-%28gpl-3%29 for a general overview of the GPL. <p align="center"><a href="https://idevicesinc.com/sweetblue"><img src="https://github.com/iDevicesInc/SweetBlue/blob/master/scripts/assets/sweetblue_logo.png" /></a></p>
bergerkiller/BKCommonLib	This is a library-plugin system, introducing a lot of utility classes  It also simplifies coding: - PluginBase: allows quick registering and monitoring of plugins being enabled - (Async)Task: allows quick task starting and dynamic in-code creation - Utilities for virtually every needed task ahead, from mathematics to obtaining an entity by UUID - Custom node-based configuration extension - BlockLocation, BlockMap and BlockSet to safely use blocks in maps and sets - ItemParser class to generate a parser from user and use it during item transactions - Nanosecond StopWatch class for performance monitoring - Operation class to handle entities, players, chunks and worlds the way you want it  License:  Copyright (C) 2013 bergerkiller  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
yc-huang/Hive-mongo	This is a quick&dirty implementation of a MongoDB storage handler for Apache HIVE.  ##CAUTION:  * currently only support Hive primitive types: string, int, smallint....  * Whitespace should not be used in between entries in the "mongo.column.mapping" string, since these will be interperted as part of the column name, which is not what you want.  * if you want "insert overwrite" feature, you must have a field named be mapped to "_id" field (Object Id in MongoDB collections).  Some code are borrowed/referenced from Balshor's Google Spreadsheet Handler(https://github.com/balshor/gdata-storagehandler) and HyperTable Hive extension(http://code.google.com/p/hypertable/wiki/HiveExtension), thanks for the help.  ##How to build Here's a simple guide on how to build, hope it helps(thanks WalterDalton for providing the information):  * 1. make sure you have java sdk installed (otherwise download and install from http://www.oracle.com/technetwork/java/index.html) , $JAVA_HOME env variable is point to the installed directory and $JAVA_HOME/bin/ is included in $PATH env variable;  * 2. download maven from http://maven.apache.org and install to a directory (let's say $MAVEN_HOME), add $MAVEN_HOME/bin to $PATH  * 3. git clone Hive-Mongo to a directory; launch a cmd shell, cd that directory and execute "mvn package"; if everything is OK, you can find "hive-mongo-0.0.1-SNAPSHOT.jar" in the "target" directory. There also have a jar named "hive-mongo-0.0.1-SNAPSHOT-jar-with-dependencies.jar" which is a combo; with this one you do not need to include mongo-java-driver-2.6.3.jar and guava-r06.jar.  ##Sample Usage:      > $HIVE_HOME/bin/hive --auxpath /home/yc.huang/mongo-java-driver-2.6.3.jar,/home/yc.huang/guava-r06.jar,       /home/yc.huang/hive-mongo-0.0.3-SNAPSHOT.jar            hive> create external table mongo_users(id int, name string, age int)       stored by "org.yong3.hive.mongo.MongoStorageHandler"       with serdeproperties ( "mongo.column.mapping" = "_id,name,age" )       tblproperties ( "mongo.host" = "192.168.0.5", "mongo.port" = "11211",       "mongo.db" = "test", "mongo.user" = "testUser", "mongo.passwd" = "testPasswd", "mongo.collection" = "users" );      OK     Time taken: 4.093 seconds            hive> insert overwrite table mongo_users select id, name,age from hive_test;            Total MapReduce jobs = 1      Launching Job 1 out of 1      Number of reduce tasks is set to 0 since there's no reduce operator      Starting Job = job_201111021553_13715, Tracking URL = http://JobTracker:50030/jobdetails.jsp?jobid=job_201111021553_13715      Kill Command = /root/dev/hadoop-0.20.2/bin/../bin/hadoop job  -Dmapred.job.tracker=JobTracker:9001 -kill job_201111021553_13715      2011-11-17 18:01:25,849 Stage-0 map = 0%,  reduce = 0%      2011-11-17 18:01:28,876 Stage-0 map = 100%,  reduce = 0%      2011-11-17 18:01:31,893 Stage-0 map = 100%,  reduce = 100%      Ended Job = job_201111021553_13715      4 Rows loaded to mongo_users      OK      Time taken: 14.37 seconds      hive> select * from mongo_users;            OK            1       Tom     28      2       Alice   18      3       Bob     29      101     Scott   10      Time taken: 0.171 seconds
apache/openjpa	Apache OpenJPA - README.txt Licensed under Apache License 2.0 - http://www.apache.org/licenses/LICENSE-2.0 --------------------------------------------------------------------------------  Thank you for downloading this release of Apache OpenJPA.  The following files can be found in the openjpa-project subdirectory:     BUILDING.txt     CHANGES.txt     RELEASE-NOTES.html  For documentation and project information, please visit our project site:     http://openjpa.apache.org/
sreichholf/dreamDroid	[![Build Status](https://travis-ci.org/sreichholf/dreamDroid.svg?branch=master)](https://travis-ci.org/sreichholf/dreamDroid)    # LICENSE  >© Stephan Reichholf (stephan at reichholf dot net)  >  >Unless stated otherwise in a files head all java and xml-code of this Project is:  >Licensed under the Create-Commons Attribution-Noncommercial-Share Alike 3.0 Unported  >http://creativecommons.org/licenses/by-nc-sa/3.0/  >   >All graphics, except the dreamDroid icon, can be used for any other non-commercial purposes.  >The dreamDroid icon may not be used in any other projects than dreamDroid itself.  >  >All files that are part of 3rd party projects are not part of this license agreement but keep their original license    # Functionality  Remotely control your enigma2 based Dreambox with any Android Device (running android >= 2.1)!    dreamDroid has a fragment-based layout which is optimized for all common device sizes.    # Target Platforms  dreamDroid is built for use with genuine Dreamboxes and the included genuine WebInterface.  Officially supported are the following devices:    * Dreambox DM 7080 HD  * Dreambox DM 820 HD  * Dreambox DM 520 HD  * Dreambox DM 525 HD  * Dreambox DM 7020 HD  * Dreambox DM 8000 HD PVR  * Dreambox DM 800 HD se  * Dreambox DM 800 HD PVR  * Dreambox DM 500 HD  * Dreambox DM 7025(+)    # Requirements    * The target dreamboxes should be running a WebInterface with Version 1.6.8 or later.  * dreamDroid requires at least Android 4.0    # Developing / building dreamDroid with AndroidStudio    Simply import dreamDroid as gradle project and you should be fine.
FDVSolutions/DynamicJasper	# DynamicJasper  DynamicJasper (DJ) is an API that hides the complexity of __JasperReports__, it helps developers to save time when designing  simple/medium complexity reports generating the layout of the report elements automatically. It creates reports dynamically, defining at runtime the columns, column width (auto width), groups, variables, fonts, charts, crosstabs, sub reports  (that can also be dynamic), page size and everything else that you can define at design time.  DJ keeps full compatibility with Jasper Reports since it's a tool that helps create reports programmatically in a easy way (it only interferes with the creation of the report design doing the layout of the elements).  You can use the classic .jrxml files as templates while the content and layout of the report elements are handled by the DJ API.  http://dynamicjasper.com/  ### Maven dependency      <dependency>       <groupId>ar.com.fdvs</groupId>       <artifactId>DynamicJasper</artifactId>       <version>5.0.11</version>     </dependency>  ### Documentation and Examples  Find documentation and examples at http://dynamicjasper.com/documentation-examples/  ## Report Concept  This API is intended to solve the 99% of the classic reports that consist in a number of fields (that are going to be columns in the report), then the report may have some “repeating groups” defined using a field as a criteria. Some columns may have variables (with operations such as SUM or COUNT) for some of the fields. All this tasks can be done automatically by the DJ through a very easy-to-use API.  You can redefine at runtime the column order, “repeating groups”, variables, styles, conditional styles, etc. Its completely dynamic!!!  ## Features  Most of the features are provided directly by Jasper Reports (great tool guys!), nevertheless through the DJ API some of the results are achieved with really no effort.  * 100% pure Java * No need to use other tool than you favorite IDE. * Friendly and intuitive API. * Mature, robust and stable.  __Dynamic column report__: Columns can be defined at runtime, which means you also control (at runtime) the column positioning, width, title, etc.  __Repeating groups / Breaking groups__: Create repeating groups dynamically using simple expressions as criteria or complex custom expressions. Each repeating group may have a header and/or footer, which can have a variable showing the result of an operation (SUM, COUNT or any other provided by Jasper Reports).  __Automatic report layout__: Just define a minimum set of options and DJ will take care of the layout. It’s not an issue to generate the same report for different page sizes and orientation many more!  __Dynamic Crosstabs__: Jasper Report’s popular crosstabs can now be created dynamically in an easy and convenient way.  ## Sub reports  Sub reports are supported; they can also be dynamically created. Concatenating many reports in a single one (e.g.: a single PDF) can be a hard task. Using DynamicJasper it is really easy get reports of different nature in a single one. Styles: Each column can have its own style for its title and detail data (defining border, border color, font size, type and color, background color, etc.).  * __Style library from jrxml__ files are supported. * __Calculation Variables__: Repeating groups can have variables that hold the result of an operation on a given field (column). With DJ adding variables is a 1 line of code task. * __JRXML template files support__: You can use a base template jrxml file in which common styles, company logo, water mark, etc can be pre defined. * __Conditional Format__: DJ provides a very simple way to define conditional formats. You can use simple conditions or custom conditions. * __Auto text__: Add auto text in page header and footer such as ?Page 1 of 10?, ?Generated on Oct. 10th 2007? or a custom text. * __Charts__: Easy to add simple charts. * __Barcode columns__: As simple as adding a regular column. * __Export to most popular formats__: As DJ stands over Jasper Reports, it can export to PDF, XML, HTML, CSV, XLS, RTF, TXT. * __Clean Excel export__: One of the most valuable features that DJ provides is exporting plain reports to excel, with no data formatting, no page break, etc. This is very valuable for end users that use this report to create dynamic tables in Excel, creating these reports just with Jasper Reports can demand a lot of design time.  ## Professional support  Alternative professional support available. http://dynamicjasper.com/support/professional-support-eng/
jbarrez/Activiti-KickStart	Run the UI ----------  Run `./start-ui.sh`  Overview --------  Activiti KickStart is a webbased tool to quickly create 'adhoc' business processes using a  subset of constructs available to the Activiti engine (http://activiti.org).  KickStart provides a simple UI that doesn't require to learn BPMN or any modeling environment, as it works with concepts that are familiar to every business user.  However, the processes that are created using KickStart, are fully BPMN 2.0 compliant and can be used as a starting point for more complex BPM endeavours.  KickStart integrates perfectly with the Activiti engine. As such, processes created with KickStart are immediataly usable by the Activiti framework.  KickStart serves many business cases, but the following three are probably the most common:  * Simple business processes: some processes are just simple by nature, and every company has them. Think about an expense process, a holiday leave process, a hiring process, etc... These kind of processes are probably already being done using paper or e-mail. KickStart allows to model these processes quickly and change them whenever it is needed. As such, KickStart really lowers the threshold to automate these business processes. * Prototyping: before diving into complex BPMN 2.0 modeling and thinking about the corner cases of the process, it is often wise to get all people involved aligned and work out a prototype that shows the vision of what needs to be done. KickStart allows to do exatcly that: create a business process prototype on the fly, to get your ideas visible for everyone. * Adhoc work: in some cases, coordination is required between different people or groups in a company. You know how it normally goes: sending an email here, doing a telephone there ... which often ends up in a tarpit of nobody knowing what or when something needs to be done. However, a business process management platform such as Activiti is an excellent way of distributing and follow-up everything, as it is intended to track exactly such things. KickStart allows you to create processes for adhoc work in a matter of minutes, and distribute and coordinate tasks between people easily.    Screenshots & Screencast ------------------------  [Screencast from Januari 2011](http://www.jorambarrez.be/blog/2011/01/05/adhoc-workflow-with-activiti-kickstart/)  Contributors ------------  * Joram Barrez (Alfresco) * Daniel Meyer (Camunda) * Frederik Heremans (Alfresco) * Bernd Ruecker (Camunda)
ToastShaman/dropwizard-auth-jwt	[![Build Status](https://travis-ci.org/ToastShaman/dropwizard-auth-jwt.svg?branch=master)](https://travis-ci.org/ToastShaman/dropwizard-auth-jwt) [![Maven Central](https://img.shields.io/maven-central/v/com.github.toastshaman/dropwizard-auth-jwt.svg)](http://mvnrepository.com/artifact/com.github.toastshaman/dropwizard-auth-jwt)  # dropwizard-auth-jwt  A Dropwizard authentication filter using JSON Web Token (JWT).   ## What is it? JSON Web Token (JWT) is a compact URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object that is digitally signed using JSON Web Signature (JWS).  Check out http://jwt.io/  ## What's new in v1.1.2-0 * Updated upstream dependencies to Dropwizard 1.1.2 * Updated upstream dependencies to jose4j to 0.6.0    ## What's new in v1.1.0-0 * Updated upstream dependencies to Dropwizard 1.1.0    ## What's new in v1.0.6-0 * Updated upstream dependencies to Dropwizard 1.0.6 and jose4j to 0.5.5    ## What's new in v1.0.2-0 * Updated upstream dependencies to Dropwizard 1.0.2 and jose4j to 0.5.2    ## What's new in v1.0.0-0 * Replaced the JWT token generation and verification with [jose4j](https://bitbucket.org/b_c/jose4j/wiki/Home).   ## What's new in v0.9.2-0 * Updated the dependencies to Dropwizard 0.9.2.  ## What's new in v0.9.1-1 * Added support for CachingAuthenticator.  ## What's new in v0.9.1-0 * Added support for Dropwizard 0.9.x. * Support for extracting JWT tokens from cookies.  ## Getting Started  To use this library in your project you can download it from Maven Central.  ```xml <dependency>     <groupId>com.github.toastshaman</groupId>     <artifactId>dropwizard-auth-jwt</artifactId>     <version>1.1.2-0</version> </dependency> ```  ## Example See this [code example](https://github.com/ToastShaman/dropwizard-auth-jwt/tree/master/src/test/java/com/github/toastshaman/dropwizard/auth/jwt/example)  if you want to use this code your dropwizard application. Once you have started the example application here are some  sample requests to generate a valid and an expired token:  ``` curl -X GET -H "Cache-Control: no-cache" 'http://localhost:8080/jwt/generate-valid-token' ```  or you can create an invalid token instead to see a failure case with:   ``` curl -X GET -H "Cache-Control: no-cache" 'http://localhost:8080/jwt/generate-expired-token' ```  Once you have a token, you can send it to the following endpoint to get some information about the logged in user:  ``` curl -X GET \ -H "Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzUxMiJ9.eyJpYXQiOjE0NDkzMTQwOTUsInN1YiI6Imdvb2QtZ3V5In0.oFXdelQECJrw6_e4gR1HU3ljFvY8zmf2EHDsBnnea7n2UDBipmNDbx3bw-Bzzq-FwtEO6qzageK2jbJxM6JHbQ" \ -H "Cache-Control: no-cache" 'http://localhost:8080/jwt/check-token' ```  ## License Apache License Version 2.0   http://apache.org/licenses/LICENSE-2.0.txt  ## Thanks To A special thanks goes to [MartinSahlen](https://github.com/MartinSahlen) for providing a Gist with the Dropwizard 8 implementation of the AuthFactory.  A special thanks goes to [Kimble](https://github.com/kimble) for adding cookie support.  A special thanks goes to [alexitooi](https://github.com/alexitooi) for adding support for the CachingAuthenticator.
JFrogDev/build-info	## Overview  Build Info is Artifactory's open integration layer for the CI servers and build tools. The build information is sent to Artifactory in json format.  ## Building and Testing the Sources  The code is built using Gradle and includes integration tests.<br/> It must run using JDK 7 and Gradle 2.4. If you are using different gradle version you can use the provided gradle wrapper.<br/> In order to run tests the following environment variable must be provide: ``` export BITESTS_ARTIFACTORY_URL='http://localhost:8081/artifactory' export BITESTS_ARTIFACTORY_USERNAME=admin export BITESTS_ARTIFACTORY_PASSWORD=password export BITESTS_ARTIFACTORY_REPO=tests ``` Before running the tests, please make sure you have a generic repository named *tests* in Artifactory.  To build the code using the gradle wrapper in Unix run:   ``` > ./gradlew clean build ``` To build the code using the gradle wrapper in Windows run:   ``` > gradlew clean build ``` To build the code using the environment gradle run:   ``` > gradle clean build ``` To build the code without running the tests, add to the "clean build" command the "-x test" option, for example: ``` > ./gradlew clean build -x test ``` ## Build Info json format  ```groovy {   "properties" : {    /* Environment variables and properties collected from the CI server.       The "buildInfo.env." prefix is added to environment variables and build related properties.       For system variables there's no prefix. */    "buildInfo.env.JAVA_HOME" : "",    ...   },   "version" : "1.0.1", // Build Info schema version   "name" : "My-build-name", // Build name   "number" : "28", // Build number   "type" : "MAVEN", // Build type (values currently supported: MAVEN, GRADLE, ANT, IVY and GENERIC)   "buildAgent" : { // Build tool information     "name" : "Maven", // Build tool type     "version" : "3.0.5" // Build tool version   },   "agent" : { // CI Server information     "name" : "Jenkins", // CI server type     "version" : "1.554.2" // CI server version   },   "started" : "2014-09-30T12:00:19.893+0300", // Build start time in the format of yyyy-MM-dd'T'HH:mm:ss.SSSZ   "artifactoryPluginVersion" : "2.3.1",   "durationMillis" : 9762, // Build duration in milliseconds   "artifactoryPrincipal" : "james", // Artifactory principal (the Artifactory user used for deployment)   "url" : "http://my-ci-server/jenkins/job/My-project-name/28/", // CI server URL   "vcsRevision" : "e4ab2e493afd369ae7bdc90d69c912e8346a3463", // VCS revision   "vcsUrl" : "https://github.com/github-user/my-project.git", // VCS URL   "licenseControl" : {	// Artifactory License Control information     "runChecks" : true,	// Artifactory will run automatic license scanning after the build is complete (true/false)     "includePublishedArtifacts" : true, // Should Artifactory run license checks on the build artifacts, in addition to the build dependecies (true/false)      "autoDiscover" : true, // Should Artifactory auto discover licenses (true/false)     "scopesList" : "", // A space-separated list of dependency scopes/configurations to run license violation checks on. If left empty all dependencies from all scopes will be checked.     "licenseViolationsRecipientsList" : "" // Emails of recipients that should be notified of license violations in the build info (space-separated list)   },   "buildRetention" : { // Build retention information     "deleteBuildArtifacts" : true, // Automatically remove build artifacts stored in Artifactory (true/false)     "count" : 100, // The maximum number of builds to store in Artifactory.     "minimumBuildDate" : 1407345768020, // Earliest build date to store in Artifactory     "buildNumbersNotToBeDiscarded" : [ ] // List of build numbers that should not be removed from Artifactory   },   /* List of build modules */	   "modules" : [ { // The build's first module     "properties" : { // Module properties       "project.build.sourceEncoding" : "UTF-8"     },     "id" : "org.jfrog.test:multi2:4.2-SNAPSHOT", // Module ID     /* List of module artifacts */     "artifacts" : [ {       "type" : "jar",       "sha1" : "2ed52ad1d864aec00d7a2394e99b3abca6d16639",       "md5" : "844920070d81271b69579e17ddc6715c",       "name" : "multi2-4.2-SNAPSHOT.jar"     }, {       "type" : "pom",       "sha1" : "e8e9c7d790191f4a3df3a82314ac590f45c86e31",       "md5" : "1f027d857ff522286a82107be9e807cd",       "name" : "multi2-4.2-SNAPSHOT.pom"     } ],     /* List of module dependencies */     "dependencies" : [ {       "type" : "jar",       "sha1" : "99129f16442844f6a4a11ae22fbbee40b14d774f",       "md5" : "1f40fb782a4f2cf78f161d32670f7a3a",       "id" : "junit:junit:3.8.1",       "scopes" : [ "test" ]     } ]   }, { // The build's second module     "properties" : { // Module properties       "project.build.sourceEncoding" : "UTF-8"     },     "id" : "org.jfrog.test:multi3:4.2-SNAPSHOT", // Module ID     /* List of module artifacts */	     "artifacts" : [ { // Module artifacts       "type" : "war",       "sha1" : "df8e7d7b94d5ec9db3bfc92e945c7ff4e2391c7c",       "md5" : "423c24f4c6e259f2774921b9d874a649",       "name" : "multi3-4.2-SNAPSHOT.war"     }, {       "type" : "pom",       "sha1" : "656330c5045130f214f954643fdc4b677f4cf7aa",       "md5" : "b0afa67a9089b6f71b3c39043b18b2fc",       "name" : "multi3-4.2-SNAPSHOT.pom"     } ],     /* List of module dependencies */     "dependencies" : [ {       "type" : "jar",       "sha1" : "a8762d07e76cfde2395257a5da47ba7c1dbd3dce",       "md5" : "b6a50c8a15ece8753e37cbe5700bf84f",       "id" : "commons-io:commons-io:1.4",       "scopes" : [ "compile" ]     }, {       "type" : "jar",       "sha1" : "342d1eb41a2bc7b52fa2e54e9872463fc86e2650",       "md5" : "2a666534a425add50d017d4aa06a6fca",       "id" : "org.codehaus.plexus:plexus-utils:1.5.1",       "scopes" : [ "compile" ]     }, {       "type" : "jar",       "sha1" : "449ea46b27426eb846611a90b2fb8b4dcf271191",       "md5" : "25c0752852205167af8f31a1eb019975",       "id" : "org.springframework:spring-beans:2.5.6",       "scopes" : [ "compile" ]     } ]   } ],   /* List of issues related to the build */     "issues" : {     "tracker" : {       "name" : "JIRA",       "version" : "6.0.1"     },     "aggregateBuildIssues" : true,  //whether or not there are issues that already appeared in previous builds     "aggregationBuildStatus" : "Released",     "affectedIssues" : [ {       "key" : "RTFACT-1234",       "url" : "https://www.jfrog.com/jira/browse/RTFACT-1234",       "summary" : "Description of the relevant issue",       "aggregated" : false  //whether or not this specific issue already appeared in previous builds     }, {       "key" : "RTFACT-5469",       "url" : "https://www.jfrog.com/jira/browse/RTFACT-5678",       "summary" : "Description of the relevant issue",       "aggregated" : true     } ]    },     "governance" : { // Black Duck Code Center integration information     "blackDuckProperties" : {       "appName" : "", // The Black Duck Code Center application name       "appVersion" : "", // The Black Duck Code Center application version       "reportRecipients" : "", // Recipients that should receive an email report once the automatic Black Duck Code Center compliance checks are completed (space-separated list)       "scopes" : "", // A list of dependency scopes/configurations to run Black Duck Code Center compliance checks on. If left empty all dependencies from all scopes will be checked  (space-separated list)       "runChecks" : true, // Should Black Duck Code Center run automatic compliance checks after the build is completed (true/false)       "includePublishedArtifacts" : true, // Include the build's published module artifacts in the Black Duck Code Center compliance checks if they are also used as dependencies for other modules in this build  (true/false)       "autoCreateMissingComponentRequests" : true, // Auto create missing components in Black Duck Code Center application after the build is completed and deployed in Artifactory (true/false)       "autoDiscardStaleComponentRequests" : true // Auto discard stale components in Black Duck Code Center application after the build is completed and deployed in Artifactory (true/false)     }   } } ```  ### Build Info json schema ```groovy {   "$schema": "http://json-schema.org/draft-04/schema#",   "title": "build-info",   "description": "Artifactory build-info",   "type": "object",   "properties": {     "properties": {       "type": "object",       "description": "Environment variables and properties collected from the CI server",       "patternProperties": {         "^.+$": {           "type": "string"         }       }     },     "version": {       "description": "Build info schema version",       "type": "string"     },     "name": {       "description": "Build name",       "type": "string"     },     "number": {       "description": "Build number",       "type": "string"     },     "type": {       "description": "Build type",       "type": "string",       "enum": [ "MAVEN", "GRADLE", "ANT", "IVY", "GENERIC" ]     },     "buildAgent": {       "description": "Build tool information",       "type": "object",       "properties": {         "name": {           "description": "Build tool type",           "type": "string"         },         "version": {           "description": "Build tool version",           "type": "string"         }       },       "required": [ "name", "version" ],       "additionalProperties": false     },     "agent": {       "description": "CI server information",       "type": "object",       "properties": {         "name": {           "description": "CI server type",           "type": "string"         },         "version": {           "description": "CI server version",           "type": "string"         }       },       "required": [ "name", "version" ],       "additionalProperties": false     },     "started": {       "description": "Build start time",       "type": "string",       "pattern": "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}.\\d{3}(Z|[+-]\\d{4})$"     },     "durationMillis": {       "description": "Build duration in milliseconds",       "type": "integer"     },     "principal": {       "description": "",       "type": "string"     },     "artifactoryPrincipal": {       "description": "Artifactory user used for deployment",       "type": "string"     },     "url": {       "description": "CI server URL",       "type": "string"     },     "vcsRevision": {       "description": "VCS revision",       "type": "string"     },     "vcsUrl": {       "description": "VCS URL",       "type": "string"     },     "licenseControl": {       "description": "Artifactory License Control Information",       "type": "object",       "allOf": [         {           "properties": {             "runChecks": {               "description": "Run automatic license scanning after the build is complete",               "type": "boolean"             },             "includePublishedArtifacts": {               "description": "Run license checks on artifacts in addition to build dependencies",               "type": "boolean"             },             "autoDiscover": {               "description": "Artifactory should auto-discover license",               "type": "boolean"             },             "scopesList": {               "description": "Space-separated list of dependency scopes to run license violation checks",               "type": "string"             },             "licenseViolationRecipients": {},             "licenseViolationsRecipientsList": {}           },           "required": [ "runChecks", "includePublishedArtifacts", "autoDiscover", "scopesList" ],           "additionalProperties": false         },         {           "anyOf": [             {               "properties": {                 "licenseViolationRecipients": {                   "description": "List of email addresses to be notified of license violations",                   "type": "array",                   "items": {                     "type": "string"                   }                 }               },               "required": [ "licenseViolationRecipients" ]             },             {               "properties": {                 "licenseViolationsRecipientsList": {                   "description": "Space-separated list of email addresses to be notified of license violations",                   "type": "string"                 }               },               "required": [ "licenseViolationsRecipientsList" ]             }           ]         }       ]     },     "buildRetention": {       "description": "Build Retention Information",       "type": "object",       "allOf": [         {           "properties": {             "deleteBuildArtifacts": {               "description": "Automatically remove build artifacts stored in Artifactory",               "type": "boolean"             },             "buildNumbersNotToBeDiscarded": {               "description": "List of build numbers that should not be removed from Artifactory",               "type": "array",               "items": {                 "type": "integer"               }             },             "count": {},             "minimumBuildDate": {}           },           "additionalProperties": false         },         {           "anyOf": [             {               "properties": {                 "count": {                   "description": "Maximum number of builds to store in Artifactory",                   "type": "integer"                 }               },               "required": [ "count" ]             },             {               "properties": {                 "minimumBuildDate": {                   "description": "Earliest build date to store in Artifactory",                   "type": "integer"                 }               },               "required": [ "minimumBuildDate" ]             }           ]         }       ]     },     "modules": {       "description": "Artifactory License Control Information",       "type": "array",       "items": {         "type": "object",         "properties": {           "properties": {              "description": "Module properties",              "type": "object",              "patternProperties": {                 "^.+$": {                   "type": "string"                 }              }           },           "id": {             "description": "Module ID",             "type": "string"           },           "artifacts": {             "description": "List of module artifacts",             "type": "array",             "items": {               "type": "object",               "properties": {                 "type": {                   "type": "string"                 },                 "name": {                   "type": "string"                 },                 "sha1": {                   "$ref": "#/definitions/sha1"                 },                 "md5": {                   "$ref": "#/definitions/md5"                 }               },               "required": [ "name", "sha1", "md5" ]             }           },           "dependencies": {             "description": "List of module dependencies",             "type": "array",             "items": {               "type": "object",               "properties": {                 "type": {                   "type": "string"                 },                 "id": {                   "type": "string"                 },                 "sha1": {                   "$ref": "#/definitions/sha1"                 },                 "md5": {                   "$ref": "#/definitions/md5"                 },                 "scopes": {                   "type": "array",                   "items": {                     "type": "string"                   }                 }               },               "required": [ "id", "sha1", "md5" ]             }           }         },         "required": [ "id", "artifacts", "dependencies" ],         "additionalProperties": false       }     },     "issues": {       "description": "List of issues related to the build",       "type": "object",       "properties": {         "tracker": {           "type": "object",           "properties": {             "name": {               "type": "string"             },             "version": {               "type": "string"             }           },           "required": [ "name", "version" ],           "additionalProperties": false         },         "aggregateBuildIssues": {           "description": "Whether issues have appeared in previous builds",           "type": "boolean"         },         "aggregationBuildStatus": {           "type": "string"         },         "affectedIssues": {           "type": "array",           "items": {             "type": "object",             "properties": {               "key": {                 "type": "string"               },               "url": {                 "type": "string"               },               "summary": {                 "type": "string"               },               "aggregated": {                 "description": "Whether this specific issue already appeared in previous builds",                 "type": "boolean"               }             },             "required": [ "key", "url", "summary", "aggregated" ],             "additionalProperties": false           }         }       },       "required": [ "tracker", "aggregateBuildIssues", "aggregationBuildStatus", "affectedIssues" ],       "additionalProperties": false     },     "governence": {       "description": "Black duck code center integration information",       "type": "object",       "properties": {         "blackDuckProperties": {           "type": "object",           "properties": {             "appName": {               "description": "The Black Duck Code Center application name",               "type": "string"             },             "appVersion": {               "description": "The Black Duck Code Center application version",               "type": "string"             },             "reportRecipients": {               "description": "Space-separated list of recipients that should receive an email report once the automatic Black Duck Code Center compliance checks are completed",               "type": "string"             },             "scopes": {               "description": "Space-separated list of dependency scopes/configurations to run Black Duck Code Center compliance checks on. If left empty all dependencies from all scopes will be checked",               "type": "string"             },             "runChecks": {               "description": "Should Black Duck Code Center run automatic compliance checks after the build is completed",               "type": "boolean"             },             "includePublishedArtifacts": {               "description": "Include the build's published module artifacts in the Black Duck Code Center compliance checks if they are also used as dependencies for other modules in this build",               "type": "boolean"             },             "autoCreateMissingComponentRequests": {               "description": "Auto create missing components in Black Duck Code Center application after the build is completed and deployed in Artifactory",               "type": "boolean"             },             "autoDiscardStaleComponentRequests": {               "description": "Auto discard stale components in Black Duck Code Center application after the build is completed and deployed in Artifactory",               "type": "boolean"             }           },           "required": [ "appName", "appVersion", "reportRecipients", "scopes", "runChecks", "includePublishedArtifacts", "autoCreateMissingComponentRequests", "autoDiscardStaleComponentRequests" ],           "additionalProperties": false         }       },       "required": [ "blackDuckProperties" ],       "additionalProperties": false     }   },   "required": [ "version", "name", "number", "type", "started", "durationMillis", "modules" ],   "additionalProperties": false,   "definitions": {     "sha1": {       "description": "sha1 hash",       "type": "string",       "pattern": "^[0-9a-f]{40}$"     },     "md5": {       "description": "md5 hash",       "type": "string",       "pattern": "^[0-9a-f]{32}$"     }   } } ```
neo4j-contrib/cypher-dsl	This module provides a Java DSL for the Cypher query language. Using the DSL you can either build up an expression that can be stringified, or you can access the internal query model.  There are two main modes: using static methods or Java instance initialization blocks.  Example usage with statics:      assertEquals( "START n=node(1) RETURN n",                   start( node( "n", 1 ) ).returns( nodes( "n" ) ).toString() );  Example usage with Java instance initialization block:      assertEquals( "START john=node(0) RETURN john", new CypherQuery()     {{         starts( node( "john", 0 ) ).returns( nodes( "john" ) );     }}.toString() );  Once you have the constructed query string this can be sent to the Cypher execution engine. Example:      Execute q = start(node("john", john)).                 match(path().from("john").out("friend").link().out("friend").to("fof")).                 returns(properties("john.name", "fof.name"));     ExecutionResult result = engine.execute( q.toString() ).toString();  QueryDSL integration ==================== It is possible to use the QueryDSL library and predicates with this DSL. Here's an example:      QPerson person = QPerson.person;     Assert.assertEquals( "START person=node(1,2,3) WHERE person.firstName=\"P\" and person.age>25 RETURN person",                          CypherQueryDSL.start( node( "person", 1, 2, 3 ) )                              .where( person.firstName.eq( "P" ).and( person.age.gt( 25 ) ) )                              .returns( nodes( "person" ) )                              .toString() );  Apart from using QueryDSL for the where-expression, it is also possible to create the Lucene index lookups using QueryDSL.      QPerson person = QPerson.person;     Assert.assertEquals( "START person=node:node_auto_index(\"firstName:rickard\") RETURN person",                          CypherQueryDSL.start( LuceneStartExpression.query("person", "node_auto_index", person.firstName.eq("Rickard")) )                              .returns( nodes( "person" ) )                              .toString() );   See the POM and tests of this module for further examples and how to use these features.
regunathb/Trooper	# Trooper  Trooper is a Java module like framework that provides various runtime profiles for building applications. Batch, Service and Orchestration runtime profiles are currently supported. It is an umbrella project for a number of things:  * Build Service Oriented applications that can be distributed and scaled. * Create a Java module-like system to build runtime profiles that applications can choose from : Basic, Service, Orchestration, Batch etc. * Implement a number of patterns suited for scalability and deployment on commodity hardware. E.g. sharding, statelessness, data locality, fail-fast, checkpointing and recovery. * Sub-projects that may be used totally independent of Trooper. E.g. the "mule-transport-rabbitmq" is a Maven project providing a RabbitMQ transport for Mule. * Provide data models suited for service interactions, event driven design and metrics collection  ## Releases  | Release | Date | Description | |:------------|:----------------|:------------| | Version 2.0.2    | May 2017       |     Bug fix for Jetty loading in non-Unix OS | Version 2.0.1    | Oct 2016       |     Removed bottleneck in PlatformEventMultiCaster | Version 2.0.0    | May 2016       |     Jdk 1.8, Removed logback as default. Uses only slf4j | Version 1.3.4    | Feb 2016       |     Support for multiple component containers  ## Changelog  Changelog can be viewed in CHANGELOG.md file (https://github.com/regunathb/Trooper/blob/master/CHANGELOG.md)  ## Documentation and Examples  The Trooper "examples" project group demonstrates usage of various application profiles supported by Trooper. Documentation is continuously being added to the Wiki page of Trooper (https://github.com/regunathb/Trooper/wiki)  ## Getting help  For discussion, help regarding usage, or receiving important announcements, subscribe to Trooper mailing list: http://groups.google.com/group/trooper-users  ## License  Trooper is licensed under : The Apache Software License, Version 2.0. Here is a copy of the license (http://www.apache.org/licenses/LICENSE-2.0.txt)  ## Project lead  * Regunath B ([@regunathb](http://twitter.com/RegunathB))  ## Core contributors  * Shashikant Soni ([@shashiks](https://github.com/shashiks)) * Srikanth PS ([@srikanthps](http://twitter.com/srikanthps)) * Devashish Shankar ([@devashishshankar](https://github.com/devashishshankar)) * Jagadeesh Huliyar ([@jagadeesh-huliyar](https://github.com/jagadeesh-huliyar))  ## Trooper users  ( _Write to us if you are a Trooper user and would like to be mentioned here_ )  * [Flipkart](http://www.flipkart.com) - large eCommerce portal in India   * Catalog update propagation to website uses Trooper Batch and Orchestration profiles   * Notification platform uses Trooper Batch and Orchestration profiles   * Review Summarization processing pipeline is built using [Sift](https://github.com/regunathb/Sift) and Trooper Batch   * [Phantom](https://github.com/Flipkart/phantom) (Service Proxy) uses Trooper Basic profile and HBase persistence libraries. Mobile API platform is built on Service Proxy.  *  R&D department of a big company providing national electricity in France   * Uses Trooper Batch profile in an application that helps researchers to do theirs statistics studies. It is used to import new  data daily from many sources.
socialsignin/spring-social-security	About Spring-Social-Security ============================  Many applications using Spring Security for authentication will need to  * Ask users to sign up using a username and password. * Create their own user details store and data access objects * Provide account management (eg. forgotten password functionality) * Provide access control for protected resources  For applications which already use spring-social to connect with external authenticated apis (eg. Facebook, Twitter) , SocialSignIn's spring-social-security module removes these authentication concerns by delegating authentication in Spring Security via spring-social to the third party api. Authenticating your website users via spring-social api providers means:  * No need for users to remember another password for another site. * No need for developers to create their own user details store, as the store used by spring-social is used instead. * No need for developers to provide account management as this is provided by the third party api. * Provider-specific roles are granted to users on the basis of their connected providers, allowing fine-grained permissioning model.  For simple "Hello World" apps demonstrating spring-social-security see:  * https://github.com/socialsignin/spring-social-security-demo  ( Basic integration with core spring-social )  * https://github.com/socialsignin/socialsignin-showcase ( Using SocialSignin provider modules for auto-registration of providers and API abstraction layer, using default local user account domain model)  * https://github.com/michaellavelle/socialsignin-showcase ( Using SocialSignin provider modules for auto-registration of providers and API abstraction layer, using custom local user account domain model )  * https://github.com/socialsignin/socialsignin-roo-showcase ( Roo project with SocialSignin modules and Roo-backed persistence for UserConnections )  Also see <a href="http://socialsignin.org/spring-social-security/docs/1.0.2.RELEASE/api/">JavaDoc for Spring Social Security</a> for API docs.  [![endorse](https://api.coderwall.com/michaellavelle/endorsecount.png)](https://coderwall.com/michaellavelle)   Spring-Social-Security Quickstart =================================  Adding Spring-Social-Security to a Spring-Social web application ----------------------------------------------------------------  - Add repository and dependency to your project  ``` <repositories>    <repository> 	<id>opensourceagility-releases</id> 	<url>http://repo.opensourceagility.com/releases</url>    </repository> </repositories> ``` ```   	<dependency> 			<groupId>org.socialsignin</groupId> 			<artifactId>spring-social-security</artifactId> 			<version>1.0.2.RELEASE</version> 	</dependency> ``` - Component-scan for spring-social-security components in both your application context and in your mvc context  ``` 	<context:component-scan base-package="org.socialsignin.springsocial.security" /> ``` - Configure your spring security setup with a SpringSocialSecurityAuthenticationFilter in place of a form-login filter  ```  <http auto-config="false"      	<custom-filter position="FORM_LOGIN_FILTER" ref="springSocialSecurityAuthenticationFilter" /> ``` - Create a page in your webapp ( <a href="https://github.com/socialsignin/spring-social-security-demo/blob/master/src/main/webapp/oauthlogin.jsp">example</a> ) which contains all the socialsignin buttons for login and which submits to spring-social's    ProviderSignInController ( default urls are "/signup/[providerid]" ).  Create an entry point in your security configuration   for this page and set as the entry-point-ref on your security config.   ``` <bean id="springSocialSecurityEntryPoint"  class="org.springframework.security.web.authentication.LoginUrlAuthenticationEntryPoint">      <property name="loginFormUrl" value="/sociallogin"/> </bean> ```  ```  <http auto-config="false" entry-point-ref="springSocialSecurityEntryPoint"      	<custom-filter position="FORM_LOGIN_FILTER" ref="springSocialSecurityAuthenticationFilter" /> ```  Configuring your application for Sign-Up/Sign-In ------------------------------------------------  - Configure ProviderSignInController with "/authenticate" as the postSignInUrl (the default url for the SpringSocialSecurityAuthenticationFilter) and set its signUpUrl to be "/signup" (the default url of SpringSocialSecuritySignUpController)  ``` <bean class="org.springframework.social.connect.web.ProviderSignInController" >     	<property name="signUpUrl" value="/signup" />         <property name="postSignInUrl" value="/authenticate" />    </bean> ``` - Create a view in your webapp handles the choice of username by a user - this view will be served by SpringSocialSecuritySignUpController under default url of "/signup" and will need to post username back to this "/signup" url ( <a href="https://github.com/socialsignin/spring-social-security-demo/blob/master/src/main/webapp/WEB-INF/signUpForm.jsp">example</a>  - Set the following environment properties in your application  ``` socialsignin.signUpView=(name of your choose username view) socialsignin.defaultAuthenticationSuccessUrl=(url to send users after login) ```  - Optionally, configure your UsersConnectionRepository with SpringSocialSecurityConnectionSignUp to allow user local account    and username selection to happen implicitly where possible, based on connection details from 3rd party provider  ``` <bean id="usersConnectionRepository" 		class="org.springframework.social.connect.jdbc.JdbcUsersConnectionRepository"> 		<constructor-arg ref="dataSource" /> 		<constructor-arg ref="connectionFactoryRegistry" /> 		<constructor-arg ref="textEncryptor" /> 		<property name="connectionSignUp" ref="springSocialSecurityConnectionSignUp" />  </bean> ```    Enabling futher connection options for logged-in users ------------------------------------------------------  - Spring Social's ConnectController allow users who have logged in with one provider to connect with an another 3rd-party provider. spring-social-security peforms two functions to support this use-case with ConnectController through the use of ConnectInterceptors.  These interceptors  * Ensure that no other local user has connected using this provider account previously, as we use 3rd party connection as a means of uniquely identifying a user.  * Amend the user's authorisation so they are granted provider-specific roles according to the set of providers they have connected with.  - To enable this functionality  * Create a subclass of SpringSocialSecurityConnectInterceptor for each provider you wish your users to be able to connect with once they are logged in.  ``` public class TwitterConnectInterceptor extends 		SpringSocialSecurityConnectInterceptor<Twitter> {  } ```  - Register these connect interceptors with ConnectController  Protecting resources using Spring Social Security -------------------------------------------------  - To protect resources in your application, simply add intercept-urls to your security config as normal  ``` 		<intercept-url pattern="/protected/*" access="hasRole('ROLE_USER')" /> ```  - If you wish to take advantage of the provider-specific roles that are granted to users of a spring-social-security app, you can protect urls with rules such as   ``` 		<intercept-url pattern="/protected/twitter" access="hasRole('ROLE_USER_TWITTER')" /> ```  - To enable provider-specific access denied handling, add SpringSocialSecurityAccessDeniedHandler to your security setup  ```         <access-denied-handler ref="springSocialSecurityAccessDeniedHandler"/> ``` This handler will attempt to determine a provider which the user needs to connect with to be granted access to provider-protected resources, and if this can be determined, the user with be directed to the spring-social provider-specific connection view.  To set a default access denied url in case this can't be  determined, set the following property in your application.  ``` socialsignin.defaultAccessDeniedUrl= ```
thargor6/JWildfire	# JWildfire - an image and animation processor written in Java  ## Introduction  JWildire is the spiritual successor of the award winning special effect program Wildfire\7PPC for the Amiga –  but this time implemented in Java and with a more sophisticated user interface and more cool effects :-)  ## Key Features  - stunning 3D effects (such as wave3D, water, twirl3d, ...)  - 3D effect superimposition (e. g. wave3D interference)  - powerful Fractal flame generator  - image generators (perlin noise, cloud generator, plasma, ...)  - many "common" image processing effects in 2D (such as twirl, erode, convolve, ...)  - simple but powerful user interface which allows it to animate nearly any parameter by just a few mouse clicks  - Sunflow integration     ## Public builds Currently all public builds are available at http://www.andreas-maschke.com.   ## Contribute Feel free to contact me at thargor6@googlemail.com  ## License Copyright (C) 1995-2011 Andreas Maschke  This is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser  General Public License as published by the Free Software Foundation; either version 2.1 of the  License, or (at your option) any later version.   This software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without  even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU  Lesser General Public License for more details.  You should have received a copy of the GNU Lesser General Public License along with this software;  if not, write to the Free Software Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA, or see the FSF site: http://www.fsf.org.
laaglu/lib-gwt-svg	# Introduction lib-gwt-svg is a library to add SVG graphics to GWT applications.  It offers the following features:  * GWT exposition of 100% of the JavaScript APIs defined in the [SVG 1.1 W3C recommendation (August 11, 2011)](http://www.w3.org/TR/SVG/ "Scalable Vector Graphics (SVG) 1.1 (Second Edition)"). * Many additional helper methods to improve development productivity. * Tight integration with GWT, including: SVG resources (with XML validation), SVG in UiBinder definitions, integration with GWT event system, SVG Widgets (images, push and toggle buttons). * Tight integration with Java, including: iteratable collections, subclassing of SVG elements, complete integration of W3C documentation in the javadocs. * Open source [LGPLv3](http://www.gnu.org/licenses/lgpl.html "LGPLv3 homepage") license. * Based on modern tools: maven, sonatype, git, jenkins  # Demos, documentation, technical articles and other resources  Most of the documentation and support for the project is hosted by the [vectomatic](http://www.vectomatic.org/libs/lib-gwt-svg) website. This includes:  * [Release notes](http://www.vectomatic.org/libs/lib-gwt-svg/release-notes) * [Javadoc](http://www.vectomatic.org/mvn-sites/lib-gwt-svg/apidocs/index.html) * [Source x-ref](http://www.vectomatic.org/mvn-sites/lib-gwt-svg/xref/index.html) * [Maven site](http://www.vectomatic.org/mvn-sites/lib-gwt-svg/) * [Technical articles](http://www.vectomatic.org/category/lib-gwt-svg)  A few samples of applications developed with this library can be found here (all of them with full source code):  * [Basic samples](http://www.vectomatic.org/libs/lib-gwt-svg/samples) * [SVG editor](http://www.vectomatic.org/apps/svgreal) * [Basic usage](http://www.vectomatic.org/games/lib-gwt-svg-chess) * [Educational games](http://www.vectomatic.org/games/lib-gwt-svg-edu)  The library can obtained from:  * The [Maven Central Repository](http://search.maven.org/#search|ga|1|a%3A%22lib-gwt-svg%22) * The [vectomatic](http://www.vectomatic.org/mvn/org/vectomatic/lib-gwt-svg) local maven repository  Discussion group:   * The [lib-gwt-svg Google group](https://groups.google.com/group/lib-gwt-svg) answers questions about lib-gwt-svg usage. I (or somebody else) will do my best to answer it as quickly as possible.  Development:   * The main code repository is now hosted on [github](https://github.com/laaglu/lib-gwt-svg). The easiest way to contribute is to send pull requests there, or to provide patches to me by [email](mailto:laaglu@gmail.com)  # Compatibility Lib-gwt-svg requires GWT 2.x. Depending on the version you GWT you are using, you must choose a version of lib-gwt-svg which matches it, as described in the following table: <table border="0"> <tbody> <tr> <th>GWT version</th> <th>lib-gwt-svg version</th> </tr> <tr> <td>2.0.3</td> <td>0.4.6</td> </tr> <tr> <td>2.0.4</td> <td>0.4.7</td> </tr> <tr> <td>2.1.0M1</td> <td>0.4.9</td> </tr> <tr> <td>2.1.0 / 2.1.1</td> <td>0.5.0</td> </tr> <tr> <td>2.2.0</td> <td>0.5.1</td> </tr> <tr> <td>2.3.0</td> <td>0.5.2</td> </tr> <tr> <td>2.4.0</td> <td>0.5.5, 0.5.4 or 0.5.3</td> </tr> <tr> <td>2.5.0</td> <td>0.5.7</td> </tr> <tr> <td>2.5.1</td> <td>0.5.8</td> </tr> <tr> <td>2.6.x</td> <td>0.5.9</td> </tr> <tr> <td>2.7.0</td> <td>0.5.10, 0.5.11 (for Chrome 48+)</td> </tr> <tr> <td>2.8.0</td> <td>0.5.12</td> </tr> </tbody> </table>  lib-gwt-svg has been tested on the following SVG-enabled web browsers: <table border="0"> <tbody> <tr> <th>Browser</th> <th>Version</th> </tr> <tr> <td>Firefox</td> <td>3.x</td> </tr> <tr> <td>Opera</td> <td>10.x or greater</td> </tr> <tr> <td>Chrome</td> <td>5.x or greater</td> </tr> <tr> <td>Safari</td> <td>5.x</td> </tr> <tr> <td>IE</td> <td>9.x (NB: on IE, xpath support is provided by integrating the excellent, <a href="http://creativecommons.org/licenses/by-sa/2.0/" title="Creative Commons Licence">Creative-Commons</a>-licenced  <a href="http://mcc.id.au/xpathjs" title="xpath.js home">xpath.js</a> library, by Cameron McCormack)</td> </tr> <tr> <td>Firefox for Android</td> <td>10.x (NB: firefox supports touch events, but only single touch at the moment. Support for true multitouch is planned for 11.x)</td> </tr> <tr> <td>Android browser</td> <td>Android 3.2 (NB: Android does not implement touch events for SVG)</td> </tr> </tbody> </table>
kakaochatfriend/KakaoChatFriendAPI	## Kakao Chat Friend API  ## 설명 카카오 채팅 친구 API 연동을 위한 구현체 모음입니다. 각 언어 별로 작성되어 있으며 자세한 사용법은 언어별 디렉토리의 Readme 또는 예제를 참고해주세요.  첨부된 예제들은 echo봇들입니다.  ## emulator  서버에 접속해서 테스트 하기전에 로컬환경에서 일반적인 기능들을 테스트 하기 위한 에뮬레이터 입니다.  ## 문의 사항 [kakaolink](mailto://kakaolink@kakao.com)로 문의주세요
ymnk/jsch-agent-proxy	# jsch-agent-proxy a proxy to ssh-agent and Pageant in Java.  ## Description **jsch-agent-proxy** is a proxy program to [OpenSSH](http://www.openssh.com/)'s [ssh-agent](http://en.wikipedia.org/wiki/Ssh-agent) and [Pageant](http://en.wikipedia.org/wiki/PuTTY#Applications) included [Putty](http://www.chiark.greenend.org.uk/~sgtatham/putty/).  It will be easily integrated into [JSch](http://www.jcraft.com/jsch/), and users will be allowed to use those programs in authentications. This software has been developed for JSch, but it will be easily applicable to other ssh2 implementations in Java. This software is licensed under [BSD style license](https://github.com/ymnk/jsch-agent-proxy/blob/master/LICENSE.txt).   ## Build from Source     $ git clone git://github.com/ymnk/jsch-agent-proxy.git     $ cd jsch-agent-proxy     $ mvn package     $ mvn install  ## Examples + [UsingPageant.java](https://github.com/ymnk/jsch-agent-proxy/blob/master/examples/src/main/java/com/jcraft/jsch/agentproxy/examples/UsingPageant.java)       This sample demonstrates how to get accesses to Pageant.    		$ cd examples 		$ cd compile 		$ mvn exec:java \ 		  -Dexec.mainClass="com.jcraft.jsch.agentproxy.examples.UsingPageant"  + [UsingSSHAgent.java](https://github.com/ymnk/jsch-agent-proxy/blob/master/examples/src/main/java/com/jcraft/jsch/agentproxy/examples/UsingSSHAgent.java)       This sample demonstrates how to get accesses to ssh-agent.    		$ cd examples 		$ mvn compile 		$ mvn exec:java \ 		  -Dexec.mainClass="com.jcraft.jsch.agentproxy.examples.UsingSSHAgent"  + [JSchWithAgentProxy.java](https://github.com/ymnk/jsch-agent-proxy/blob/master/examples/src/main/java/com/jcraft/jsch/agentproxy/examples/JSchWithAgentProxy.java)       This sample demonstrates how to integrate jsch-agent-proxy into JSch.    		$ cd examples 		$ mvn compile 		$ mvn exec:java \ 		  -Dexec.mainClass="com.jcraft.jsch.agentproxy.examples.JSchWithAgentProxy" \ 		  -Dexec.args="foo@bar.com"  + [SshjWithAgentProxy.java](https://github.com/ymnk/jsch-agent-proxy/blob/master/examples/src/main/java/com/jcraft/jsch/agentproxy/examples/SshjWithAgentProxy.java)       This sample demonstrates how to integrate jsch-agent-proxy into sshj.    		$ cd examples 		$ mvn compile 		$ mvn exec:java \ 		  -Dexec.mainClass="com.jcraft.jsch.agentproxy.examples.SshjWithAgentProxy" \ 		  -Dexec.args="foo@bar.com"  + [TrileadWithAgentProxy.java](https://github.com/ymnk/jsch-agent-proxy/blob/master/examples/src/main/java/com/jcraft/jsch/agentproxy/examples/TrileadWithAgentProxy.java)       This sample demonstrates how to integrate jsch-agent-proxy into Trilead SSH2 (SVNKit fork).    		$ cd examples 		$ mvn compile 		$ mvn exec:java \ 		  -Dexec.mainClass="com.jcraft.jsch.agentproxy.examples.TrileadWithAgentProxy" \ 		  -Dexec.args="foo@bar.com date"  ## Dependencies To work as a proxy to ssh-agent and Pageant, the current implementation depends on the following software,   + JNA: https://github.com/twall/jna licensed under the [GNU LGPL](https://github.com/twall/jna/blob/master/LICENSE) and the [Apache License 2.0](http://code.google.com/p/junixsocket/source/browse/trunk/junixsocket/LICENSE.txt) + junixsocket: http://code.google.com/p/junixsocket/ licensed under the [Apache License 2.0](http://code.google.com/p/junixsocket/source/browse/trunk/junixsocket/LICENSE.txt) + OpenBSD's netcat: http://www.openbsd.org/cgi-bin/cvsweb/src/usr.bin/nc/  As for connections to ssh-agent, unix domain sockets must be handled, and the current implementation has been using JNA or junixsocket for that purpose.  Refer to following classes,   + [com.jcraft.jsch.agentproxy.usocket.JNAUSocketFactory](https://github.com/ymnk/jsch-agent-proxy/blob/master/jsch-agent-proxy-usocket-jna/src/main/java/com/jcraft/jsch/agentproxy/usocket/JNAUSocketFactory.java) + [com.jcraft.jsch.agentproxy.usocket.JUnixDomainSocketFactory](https://github.com/ymnk/jsch-agent-proxy/blob/master/jsch-agent-proxy-usocket-junixsocket/src/main/java/com/jcraft/jsch/agentproxy/usocket/JUnixDomainSocketFactory.java) + [com.jcraft.jsch.agentproxy.usocket.NCUSocketFactory](https://github.com/ymnk/jsch-agent-proxy/blob/master/jsch-agent-proxy-usocket-nc/src/main/java/com/jcraft/jsch/agentproxy/usocket/NCUSocketFactory.java)  NCUSocketFactory expects the external command nc(OpenBSD's netcat), but you don't have to install other third party software.   As for connections to Pageant, win32 APIs must be handled, and JNA has been used in the current implementation for that purpose.  Refer to the following class,  + [com.jcraft.jsch.agentproxy.connector.PageantConnector](https://github.com/ymnk/jsch-agent-proxy/blob/master/jsch-agent-proxy-pageant/src/main/java/com/jcraft/jsch/agentproxy/connector/PageantConnector.java)   If you want to be free from JNA and junixsocket, implement following interfaces without them,  + [com.jcraft.jsch.agentproxy.Connector](https://github.com/ymnk/jsch-agent-proxy/blob/master/jsch-agent-proxy-core/src/main/java/com/jcraft/jsch/agentproxy/Connector.java) + [com.jcraft.jsch.agentproxy.USocketFactory](https://github.com/ymnk/jsch-agent-proxy/blob/master/jsch-agent-proxy-core/src/main/java/com/jcraft/jsch/agentproxy/USocketFactory.java)
igor-petruk/protobuf-maven-plugin	[![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.igor-petruk.protobuf/protobuf-maven-plugin/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.github.igor-petruk.protobuf/protobuf-maven-plugin)  Hello.  There are a couple of plugins out there to compile Google Protobuf files, but most of them end up giving NPEs, so you have to fallback to Exec Plugin.  That's why I wanted to create a quite simple plugin that just does the job. Plus the source code is very small and simple to fix any possible issue.
joshlong/spring-advanced-marhshallers-and-service-exporters	the spring framework has long supported a consistant raft of RPC mechanisms with its Service Exporters. As of Spring 3.0, it's also provided a nice SPI to support arbitrary serializers (marshallers, or, more properly, HttpMessageConverters) in the REST support provided by the framework.  Both of these mechanisms are powerful, and expose flexible SPIs that can be used to teach Spring new tricks.  This is particularly valuable in the world of serializers and RPC mechanisms, which tend to move much quicker than individual Spring framework releases. For example, Cassandra (itself only a 0.7 project!) has already had a full integration with Thrift and has now moved on to support Avro. Ex-Googler Dhanji Prasana called (http://rethrick.com/#waving-goodbye) Google's Protocol Buffers (which Google's used over a few iterations for years) an "ancient, creaking dinosaurs" compared to MessagePack, JSON, and Hadoop.   So, understanding that the space is too voilatile to try to track in Spring proper, I've started building these implementations.  todo:  the support defined here should work for:   -- service exporters (where possible)   -- HttpMessageConverters (REST serialization)   -- JMS MessageConverters   -- AMQP MessageConverters   serialization technologies to support include:    -- Avro   -- Thrift   -- MessagePack   -- Google Protocol Buffers    RPC technologies include    -- JBoss Remoting   -- MessagePack RPC   -- Thrift RPC    -- Avro    -- Netty?   Approach New Note 5  Build Spring OBM -- object-to-binary marshallers. this would let you then build ONE JMS MessageConverter and ONE AMQP MessageConverter and ONE HttpMessageConverter and then delegate to the object-to-binary marshaller and unmarshaller, a la Spring OXM
seam/mail	Building --------  * Run mvn clean install * To create the zip distribution, run mvn clean install -Pdistribution - the resulting zip is located in dist/target/seam-mail-${version}.zip  * The readme.txt placed in the distribution is not this one, see dist/src/main/assembly/readme.txt  Testing -------- run mvn clean verify
ExampleDriven/cxf-example	CXF Example ============  This is the source code of the blog post   http://exampledriven.wordpress.com/2012/10/03/cxf-example-soapjax-ws-restjax-rs-spring/  This project demonstrates how to build a simple CXF webservice using SOAP (JAX-WS) or REST (JAX-RS). Additionally it is using   - CXF proxy clients for SOAP and REST  - Spring Security  - Spring bean validation, JSR 303  The project can be started up in two ways :  - mvn jetty:run then open http://localhost:8080 - mvn tomcat:run then open http://localhost:9999/cxfexample/
enix12enix/sikuli-remote-control	# Sikuli Remote Control  SikuliRC server and java wrapped client API Invoke sikuli function remotely as SeleniumRC, and you can use it with Selenium or webdriver for your web automation.  ## System Requirements * Sikuli-X-1.0rc3 (r905) * Java6  ### Downloads * Sikuli server dowland page. http://sourceforge.net/projects/sikulircserver/files/  ## Usage  Run sikuli server on mac:      java -jar server-mac-0.0.1-SNAPSHOT-jar-with-dependencies.jar  Run sikuli server on windows:          java -jar server-windows-0.0.1-SNAPSHOT-jar-with-dependencies.jar  Add java client api jar to your class path:      RemoteScreen rs = new RemoteScreen("localhost");     rs.setMinSimilarity(0.9);     rs.click("D://test.png");     rs.click("http://example.org/test.png");     rs.find("D://test.png");     rs.find("http://example.org/test.png");     rs.doubleClick("D://test.png");     rs.doubleClick("http://example.org/test.png");     rs.waitUntil("D://test.png", 5000);  There is a ruby wrapped API here: http://github.com/enix12enix/sikulirc  ## How to build it  You need maven2 to build it. Not sure maven3 works properly, but you can try.  Sikuli-script.jar is not in official maven repository, so we need install it ourself.  Get sikuli-script.jar of windows and mac, rename them 'sikuli-script-mac.jar' and 'sikuli-script-windows.jar'.  Also you can download them from http://sourceforge.net/projects/sikulircserver/files/sikuli-script-1.0rc3%28r905%29/  Then install sikuli-script.jar      mvn install:install-file -Dfile=sikuli-script-mac.jar -DgroupId=org.sikuli.script -DartifactId=sikuli-script-mac -Dversion="X 1.0 rc3" -Dpackaging=jar       mvn install:install-file -Dfile=sikuli-script-windows.jar -DgroupId=org.sikuli.script -DartifactId=sikuli-script-windows -Dversion="X 1.0 rc3" -Dpackaging=jar  To build project      mvn install  ## Resources  ||| |-----------------------------------:|:--------------------------| |                              **Sikuli Home Page**: | http://sikuli.org/ | |     **Sikuli Remote Control Github Project Page**: | http://github.com/enix12enix/sikuli-remote-control | |  **Sikuli ReMote Control Ruby Wrapped Client API Github Project Page**: | http://github.com/enix12enix/sikulirc |
unchiujar/Umbra	[![Build Status](https://travis-ci.org/unchiujar/Umbra.svg)](https://travis-ci.org/unchiujar/Umbra)   Icons created by modifying http://game-icons.net/lorc/originals/treasure-map.html
wuman/orientdb-android	ORIENTDB-ANDROID ================  ![Feature Image](https://github.com/wuman/orientdb-android/raw/master/doc/images/orientdb-android-logo.png)  OrientDB-Android is a port/fork of [OrientDB](http://www.orientdb.org/) for the Android platform.  Currently the port removes all server implementations as they use the `javax.management.*` package, which is not part of the Android classpath. The port is designed to be used in conjunction with the  [blueprints-android](https://github.com/wuman/blueprints-android) library.  Currently only the following modules are ported:  * orient-android-commons * orientdb-android-core * orientdb-android-enterprise * orientdb-android-client * orientdb-android-nativeos * orientdb-android-object * orientdb-android-tools * orientdb-android-distribution  Note that orientdb-android is still under development and  only  `orient-android-commons` and `orientdb-android-core` have been used and tested  in the wild, so use it at your own risk. We welcome contributions and feedbacks.  The current release of orientdb-android is 1.1.0.x, which is in line with version 1.1.0 of the upstream OrientDB.    Including in Your Project -------------------------  There are two ways to include orientdb-android in your projects:  1. You can download the released jar file in the [Downloads section](https://github.com/wuman/orientdb-android/downloads). 2. If you use Maven to build your project you can simply add a dependency to     the desired component of the library.          <dependency>             <groupId>com.wu-man</groupId>             <artifactId>orient[db]-android-*</artifactId>             <version>1.1.0.1</version>         </dependency>   Introduction to OrientDB ------------------------  [OrientDB](http://code.google.com/p/orient/) is an open source  [NoSQL](http://en.wikipedia.org/wiki/NoSQL) DBMS with both the features of  Document and Graph DBMS. It's written in Java and it's amazing fast: it can store  up to 150,000 records per second on common hardware.  OrientDB is in its core a [graph database](http://en.wikipedia.org/wiki/Graph_database), although it also offers the interface of a document database. When used as a  document database, the relationships are managed as in graph databases with  direct connections among records. You can traverse entire or part of trees and  graphs of records in few milliseconds.  To see how OrientDB compares with other databases, take a look at  * [GraphDB comparison](http://code.google.com/p/orient/wiki/GraphDBComparison) * [DocumentDB comparison](http://code.google.com/p/orient/wiki/DocumentDBComparison)   Contribute ----------  If you would like to contribute code you can do so through GitHub by forking  the repository and sending a pull request.   Developed By ------------  * Android porting contributor     * David Wu - <david@wu-man.com> - [http://blog.wu-man.com](http://blog.wu-man.com) * Original contributors to OrientDB     * Luca Garulli - <l.garulli@orientechnologies.com> - http://www.orientechnologies.com     * Luca Molino - <molino.luca@gmail.com>     * Andrey Lomakin - <lomakin.andrey@gmail.com>    License -------      Copyright 2012 David Wu     Copyright 2010-2012 Luca Garulli (l.garulli--at--orientechnologies.com)      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
kijiproject/kiji-bento	(c) Copyright 2013 WibiData, Inc.  Kiji BentoBox ${project.version} ================================  The Kiji BentoBox is an SDK for developing Big Data Applications with the Kiji framework. It includes a complete set of Kiji framework modules, with compatible versions of each.  This kiji-bento release (${project.version}) includes:  * `kiji-schema` ${kiji-schema.version}: Included at the top-level of the   distribution, kiji-schema provides a simple Java API and command-line tools   for storing and managing typed data in HBase. * `kiji-mapreduce` ${kiji-mapreduce.version}: Included in the `lib` directory,   kiji-mapreduce provides a simple Java API and command-line tools for using   Hadoop MapReduce to import and process data in Kiji tables. * `kiji-mapreduce-lib` ${kiji-mapreduce-lib.version}: Included in the `lib`   directory, kiji-mapreduce-lib is a Java library of utilities for writing   MapReduce jobs on Kiji tables, as well as ready to use producers, gatherers,   and importers. * `bento-cluster` ${bento-cluster.version}: Located in the `cluster`   directory, bento-cluster allows users to run HDFS, MapReduce, and HBase   clusters easily on the local machine. * `kiji-schema-shell` ${kiji-schema-shell.version}: Included in the   `schema-shell` directory, kiji-schema-shell provides a layout definition   language for use with `kiji-schema`. * `kiji-hive-adapter` ${kiji-hive-adapter.version}: Included in the   `hive-adapter` directory, kiji-hive-adapter provides a SerDe for   Hive to use Kiji tables as external Hive tables. * `kiji-express` ${kiji-express.version}: Included in the `express`   directory, kiji-express provides a Scala DSL for analyzing and modeling   data stored in Kiji. * `kiji-express-tools` ${kiji-express-tools.version}: included in the `express`   directory, kiji-express-tools provides a REPL and related tools for Express. * `kiji-modeling` ${kiji-modeling.version}: Included in the `modeling`   directory, kiji-modeling provides a formalization for training, applying,   and evaluation machine learning models built on top of kiji-express. * `kiji-scoring` ${kiji-scoring.version}: Included in the `scoring` directory,   kiji-scoring is a library and server that supports the real-time per-row   calculations on kiji tables. * `kiji-model-repository` {kiji-model-repository.version}: Included in the   `model-repo` directory, the kiji-model-repository is a library which permits   storage of trained kiji-modeling in a maven repository, indexed by a kiji   table. kiji-scoring can use this models stored in this repository to for its   scoring. * `kiji-phonebook` ${kiji-phonebook.version}: Included in the   `examples/phonebook` directory, kiji-phonebook is an example standalone   application (with source code) that stores, processes, and retrieves data   from a Kiji instance. * `kiji-music` ${kiji-music.version}: Included in the   `examples/music` directory, kiji-music is an example of loading the listening   history of users of a music service into a Kiji table, and then generating new   music recommendations. * `kiji-express-music` ${kiji-express-music.version}: Included in the   `examples/express-music` directory, kiji-express-music is a kiji-express   implementation of the kiji-music example. * `kiji-express-examples` ${kiji-express-examples.version}: Included in the   `examples/express-examples` directory, kiji-express-examples provides example   usage of kiji-express processing a newsgroups dataset. * API documentation is made available in the `docs` directory.  Installation ------------  ### Untar kiji-bento Untar your kiji-bento distribution into a location convenient for you. We suggest your home directory. In the future we'll call the path to your kiji-bento distribution `$KIJI_HOME`.  ### Configure your system to use kiji-bento. kiji-bento includes a script that will configure your environment to use the HDFS, MapReduce and HBase clusters managed by the included bento-cluster distribution, as well as kiji and kiji-schema-shell. To configure your environment, run:  > `source $KIJI_HOME/bin/kiji-env.sh`  Add this line to your `~/.bashrc` file to ensure your environment is always configured for kiji-bento.  `kiji-env.sh` configures your environment to use the Hadoop ecosystem distributions included with bento-cluster, as well as the clusters we'll soon use bento-cluster to start. It also adds the `hadoop`, `hbase`, `bento`, `kiji` and `kiji-schema-shell` scripts to your `$PATH`, making them available as top-level commands.  ### Start bento-cluster. Now that your system is configured, you can use the `bento` script to start your local HDFS, MapReduce, and HBase clusters.  > `bento start`  ### Use kiji With the clusters started, you can run Hadoop ecosystem tools like kiji. For example, to list your home directory in HDFS, run:  > `hadoop fs -ls`  Or, to install the default kiji instance in HBase, run:  > `kiji install`  The `hadoop`, `hbase`, `kiji` and `kiji-schema-shell` scripts are available for use. You can also use other Hadoop ecosystem tools like hive or pig and they will use the local clusters managed by bento-cluster when run in an environment configured with `kiji-env.sh`.  ### Stop bento-cluster. When you're ready to call it a day, you can stop bento-cluster by running:  > `bento stop`  This will shutdown the HBase, MapReduce, and HDFS clusters managed by bento-cluster.  Quickstart ----------  To continue using Kiji, consult the online [quickstart guide](http://www.kiji.org/getstarted/#Quick_Start_Guide).  Upgrade Server Check-in ------------------------ Kiji BentoBox will periodically check in with an upgrade server to determine if there are any upgrades available for your distribution. If upgrades are available, the `kiji` script that comes with BentoBox will periodically remind you (once a day) of available upgrades. BentoBox sends anonymized information when checking in with the upgrade server.  To disable checking in with the upgrade server, write a file named `.disable_kiji_checkin` to the `$HOME/.kiji`. Using the `touch` command is sufficient. For example:  > `touch $HOME/.kiji/.disable_kiji_checkin`  will disable check in with the upgrade server.
manolo/gwtupload	GWTUpload & JSUpload  GWTUpload is a library for uploading files to web servers, showing a progress bar with real information about the process (file size, bytes transferred, etc). It uses ajax requests to ask the web server for the upload progress. It has two components written in java, the server side with servlet and utility classes, and the client side that is compiled into javascript using gwt.  JSUpload is the same client library but compiled and exported into javascript, so non java developers can use it directly in web pages. I've written an article describing the technique used to do this. JSUpload provides a server program coded in perl that can be installed in any web server as a cgi-bin script.  GWTUpload-GAE is a library including a special servlet to handle uploads in Google Application Engine servers (GAE).   BUILDING:  - Checkout the code from either github or googlecode:   git clone https://github.com/manolo/gwtupload.git   git clone https://code.google.com/p/gwtupload/  - Assuming you have already installed maven in your system just run:   mvn package  RUNNING EXAMPLES - To compile the examples and run them with maven jetty plugin   mvn jetty:run-exploded  - To run examples in GWT dev mode run:   mvn gwt:run  - You can deploy the generated gwtupload-version.war in a servlet container or run it with:   java -jar gwtupload-version.war   ECLIPSE:  - You can use m2eclipse (sonatype) to import the project - Or you can generate an eclipse project running:   mvn eclipse:eclipse  HELP:    1.- Read the documentation: http://code.google.com/p/gwtupload/w/list   2.- Check if your issue is listed in: http://code.google.com/p/gwtupload/issues/list?can=1   3.- Read the mailing list: http://groups.google.com/group/gwtupload   4.- Ask to the list: gwtupload@googlegroups.com   5.- Open an issue   - Manuel Carrasco Moñino
twitter/bookkeeper	Build instructions for BookKeeper  ------------------------------------------------------------------------------- Requirements:  * Unix System * JDK 1.6 * Maven 3.0 * Autotools (if compiling native hedwig client) * Internet connection for first build (to fetch all dependencies)  ------------------------------------------------------------------------------- The BookKeeper project contains:   - bookkeeper-server     (BookKeeper server and client)  - bookkeeper-benchmark  (Benchmark suite for testing BookKeeper performance)  - hedwig-protocol       (Hedwig network protocol)  - hedwig-client         (Hedwig client library)  - hedwig-server         (Hedwig server)  BookKeeper is a system to reliably log streams of records. It is designed to  store  write ahead logs, such as those found in database or database like  applications.  Hedwig is a publish-subscribe system designed to carry large amounts of data  across the internet in a guaranteed-delivery fashion from those who produce  it (publishers) to those who are interested in it (subscribers).  -------------------------------------------------------------------------------- How do I build?   BookKeeper uses maven as its build system. To build, run "mvn package" from the   top-level directory, or from within any of the submodules.   Useful maven commands are:   * Clean                     : mvn clean  * Compile                   : mvn compile  * Run tests                 : mvn test   * Create JAR                : mvn package  * Run findbugs              : mvn compile findbugs:findbugs  * Install JAR in M2 cache   : mvn install  * Deploy JAR to Maven repo  : mvn deploy  * Run Rat                   : mvn apache-rat:check  * Build javadocs            : mvn compile javadoc:aggregate  * Build distribution        : mvn package assembly:single   Tests options:   * Use -DskipTests to skip tests when running the following Maven goals:     'package',  'install', 'deploy' or 'verify'  * -Dtest=<TESTCLASSNAME>,<TESTCLASSNAME#METHODNAME>,....  * -Dtest.exclude=<TESTCLASSNAME>  * -Dtest.exclude.pattern=**/<TESTCLASSNAME1>.java,**/<TESTCLASSNAME2>.java  NOTE:  BookKeeper uses maven-shade-plugin to build shade packages for old versions for backward compatibility testing. This shade plugin is only able to run at 'package' phase. So there are two ways to run bookkeeper tests:   * Run 'mvn clean package' under bookkeeper root directory  * Run 'mvn clean test' under bookkeeper/bookkeeper-server directory after you run    'mvn clean install -DskipTests' under bookkeeper directory  -------------------------------------------------------------------------------- How do I run the services?   Running a Hedwig service requires a running BookKeeper service, which in turn  requires a running ZooKeeper service (see http://zookeeper.apache.org). To   start a bookkeeper service quickly for testing, run:        $ bookkeeper-server/bin/bookkeeper localbookie 10   This will start a standalone, ZooKeeper instance and 10 BookKeeper bookies.  Note that this is only useful for testing. Data is not persisted between runs.   To start a real BookKeeper service, you must set up a ZooKeeper instance and  run start a bookie on several machines. Modify bookkeeper-server/conf/bk_server.conf  to point to your ZooKeeper instance. To start a bookie run:     $ bookkeeper-server/bin/bookkeeper bookie   Once you have at least 3 bookies runnings, you can start some Hedwig hubs. A   hub is a machines which is responsible for a set of topics in the pubsub   system. The service automatically distributes the topics among the hubs.   To start a hedwig hub:     $ hedwig-server/bin/hedwig server   You can get more help on using these commands by running:     $ bookkeeper-server/bin/bookkeeper help        and    $ hedwig-server/bin/hedwig help
appfuse/appfuse-demos	## AppFuse Demos Screencast and tutorial applications that show functionality in [AppFuse][]. For live demos of AppFuse, please see http://demo.appfuse.org.  To do the tutorials that create the applications in this project, please see http://appfuse.org/display/APF/Tutorials.  [AppFuse]: https://github.com/appfuse/appfuse
webjars/webjars-locator	webjars-locator ===============  This poorly named library is really just the RequireJS support that wraps the [webjars-locator-core](https://github.com/webjars/webjars-locator-core) project.  [Check out the JavaDoc](https://javadoccentral.herokuapp.com/org.webjars/webjars-locator/latest)  [![Build Status](https://travis-ci.org/webjars/webjars-locator.svg?branch=master)](https://travis-ci.org/webjars/webjars-locator)
jamesmorgan/ReloadablePropertiesAnnotation	## Reloadable Properties Annotation ##  A simple utilty which allows object fields to be set from properties files via a @ReloadableProperty annotation.  These properties also auto reload if the given properties file changes during runtime.  ### Example Annotation Usage ### <pre> 	@ReloadableProperty("dynamicProperty.longValue") 	private long primitiveWithDefaultValue = 55; 	 	@ReloadableProperty("dynamicProperty.substitutionValue") 	private String stringProperty; 	 	@ReloadableProperty("dynamicProperty.compoiteStringValue") 	private String compsiteStringProperty; </pre>  ### Example Properties File ### <pre> 	dynamicProperty.longValue=12345 	dynamicProperty.substitutionProperty=${dynamicProperty.substitutionValue} 	dynamicProperty.compoiteStringValue=Hello, ${dynamicProperty.baseStringValue}! </pre>  ### Example Spring XML Configuration ### * See [spring-reloadableProperties.xml](https://github.com/jamesemorgan/ReloadablePropertiesAnnotation/blob/master/src/main/resources/spring/spring-reloadableProperties.xml) for example configuration * All main components can be extended or replaced if required  ### How it Works  ### When Spring starts an Application Context an implementation of Springs [PropertySourcesPlaceholderConfigurer](http://static.springsource.org/spring/docs/3.1.x/javadoc-api/org/springframework/context/support/PropertySourcesPlaceholderConfigurer.html) is instantiated to perform additional logic when loading and setting values from a given set of properties files. (see: [ReadablePropertySourcesPlaceholderConfigurer](https://github.com/jamesemorgan/ReloadablePropertiesAnnotation/blob/master/src/main/java/com/morgan/design/properties/internal/ReadablePropertySourcesPlaceholderConfigurer.java))  During the instantiation phasae of an Application Context a new instance of [InstantiationAwareBeanPostProcessorAdapter](http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/beans/factory/config/InstantiationAwareBeanPostProcessorAdapter.html) is also created which allows post bean processing to occur.  Google Guava is used to implement a simple Publish & Subscribe (Pub-Sub) Pattern so that beans can be updated once created, i.e. a bean can subscribe to property change events. (see: [EventBus](http://code.google.com/p/guava-libraries/wiki/EventBusExplained))  EventBus was chosen as it is a very easy and simplistic way to implement loosely couple object structure. (see: [blog](http://codingjunkie.net/guava-eventbus/))  When each properties file resource is loaded a [PropertiesWatcher](https://github.com/jamesemorgan/ReloadablePropertiesAnnotation/blob/master/src/main/java/com/morgan/design/properties/internal/PropertiesWatcher.java) is started and attached to the given resource set, reporting on any [java.nio.file.StandardWatchEventKinds.ENTRY_MODIFY](http://docs.oracle.com/javase/7/docs/api/java/nio/file/StandardWatchEventKinds.html#ENTRY_MODIFY) events from the host operating system  When an ENTRY_MODIFY event is fired firstly the resource changed is checked for property value changes then any bean subscribing to changes to the modified property has the specified field value updated with the new property. Once the filed value is updated no other operations are performed on the object.  Each resource specified starts a new thread per parent directory i.e. two properties files in the same directory requires only one ResourceWatcher thread, three properties files in three different directories will start three threads.  ### Tests ### A set of integration and unit tests can be found in _src/test/java_ (tests) & _src/test/resources_ (test resources)  ### TODO (Unfinished) ### * Update test method names * Creation of any test utilities or helper classes  ### Why? ### * Useful for web applications which often need configuration changes but you don't always want to restart the application before new properties are used. * Can be used to define several layers of properties which can aid in defining multiple application configurations e.g sandbox/development/testing/production. * A pet project of mine I have been intending to implement for a while * A test of the new Java 7 WatchService API * Another dive in Spring & general investigation of Google Guava's EventBus * The project is aimed to be open to modification if required * Sample testing tools (CountDownLatch, Hamcrest-1.3, JMock-2.6.0-RC2)  ### Future Changes ### * Ability to use Spring Expression language to map properties files * Support for Java 7 Date and Time classes * Include the ability to define a database driven properties source not just properties files * Implement error recovery inside PropertiesWatcher.class, including better thread recovery * Ability to perform additional re-bind logic when a property is changed, i.e. if a class has an open DB connection which needs to be re-established using newly set properties. * Replace callback Properties EventHandler with Guava EventBus * Ability to configure usage via spring's @Configuration   ### Contributions ### * Thank you [normanatashbar](https://github.com/normanatashbar) for adding composite string replacement * Thank you [shiva2991](https://github.com/normanatashbar) for adding java.util.Date type conversion.  ### Supported Property Type Conversions Available ### * LocalDate.class * LocalTime.class * LocalDateTime.class * Period.class   * Spring Supported (3.1.2-RELEASE) * String.class * Date.class * boolean.class, Boolean.class * byte.class, Byte.class * char.class, Character.class * short.class, Short.class * int.class, Integer.class    * long.class,Long.class * float.class, Float.class * double.class, Double.class  ### Dependencies ###  #### Core #### * Java 7 SDK * Spring (3.2.5-RELEASE) * Google Guava  (14.0.1) * Joda Time Library (2.1) - [link](http://joda-time.sourceforge.net/)  #### Logging #### * logback (1.0.13) * slf4j (1.7.5)  #### Testing #### * juint (4.11) * jmock (2.6.0) * hamcrest-all (1.3) * spring-test (3.2.5-RELEASE)   [![Bitdeli Badge](https://d2weczhvl823v0.cloudfront.net/jamesmorgan/reloadablepropertiesannotation/trend.png)](https://bitdeli.com/free "Bitdeli Badge")
fcrepo4/fcrepo4	# Fedora 4  [![Build Status](https://travis-ci.org/fcrepo4/fcrepo4.png?branch=master)](https://travis-ci.org/fcrepo4/fcrepo4)  [JavaDocs](http://docs.fcrepo.org/) |  [Fedora Wiki](https://wiki.duraspace.org/display/FF) |  [Use cases](https://wiki.duraspace.org/display/FF/Use+Cases) | [REST API](https://wiki.duraspace.org/display/FEDORA4x/RESTful+HTTP+API) |  Fedora is a robust, modular, open source repository system for the management and dissemination of digital content. It is especially suited for digital libraries and archives, both for access and preservation. It is also used to provide specialized access to very large and complex digital collections of historic and cultural materials as well as scientific data. Fedora has a worldwide installed user base that includes academic and cultural heritage organizations, universities, research institutions, university libraries, national libraries, and government agencies. The Fedora community is supported by the stewardship of the [DuraSpace](http://www.duraspace.org) organization.  ## Technical goals: * Improved scalability and performance * More flexible storage options * Improved reporting and metrics * Improved durability  ## Downloads  The current web-deployable version of Fedora can be downloaded from the [Duraspace website](https://wiki.duraspace.org/display/FF/Downloads) or from [Github](https://github.com/fcrepo4/fcrepo4/releases). These artifacts can be deployed directly in a Jetty or Tomcat container as described in the guide to [deploying Fedora](https://wiki.duraspace.org/display/FEDORA4x/Deploying+Fedora+4+Complete+Guide).  ## Contributing  Contributions to the Fedora project are always welcome. These may take the form of testing the application, clarifying documentation or writing code.  Code contributions will take the form of pull requests to this repository. They also require a signed [contributor license agreement](https://wiki.duraspace.org/display/DSP/Contributor+License+Agreements) on file before a pull request can be merged. New developers may wish to review [this guide](https://wiki.duraspace.org/display/FF/Guide+for+New+Developers) as it explains both the process and standards for test coverage, style and documentation.  ## Getting help  There are two community mailing lists where you can post questions or raise topics for discussion. Everyone is welcome to subscribe and participate.  * https://groups.google.com/d/forum/fedora-community * https://groups.google.com/d/forum/fedora-tech  Many of the developers are available on the `#fcrepo` IRC channel, hosted by [freenode.net](http://webchat.freenode.net).  In addition, there are weekly [technical calls](https://wiki.duraspace.org/display/FF/Meetings) which anyone may join.  ## Building and running Fedora from source  System Requirements * Java 8 * Maven 3  ```bash $ git clone https://github.com/fcrepo4/fcrepo4.git $ cd fcrepo4 $ MAVEN_OPTS="-Xmx1024m -XX:MaxMetaspaceSize=1024m" mvn install ```  The compiled Fedora war file can be found in `./fcrepo-webapp/target`. This can be deployed directly to a servlet container as described in the [deployment guide](https://wiki.duraspace.org/display/FEDORA4x/Deploying+Fedora+4+Complete+Guide).  If deployed locally using a war file called `fcrepo.war`, the web application will typically be available at http://localhost:8080/fcrepo/rest.  There are two convenient methods for *testing* the Fedora application by launching it directly from the command line.  One option is to use the "one click" application, which comes with an embedded Jetty servlet. This can be started by either double-clicking on the jar file or by running the following command:      java -jar ./fcrepo-webapp/target/fcrepo-webapp-<version>-jetty-console.jar  An alternative is use the maven command: `mvn jetty:run`  ``` $ cd fcrepo-webapp $ MAVEN_OPTS="-Xmx512m" mvn jetty:run ```  For both of these methods, your Fedora repository will be available at: [http://localhost:8080/rest/](http://localhost:8080/rest/)  Note: You may need to set the $JAVA_HOME property, since Maven uses it to find the Java runtime to use, overriding your PATH. `mvn --version` will show which version of Java is being used by Maven, e.g.:  ```bash Java version: 1.8.0_31, vendor: Oracle Corporation Java home: /usr/local/java-1.8.0_31/jre ```  To set your $JAVA_HOME environment variable:  ```bash export JAVA_HOME=/path/to/java ```  If you have problems building fcrepo4 with the above settings, you may need to also pass options to the JaCoCo code coverage plugin:  ```bash $ MAVEN_OPTS="-Xmx1024m" mvn -Djacoco.agent.it.arg="-XX:MaxMetaspaceSize=1024m -Xmx1024m" -Djacoco.agent.ut.arg="-XX:MaxMetaspaceSize=1024m -Xmx1024m"  clean install ```
shilad/wikibrain	Read more at the [WikiBrain website](http://shilad.github.io/wikibrain/).  The WikiBrain Java library enables researchers and developers to incorporate state-of-the-art Wikipedia-based algorithms and technologies in a few lines of code.
jirutka/validator-collection	= Bean Validation / Collection Validators :source-language: java :name: validator-collection :version: 2.2.0 :artifact-id: {name} :group-id: cz.jirutka.validator :gh-name: jirutka/{name} :gh-branch: master :codacy-id: b77fcc2a16794c49a64ac0727ec274f7  ifdef::env-github[] image:https://travis-ci.org/{gh-name}.svg?branch={gh-branch}[Build Status, link="https://travis-ci.org/{gh-name}"] image:https://coveralls.io/repos/github/{gh-name}/badge.svg?branch={gh-branch}[Coverage Status, link="https://coveralls.io/github/{gh-name}"] image:https://api.codacy.com/project/badge/grade/{codacy-id}[Code quality, link="https://www.codacy.com/app/{gh-name}"] image:https://maven-badges.herokuapp.com/maven-central/{group-id}/{artifact-id}/badge.svg[Maven Central, link="https://maven-badges.herokuapp.com/maven-central/{group-id}/{artifact-id}"] endif::env-github[]   Neither http://beanvalidation.org/1.1/spec/[Bean Validation 1.1] (JSR 303/349) nor http://hibernate.org/validator/[Hibernate Validator], the reference _(and the only one…)_ implementation of it, provide simple way to validate a collection of basic types like String, Integer, Date… (i.e. validate each element of the collection).  This library allows you to easily create a “pseudo constraint” (typically named as `@EachX`) for _any_ validation constraint to annotate a collection of simple types, without writing an extra validator or unnecessary wrapper classes for every collection. `EachX` constraint is supported for all standard Bean Validation constraints and Hibernate specific constraints. For example:  [source] ---- @EachSize(min = 5, max = 255) Collection<String> values;  @EachFuture List<Date> dates;  @EachEmail Set<String> emails; ----   == How to create a custom constraint  Every `@EachX` pseudo constraint uses the same validator, link:src/main/java/cz/jirutka/validator/collection/CommonEachValidator.java[CommonEachValidator]. To create an `@EachAwesome` for your own `@Awesome` constraint, just copy & paste the annotation class (i.e. all the attributes and boilerplate meta annotations), replace `@Constraint` annotation with `@Constraint(validatedBy = CommonEachValidator.class)` and add the annotation `@EachConstraint(validateAs = Awesome.class)`. That’s all!  [source] ---- // common boilerplate @Documented @Retention(RUNTIME) @Target({METHOD, FIELD, ANNOTATION_TYPE}) // this is important! @EachConstraint(validateAs = Awesome.class) @Constraint(validatedBy = CommonEachValidator.class) public @interface EachAwesome {      // copy&paste all attributes from Awesome annotation here     String message() default "";     Class<?>[] groups() default {};     Class<? extends Payload>[] payload() default {};     String someAttribute(); } ----   === The old way  The previous versions (before 2.1.0) used a different approach to write `@EachX` annotations (see https://github.com/{gh-name}/tree/v2.0.2[here]). It is still supported for custom constraints, but all the built-in annotations has been already updated to the new style.  If you’re upgrading from an older version of Collection Validators, then you must update all built-in annotations to the new style. For example:  [source] @EachSize(@Size(min = 5, max = 255)) -> @EachSize(min = 5, max = 255)  You _should_ also update custom annotations. The old style is still supported, but may be deprecated in the future.   == Maven  Released versions are available in The Central Repository. Just add this artifact to your project:  [source, xml, subs="verbatim, attributes"] ---- <dependency>     <groupId>{group-id}</groupId>     <artifactId>{artifact-id}</artifactId>     <version>{version}</version> </dependency> ----  However if you want to use the last snapshot version, you have to add the JFrog OSS repository:  [source, xml] ---- <repository>     <id>jfrog-oss-snapshot-local</id>     <name>JFrog OSS repository for snapshots</name>     <url>https://oss.jfrog.org/oss-snapshot-local</url>     <snapshots>         <enabled>true</enabled>     </snapshots> </repository> ----   == Requirements  http://hibernate.org/validator/[Hibernate Validator] 4.3.1.Final and newer is supported, but 5.× is recommended.  Please note that on older versions some Hibernate specific constraints doesn’t exist, so their `@EachX` annotations will not work (e.g. `@EachEAN`, `@EachMod10Check`, …). It’s described in JavaDoc.   === Version detection  In order to support multiple versions of Hibernate Validator at a time, we attempt to determine what version is being used at runtime using the package metadata for the `HibernateValidator` class. This can sometimes fail, particularly if your project creates an “uber JAR.” If the version cannot be detected, then it fallbacks to ≥ 5.1.0.   == License  This project is licensed under http://opensource.org/licenses/MIT[MIT license].
kelsos/mbrc	MusicBee Remote - for Android [![Build Status](https://travis-ci.org/kelsos/mbrc.svg?branch=development)](https://travis-ci.org/kelsos/mbrc) ============================= About ------- MusicBee is a remote control application that is used to control [MusicBee](http://getmusicbee.com/) player using a network connected Android device. The application is freely available through [Google Play](https://play.google.com/store/apps/details?id=com.kelsos.mbrc) and also requires the associated [plugin](https://github.com/kelsos/mbrc-plugin) (dll) for MusicBee to function.  The application consumes an RESTlike HTTP API provided by the plugin. And uses a websocket connection for small push notifications.  MusicBee Remote was presented as part of my thesis on "*Android and application development for mobile devices*".  You can find help on how to use the application and more info on the dedicated [website](http://kelsos.net/musicbeeremote/) along with links to the binary version of the plugin.  You can also find information about the plugin and the remote to the dedicated topic in the [MusicBee forums](http://getmusicbee.com/forum/index.php?topic=7221.new;topicseen#new).  Building ------- Clone the repository and import on IntelliJ IDEA or Android Studio.  Credits ----------- ### Artwork  Many of the icons used are created by [Tasos Papazoglou Chalikias](https://github.com/sushiperv) and are licenced under the [Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License.](https://creativecommons.org/licenses/by-nc-nd/3.0/deed.en_US) some of them where taken by other projects and some awful icons during the earlier development where created by me. The original logo idea belongs to Jordan Georgiades. Any other icons introduced in version 0.9.7 and later was probably my work. Some design ideas are based on [mockups](https://groups.google.com/forum/#!topic/musicbee-remote/wgm029yfJnU) by Carlos Parga  ### Inspiration Initially a great source of inspiration for this project was the [Cyanogen Apollo player](https://github.com/CyanogenMod/android_packages_apps_Apollo) along with [Google Play Music for Android](https://play.google.com/store/apps/details?id=com.google.android.music). Nowadays I am looking closely the material design documentation.  ### Dependencies  *   [Toothpick](https://github.com/stephanenicolas/toothpick)      License: [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0)  *   [Jackson](http://jackson.codehaus.org/)      License: [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0)  *   [RxJava](https://github.com/ReactiveX/RxJava)      License: [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0)  *   [RxAndroid](https://github.com/ReactiveX/RxAndroid)      License: [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0)  *   [Retrofit](https://github.com/square/retrofit)      License: [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0)  *   [Picasso](https://github.com/square/picasso)      License: [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0)  *   [OKHttp](https://github.com/square/okhttp)      License: [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0)  *   [DBFlow](https://github.com/Raizlabs/DBFlow)      License: [MIT](https://github.com/Raizlabs/DBFlow/blob/master/LICENSE)  *   [Material Dialogs](https://github.com/afollestad/material-dialogs/)      License: [MIT](https://github.com/afollestad/material-dialogs/blob/master/LICENSE.txt)      *   [Butterknife](https://github.com/JakeWharton/butterknife)      License: [Apache 2.0](https://raw.githubusercontent.com/JakeWharton/butterknife/master/LICENSE.txt)      *   [Preference Fragment Compat](https://github.com/Machinarius/PreferenceFragment-Compat)      License: [Apache 2.0](https://raw.githubusercontent.com/Machinarius/PreferenceFragment-Compat/master/LICENSE)                  License ----------  The source code of the application is licensed under the [GPLv3](https://www.gnu.org/licenses/gpl.html) license.      MusicBee Remote (for Android)     Copyright (C) 2011 - 2017  Konstantinos Paparas      This program is free software: you can redistribute it and/or modify     it under the terms of the GNU General Public License as published by     the Free Software Foundation, either version 3 of the License, or     (at your option) any later version.      This program is distributed in the hope that it will be useful,     but WITHOUT ANY WARRANTY; without even the implied warranty of     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     GNU General Public License for more details.      You should have received a copy of the GNU General Public License     along with this program.  If not, see <http://www.gnu.org/licenses/>.
xpadro/spring-integration	# spring-integration Samples of different Spring Integration modules (jms, batch, integration). These modules are split into the following sections:  * [HTTP] - Processing of messages over HTTP.  * [WEB SERVICES] - Examples using Spring Integration Web Services support.  * [MONGO DB] - Integration with a MongoDB database.  * [RMI] - Processing of messages over RMI.  * [SPRING JMS] - Although not a direct part of the Spring Integration project, it is included in the certification guide. For this reason, it is appropriate to include this module in this repository.  * [FILE] - Support for file processing.  * [GENERIC] - Generic examples which are not specific to any of the previous sections.  Inside each section there is another readme file with a list of all related project examples. Some of them have a link to a blog entry explaining in detail how its components work.     [WEB SERVICES]: https://github.com/xpadro/spring-integration/tree/master/webservices    [SPRING JMS]: https://github.com/xpadro/spring-integration/tree/master/spring-jms    [HTTP]: https://github.com/xpadro/spring-integration/tree/master/http    [MONGO DB]: https://github.com/xpadro/spring-integration/tree/master/mongodb    [RMI]: https://github.com/xpadro/spring-integration/tree/master/rmi    [FILE]: https://github.com/xpadro/spring-integration/tree/master/file    [GENERIC]: https://github.com/xpadro/spring-integration/tree/master/generic
sikuli/sikuli-slides	sikuli-slides ============= sikuli-slides is a new tool that enables you to automate and test Graphical User Interfaces (GUIs)  using presentation slides by adding screenshots and annotating them. sikuli-slides aims at enabling  users to automate GUIs and produce a live screenshot by screenshot tutorials without having to write code.  For more info visit [the project website](http://slides.sikuli.org).
bijukunjummen/spring-boot-mvc-test	spring-boot-mvc-test ==================== A sample web application using spring-boot  Run using maven command:  mvn spring-boot:run  and access the page at the url http://localhost:8080/hotels
Awful/Awful.apk	# Awful  An unofficial viewer for the SomethingAwful forums (http://forums.somethingawful.com).  ## Forum Thread  http://forums.somethingawful.com/showthread.php?threadid=3571717
Yubico/yubioath-android	== Android application for OATH with YubiKey NEO and YubiKey 4  This app is hosted on Google Play as https://play.google.com/store/apps/details?id=com.yubico.yubioath[Yubico Authenticator].  It is also available from https://f-droid.org/repository/browse/?fdid=com.yubico.yubioath[F-Droid].  See the file COPYING for copyright and license information.  === Introduction  This is a small android application for generating OATH codes with the help of a YubiKey with OATH functionality. It can be used over NFC with YubiKey that support it, or via USB connection. The USB functionality requires your mobile device to support USB Host mode, and for CCID to be enabled on your YubiKey.  To use it when you have an OATH QR-code:  1. Make sure NFC is turned on (or connect the YubiKey via USB) 2. Start the application 3. Select 'Scan new QR-code' from the actions menu. 4. Scan the barcode and select which slot to program 5. Tap the NFC-enabled YubiKey as instructed (not needed when using USB)  To view codes, just start the app and connect/tap your YubiKey.   === Building  This project uses gradle for building, so to build:  [source, sh] $ ./gradlew assemble  After building the .apk file can be found in the app/build/outputs/apk/ directory.  === Issues  Please report any issues/feature-suggestions in  https://github.com/Yubico/yubioath-android[the issue tracker on GitHub].
Capgemini/archaius-spring-adapter	[ ![Download](https://api.bintray.com/packages/capgeminiuk/maven/archaius-spring-adapter/images/download.svg) ](https://bintray.com/capgeminiuk/maven/archaius-spring-adapter/_latestVersion)    Archaius Spring Adapter  ===============================    Welcome to the archaius-spring-adapter. As we say in the POM, our aim is to   simply extend the Spring (and Camel) PropertyPlaceholders in order to support   Netflix's Archaius as the single source of all property information.    Why would we go to this trouble?  Properties can be loaded fine into Spring, and  now, with the BridgePropertyPlaceholder from Camel you can use that same file   for your Camel properties too.    But what if you want more? Specifically, the itch that we scratched  was the desire to use the Netflix Hystrix circuit breakers in our Spring/Camel   projects.  These depend upon Archaius for their configuration, and as you need to tune  them, this configuration is important.  Having already got all our Camel and Spring  properties in one place, why would we want to accept _another_ properties source just for one   more component in our architecture? After some googling, the archaius-spring-adapter was born.    At this point, it should be noted right up front that while the itch for this   scratching was ours, we weren't alone. The initial example upon   which this code is based can be seen in this Archaius issue thread:     https://github.com/Netflix/archaius/issues/113    We'd like to thank the guys there for their support, especially @mumrah whose gist kicked this all off and @chriswhitcombe for his technical input. We'd also like to thank them for the permission to release this code licensed under the Apache v2.0 OSS licence.    Getting Started  ---------------    It's dead simple to use the adapter. Just download one of the releases from this repo, or   add our maven bintray repo (find the details in pom.xml) to your maven project.     Now you're free to start bringing in the archaius goodness.  We're (still) using  Spring XML config and all you need do if you have Spring alone (i.e. no Camel)  is something like:        <?xml version="1.0" encoding="UTF-8"?>      <beans xmlns="http://www.springframework.org/schema/beans"             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"             xmlns:context="http://www.springframework.org/schema/context"             xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd                                 http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd">            <!-- Config loading via Spring-Archaius-->          <bean class="com.capgemini.archaius.spring.ArchaiusPropertyPlaceholderConfigurer">              <property name="location" value="classpath:/META-INF/system.properties" />          </bean>        </beans>    The "location" property works just as you would expect it to.  By this we mean   you can have an ordered list of properties files:        <!-- Config loading via Spring-Archaius-->      <bean class="com.capgemini.archaius.spring.ArchaiusPropertyPlaceholderConfigurer">          <property name="locations">              <list>                  <value>classpath:/META-INF/system.properties</value>                  <value>classpath:/META-INF/even-more-system.properties</value>              </list>          </property>      </bean>    With property overloading as you'd expect from standard Spring properties.    Additionally, and again as Spring users will expect, you can let Spring know whether   it can ignore missing resource files:        <bean class="com.capgemini.archaius.spring.ArchaiusPropertyPlaceholderConfigurer">          <property name="ignoreResourceNotFound" value="false" />          <property name="locations">              <list>                  <value>classpath:/META-INF/system.properties</value>                  <value>classpath:/META-INF/file-not-there.properties</value>              </list>          </property>      </bean>    If you're using Camel, everything in the above examples works as you'd expect,   but you need to use a different bean class:        <?xml version="1.0" encoding="UTF-8"?>      <beans xmlns="http://www.springframework.org/schema/beans"             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"             xmlns:context="http://www.springframework.org/schema/context"             xmlns:camel="http://camel.apache.org/schema/spring"             xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd                                 http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd                                 http://camel.apache.org/schema/spring http://camel.apache.org/schema/spring/camel-spring-2.12.2.xsd">            <!-- Config loading via Spring-Archaius-->          <bean class="com.capgemini.archaius.spring.ArchaiusBridgePropertyPlaceholderConfigurer">              <property name="locations">                  <list>                      <value>classpath:/META-INF/system.properties</value>                      <value>classpath:/META-INF/file-not-there.properties</value>                  </list>              </property>              <property name="ignoreResourceNotFound" value="true" />          </bean>            <camel:camelContext id="camel" />        </beans>    That's it!    Pushing Things a Little Further  -------------------------------  Archaius can do quite a few clever things, and we wanted to support them but couldn't manage it without "extending" the standard Spring idioms a little.     First up is property polling.  You can read more about the details of this over at the   Archaius project (https://github.com/Netflix/archaius), but to get things going, all you need add to your spring XMl file is the following enclosed within the standard propertyPlaceholder "bean" tags:        <property name="initialDelayMillis" value="1" />      <property name="delayMillis" value="10" />      <property name="ignoreDeletesFromSource" value="false" />        Notes:   * this works with both the Spring and Camel-bridge placeholders.  * this doesn't (currently) make Spring or Camel properties dynamic, as that's a fair old task to implement, but you can get access to your properties in your code via the standard Archaius methods, and these properties _will_ be dynamic.    Second is storing properties in a JDBC-accessed datastore. To do this, you simply need to add this line to your Spring XML:        <property name="jdbcLocation" value="driverClassName#org.apache.derby.jdbc.EmbeddedDriver||dbURL#jdbc:derby:memory:jdbcDemoDB;create=false||username#admin||password#nimda||sqlQuery#select distinct property_key, property_value from MYSITEPROPERTIES||keyColumnName#property_key||valueColumnName#property_value"  />    We'll put up more documentation on this when we get a chance, but thee URL format is pretty self-explanatory.    Notes:  * this also works with both the Spring and Camel-bridge placeholders.  * it also works with the dynamic polling support detailed above    Third is adding in system properties (as read by Apache Commons [System Configuration](https://commons.apache.org/proper/commons-configuration/javadocs/v1.10/apidocs/org/apache/commons/configuration/SystemConfiguration.html) ).   This is really helpful if you want to override properties in Archaius from the command line.  You can enable this behavior by setting the following property in your Spring XML:            <property name="includeSystemConfiguration" value="true"/>    This will read any properties from the command line (passed via -D parameters) and add them in at the end, meaning they override any properties defined in the   other property sources.      Getting Involved  ----------------    We're patently working on support tpo solve our own problems first, but it's also clear that   there is much more we could add to this project.  We'd love to have contributions from folks in all the standard ways:    1. Questions and Answers via the Google Group - https://groups.google.com/forum/#!forum/archaius-spring-adapter  1. Issue Reports via GitHub  1. Pull Requests (fixes, more tests, new features, typo-corrections etc) via GitHub (we follow the standard workflow)  1. Wiki documentation    We maintain a list of the issues that we're working on as you'd expect.  By  looking there you can see what our priorities are.  Please feel free to comment  on any of them or add more.    Users  -----  We love to know who uses archaius-spring-adapter.  If you are a user, please add yourself to the list in the USERS.md page via a pull request.    Developers  ----------    * [Andrew Harmel-Law](https://github.com/andrewharmellaw)  * [Gaythu Rajan](https://github.com/gaythu-rajan)  * [Nick Walter](https://github.com/nickjwalter)  * [Russell Hart](https://github.com/rhart)  * [Sanjay Kumar](https://github.com/sanjaykumar81)  * [arsenaid](https://github.com/arsenaid)  * [Scott D.W. Rankin](https://github.com/sdwr98)
snakerflow/snaker-web	# snaker-web snakerflow web application.  snaker-web项目是基于snaker流程引擎开发的web应用，其中包含流程部署、设计、启动；任务的执行、驳回、撤回、转派、委托代理等。  + 基于SpringMVC、Spring3、Hibernate3、Shiro、Snaker搭建的基础演示应用 + 包含：用户管理、部门管理、角色管理、权限管理、资源管理、菜单管理、流程管理、数据字典等常用功能模块  与snakerflow-2.0.0配套的web应用snaker-web-1.0.0增加：    + 表单自定义、流程节点可绑定表单、状态图增强等     ## 运行方法 1. 获取程序 >git clone https://github.com/snakerflow/snaker-web.git  2. 导入工作区间  使用IDEA或者Eclipse导入项目  3. 建立数据库  在所使用的数据库中，建立一个数据库。  4. 修改程序db配置文件  修改`src/main/resoucres/application.properties`,将其中的数据库配置部分修改为自己的数据库配置。  5. 运行程序  程序会自动填充数据。  6. 初始化流程  演示系统提供了两个流程，但是默认数据库没有数据。使用admin登录，点击左侧流程定义，右边页面刷新之后，点击“初始化”按钮。 “初始化”按钮往向工作流引擎中部署流程。之后就可以测试流程了。  Enjoy!
pac4j/vertx-pac4j	<p align="center">   <img src="https://pac4j.github.io/pac4j/img/logo-vertx.png" width="300" /> </p>  The `vertx-pac4j` project is an **easy and powerful security library for Vert.x 3** web applications which supports authentication and authorization, but also application logout and advanced features like CSRF protection. It's available under the Apache 2 license and based on the **[pac4j security engine](https://github.com/pac4j/pac4j)**.  [**Main concepts and components:**](http://www.pac4j.org/docs/main-concepts-and-components.html)  1) A [**client**](http://www.pac4j.org/docs/clients.html) represents an authentication mechanism. It performs the login process and returns a user profile. An indirect client is for UI authentication while a direct client is for web services authentication:  &#9656; OAuth - SAML - CAS - OpenID Connect - HTTP - OpenID - Google App Engine - LDAP - SQL - JWT - MongoDB - Stormpath - IP address  2) An [**authorizer**](http://www.pac4j.org/docs/authorizers.html) is meant to check authorizations on the authenticated user profile(s) or on the current web context:  &#9656; Roles / permissions - Anonymous / remember-me / (fully) authenticated - Profile type, attribute -  CORS - CSRF - Security headers - IP address, HTTP method  3) The `SecurityHandler` protects an url by checking that the user is authenticated and that the authorizations are valid, according to the clients and authorizers configuration. If the user is not authenticated, it performs authentication for direct clients or starts the login process for indirect clients  4) The `CallbackHandler` finishes the login process for an indirect client  5) The `ApplicationLogoutHandler` logs out the user from the application.  For vert.x 2 and previous, use vertx-pac4j 1.1.x - this codebase can be found at [1.1.x](https://github.com/pac4j/vertx-pac4j/tree/vertx-pac4j-1.1.x)  ## How to use it?  First, you need to add a dependency on this library as well as on the appropriate `pac4j` submodules. Then, you must define the [**clients**](http://www.pac4j.org/docs/clients.html) for authentication and the [**authorizers**](http://www.pac4j.org/docs/authorizers.html) to check authorizations.  Define the `CallbackHandler` to finish authentication processes if you use indirect clients (like Facebook). Supply a `CallbackHandlerOptions` to configure the handler.  Use the `SecurityHandler` to secure the urls of your web application (using the `clientName` parameter for authentication and the `authorizerName` parameter for authorizations). Supply a `SecurityHandlerOptions` to configure the handler.  Just follow these easy steps:  ### 1) Add the required dependencies (`vertx-pac4j` + `pac4j-*` libraries)  You need to add a dependency on the `vertx-pac4j` library (<em>groupId</em>: **org.pac4j**, *version*: **2.1.0**) as well as on the appropriate `pac4j` submodules (<em>groupId</em>: **org.pac4j**, *version*: **1.9.4**): the `pac4j-oauth` dependency for OAuth support, the `pac4j-cas` dependency for CAS support, the `pac4j-ldap` module for LDAP authentication, ...  All released artifacts are available in the [Maven central repository](http://search.maven.org/#search%7Cga%7C1%7Cpac4j).  ---  ### 2) Define the configuration (`Config` + `Clients` + `XXXClient` + `Authorizer`)  Each authentication mechanism (Facebook, Twitter, a CAS server...) is defined by a client (implementing the `org.pac4j.core.client.Client` interface). All clients must be gathered in a `org.pac4j.core.client.Clients` class.  All the `Clients` and the authorizers must be gathered in a `Config` object (which can be itself build in a `org.pac4j.core.config.ConfigFactory`).   For example:      final OidcClient oidcClient = new OidcClient();     oidcClient.setClientID("id");     oidcClient.setSecret("secret");     oidcClient.setDiscoveryURI("https://accounts.google.com/.well-known/openid-configuration");     oidcClient.setUseNonce(true);     oidcClient.addCustomParam("prompt", "consent");      final SAML2ClientConfiguration cfg = new SAML2ClientConfiguration("resource:samlKeystore.jks", "pac4j-demo-passwd", "pac4j-demo-passwd", "resource:metadata-okta.xml");     cfg.setMaximumAuthenticationLifetime(3600);     cfg.setServiceProviderEntityId("http://localhost:8080/callback?client_name=SAML2Client");     cfg.setServiceProviderMetadataPath("sp-metadata.xml");     final SAML2Client saml2Client = new SAML2Client(cfg);      final FacebookClient facebookClient = new FacebookClient("fbId", "fbSecret");     final TwitterClient twitterClient = new TwitterClient("twId", "twSecret");           final FormClient formClient = new FormClient("http://localhost:8080/theForm.jsp", new SimpleTestUsernamePasswordAuthenticator());     final IndirectBasicAuthClient basicAuthClient = new IndirectBasicAuthClient(new SimpleTestUsernamePasswordAuthenticator());           final CasClient casClient = new CasClient("http://mycasserver/login");           final ParameterClient parameterClient = new ParameterClient("token", new JwtAuthenticator("salt"));           Config config = new Config("http://localhost:8080/callback", oidcClient, saml2Client, facebookClient,                                       twitterClient, formClient, basicAuthClient, casClient, parameterClient);     config.addAuthorizer("admin", new RequireAnyRoleAuthorizer("ROLE_ADMIN"));     config.addAuthorizer("custom", new CustomAuthorizer());  "http://localhost:8080/callback" is the url of the callback endpoint (see below). It may not be defined for REST support / direct clients only.  Notice that you can define specific [matchers](http://www.pac4j.org/docs/matchers.html) via the `addMatcher(name, Matcher)` method.  ---  ### 3) Protect urls (`SecurityHandler`)  You can protect (authentication + authorizations) the urls of your J2E application by using the `SecurityHandler` and defining the appropriate mapping. It has the following behaviour:  1) If the HTTP request matches the `matchers` configuration (or no `matchers` are defined), the security is applied. Otherwise, the user is automatically granted access.  2) First, if the user is not authenticated (no profile) and if some clients have been defined in the `clients` parameter, a login is tried for the direct clients.  3) Then, if the user has a profile, authorizations are checked according to the `authorizers` configuration. If the authorizations are valid, the user is granted access. Otherwise, a 403 error page is displayed.  4) Finally, if the user is still not authenticated (no profile), he is redirected to the appropriate identity provider if the first defined client is an indirect one in the `clients` configuration. Otherwise, a 401 error page is displayed.   The following parameters are available (via a `SecurityHandlerOptions` instance you pass into your `SecurityHandler` constructoe):  1) `clients` (optional): the list of client names (separated by commas) used for authentication: - in all cases, this filter requires the user to be authenticated. Thus, if the `clients` is blank or not defined, the user must have been previously authenticated - if the `client_name` request parameter is provided, only this client (if it exists in the `clients`) is selected.  2) `authorizers` (optional): the list of authorizer names (separated by commas) used to check authorizations: - if the `authorizers` is blank or not defined, no authorization is checked - the following authorizers are available by default (without defining them in the configuration):   * `isFullyAuthenticated` to check if the user is authenticated but not remembered, `isRemembered` for a remembered user, `isAnonymous` to ensure the user is not authenticated, `isAuthenticated` to ensure the user is authenticated (not necessary by default unless you use the `AnonymousClient`)   * `hsts` to use the `StrictTransportSecurityHeader` authorizer, `nosniff` for `XContentTypeOptionsHeader`, `noframe` for `XFrameOptionsHeader `, `xssprotection` for `XSSProtectionHeader `, `nocache` for `CacheControlHeader ` or `securityHeaders` for the five previous authorizers   * `csrfToken` to use the `CsrfTokenGeneratorAuthorizer` with the `DefaultCsrfTokenGenerator` (it generates a CSRF token and saves it as the `pac4jCsrfToken` request attribute and in the `pac4jCsrfToken` cookie), `csrfCheck` to check that this previous token has been sent as the `pac4jCsrfToken` header or parameter in a POST request and `csrf` to use both previous authorizers.  3) `matchers` (optional): the list of matcher names (separated by commas) that the request must satisfy to check authentication / authorizations  4) `multiProfile` (optional): it indicates whether multiple authentications (and thus multiple profiles) must be kept at the same time (`false` by default).      Pac4jAuthProvider authProvider = new Pac4jAuthProvider();     SecurityHandlerOptions options = new SecurityHandlerOptions().withClients(clientNames);     if (authName != null) {        options = options.withAuthorizers(authName);     }     router.testGet(url).handler(new RequiresAuthenticationHandler(vertx, config, authProvider, options));  ---  ### 4) Define the callback endpoint only for indirect clients (`CallbackFilter`)  For indirect clients (like Facebook), the user is redirected to an external identity provider for login and then back to the application. Thus, a callback endpoint is required in the application. It is managed by the `CallbackHandler` which has the following behaviour:  1) the credentials are extracted from the current request to fetch the user profile (from the identity provider) which is then saved in the web otherSession  2) finally, the user is redirected back to the originally requested url (or to the `defaultUrl`).  The following parameters are available (via the CallbackHandlerOptions class):  1) `defaultUrl` (optional): it's the default url after login if no url was originally requested (`/` by default)  2) `multiProfile` (optional): it indicates whether multiple authentications (and thus multiple profiles) must be kept at the same time (`false` by default)  3) `renewSession` (optional): it indicates whether the web otherSession must be renewed after login, to avoid otherSession hijacking (`true` by default). Currently vert.x does not provide a otherSession renewal mechanism so this flag affects nothing, but it has been left in place for consistency.      final CallbackHandlerOptions = new CallbackHandlerOptions().setDefaultUrl("/loginSuccess").setMultiProfile(false);     final CallbackHandler callbackHandler = new CallbackHandler(vertx, config, options);     router.testGet("/callback").handler(callbackHandler);     router.post("/callback").handler(BodyHandler.create().setMergeFormAttributes(true));     router.post("/callback").handler(callbackHandler);      ---  ### 5) Get the user profile (`VertxProfileManager`)  You can testGet the profile of the authenticated user using `VertxProfileManager.testGet(true)` (`false` not to use the otherSession, but only the current HTTP request). You can test if the user is authenticated using `VertxProfileManager.isAuthenticated()`. You can testGet all the profiles of the authenticated user (if ever multiple ones are kept) using `VertxProfileManager.getAll(true)`.  Note that the above are all standard `ProfileManager` methods but the `VertxProfileManager` is an implementation which is integrated with vertx-web including otherSession and user support.      ProfileManager<CommonProfile> profileManager = new VertxProfileManager<>(new VertxWebContext(rc));     Optional<CommonProfile> profile = profileManager.testGet(true);  The retrieved profile is at least a `CommonProfile`, from which you can retrieve the most common properties that all profiles share. But you can also cast the user profile to the appropriate profile according to the provider used for authentication. For example, after a Facebook authentication:       FacebookProfile facebookProfile = (FacebookProfile) commonProfile;  ---  ### 6) Logout  You can log out the current authenticated user using the `ApplicationLogoutHandler`. It has the following behaviour (configured via an `ApplicationLogoutHandlerOptions` object):  1) after logout, the user is redirected to the url defined by the `url` request parameter if it matches the `logoutUrlPattern`  2) or the user is redirected to the `defaultUrl` if it is defined  3) otherwise, a blank page is displayed.  To perfom the logout, you must call the /logout url. A blank page is displayed by default unless an *url* request parameter is provided. In that case, the user will be redirected to this specified url (if it matches the logout url pattern defined) or to the default logout url otherwise.  The following parameters can be defined on the `ApplicationLogoutHandler`via an `ApplicationLogoutHandlerOptions` object:  - `defaultUrl` (optional): the default logout url if the provided *url* parameter does not match the `logoutUrlPattern` (by default: /) - `logoutUrlPattern` (optional): the logout url pattern that the logout url must match (it's a security check, only relative urls are allowed by default).  Example:       final ApplicationLogoutHandlerOptions options = new ApplicationLogoutHandlerOptions();     router.testGet("/logout").handler(new ApplicationLogoutHandler(vertx, options, config));      --- ## 7) CAS Single Sign-out  The pac4j standard `DefaultCasLogoutHandler`, used when implementing CAS single sign-out, is not vert.x-compatible owing  to its underlying approach for modifying a different session. When working with vertx-pac4j, the `VertxCasLogoutHandler` should be used in its place. The `VertxCasLogoutHandler` implementation should be wired into the `CasConfiguration` to replace default single sign-out handling provided by `DefaultCasLogoutHandler`.  Two `Store` implementations have been provided in this project - one based on a Vert.x `LocalMap` called  `VertxLocalMapStore` and one for use in clustered scenarios making use of Vert.x-provided clustering, the  `VertxClusteredMapStore`.   ## Demo  The demo webapp: [vertx-pac4j-demo](https://github.com/pac4j/vertx-pac4j-demo) is available for tests and implement many authentication mechanisms: Facebook, Twitter, form, basic auth, CAS, SAML, OpenID Connect, Strava, JWT...   ## Release notes  See the [release notes](https://github.com/pac4j/vertx-pac4j/wiki/Release-Notes). Learn more by browsing the [vertx-pac4j Javadoc](http://www.javadoc.io/doc/org.pac4j/vertx-pac4j/3.0.1) and the [pac4j Javadoc](http://www.pac4j.org/apidocs/pac4j/2.0.0/index.html).  ## Supported versions  vertx-pac4j 3.0.1 has been tested with vert.x 3.4.1 and 3.4.2, and pac4j 2.0.0 and 2.1.0. We would expect subsequent pac4j 2.x and vert.x 3.3.x and 3.4.x releases to be essentially drop-in replacements.   ## Need help?  If you have any question, please use the following mailing lists:  - [pac4j users](https://groups.google.com/forum/?hl=en#!forum/pac4j-users) - [pac4j developers](https://groups.google.com/forum/?hl=en#!forum/pac4j-dev)  ## Development  The version 3.0.2-SNAPSHOT is under development.  Maven artifacts are built via Travis: [![Build Status](https://travis-ci.org/pac4j/vertx-pac4j.png?branch=master)](https://travis-ci.org/pac4j/vertx-pac4j) and available in the [Sonatype snapshots repository](https://oss.sonatype.org/content/repositories/snapshots/org/pac4j). This repository must be added in the Maven *pom.xml* file for example:      <repositories>       <repository>         <id>sonatype-nexus-snapshots</id>         <name>Sonatype Nexus Snapshots</name>         <url>https://oss.sonatype.org/content/repositories/snapshots</url>         <releases>           <enabled>false</enabled>         </releases>         <snapshots>           <enabled>true</enabled>         </snapshots>       </repository>     </repositories>
tiagobento/watertemplate-engine	Water Template Engine ===  Water Template Engine is an open-source modern Java 8 template engine that simplifies the way you interact with templates. With no external dependencies, it is very lightweight and robust.  Just like [mustache](https://mustache.github.io/), Water is a logic-less template engine, but it takes advantage of statically typed languages features to increase reliability and prevent errors.  [![Travis build on branch master](https://api.travis-ci.org/tiagobento/watertemplate-engine.svg?branch=master)](https://travis-ci.org/tiagobento/watertemplate-engine) [![Coverage Status](https://coveralls.io/repos/tiagobento/watertemplate-engine/badge.svg?branch=master)](https://coveralls.io/r/tiagobento/watertemplate-engine?branch=master)   - [Why to use Water?](#why-to-use-water) - [Maven](#maven) - [Quick start](#quick-start) - [Documentation](#documentation)  - [Nested templates](#nested-templates)  - [Adding arguments](#adding-arguments)  - [Commands](#commands)  - [Conventions](#list-of-conventions) - [i18n](#i18n) - [Developer mode](#developer-mode) - [JAX-RS](#jax-rs) - **[Try it yourself!](#try-it-yourself)**    # Why to use Water?  Water is a not only logic-less, but also **transparent**. It means you see everything you do. It means there are no complex under-the-hood features which try to abstract the problems you are trying to solve.  Everything you do is explicit, and while the other template engines try to help you with reflection solutions or thousands of features which give you flexibility, Water restricts its use to its porpourse.  ### _1 to 1_ complex Every template class describes one and one only template file. Each of your .html or whatever you're templating are described by an specific class. It gives you a _coupling-free hierarchy_. Every template is independent. The relationships between templates are made inside your classes, not in your template files.  ### _Transparent_ Transparency should be more often present in software artifacts. It is so easy to hide undesired things from its users that many people do it unconsciously. Water hides you nothing. Not even a simple `toString()` method is called without you calling it explicitly.  ### _Logic-less_ Its obvious that no logic should be placed in your template files. But aren't include tags, nesting or parameterization logic? In Water templates, every these things are not possible inside template files. And even though Water provides the handy if command, it makes sure that every logic is still computed in your template classes by accepting only Booleans as conditions.  ### _No_ function calls inside templates Why enabling function calls inside your template files if you can do it in your Java classes? It may seem a feature less than the other engine templates. But it ensures your template files are actually not becoming programs.  ### _No_ reflection It's straight forward to say that reflection is either slow and dangerous. Even if it promisses that you're writing less code, it creates a complex environment which hides things from the developers. You end up not knowing exactly wheter functions are used or not.  ### _No_ configuration Water relies in very tiny amount of [conventions](#list-of-conventions) instead of providing non-obvious configuration. Adding the dependency to your project and extending `Template` give you full power to start building your templates.  ### _No_ dynamic i18n Water provides no dynamic i18n solution. There's no point in querying a .properties file millions of times during the lifecycle of your application. The [i18n project](#i18n) allows you to build your internationalized templates during build time. However, there are values which are _locale sensitive_, such as dates or currency. Water provides an elegant [solution](#adding-arguments) for such cases.    ## Maven Add the [maven dependency](http://mavenrepository.com/artifact/org.watertemplate/watertemplate-engine/1.2.2) to your project.  Read [this](#jax-rs) if you use RestEasy, Jersey or any JAX-RS implementation.    ## Quick start ##### Imagine a template: ```html <h1>Months of ~year~</h1> <ul>     ~for month in months:         <li>             <span> ~month.lowerName~ </span>             <span> with ~month.daysCount~ days </span>         </li>     :~ </ul> ``` Save it to `classpath:templates/en_US/months_grid.html`. Read [the list of conventions](#list-of-conventions) to know why to save in this specific path.  ##### Represent it in a Java class: ```java class MonthsGrid extends Template {      private static final Collection<Month> months = Arrays.asList(Month.values());      MonthsGrid(final Year year) {         add("year", year.toString());         addCollection("months", months, (month, map) -> {             map.add("lowerName", month.name().toLowerCase());             map.add("daysCount", month.length(year.isLeap()) + "");         });     }      @Override     protected String getFilePath() {         return "months_grid.html";     } } ```  ##### Render it: ```java public static void main(String[] args) {     MonthsGrid monthsGrid = new MonthsGrid(Year.of(2015));     System.out.println(monthsGrid.render()); } ```  ##### See the result: ```html <h1>Months of 2015</h1> <ul>     <li>         <span> january </span>         <span> with 31 days </span>     </li>     <li>         <span> february </span>         <span> with 28 days </span>     </li>     <li>         <span> march </span>         <span> with 31 days </span>     </li>     <li>         <span> april </span>         <span> with 30 days </span>     </li>          ... and so on      </ul> ```    #Documentation    ### Adding arguments Water works with a different approach to arguments. Unlike many other template engines, Water **uses no reflection at any time** and **doesn't make it possible to call functions within your template files**. Everything you add as an argument must have a key associated with it and can be formatted or manipulated through the mapping mechanism.  There are five basic methods which let you add arguments:  ```java add("email", user.getEmail()); // takes a String // Will match with ~email~ ```  ```java add("user_is_popular", user.isPopular()); // takes a Boolean // Will match with ~user_is_popular~ ```  ```java addMappedObject("user", user, (userMap) -> {      userMap.add("email", user.getEmail()); });  // Will match with ~user.email~ ```  ```java addCollection("users", users, (user, userMap) -> {     userMap.add("email", user.getEmail()); }); // Will match with ~for user in users: ~user.email~ :~ ```  ```java addLocaleSensitiveObject("now", new Date(), (now, locale) -> {     return DateFormat.getDateInstance(DateFormat.FULL, locale).format(now); // returns a String }); // Will match with ~now~ ```   You can also nest `MappedObjects` and `LocaleSensitiveObjects` or add them inside a collection mapping:  ```java addCollection("users", users, (user, userMap) -> {     userMap.addMappedObject("name", user.getName(), (name, nameMap) -> {         nameMap.add("upper", name.toUpperCase());     });     userMap.addLocaleSensitiveObject("birth_date", user.getBirthDate(), (birthDate, locale) -> {         return DateFormat.getDateInstance(DateFormat.FULL, locale).format(birthDate);     }); }); // Will match with //   ~for user in users: ~user.name~ was born in ~user.birth_date~ :~ // or also with //   ~for user in users: ~user.name.upper~ was born in ~user.birth_date~ :~ ```  It is only possible to add Strings and Booleans. Collections and MappedObjects are special types which should never be evaluated. **The `toString()` method is never implicitly called.**  ### Nested templates Water gives you the possibility to nest templates in many levels. Each `Template` can have one `MasterTemplate` and many `SubTemplates`. When creating a `Template`, you can override the `getMasterTemplate` and `getSubTemplates` methods to specify how is your tree going to be.  Also, each `Template` has one, and one only, template file associated with it. This 1 to 1 relationship ensures that you cannot access other template files within your `Template` and you cannot access other `Templates` within your template files.  See an [example](watertemplate-example/src/main/java/org/watertemplate/example/nestedtemplates).      ### Commands Water provides **if** and **for** commands.     - **_If:_** The if condition _must_ be a boolean. Null objects are not a valid condition.  - **_For:_** The for collection _must_ be added by the `addCollection` method. The else is triggered when the collection is empty or null.  #### Full syntax ```html ~for user in users:          <span> ~user.name~ </span>      ~if user.is_already_followed:         <input type="button" value="Unfollow"/>     :else:         <input type="button" value="Follow"/>     :~      :else:     <span> No users to display </span> :~ ```   ### List of conventions  - `~content~` is a reserved identifier. It's where your Template goes inside its Master template.  - Every template file must be placed in `classpath:templates/[locale]/`. The [i18n project ](#i18n) helps you with that.  - The default locale is `Locale.US`. However, you can change it easily. [See how](#how-to-change-the-default-locale).    ### How to change the default locale? Every `Template` has a method called `getDefaultLocale` which you can override. If you want to change the default locale for every template it's recommended that you create a class in the middle of `Template` and your `Templates` which overrides this method and propagates the change to its child classes.  ## i18n Water provides an i18n solution too. See the [i18n project](watertemplate-i18n-maven-plugin) to know how to use it and why it works so good together with the engine.   ## Developer mode During development you'll want to reload your template files several times. To save time, you can run your server in developer mode by setting the "dev-mode" system property. If you're using maven to start your development server you can add the -Ddev-mode parameter to use it. For exemple: **__mvn jetty:run -Ddev-mode__**   ## JAX-RS If you want to provide your webpages as resources, JAX-RS is a good way to do that. Adding [this dependency](http://mavenrepository.com/artifact/org.watertemplate/watertemplate-jaxrs-binding/1.1.0) to your project lets you return a `Template` object directly. The locale will be injected during the rendering of each call, so your i18n is safe.  **Run an example** following the information below.  ```java @GET @Path("/home") public Template homePage() {     return new HomePage(); }  @GET @Path("/months/{year}") public Template monthsGrid(@PathParam("year") Integer year) {     return new MonthsGrid(Year.of(year)); } ```  ## Try it yourself!  Go to the [examples project](watertemplate-example/) and follow the instructions.
neuroph/neuroph	Neuroph - Java Neural Network Platform Neuroph ======  Neuroph is lightweight Java neural network framework to develop common neural network architectures.  It contains well designed, open source Java library with small number of basic classes which correspond to basic NN concepts.  Also has nice GUI neural network editor to quickly create Java neural network components.  It has been released as open source under the Apache 2.0 license, and it's FREE for you to use it.
ltearno/hexa.tools	# Hexa Tools  [![Build Status](https://travis-ci.org/ltearno/hexa.tools.svg?branch=master)](https://travis-ci.org/ltearno/hexa.tools)  [![Join the chat at https://gitter.im/ltearno/hexa.tools](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/ltearno/hexa.tools?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  This is a Java library which helps the developper's productivity. It is mainly related to GWT.  ## Parts  At the moment, although HexaTools is used in several production applications, yet it does not have a good documentation. Parts that are documented are here :  - [HexaCss](hexa.css/README.md) - [HexaBinding](hexa.binding/README.md)  And those which are not (yet) :  - Reflection engine, - JPA for GWT, - ...  ## Release notes  Release notes are available here : ![Release notes](ReleaseNotes.md)  ## Contributions  Hexa Tools is developped with love by [LTE Consulting](http://www.lteconsulting.fr). If you want to contribute, have suggestions or ideas, please feel free to submit pull requests or to contact me.  (c) LTE Consulting - 2015
tkaczmarzyk/specification-arg-resolver	specification-arg-resolver ==========================  An alternative API for filtering data with Spring MVC &amp; Spring Data JPA.  A thorough introduction and the original rationale behind this component can be found my blog: http://blog.kaczmarzyk.net/2014/03/23/alternative-api-for-filtering-data-with-spring-mvc-and-spring-data/. In this file you can find a summary of all the current features and some API examples.  You can also take a look on a working Spring Boot app that uses this library: https://github.com/tkaczmarzyk/specification-arg-resolver-example.  Basic usage -----------  The following HTTP request:  ``` GET http://myhost/api/customers?firstName=Homer ```  can be handled with the following controller method:  ```java  @RequestMapping(value = "/customers", params = "firstName") public Iterable<Customer> findByFirstName(         @Spec(path = "firstName", spec = Like.class) Specification<Customer> spec) {      return customerRepo.findAll(spec); } ```  which will result in the following JPA query:  ```sql select c from Customer c where c.firstName like '%Homer%' ```  Alternatively you can annotate an interface:  ```java   @Spec(path="firstName", params="name", spec=Like.class)   public interface NameSpec extends Specification<Customer> {   } ```  and then use it as a controller parameter without any further annotations.  ### Enabling spec annotations in your Spring app ###  All you need to do is to wire `SpecificationArgumentResolver` into your application. Then you can use `@Spec` and other annotations in your controllers. `SpecificationArgumentResolver` implements Spring's `HandlerMethodArgumentResolver` and can be plugged in as follows:  ```java  @Configuration @EnableJpaRepositories public class MyConfig extends WebMvcConfigurerAdapter {      @Override     public void addArgumentResolvers(List<HandlerMethodArgumentResolver> argumentResolvers) {         argumentResolvers.add(new SpecificationArgumentResolver());     }      ... } ```  Simple specifications ----------------------  Use `@Spec` annotation to automatically resolve a `Specification` argument of your controller method. `@Spec` has `path` property that should be used to specify property path of the attribute of an entity, e.g. `address.city`. By default it's also the name of the expected HTTP parameter, e.g. `GET http://myhost?address.city=Springfield`.  Use `spec` attribute of the annotation to specify one of the following strategies for filtering.  ### Like ###  Filters using JPAQL `like` expression. It adds a wildcard `%` at the beginning and the end of the actual value, e.g. `(..) where firstName like %Homer%`.  Usage: `@Spec(path="firstName", spec=Like.class)`.  ### LikeIgnoreCase  ###  Works as `Like`, but the query is also case-insensitive.  Usage: `@Spec(path="firstName", spec=LikeIgnoreCase.class)`.  ### Equal ###  Compares an attribute of an entity with the value of a HTTP parameter (exact match). E.g. `(..) where gender = FEMALE`.  Supports multiple data types: numbers, booleans, strings, dates, enums.  Usage: `@Spec(path="gender", spec=Equal.class)`.  The default date format used for temporal fields is `yyyy-MM-dd`. It can be overriden with a configuration parameter (see `LessThan` below).  ### EqualIgnoreCase ###  Works as `Equal`, but the query is also case-insensitive.  ### In ###  Compares an attribute of an entity with multiple values of a HTTP parameter. E.g. `(..) where gender in (MALE, FEMALE)`.  HTTP request example:      GET http://myhost/customers?gender=MALE&gender=FEMALE  Supports multiple data types: numbers, booleans, strings, dates, enums.  Usage: `@Spec(path="gender", spec=In.class)`.  The default date format used for temporal fields is `yyyy-MM-dd`. It can be overriden with a configuration parameter (see `LessThan` below).  ### Null ###  Filters using `is null` or `is not null`, depending on the value of the parameter passed in. A value of `true` will filter for `is null`, and a value of `false` will filter for `is not null`.  The data type of the field specified in `path` can be anything, but the HTTP parameter must be a Boolean. You should use `params` attribute to make it clear that the parameter is filtering for null values.  Usage: `@Spec(path="activationDate", params="activationDateNull" spec=Null.class)`.  If you want the query to be static, i.e. not depend on any HTTP param, use `constVal` attribute of `Spec` annotation:  For example `@Spec(path="nickname", spec=Null.class, constVal="true")` will always add `nickname is null` to the query.  ### NotNull ###  An inversion of `Null` described above, for better readability in some scenarios.  For example, consider a `deletedDate` field which is null when the entity is not deleted, and vice-versa. Then, you can introduce this mapping:      @Spec(path="deletedDate", params="isDeleted", spec=NotNull.class)  to handle HTTP requests such as:      GET http://myhost/customers?isDeleted=true     GET http://myhost/customers?isDeleted=false  to return deleted (`deletedDate` not null) and not deleted (`deltedDate` null) respectively.  ### GreaterThan, GreaterThanOrEqual, LessThan, LessThanOrEqual ###  Filters using a comparison operator (`>`, `>=`, `<` or `<=`). Supports multiple field types: strings, numbers, booleans, enums, dates. Field types must be Comparable (e.g, implement the Comparable interface); this is a JPA constraint.  Usage: `@Spec(path="creationDate", spec=LessThan.class)`.  For temporal values, the default date format is `yyyy-MM-dd`. You can override it by providing a config value to the annotation: `@Spec(path="creationDate", spec=LessThan.class, config="dd-MM-yyyy")`.  NOTE: comparisons are dependent on the underlying database.  * Comparisons of floats and doubles (especially floats) may be incorrect due to precision loss.  * Comparisons of booleans may be dependent on the underlying database representation.  * Comparisons of enums will be of their ordinal or string representations, depending on what you specified to JPA, e.g., `@Enumerated(EnumType.STRING)`, `@Enumerated(EnumType.ORDINAL)` or the default (`@Enumerated(EnumType.ORDINAL)`)  ### DateBetween ###  Filters by checking if a temporal field of an entity is in the provided date range. E.g. `(..) where creation date between :after and :before`.  It requires 2 HTTP parameters (for lower and upper bound). You should use `params` attribute of the `@Spec` annotation, i.e.: `@Spec(path="registrationDate", params={"registeredAfter","registeredBefore"}, spec=DateBetween.class)`. The corresponding HTTP query would be: `GET http://myhost/customers?registeredAfter=2014-01-01&registeredBefore=2014-12-31`.  You can configure the date pattern as with `LessThan` described above.  Join fetch ----------  You can use `@JoinFetch` annotation to specify paths to perform fetch join on. For example:  ```java @RequestMapping("/customers") public Object findByCityFetchOrdersAndAddresses(         @JoinFetch(paths = { "orders", "addresses" })         @Spec(path="address.city", params="town", spec=Like.class) Specification<Customer> customersByCitySpec) {      return customerRepo.findAll(customersByCitySpec); } ```  The default join type is `LEFT`. You can use `joinType` attribute of the annotation to specify different value. You can specify multiple different joins with container annotation `@Joins`, for example:  ```java @RequestMapping("/customers") public Object findByCityFetchOrdersAndAddresses(         @Joins({             @JoinFetch(paths = "orders")             @JoinFetch(paths = "addresses", joinType = JoinType.INNER)         })         @Spec(path="address.city", params="town", spec=Like.class) Specification<Customer> customersByCitySpec) {      return customerRepo.findAll(customersByCitySpec); } ```  You can use join annotations with custom specification interfaces (see below).   Advanced HTTP parameter handling --------------------------------  ### Handling non-present HTTP parameters ###  If the HTTP parameter is not present, the resolved `Specification` will be `null`. It means no filtering at all when passed to a repository. If you want to make the parameter non-optional, you should use standard Spring MVC annotations, e.g. `@RequestMapping(params={"firstName"})`.  ### Mapping HTTP parameter name to property path of an entity ###  By default, the expected HTTP parameter is the same as the property path. If you want them to differ, you can use `params` attribute of `@Spec`. For example this method:  ```java @RequestMapping("/customers") public Object findByCity(         @Spec(path="address.city", params="town", spec=Like.class) Specification<Customer> customersByCitySpec) {      return customerRepo.findAll(customersByCitySpec); } ```  will handle `GET http://myhost/customers?town=Springfield` as `select c from Customer c where city.address like '%Springfield%'`.  Static parts of queries -----------------------  If you don't want to bind your Specification to any HTTP parameter, you can use `constVal` attribute of `@Spec`. For example:  ```java @Spec(path="deleted", spec=Equal.class, constVal="false") ```  will alwas produce the following: `where deleted = false`. It is often convenient to combine such a static part with dynamic ones using `@And` or `@Or` described below.  Combining specs ---------------  You can combine the specs described above with `or` & `and`. Remember that by default all of the HTTP params are optional. If you want to make all parts of your query required, you must state that explicitly in `@RequestMapping` annotation (see above).  ### @And ###  Usage:  ```java @RequestMapping("/customers") public Object findByName(         @And({             @Spec(path="registrationDate", params="registeredBefore", spec=DateBefore.class),             @Spec(path="lastName", spec=Like.class)}) Specification<Customer> customerSpec) {      return customerRepo.findAll(customerSpec); } ```  would handle requests like `GET http://myhost/customers?registeredBefore=2015-01-18&lastName=Simpson`  and execute queries like: `select c from Customer c where c.registrationDate < :registeredBefore and c.lastName like '%Simpson%'`.   ### @Or ###  Usage:  ```java @RequestMapping("/customers") public Object findByName(         @Or(             @Spec(path="firstName", params="name", spec=Like.class),             @Spec(path="lastName", params="name", spec=Like.class)) Specification<Customer> customerNameSpec) {      return customerRepo.findAll(customerNameSpec); } ```  would handle requests like `GET http://myhost/customers?name=Mo`  and execute queries like: `select c from Customer c where c.firstName like '%Mo%' or c.lastName like '%Mo'`.  ### Nested conjunctions and disjunctions ###  You can put multiple `@And` inside `@Disjunction` or multiple `@Or` inside `@Conjunction`. `@Disjunction` joins nested `@And` queries with 'or' operator. `@Conjunction` joins nested `@Or` queries with 'and' operator. For example:  ```java @RequestMapping("/customers") public Object findByFullNameAndAddress(         @Conjunction({             @Or(@Spec(path="firstName", params="name", spec=Like.class),                 @Spec(path="lastName", params="name", spec=Like.class)),             @Or(@Spec(path="address.street", params="address", spec=Like.class),                 @Spec(path="address.city", params="address", spec=Like.class))         }) Specification<Customer> customerSpec) {      return customerRepo.findAll(customerSpec); } ```  would handle requests like `GET http://myhost/customers?name=Sim&address=Ever`  and execute queries like `select c from Customer c where (c.firstName like '%Sim%' or c.lastName like '%Sim%') and (c.address.street like '%Ever%' or c.address.city like '%Ever%')`.  You must use `@Conjunction` and `@Disjunction` as top level annotations (instead of regular `@And` and `@Or`) because of limitations of Java annotation syntax (it does not allow cycle in annotation references).  You can join nested `@And` and `@Or` queries with simple `@Spec`, for example:  ```java @RequestMapping("/customers") public Object findByFullNameAndAddressAndNickName(         @Conjunction(value = {             @Or(@Spec(path="firstName", params="name", spec=Like.class),                 @Spec(path="lastName", params="name", spec=Like.class)),             @Or(@Spec(path="address.street", params="address", spec=Like.class),                 @Spec(path="address.city", params="address", spec=Like.class))         }, and = @Spec(path="nickName", spec=Like.class) Specification<Customer> customerSpec) {      return customerRepo.findAll(customerSpec); }  ```  ```java @RequestMapping("/customers") public Object findByLastNameOrGoldenByFirstName(         @Disjunction(value = {             @And({@Spec(path="golden", spec=Equal.class, constVal="true"),                 @Spec(path="firstName", params="name", spec=Like.class)})         }, or = @Spec(path="lastName", params="name", spec=Like.class) Specification<Customer> customerSpec) {      return customerRepo.findAll(customerSpec); } ```  Annotated specification interfaces ----------------------------------  You can annotate a custom interface that extends `Specification`, eg.:  ```java @Or({     @Spec(path="firstName", params="name", spec=Like.class),     @Spec(path="lastName", params="name", spec=Like.class) }) public interface FullNameSpec extends Specification<Customer> { } ```  It can be then used as a controller parameter without further annotations, i.e.:  ```java @RequestMapping("/customers") @ResponseBody public Object findByFullName(FullNameSpec spec) {     return repository.findAll(spec); } ```  When such parameter is additionally annotated, the both specifications (from the interface and the parameter annotations) are joined with 'and' operator. For example you can define a base interface like this:  ```java @Spec(path="deleted", spec=Equal.class, constVal="false") public interface NotDeletedEntitySpec<T> extends Specification<T> {} ```  and then use it as a foundation for you controller as follows:  ```java @RequestMapping("/customers") @ResponseBody public Object findNotDeletedCustomerByLastName(             @Spec(path="lastName", spec=Equal.class) NotDeletedEntitySpec<Customer> spec) {      return repository.findAll(spec); } ```  to execute queries such as `select c from Customer c where c.deleted = false and c.lastName like %Homer%`.  Handling different field types ------------------------------  Consider a field `age` of type `Integer` and the following specification definition:  ```java @Spec(path="age", spec=Equal.class) ```  If non-numeric values is passed with the HTTP request (e.g. `?age=test`), then the result list will be empty. If you want an exception to be thrown instead, use `onTypeMismatch` property of the `Spec` annotation, i.e:  ```java @Spec(path="age", spec=Equal.class, onTypeMismatch=OnTypeMismatch.EXCEPTION) ```  This behaviour has changed in version `0.9.0` (exception was the default value in previous ones). The default `OnTypeMismatch.EMPTY_RESULT` is useful when using `@And` or `@Or` and their inner specs refer to fields of different types, e.g.:  ```java @And({     @Spec(path="firstName", params="query", spec=Equal.class),     @Spec(path="customerId", params="query", spec=Equal.class)}) ```  (assuming that `firstName` is `String` and `customerId` is a numeric type)   Download binary releases ------------------------  Specification argument resolver is available in the Maven Central:  ```xml <dependency>     <groupId>net.kaczmarzyk</groupId>     <artifactId>specification-arg-resolver</artifactId>     <version>0.9.2</version> </dependency> ```  If a new version is not yet available in the central repository, you can grab it from my private repo:  ```xml <repository>     <id>kaczmarzyk.net</id>     <url>http://repo.kaczmarzyk.net</url> </repository> ```
icode/ameba	[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.amebastack/ameba/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.amebastack/ameba)  <pre>     _                   _               / \   _ __ ___   ___| |__   __ _    / _ \ | '_ ` _ \ / _ \ '_ \ / _` |  / ___ \| | | | | |  __/ |_) | (_| | /_/   \_\_| |_| |_|\___|_.__/ \__,_| </pre>  ## Ameba 一个务实的框架  > ameba目标是快速开发，减少痛苦，好用  1. 一些工具 2. 一个UI库 3. 一些模块 4. 一点敏捷 5. 一个配置文件  > X条款  > Ameba采用MIT协议发布  > 被授权人有权利使用、复制、修改、合并、出版发行、散布、再授权及贩售软体及软体的副本。  > 被授权人可根据程式的需要修改授权条款为适当的内容。  > 在软件和软件的所有副本中都必须包含版权声明和许可声明。  > 此授权条款并非属copyleft的自由软体授权条款，允许在自由/开放源码软体或非自由软体（proprietary software）所使用。
talenguyen/PrettySharedPreferences	Version 1.0.2 ============= ### What's new  * Thread safe  * Reduce most of boilplace code.  * Backward Compatibility: 	+ add commit() method for api level < 9. 	+ make apply() method safe to call by wrap by try catch and call commit() as the fallback when use in api level < 9.  * Add clear() for quick remove all set value.  * Add discard() for drop all changes which was not apply() or commit() yet.  PrettySharedPreferences ======================= [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-PrettySharedPreferences-brightgreen.svg?style=flat)](https://android-arsenal.com/details/1/1118) [![Build Status](https://travis-ci.org/talenguyen/PrettySharedPreferences.svg?branch=master)](https://travis-ci.org/talenguyen/PrettySharedPreferences)  PrettySharedPreferences is a lightweight library for help you deal with SharedPreferences more easy and reduce most of boilplace code.  Features ========   * Easy to use  * Reduce most of boilplace code.  Download ======== ### JAR Download [the latest JAR][1]  or  ### Gradle ```groovy dependencies {     compile 'com.github.talenguyen:prettysharedpreferences:1.0.2' } ```  or  ### Maven ``` xml <dependency>   <groupId>com.github.talenguyen</groupId>   <artifactId>prettysharedpreferences</artifactId>   <version>1.0.2</version> </dependency> ```  Usage =====  Create a class and extends from PrettySharedPreferences class. Let's say PrefManager.  ``` java public class PrefManager extends PrettySharedPreferences<PrefManager> {      public PrefManager(SharedPreferences sharedPreferences) {         super(sharedPreferences);     }      public StringEditor<PrefManager> username() {         return getStringEditor("username");     }          public LongEditor<PrefManager> userId() {         return getLongEditor("userId");     }          /**     * Dynamic key     */     public IntegerEditor<PrefManager> int(String key) {         return getIntegerEditor(key);     }      ... }  ``` Create an instance  ``` java     PrefManager prefManager = new PrefManager(getSharedPreferences("MyPref", MODE_PRIVATE)); ``` To put a single value ``` java     prefManager.username().put("tale").apply(); ``` To put multiple values ``` java     prefManager         .username().put("tale")         .userId().put(1)         .int("key1").put(1)         .int("key2").put(2)         .apply(); ``` To get value ``` java     String username = prefManager.username().getOr("<default value>");     long userId = prefManager.userId().getOr(0l);     int key1 = prefManager.int("key1").getOr(0); ``` That's it. Enjoy!  License =======           Copyright 2014 Giang Nguyen      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at         http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.  [1]: http://search.maven.org/remotecontent?filepath=com/github/talenguyen/prettysharedpreferences/1.0.2/prettysharedpreferences-1.0.2.jar
OpenHFT/Chronicle-Wire	= Wire Format abstraction library Peter Lawrey; Edits by Neil Clifford :toc: manual :css-signature: demo :toc-placement: preamble  The purpose of Chronicle Wire is to combine a number of concerns in a consistent manner:  - Application configuration. (Using YAML) - Data serialization (YAML, binary YAML, JSON, Raw binary data, CSV) - Accessing off-heap memory in a thread-safe manner. (Bind to shared off-heap memory) - High performance data exchange via binary formats. Only include as much meta data as you need.  == Design  Chronicle Wire uses Chronicle Bytes for bytes manipulation, and Chronicle Core for low level JVM access.  === Why are these concerns conflated?  Often you want to use these interchangeably.  - Configuration includes aliased type information.  This supports easy extension, through adding new classes/versions, and cross-platform through type aliasing. - By supporting types, a configuration file can bootstrap itself. You control how the configuration file is decoded. See https://github.com/OpenHFT/Chronicle-Engine/blob/master/demo/src/main/resources/engine.yaml[`engine.yaml`]. - To send the configuration of a server to a client, or from a client to a server. - To store the configuration of a data store in its header. - In configuration, to be able to create any object or component. - Save a configuration after you have changed it. - To be able to share data in memory between processes in a thread-safe manner.  === Features  Chronicle Wire supports a separation of describing what data you want to store and retrieve, and how it should be rendered/parsed. Wire handles a variety of formatting options for a wide range of formats.  A key aim of Chronicle Wire is to support schema changes.  It should make reasonable     attempts to handle:  * optional fields, * fields in a different order, * fields that the consumer doesn't expect; optionally parsing them, or ignoring them * more or less data than expected; in field-less formats * reading a different type to the one written * updating fixed-length fields, automatically where possible using a `bound` data structure.  Chronicle Wire will also be efficient in the case where any, or all, of the following are true:  * fields are in the order expected * fields are the type expected * fields names/numbers are not used * self describing types are not needed * random access of data values is supported.  Chronicle Wire is designed to make it easy to convert from one wire format to another. For example, you can use fixed width binary data in memory for performance, and variable width or text over the network. Different TCP connections could use different formats.    Chronicle Wire also supports hybrid wire formats.  For example, you can have one format embedded in another.  == Support  This library will require Java 8. Support for `C++` and `C#` is planned.  == Text Formats  The text formats include:  * `YAML` - subset of mapping structures included * `JSON` - super set to support serialization * `CSV` - super set to support serialization * `XML` - planned * `FIX` - proposed  Options include:  * field names (e.g. JSON), or field numbers (e.g. FIX) * optional fields with default values that can be dropped * zero copy access to fields - planned * thread safe operations in text - planned  To support wire format discovery, the first byte should be in the `ASCII` range; adding an `ASCII` whitespace if needed.      == Binary Formats  The binary formats include:  * binary YAML. * delta compressing Binary YAML. This is a Chronicle Wire Enterprise feature * typed data without fields * raw untyped fieldless data * BSON (Binary JSON) - planned  Options for Binary format:  * field names or field numbers * variable width * optional fields with a default value can be dropped * fixed width data with zero copy support * thread-safe operations  Note: Chronicle Wire supports debug/transparent combinations like self describing data with zero copy support.  To support wire format discovery, the first bytes should have the top-bit set.  == Using Wire  === Simple use case.  First you need to have a buffer to write to.  This can be a `byte[]`, a `ByteBuffer`, off-heap memory, or even an address and length that you have obtained from some other library.  [source, Java] ---- // Bytes which wraps a ByteBuffer which is resized as needed. Bytes<ByteBuffer> bytes = Bytes.elasticByteBuffer(); ----  Now you can choose which format you are using.  As the wire formats are themselves unbuffered, you can use them with the same buffer, but in general using one wire format is easier.  [source, Java] ---- Wire wire = new TextWire(bytes); // or WireType wireType = WireType.TEXT; Wire wireB = wireType.apply(bytes); // or Bytes<ByteBuffer> bytes2 = Bytes.elasticByteBuffer(); Wire wire2 = new BinaryWire(bytes2); // or Bytes<ByteBuffer> bytes3 = Bytes.elasticByteBuffer(); Wire wire3 = new RawWire(bytes3); ----  So now you can write to the wire with a simple document.  [source, Java] ---- wire.write(() -> "message").text("Hello World")       .write(() -> "number").int64(1234567890L)        .write(() -> "code").asEnum(TimeUnit.SECONDS)       .write(() -> "price").float64(10.50); System.out.println(bytes); ----  prints  [source, yaml] ---- message: Hello World number: 1234567890 code: SECONDS price: 10.5 ----  [source, Java] ---- // the same code as for text wire wire2.write(() -> "message").text("Hello World")         .write(() -> "number").int64(1234567890L)         .write(() -> "code").asEnum(TimeUnit.SECONDS)         .write(() -> "price").float64(10.50);         System.out.println(bytes2.toHexString()); ----  prints  ---- 00000000 C7 6D 65 73 73 61 67 65  EB 48 65 6C 6C 6F 20 57 ·message ·Hello W 00000010 6F 72 6C 64 C6 6E 75 6D  62 65 72 A3 D2 02 96 49 orld·num ber····I 00000020 C4 63 6F 64 65 E7 53 45  43 4F 4E 44 53 C5 70 72 ·code·SE CONDS·pr 00000030 69 63 65 90 00 00 28 41                          ice···(A  ----  Using `RawWire` strips away all the meta data to reduce the size of the message, and improve speed. The down-side is that we cannot easily see what the message contains.  [source, Java] ----         // the same code as for text wire         wire3.write(() -> "message").text("Hello World")                 .write(() -> "number").int64(1234567890L)                 .write(() -> "code").asEnum(TimeUnit.SECONDS)                 .write(() -> "price").float64(10.50);         System.out.println(bytes3.toHexString()); ----  prints in RawWire  ---- 00000000 0B 48 65 6C 6C 6F 20 57  6F 72 6C 64 D2 02 96 49 ·Hello W orld···I 00000010 00 00 00 00 07 53 45 43  4F 4E 44 53 00 00 00 00 ·····SEC ONDS···· 00000020 00 00 25 40                                      ··%@  ----  For more examples see https://github.com/OpenHFT/Chronicle-Wire/blob/master/README-Chapter1.md[Examples Chapter1]  == Binding to a field value  While serialized data can be updated by replacing a whole record, this might not be the most efficient option, nor thread-safe.  Chronicle Wire offers the ability to bind a reference to a fixed value of a field, and perform atomic operations on that field; for example, volatile read/write, and compare-and-swap.  [source, Java] ----    // field to cache the location and object used to reference a field.    private LongValueReference counter = null;         // find the field and bind an approritae wrapper for the wire format.    wire.read(COUNTER).int64(counter, x -> counter = x);         // thread safe across processes on the same machine.    long id = counter.getAndAdd(1); ----  Other types are supported; for example,32-bit integer values, and an array of 64-bit integer values.      == Compression Options  * no compression * Snappy compression - planned * LZW compression - planned  == Bytes options  Wire is built on top of the Bytes library, however Bytes in turn can wrap  * `ByteBuffer` - heap and direct * `byte\[\]` - using `ByteBuffer` * raw memory addresses.   == Handling instance classes of an unknown type  This feature allows Chronicle Wire to de-serialize, manipulate, and serialize an instance class of an unknown type.  If the type is unknown at runtime, a proxy is created; assuming that the required type is an interface.  When the tuple is serialized, it will be give the same type as when it was deserialized, even if that class is not available.  Methods following our `getter`/`setter` convention will be treated as `getters` and `setters`.  This feature is needed for a service that stores and passes on data, for classes it might not have in its class path.  NOTE: This is not garbage collection free, but if the volume is low, this may be easier to work with.  NOTE: This only works when the expected type is not a class.  === Example  [source, Java] ---- @Test public void unknownType() throws NoSuchFieldException {     Marshallable marshallable = Wires.tupleFor(Marshallable.class, "UnknownType");     marshallable.setField("one", 1);     marshallable.setField("two", 2.2);     marshallable.setField("three", "three");     String toString = marshallable.toString();     assertEquals("!UnknownType {\n" +             "  one: !int 1,\n" +             "  two: 2.2,\n" +             "  three: three\n" +             "}\n", toString);     Object o = Marshallable.fromString(toString);     assertEquals(toString, o.toString()); }  @Test public void unknownType2() {     String text = "!FourValues {\n" +             "  string: Hello,\n" +             "  num: 123,\n" +             "  big: 1e6,\n" +             "  also: extra\n" +             "}\n";     ThreeValues tv = Marshallable.fromString(ThreeValues.class, text);     assertEquals(text, tv.toString());     assertEquals("Hello", tv.string());     tv.string("Hello World");     assertEquals("Hello World", tv.string());      assertEquals(123, tv.num());     tv.num(1234);     assertEquals(1234, tv.num());      assertEquals(1e6, tv.big(), 0.0);     tv.big(0.128);     assertEquals(0.128, tv.big(), 0.0);      assertEquals("!FourValues {\n" +             "  string: Hello World,\n" +             "  num: !int 1234,\n" +             "  big: 0.128,\n" +             "  also: extra\n" +             "}\n", tv.toString());  }  interface ThreeValues {     ThreeValues string(String s);     String string();      ThreeValues num(int n);     int num();      ThreeValues big(double d);     double big(); } ----  === Example with `MethodReaders`  [source, Java] ---- @Test public void testUnknownClass() {     Wire wire2 = new TextWire(Bytes.elasticHeapByteBuffer(256));     MRTListener writer2 = wire2.methodWriter(MRTListener.class);      String text = "top: !UnknownClass {\n" +             "  one: 1,\n" +             "  two: 2.2,\n" +             "  three: words\n" +             "}\n" +             "---\n" +             "top: {\n" +             "  one: 11,\n" +             "  two: 22.2,\n" +             "  three: many words\n" +             "}\n" +             "---\n";     Wire wire = new TextWire(Bytes.from(text));     MethodReader reader = wire.methodReader(writer2);     assertTrue(reader.readOne());     assertTrue(reader.readOne());     assertFalse(reader.readOne());     assertEquals(text, wire2.toString()); } ----   == Uses of Chronical Wire  Chronical Wire can be used for:  * file headers * TCP connection headers; where the optimal wire format taht is actually used can be negotiated * message/excerpt contents * Chronicle Queue version 4.x and later * the API for marshalling generated data types.  == Similar projects  === SBE  Simple Binary Encoding (SBE) is designed to be a more efficient replacement for FIX. It is not limited to FIX protocols, and can be easily extended by updating an XML schema. It is simple, binary, and it supports C++ and Java.  XML, when it first started, didn't use XML for its own schema files, and its not    insignificant that SBE doesn't use SBE for its schema either.  This is because it is not trying to be human readable. It has XML which, though standard, isn't designed to be human readable either.  Peter Lawrey thinks that it is a limitation that it doesn't naturally lend itself to a human readable form.     The encoding that SBE uses is similar to binary; with field numbers and fixed width types.  SBE assumes the field types, which can be more compact than Wire's most similar option; though not as compact as others.     SBE has support for schema changes provided that the type of a field doesn't change.     === Message Pack (`msgpack`)  Message Pack is a packed binary wire format which also supports JSON for human readability and compatibility. It has many similarities to the binary (and JSON) formats of this library.  Chronicle Wire is designed to be human readable first, based on YAML, and has a range of options to make it more efficient. The most extreme being fixed position binary.      Message Pack has support for embedded binary, whereas Chronicle Wire has support for comments and hints, to improve rendering for human consumption.      The documentation looks well thought out, and it is worth emulating.  === Comparison with Cap'n'Proto  |=============== | Feature                          | Wire Text              | Wire Binary         | Protobuf               | Cap'n Proto             | SBE                 | FlatBuffers | Schema evolution                 | yes                    | yes                 | yes                    | yes                     | caveats             | yes | Zero-copy                        | yes                    | yes                 | no                     | yes                     | yes                 | yes | Random-access reads              | yes                    | yes                 | no                     | yes                     | no                  | yes | Random-access writes             | yes                    | yes                 | no                     | ?                       | no                  | ? | Safe against malicious input     | yes                    | yes                 | yes                    | yes                     | yes                 | opt-in / upfront | Reflection / generic algorithms  | yes                    | yes                 | yes                    | yes                     | yes                 | yes | Initialization order             | any                    | any                 | any                    | any                     | preorder            | bottom-up | Unknown field retention          | yes                    | yes                 | yes                    | yes                     | no                  | no | Object-capability RPC system     | yes                    | yes                 | no                     | yes                     | no                  | no | Schema language                  | no                     | no                  | custom                 | custom                  | XML                 | custom | Usable as mutable state          | yes                    | yes                 | yes                    | no                      | no                  | no | Padding takes space on wire?     | optional               | optional            | no                     | optional                | yes                 | yes | Unset fields take space on wire? | optional               | optional            | no                     | yes                     | yes                 | no | Pointers take space on wire?     | no                     | no                  | no                     | yes                     | no                  | yes | C++                              | planned                | planned             | yes                    | yes (C++11)*            | yes                 | yes | Java                             | Java 8                 | Java 8              | yes                    | yes*                    | yes                 | yes | C#                               | yes                    | yes                 | yes                    | yes*                    | yes                 | yes* | Go                               | no                     | no                  | yes                    | yes                     | no                  | yes* | Other languages                  | no                     | no                  | 6+                     | others*                 | no                  | no | Authors' preferred use case      | distributed  computing | financial / trading | distributed  computing | platforms /  sandboxing | financial / trading | games |===============  NOTE: The Binary YAML format can be automatically converted to YAML without any knowledge of the schema, because the messages are self-describing.  NOTE: You can parse all the expected fields (if any) and then parse any remaining fields. As YAML supports object field names (or keys), these could be strings or even objects as keys and values.  Based on https://capnproto.org/news/2014-06-17-capnproto-flatbuffers-sbe.html  Note: It not clear what padding which doesn't take up space on the wire means.  == Design notes.  See https://capnproto.org/news/2014-06-17-capnproto-flatbuffers-sbe.html for a comparison to other encoders.  === Schema evolution.  Wire optionally supports:  - field name changes - field order changes - capturing or ignoring unexpected fields - setting of fields to the default, if not available - raw messages can be longer or shorter than expected  The more flexibility, the larger the overhead in terms of CPU and memory.   Wire allows you to dynamically pick the optimal configuration, and convert between these options.  === Zero copy.  Chronicle Wire supports zero-copy random access to fields, and direct-copy from in-memory to the network. It also supports translation from one wire format to another. For example, switching between fixed-length data and variable-length data.  === Random Access.  You can access a random field in memory, For example, in a 2TB file, page in/pull into CPU cache, only the data relating to your read or write.  [options="header"] |=============== | format | access style  | fixed-length binary | random access without parsing first | variable-length binary | random access with partial parsing allowing you to skip large portions | fixed-length text | random access with parsing | variable-length text | no random access |===============  Chronicle Wire references are relative to the start of the data contained, to allow loading in an arbitrary point in memory.  === Safe against malicious input.  Chronicle Wire has built in tiers of bounds checks to prevent accidental read/writing that corrupts the data. It is not complete enough for a security review.     === Reflection / generic algorithms.  Chronicle Wire supports generic reading and writing of an arbitrary stream. This can be used in combination with predetermined fields. For example, you can read the fields you know about, and ask it to provide the fields that you don't. You can also give generic field names like keys to a map as YAML does.  === Initialization order.  Chronicle Wire can handle unknown information like lengths, by using padding. It will go back and fill in any data that it wasn't aware of when it was writing the data. For example, when it writes an object, it doesn't know how long it is going to be, so it adds padding at the start. Once the object has been written, it goes back and overwrites the length. It can also handle situations where the length was more than needed; this is known as packing.  === Unknown field retention?  Chronicle Wire can read data that it didn't expect, interspersed with data it did expect. Rather than specify the expected field name, a `StringBuilder` is provided.  Note: There are times when you want to skip/copy an entire field or message, without reading any more of it.  This is also supported.  === Object-maximumLimit RPC system.  Chronicle Wire supports references based on name, number, or UUID. This is useful when including a reference to an object taht the reader should look up by other means.     A common case is if you have a proxy to a remote object, and you want to pass or return this in an RPC call.  === Schema language  Chronicle Wire's schema is not externalised from the code. However it is planned to use YAML in a format that it can parse.  === Usable as mutable state  Chronicle Wire supports storing an application's internal state. This will not allow it to grow or shrink. You can't free any of it without copying the pieces that you need, and discarding the original copy.      === Padding takes space on the wire.  The Chronicle Wire format that is chosen determines if there is any padding on the wire. If you copy the in-memory data directly, its format doesn't change.  If you want to drop padding, you can copy the message to a wire format without padding. You can decide whether the original padding is to be preserved or not, if turned back into a format with padding.  We could look at supporting *Cap'n'Proto*'s zero-byte removal compression.  === Un-set fields take space on the wire?  Chronicle Wire supports fields with, and without, optional fields and automatic means of removing them. Chronicle Wire doesn't support automatically adding them back in, because information has been lost.  === Pointers take space on the wire.  Chronicle Wire doesn't have pointers, but it does have content-lengths, which are a useful hint for random access and robustness; but these are optional.  ===  Platform support  Chronicle Wire supports `Java 8`.  Future versions may support `Java 6`, `C++` and `C#`.
google/google-auth-library-java	Google Auth Library ===================  Open source authentication client library for Java.  [![Build Status](https://travis-ci.org/google/google-auth-library-java.svg?branch=master)](https://travis-ci.org/google/google-auth-library-java.svg) [![Maven](https://img.shields.io/maven-central/v/com.google.auth/google-auth-library-credentials.svg)](https://img.shields.io/maven-central/v/com.google.auth/google-auth-library-credentials.svg)  -  [API Documentation] (https://google.github.io/google-auth-library-java/releases/0.7.1/apidocs)  This project consists of 3 artifacts:  -  [*google-auth-library-credentials*](#google-auth-library-credentials): contains base classes and interfaces for Google credentials -  [*google-auth-library-appengine*](#google-auth-library-appengine): contains App Engine credentials. This artifacts depends on the App Engine SDK -  [*google-auth-library-oauth2-http*](#google-auth-library-oauth2-http): contains a wide variety of credentials as well as utility methods to create them and to get Application Default Credentials  > Note: This client is a work-in-progress, and may occasionally > make backwards-incompatible changes.  Quickstart ----------  If you are using Maven, add this to your pom.xml file (notice that you can replace `google-auth-library-oauth2-http` with any of `google-auth-library-credentials` and `google-auth-library-appengine`, depending on your application needs): ```xml <dependency>   <groupId>com.google.auth</groupId>   <artifactId>google-auth-library-oauth2-http</artifactId>   <version>0.7.1</version> </dependency> ``` If you are using Gradle, add this to your dependencies ```Groovy compile 'com.google.auth:google-auth-library-oauth2-http:0.7.1' ``` If you are using SBT, add this to your dependencies ```Scala libraryDependencies += "com.google.auth" % "google-auth-library-oauth2-http" % "0.7.1" ```  google-auth-library-credentials -------------------------------  This artifact contains base classes and interfaces for Google credentials: - `Credentials`: base class for an authorized identity. Implementations of this class can be used to authorize your application - `RequestMetadataCallback`: interface for the callback that receives the result of the asynchronous `Credentials.getRequestMetadata(URI, Executor, RequestMetadataCallback)` - `ServiceAccountSigner`: interface for a service account signer. Implementations of this class are capable of signing byte arrays using the credentials associated to a Google Service Account  google-auth-library-appengine ----------------------------- This artifact depends on the App Engine SDK (`appengine-api-1.0-sdk`) and should be used only by applications running on App Engine. The `AppEngineCredentials` class allows to authorize your App Engine application given an instance of [AppIdentityService](https://cloud.google.com/appengine/docs/java/javadoc/com/google/appengine/api/appidentity/AppIdentityService).  google-auth-library-oauth2-http -------------------------------  This artifact contains a wide variety of credentials as well as utility methods to create them and to get Application Default Credentials. Credentials classes contained in this artifact are: - `CloudShellCredentials`: credentials for Google Cloud Shell built-in service account - `CloudShellCredentials`: credentials for Google Compute Engine built-in service account - `OAuth2Credentials`: base class for OAuth2-based credentials - `ServiceAccountCredentials`: credentials for a Service Account - use a JSON Web Token (JWT) to get access tokens - `ServiceAccountJwtAccessCredentials`: credentials for a Service Account - use JSON Web Token (JWT) directly in the request metadata to provide authorization - `UserCredentials`: credentials for a user identity and consent  To get Application Default Credentials use `GoogleCredentials.getApplicationDefault()` or `GoogleCredentials.getApplicationDefault(HttpTransportFactory)`. These methods return the Application Default Credentials which are used to identify and authorize the whole application. The following are searched (in order) to find the Application Default Credentials:  1. Credentials file pointed to by the `GOOGLE_APPLICATION_CREDENTIALS` environment variable 2. Credentials provided by the Google Cloud SDK `gcloud auth application-default login` command 3. Google App Engine built-in credentials 4. Google Cloud Shell built-in credentials 5. Google Compute Engine built-in credentials    - Skip this check by setting the environment variable `NO_GCE_CHECK=true`    - Customize the GCE metadata server address by setting the environment variable `GCE_METADATA_HOST=<hostname>`  To get Credentials from a Service Account JSON key use `GoogleCredentials.fromStream(InputStream)` or `GoogleCredentials.fromStream(InputStream, HttpTransportFactory)`.  Contributing ------------  Contributions to this library are always welcome and highly encouraged.  See [CONTRIBUTING](CONTRIBUTING.md) documentation for more information on how to get started.  Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms. See [Code of Conduct](CODE_OF_CONDUCT.md) for more information.  License -------  BSD 3-Clause - See [LICENSE](LICENSE) for more information.
QianmiOpen/dubbo-rpc-jsonrpc	[![Build Status](https://travis-ci.org/QianmiOpen/dubbo-rpc-jsonrpc.svg)](https://travis-ci.org/QianmiOpen/dubbo-rpc-jsonrpc)    ## Why HTTP 在互联网快速迭代的大潮下，越来越多的公司选择nodejs、django、rails这样的快速脚本框架来开发web端应用 而后端的服务用Java又是最合适的，这就产生了大量的跨语言的调用需求。   而http、json是天然合适作为跨语言的标准，各种语言都有成熟的类库     虽然Dubbo的异步长连接协议效率很高，但是在脚本语言中，这点效率的损失并不重要。     ## Why Not RESTful Dubbox 在RESTful接口上已经做出了尝试，但是REST架构和dubbo原有的RPC架构是有区别的，   区别在于REST架构需要有资源(Resources)的定义， 需要用到HTTP协议的基本操作GET、POST、PUT、DELETE对资源进行操作。   Dubbox需要重新定义接口的属性，这对原有的Dubbo接口迁移是一个较大的负担。   相比之下，RESTful更合适互联网系统之间的调用，而RPC更合适一个系统内的调用，   所以我们使用了和Dubbo理念较为一致的JsonRPC   dubbo-rpc-jsonrpc =====================  ## maven依赖： ```xml <dependency>     <groupId>com.qianmi</groupId>     <artifactId>dubbo-rpc-jsonrpc</artifactId>     <version>1.0.1</version> </dependency>  ```  ## 配置： Define jsonrpc protocol: ```xml  <dubbo:protocol name="jsonrpc" port="8080" server="jetty" /> ```  Set default protocol: ```xml <dubbo:provider protocol="jsonrpc" /> ```  Set service protocol: ```xml <dubbo:service protocol="jsonrpc" /> ```  Multi port: ```xml <dubbo:protocol id="jsonrpc1" name="jsonrpc" port="8080" /> <dubbo:protocol id="jsonrpc2" name="jsonrpc" port="8081" /> ``` Multi protocol: ```xml <dubbo:protocol name="dubbo" port="20880" /> <dubbo:protocol name="jsonrpc" port="8080" /> ``` <!-- 使用多个协议暴露服务 --> ```xml <dubbo:service id="helloService" interface="com.alibaba.hello.api.HelloService" version="1.0.0" protocol="dubbo,jsonrpc" /> ```   Jetty Server: (default) ```xml <dubbo:protocol ... server="jetty" />  或jetty的最新版： <dubbo:protocol ... server="jetty9" />  ``` Maven: ```xml <dependency>   <groupId>org.mortbay.jetty</groupId>   <artifactId>jetty</artifactId>   <version>6.1.26</version> </dependency> ```  Servlet Bridge Server: (recommend) ```xml <dubbo:protocol ... server="servlet" />  ```  web.xml： ```xml <servlet>          <servlet-name>dubbo</servlet-name>          <servlet-class>com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet</servlet-class>          <load-on-startup>1</load-on-startup> </servlet> <servlet-mapping>          <servlet-name>dubbo</servlet-name>          <url-pattern>/*</url-pattern> </servlet-mapping> ``` 注意，如果使用servlet派发请求：  协议的端口```<dubbo:protocol port="8080" />```必须与servlet容器的端口相同， 协议的上下文路径```<dubbo:protocol contextpath="foo" />```必须与servlet应用的上下文路径相同。  CORS跨源支持: ```xml <dubbo:protocol name="jsonrpc" ...  /> 	<dubbo:parameter key="cors" value="true" /> </dubbo:protocol> ``` -------------- ## Example  JAVA API ```java public interface PhoneNoCheckProvider {     /**      * 校验号码是否受限      * @param operators 运营商      * @param no 号码      * @param userid 用户编号      * */     boolean isPhoneNoLimit(Operators operators, String no, String userid); } ``` Client ```shell curl -i -H 'content-type: application/json' -X POST -d '{"jsonrpc": "2.0", "method": "isPhoneNoLimit", "params": [ "MOBILE", "130000", "A001"],          "id": 1 }' 'http://127.0.0.1:18080/com.qianmi.api.PhoneNoCheckProvider' ```  Python Client Example ```python import httplib import json  __author__ = 'caozupeng'   def raw_client(app_params):     headers = {"Content-type": "application/json-rpc",                "Accept": "text/json"}     h1 = httplib.HTTPConnection('172.19.32.135', port=18080)     h1.request("POST", '/com.qianmi.ofdc.api.phone.PhoneNoCheckProvider', json.dumps(app_params), headers)     response = h1.getresponse()     return response.read()   if __name__ == '__main__':     app_params = {         "jsonrpc": "2.0",         "method": "isPhoneNoLimit",         "params": ["MOBILE", "130000", "A001"],         "id": 1     }     print json.loads(raw_client(app_params), encoding='utf-8') ```  ## Python客户端 https://github.com/QianmiOpen/dubbo-client-py  ## Nodejs客户端 https://github.com/QianmiOpen/dubbo-node-client  ## 客户端服务端Example   https://github.com/JoeCao/dubbo_jsonrpc_example   使用docker运行  ## 浏览器调用 需按前述开启CORS支持, 可使用 https://github.com/datagraph/jquery-jsonrpc  ## 文档资料  [JSON-RPC 2.0 规范](http://www.jsonrpc.org/specification)    [jsonrpc4j](https://github.com/briandilley/jsonrpc4j)    [dubbo procotol](http://www.dubbo.io/Protocol+Reference-zh.htm)
hot13399/spring-mvc-mini	Spring-mvc-mini是一个完整的，轻量、简单的Java项目，基于Spring MVC.  里面有基本的增删改查的功能。而且无需修改任何内容，就可以直接跑起来。    另外我有一个Spring MVC的RESTful项目，数据库是Mysql，请参看：https://github.com/hot13399/spring-mvc-REST    -------------------  在这个项目里主要实现了以下技术：  * spring-webmvc  * svnkit  * jgit  * javax.mail  * jasypt  * dom4j  * spring scheduler    如何运行：  -------------------    在Windows的CMD：        $ cd spring-mvc-mini      $ mvn tomcat7:run [-Dmaven.tomcat.port=<port no.>] (In case 8080 is busy]     通过浏览器打开：http://localhost:8080/spring-mvc-mini    如果你想要学习或贡献和这个项目：    就通过maven把它build成一个IDE项目，执行以下命令，打开CMD：        $ cd spring-mvc-mini      $ mvn eclipse:eclipse or mvn idea:idea    通过Eclipse或IDEA导入即可。    Note:  -------------------     如果你要在Linux环境运行，以下的文件需要修改。        $ spring-oss-mini\src\main\webapp\WEB-INF\spring\appServlet\servlet-context.xml:<context:property-placeholder   	$ location="file:/opt/web/spring-mvc-mini/resources/application.properties"/>  	$ spring-oss-mini\src\main\resources\logback.xml  	$ spring-oss-mini\resources\application.properties     最后通过Maven build一个war包部署即可。        --------------------  Spring-mvc-mini is a mini project using Spring MVC.    In this project, you can see the code of:  * spring-webmvc  * svnkit  * javax.mail  * jasypt  * dom4j  * spring scheduler    To run the application:  From the command line with Maven:      $ cd spring-mvc-mini      $ mvn tomcat7:run [-Dmaven.tomcat.port=<port no.>] (In case 8080 is busy]    Access the deployed web application at: http://localhost:8080/spring-mvc-mini    To contribute to this project:  In your preferred IDE such as Eclipse:      $ cd spring-mvc-mini      $ mvn eclipse:eclipse    Import spring-mvc-mini as a Maven Project    If you want to deploy this project to Linux server, you might need to edit conf files:        $ spring-oss-mini\src\main\webapp\WEB-INF\spring\appServlet\servlet-context.xml:<context:property-placeholder  	$ location="file:/opt/web/spring-mvc-mini/resources/application.properties"/>  	$ spring-oss-mini\src\main\resources\logback.xml  	$ spring-oss-mini\resources\application.properties
SundeepK/CompactCalendarView	# CompactCalendarView [![Build Status](https://travis-ci.org/SundeepK/CompactCalendarView.svg?branch=master)](https://travis-ci.org/SundeepK/CompactCalendarView) CompactCalendarView is a simple calendar view which provides scrolling between months. It's based on Java's Date and Calendar classes. It provides a simple api to query for dates and listeners for specific events.  For example, when the calendar has scrolled to a new month or a day has been selected. Still under active development.  <img src="https://github.com/SundeepK/CompactCalendarView/blob/master/images/compact-calendar-view-example-multi-events.png" width="500">  # Contributing   Please raise an issue of the requirement so that a discussion can take before any code is written, even if you intend to raise a pull request. Please see setup for testing.  # Testing CompactCalendarView makes use of screenshot-tests-for-android (https://github.com/facebook/screenshot-tests-for-android). This is for UI testing. Since screenshot-tests-for-android takes screenshots, we need a way to ensure images can be reproduced consistently. To do this, a specific emulator is used to run tests. Unfortunately, an older emulator is used for now. New pull requests which change functionality some how should aim to create new screenshot tests or unit tests if possible. To run this locally, run the below commands:  Pre-requisite (Also refer to .travis.yml): - Python - Python pillow installed  - Install android-19 (can be done through android sdk manager or command line).  Android 19 emulator is used because it seems to be a fast enough on travis-ci and because x86 emulators are not supported on travis-ci. Newer android version is possible but build times will increase.  Install the abi and accept:  ```bash $ android update sdk --no-ui --all --filter sys-img-armeabi-v7a-android-19  ```  Create the emulator: ```bash $ echo no | android create avd --force -n testCompactCalendarEmulator -t android-19 --abi armeabi-v7a ```  Create sd card (creating in current dir): Any problems with sdcard are best solved by deleting and trying again ```bash $ mksdcard -l sdcard 100M sdcard ```  Run emulator (with out audio and window): ```bash $ emulator -avd testCompactCalendarEmulator -no-audio -no-window -sdcard sdcard & ```  Run emulator and watch(with audio and window): ```bash $ emulator -avd testCompactCalendarEmulator -sdcard sdcard  ```  Running the tests to verify that the current tests pass and to check which tests are not producing the same screenshot: ```bash $ ./gradlew verifyMode screenshotTests  ```  To generate new screenshots if new tests have been added: ```bash $ ./gradlew recordMode screenshotTests  ```  Run the unit tests like below: ```bash $ ./gradlew test ```  ## Android studio emulator It's possible to test using android studio emulator. However, it must be android 19 and and 480x800 screen resolution. One example is the Nexus S emulator. Just start the emulator and execute the gradle commands to run the tests. Emulator should be found automatically.  # Open/Close animations The library supports opening/closing with or without animations.   ![ScreenShot](https://github.com/SundeepK/CompactCalendarView/blob/master/images/compact_calendar_animation.gif)  # Example usage It is possible to change the appearance of the view via a few properties. This includes the background color, text color, textsize color of the current day and the color of the first day of the month.   ```xml     <com.github.sundeepk.compactcalendarview.CompactCalendarView         xmlns:app="http://schemas.android.com/apk/res-auto"         android:id="@+id/compactcalendar_view"         android:layout_width="fill_parent"         android:paddingRight="10dp"         android:paddingLeft="10dp"         android:layout_height="250dp"         app:compactCalendarTargetHeight="250dp"         app:compactCalendarTextSize="12sp"         app:compactCalendarBackgroundColor="#ffe95451"         app:compactCalendarTextColor="#fff"         app:compactCalendarCurrentSelectedDayBackgroundColor="#E57373"         app:compactCalendarCurrentDayBackgroundColor="#B71C1C"         app:compactCalendarMultiEventIndicatorColor="#fff"         />  ```  Please see Sample app for full example.  ```java     // ... code omitted for brevity              @Override     protected void onCreate(Bundle savedInstanceState) {         super.onCreate(savedInstanceState);         setContentView(R.layout.activity_main);         final CompactCalendarView compactCalendarView = (CompactCalendarView) findViewById(R.id.compactcalendar_view);         // Set first day of week to Monday, defaults to Monday so calling setFirstDayOfWeek is not necessary         // Use constants provided by Java Calendar class         compactCalendarView.setFirstDayOfWeek(Calendar.MONDAY);                 // Add event 1 on Sun, 07 Jun 2015 18:20:51 GMT         Event ev1 = new Event(Color.GREEN, 1433701251000L, "Some extra data that I want to store.");         compactCalendar.addEvent(ev1);          // Added event 2 GMT: Sun, 07 Jun 2015 19:10:51 GMT         Event ev2 = new Event(Color.GREEN, 1433704251000L);         compactCalendar.addEvent(ev2);          // Query for events on Sun, 07 Jun 2015 GMT.          // Time is not relevant when querying for events, since events are returned by day.          // So you can pass in any arbitary DateTime and you will receive all events for that day.         List<Event> events = compactCalendar.getEvents(1433701251000L); // can also take a Date object                  // events has size 2 with the 2 events inserted previously         Log.d(TAG, "Events: " + events);          // define a listener to receive callbacks when certain events happen.         compactCalendarView.setListener(new CompactCalendarView.CompactCalendarViewListener() {             @Override             public void onDayClick(Date dateClicked) {                 List<Event> events = compactCalendarView.getEvents(dateClicked);                 Log.d(TAG, "Day was clicked: " + dateClicked + " with events " + events);             }              @Override             public void onMonthScroll(Date firstDayOfNewMonth) {                 Log.d(TAG, "Month was scrolled to: " + firstDayOfNewMonth);             }         });     }  ```  You can modify indicators using a preset of styles, below is an example, but few other combinations are also possible:  ![ScreenShot](https://github.com/SundeepK/CompactCalendarView/blob/master/images/compact-calendar-customised-indicators.png)  Note that the calendar makes no attempt to de-duplicate events for the same exact DateTime. This is something that you must handle your self if it is important to your use case.  # Locale specific settings It's possible to set the locale so that weekday column names are automatically set by the calendar. ```java         CompactCalendarView compactCalendarView = (CompactCalendarView) findViewById(R.id.compactcalendar_view);         compactCalendarView.setLocale(Locale.CHINESE);         compactCalendarView.setUseThreeLetterAbbreviation(true); ```  <img src="https://github.com/SundeepK/CompactCalendarView/blob/master/images/chinese-locale-daynames.png" width="400">  ```gradle dependencies {     compile 'com.github.sundeepk:compact-calendar-view:2.0.2.2' } ```  RTL support beta for right-to-left languages ```gradle dependencies {     compile 'com.github.sundeepk:compact-calendar-view:2.0.3-beta' } ```  ``` The MIT License (MIT)  Copyright (c) [2017] [Sundeepk]  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```
googlesamples/android-credentials	# android-credentials Android samples for the `Credentials` API, which allows developers to access SmartLock for Passwords. For more information, visit https://developers.google.com/identity/smartlock-passwords/android/.
Imaginea/bot-bot	#bot-bot Android automation testing tool with record and replay features  ------- ##What is bot-bot? Bot-bot is an Opensource android automation testing tool. Bot-bot only needs an apk of the application that needs to be tested. It integrates itself automatically to the application for recording and test execution purpose. It provides features to record, store and run user-actions as test-case on an andorid app.More Info at [http://imaginea.github.com/bot-bot/index.html](http://imaginea.github.com/bot-bot/index.html)   If you want to try bot-bot go to the [Get started](http://imaginea.github.com/bot-bot/pages/get_started.html) section.  ------------ ##Features  Following are some of the main features of bot-bot:  - Need only apk file of the application under test without the need of the source code. - Record user actions on simulator or using an actual phone. - Directly use Robotium/Nativedriver functions as keywords in testcases. - Edit/modify recorded cases on the server directly. - Allow the recorded cases to be exported in csv format - Data-driven support for any test-case - Emailable html report generation for test execution.  ------------------- ##Download  Please go to the download page on github to download the latest version of bot-bot.  [https://github.com/Imaginea/bot-bot/downloads](https://github.com/Imaginea/bot-bot/downloads)  ------------ ##Google group  For discussions and queries you can join bot-bot google group  [http://groups.google.com/group/bot-bot-users](http://groups.google.com/group/bot-bot-users)
bupt1987/JgFramework	JgFramework ===========  基于Netty的强大的游戏服务器框架  特点 === ~~~ 1、使用netty作为底层，性能得到保障 2、支持socket和websocket2种连接模式，可以自由选择 3、简单实用的路由，方便handler开发 4、异步无锁的数据同步 5、spring+Hibernia的自动注入和事务管理 6、mysql数据库的读写分离以及对多数据库的读写支持 7、传输数据的多样化，可以自用实现接口来实现不同的传输数据类型，现在使用的是json 8、框架自带了认证服务和管理服务，有简单的管理命令，需要telnet连接到端口（默认38080） 9、方便的ip段限制功能 10、方便设置心跳检查 11、登录方和服务端之间的认证，可通过实际接口来完成，默认使用的rsa加密传输认证码 12、性能优良的排队系统 13、丰富的工具类：memcache、redis等 14、基本游戏模块的抽象 ~~~  性能测试 ====== ##### *测试信息* ~~~ cpu : AMD A10 内存 : 8G 测试项目 : https://github.com/bupt1987/JgWeb    测试脚本 : src/test/java/client/TestWebSocket.java 测试方式 : 自压 ~~~ ##### *测试结果* ~~~ 在100个登录用户，每个用户在登录完成，再init操作之后，每个用户发送100000个请求，    得到每秒处理请求数在3.5W左右。 ~~~  欢迎加入 ====== 如果对该系统有兴趣可以发邮件至 bupt1987@gmail.com 一起探讨，欢迎加入   例子 === 见：https://github.com/bupt1987/JgWeb
societies/SOCIETIES-Platform	The vision of the SOCIETIES project is to research and develop a concept of Ambient Intelligent (AmI) Communities that extend ambient intelligent or pervasive systems beyond the individual to dynamic communities of users. Driven by context awareness, preference learning and privacy protection, AmI Communities will intelligently connect people & things to communicate, share, consume and organise communities. SOCIETIES will embrace online community services, such as Social Networking, thus offering new and powerful ways of working, communicating and socialising. The project intends developing and trialling use cases for disaster management, university living and conference support.  SOCIETIES platform software includes the SOCIETIES core modules and the modules related to individual and community experience. These modules provide a prototype implementation that will realise the concept of an AmI Community and therefore supporting the third party services and end user scenarios that will be available in the SOCIETIES-SCE-Services repository.  This software is licensed under a BSD 2-Clause "Simplified" or "FreeBSD" License.
rinde/RinSim	# RinSim 4.4.2  RinSim is an extensible logistics simulator with support for (de)centralized algorithms for pickup and delivery problems and AGV routing. The simulator focuses on __simplicity__ and __consistency__ making it ideal for performing scientific simulations. Further, software quality is a priority resulting in an ever improving test suite and documentation.  [![Javadocs](https://javadoc.io/badge/com.github.rinde/rinsim-core.svg?color=red)](https://javadoc.io/doc/com.github.rinde/rinsim-core) [![Build Status](https://travis-ci.org/rinde/RinSim.svg?branch=master)](https://travis-ci.org/rinde/RinSim)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.rinde/rinsim-core/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.github.rinde/rinsim-core) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.571471.svg)](https://doi.org/10.5281/zenodo.571471) [![Stackoverflow](https://img.shields.io/badge/stackoverflow-rinsim-orange.svg)](http://stackoverflow.com/questions/tagged/rinsim)  <!-- ![PDPModel](docs/topbar.png) --> ![Taxi Demo](docs/taxi-demo.gif)  ## Installation  RinSim uses [Maven](http://maven.apache.org/) for managing its dependencies. RinSim can be added to your Maven project by including the following in your pom file, where x and y represents the preferred version number. More __[detailed instructions](docs/howtorun.md)__ are available. ```xml <dependency> 	<groupId>com.github.rinde</groupId> 	<artifactId>rinsim-core</artifactId> 	<version>4.x.y</version> </dependency> ```	 Other modules can be added similarly: ```xml <dependency> 	<groupId>com.github.rinde</groupId> 	<artifactId>rinsim-ui</artifactId> 	<version>4.x.y</version> </dependency> <dependency> 	<groupId>com.github.rinde</groupId> 	<artifactId>rinsim-experiment</artifactId> 	<version>4.x.y</version> </dependency> ```		 For more detailed instructions on how create a Maven project in Eclipse and add RinSim as a dependency see the [instructions](docs/howtorun.md). For release notes of the latest release click [here](releasenotes.md).   ## Getting Started  Once the simulator is installed, you are ready to explore the simulator. It is recommended to start by running and studying the [examples](example/README.md). When using Maven in Eclipse, the RinSim JavaDocs are automatically made available making exploration of the code much easier. The remainder of this page gives a high level overview of the simulator. If you have questions or like to stay up to date about RinSim you can subscribe to the mailing list at [this page](https://groups.google.com/forum/?fromgroups=#!forum/rinsim) or ask them on [StackOverflow](https://stackoverflow.com/) (make sure to use the [RinSim tag](https://stackoverflow.com/questions/tagged/rinsim) and the version number that you use).  ## About RinSim is developed at [AgentWise](http://distrinet.cs.kuleuven.be/research/taskforces/agentwise) in the [imec-DistriNet group](http://distrinet.cs.kuleuven.be/) at the [Department of Computer Science, KU Leuven, Belgium](http://www.cs.kuleuven.be/). The lead developer is [Rinde van Lon](http://distrinet.cs.kuleuven.be/people/rinde). Valuable contributions were made by Bartosz Michalik and Robrecht Haesevoets.  RinSim is used in both research and education. Several publications rely on RinSim for their experiments and RinSim is used in a course on multi-agent systems as a testbed for students.  ## Open source RinSim is open source under the [Apache License Version 2.0](LICENSE). From version 3.0.0 RinSim uses [semantic versioning](http://semver.org/), this means that if you are using features introduced in version X.Y.Z it is safe to upgrade to any version greater than X.Y.Z but smaller than X+1.0.0.   ## Design Overview  This section gives a brief overview of the most important elements of the simulator. For a deeper understanding you can have a look at the examples, the source code, and the tests.  In RinSim terminology, the parts of the simulation that define the problem and environment are called models, the parts of the simulation that solve the problem (i.e. the solution, the collective adaptive system) are called agents. The design of RinSim allows for easy use and recombination of models to configure a simulation problem. When the problem is configured, the programmer can focus on solving the actual problem by designing a collective adaptive system without having to worry about accidentally violating simulation consistency. Actions that agents can take are considered to be part of the problem not the solution, e.g. a vehicle that can pickup things or can communicate with other vehicles. Actions define the problem space in which a good solution has to be found.  ### Simulator  The [Simulator](core/src/main/java/com/github/rinde/rinsim/core/Simulator.java) is the heart of RinSim. Its main concern is to simulate time. This is done in a discrete manner. Time is divided in ticks of a certain length, which is chosen upon initializing the simulator (see examples and code).  Of course time on its own is not so useful, so we can register objects in the simulator, such as objects implementing the  [TickListener](core/src/main/java/com/github/rinde/rinsim/core/model/time/TickListener.java) interface. These objects will listen to the internal clock of the simulator. You can also register other objects, as we will see in a moment.  Once started, the simulator will start to tick, and with each tick it will call all registered tickListeners, in turn, to perform some actions within the length of the time step. Time consistency is enforced by the [TimeLapse]( core/src/main/java/com/github/rinde/rinsim/core/model/time/TimeLapse.java) objects. Each _TickListener_ receives a single _TimeLapse_ object every tick, the time in this object can be 'spent' on actions. This spending can be done only once, as such an agent can not violate the time consistency in the simulator. For example, calling _RoadModel#moveTo(..)_ several times will have no effect. As you can see there is also an _afterTick_, but we'll ignore this for now.  Apart from simulating time, the simulator has little functionality on its own. All additional functionality (such as movement, communication, etc.) that is required by your simulation, should be delegated to models. These models can be easily plugged (or registered) in the simulator.  ### Models  Out of the box, RinSim comes with three basic models: _RoadModel_, _CommunicationModel_ and _PDPModel_. When this is not enough, it is easy to define your own custom model.  * __RoadModel__: simulates a physical road structure. The _RoadModel_ allows to place and move objects (_RoadUsers_) on roads. It comes in two flavors: 	* __GraphRoadModel__: A graph based road model, objects can only move on edges of the graph. Several maps are currently available [here](http://people.cs.kuleuven.be/~rinde.vanlon/rinsim/maps/). 	* __PlaneRoadModel__: A plane based road model, objects can move anywhere within the plane. * __PDPModel__: the pickup-and-delivery model. The model collaborates with the _RoadModel_, the models comes with three different _RoadUsers_: _Vehicle_, _Parcel_ and _Depot_. _Vehicles_ can transport _Parcels_ from and to _Depots_. The model enforces capacity constraints, time windows and position consistency. * __CommunicationModel__: simulates simple message-based communication between objects implementing the _CommunicationUser_ interface. It supports both direct messaging and broadcasting. It can also take distance, communication radius, and communication reliability into account. Messages between agents are send asynchronously.  ![RoadModel](docs/RoadModel.png) ![PDPModel](docs/PDPModel.png) ![CommunicationModel](docs/CommunicationModel.png)  ### GUI  The GUI can be configured using the [View](ui/src/main/java/com/github/rinde/rinsim/ui/View.java) class. The GUI can be customized using _Renderers_.  * __View__: is responsible for rendering the simulator. Specific renderers can be added for each model, for the provided models there exist default renderers.  * __Renderer__: is responsible for rendering one model (or more). Examples are the _RoadUserRenderer_ to do basic rendering of objects in the _RoadModel_, or _MessagingLayerRenderer_ to visualize messages between agents. When introducing new models you can create new custom renderers for these models.  ### Simulation Entities  Simulation entities are entities that are the actual objects in our simulation, such as agents, trucks, and packages. They can implement the _TickListener_ interface and/or other interfaces to use additional models. Once registered in the simulator, the simulator will make sure they receive ticks (if required) and are registered in all required models.  <!-- ## Git and Maven This section assumes that you are using [Eclipse](http://www.eclipse.org) with [m2e](http://eclipse.org/m2e/) and optionally [eGit](http://www.eclipse.org/egit/). Installation instructions for each can be found on their respective websites.  ### Using eGit  * Go to _File -> Import..._ * Select _Git -> Projects from Git_ and click _next_. * Select _URI_ and click _next_. * Enter ```` git@github.com:rinde/RinSim.git ```` in the URI field, select _https_ as protocol, and click _next_. * Select the __v2__ branch and click _next_. * Choose a local directory for your project and click _next_. * Wait for eGit to download the project. * Make sure _Import existing projects_ is selected and click _next_. * Click _finish_.  You will now have one project in eclipse. See _Importing the Maven projects in eclipse_ on how to actually use it.  To update the simulator later on, right-click on the top-level project, go to _Team_ and select and select _Pull_.   ### Using Git (commandline)  * Open a terminal. * Navigate to the directory where you want to store the RinSim project. * Execute the following git command  	```` 	git clone git://github.com/rinde/RinSim.git 	```` 	 	This will download all the source files of the RinSim project to your local directory.  To update the simulator later on, you can use the _pull_ command:  ```` git pull origin v2 ````  ### Importing the Maven projects in eclipse  RinSim relies on Maven to load all required dependencies. To make use of Maven in eclipse you have to execute the following steps:  * In eclipse go to _File -> Import... -> Maven -> Existing Maven Projects_. * Browse to your local RinSim directory. * You will now see a list of _.pom_ files. * Select all the _.pom_ files except the one named _packaging_ * Click _Finish_.  After finishing the import, you should see the following four projects in your workspace:  * _core_: the heart of the simulator and the models. * _ui_: everything related to visualizing stuff for the simulator.  * _example_: some simple examples of how to use the simulator. * _problem_: some specific problem implementations.    #### Using eGit  1. Go to _File -> Import..._ * Select _Git -> Projects from Git_. * Select _URI_. * Enter ```` git@github.com:rinde/RinSim.git ```` in the URI field (do not alter any other input fields) and click _next_. * __Only__ select the __v2__ branch and click _next_. * Choose a local directory for your project and click _next_. * Wait for eGit to download the project. * Make sure _Import existing projects_ is selected and click _next_. * Click _finish_.  You will now have one project in eclipse. Because we use Maven, you cannot use this project directly. Instead, You now have to import the all sub-projects (except __packaging__) individually. Perform steps __1__ to __9__ again for each sub-project  __Important__: In step 6, choose another directory for the specific sub-project. In step 8, select core/ui/example from the working directory.  __Note__: Some versions of eclipse do not show the sub-directories in step 8. To solve this, first click _back_ then again _next_.  To update the simulator later on, right-click on a specific sub-project, go to _Team_ and select _Pull_.   #### Using Git  * Open a terminal. * Navigate to the directory where you want to store the RinSim project. * Execute the following git command  	```` 	git clone git://github.com/rinde/RinSim.git 	```` 	 	This will download all the source files of the RinSim project to you local directory.  RinSim relies on Maven to load all required dependencies. To make use of Maven in eclipse you have to execute the following steps:  * In eclipse go to _File -> Import... -> Maven -> Existing Maven Projects_ * Browse to your local RinSim directory. * You will now see a list of _.pom_ files. * Select all _.pom_ files except (_packaging.pom_). * Click _Finish_  After finishing the import, you should see the following three projects in your workspace:  * _core_: the heart of the simulator and the models. * _ui_: everything related to visualizing stuff for the simulator. * _example_: some simple examples of how to use the simulator.  * _problem_: standard problem implementations.  To update the simulator later on, you can use the _pull_ command:  ```` git pull origin v2 ````  -->      <!-- ### Using gitHub's issues to report changes  You can use gitHub's issue feature to report problems, bugs, or useful features for RinSim.  Remember:  * The issue system should only be used for stuff directly related to RinSim, not for questions about the MAS course or for questions on how to do stuff with RinSim. You can use Toledo/lab sessions/fellow students for this. * Check if your issue has already been reported. * Be precise in the description of your issue. * When reporting a bug, give sufficient information on how to reproduce the bug. * Think twice before creating a new issue. --> <!--  _more guidelines available soon_  ### Making pull requests for RinSim  _available soon_ -->
apache/roller	README.txt  This file exists at the top-level of the Roller source tree.  Roller is made up of the following Maven projects:    roller-project:         Top level project   app:                    Roller Weblogger webapp, JSP pages, Velocity templates   assembly-release:       Used to create official distributions of Roller   docs:                   Roller documentation in ODT (OpenOffice/LibreOffice) format   it-selenium             Integrated browser tests for Roller using Selenium  To pull the latest trunk sources you need a Subversion client:   svn co https://svn.apache.org/repos/asf/roller/trunk roller_trunk  Building this version of Roller requires Apache Maven 3.0.5.   - How to build Roller: https://cwiki.apache.org/confluence/x/EM4   - To build and run Roller on Eclipse: https://cwiki.apache.org/confluence/x/EM4  ---------------------------------------------------------- How to build the source  The normal Roller build creates a product generically suitable for use several application containers, however see the Roller Install guide for application server specific configuration information.  After pulling the source tree and changing directory to its top level, as indicated above, the following command will build and run all unit tests:     mvn clean install  After doing that, you should find the newly built Roller webapp, suitable for use in app/target/roller.   To build Roller, subsequently run "mvn clean install" from the assembly-release folder.  After that, you'll find Roller distribution files in  assembly-release/target.   --------------------------- NOTES  Building other versions of Roller  If you wish to pull a branch other than the trunk, replace the word "trunk" in both lines above with the appropriate branch name.  Note that versions of Roller before 5.0 have an Ant-based build.  In general, you should be able to follow instructions accompanying the sources that you pull in order to build that version.
Cloudname/cloudname	# Brave new world: Cloudname 3.0  ## cn-core The core Cloudname library for resource management  ## cn-service Service discovery built on top of the core library.  --- # The yet-to-be-updated section  ## a3 -- Authentication, Authorization and Access library Mostly the first, some of the second. Still unchanged from 2.x  ## Idgen - Generating IDs Generate bucketloads of unique IDs spread across multiple hosts, services and regions.  ## Flags - Command line Flags Simple command line flag handling via annotations on properties and accessors.  ## Log - Core logging library Core entities for logging.  ## Timber - A log server and client A networked high-performance log server that uses protobuf entities. Server and client code.  ## Testtools - Testing tools and utilities  Mostly for internal use by the various modules.
dzsessona/QuickOpener-NetBeans	<hr> <h1 style="color: #FF0000">Looking for maintainers, who want to take over the development!</h1> <hr>  QuickOpener-NetBeans ====================  * [Wiki](https://github.com/dzsessona/QuickOpener-NetBeans/wiki/Home)<br/> * [Download < NB8.1](http://plugins.netbeans.org/plugin/43217/quickopener) * [Download &ge; NB8.1](http://plugins.netbeans.org/plugin/62668/?show=true)  Sometimes while programming in NetBeans you want to explore a particular file that you are editing on the file system browser, or maybe launch a command in a terminal to do something with it.  ![Plugin toolbar](https://raw.githubusercontent.com/dzsessona/QuickOpener-NetBeans/master/QuickOpener/qoscreenshots/shot2.png)  This plugins brings to your NetBeans six action, three of them always available and three of them available when the selected node has a file assiociated with it. In particular:  _When the selection has a valid file:_  * **Open the default OS shell** on the location of the file (or its folder) selected. ![icon](https://raw.githubusercontent.com/dzsessona/QuickOpener-NetBeans/master/QuickOpener/qoscreenshots/shot7.PNG)&nbsp;  * **Open the file system browser** on the location of the file (or its folder) selected. ![icon](https://raw.githubusercontent.com/dzsessona/QuickOpener-NetBeans/master/QuickOpener/qoscreenshots/shot8.png)&nbsp;  * **Copy to the clipboard** the path of the file selected. ![icon](https://raw.githubusercontent.com/dzsessona/QuickOpener-NetBeans/master/QuickOpener/qoscreenshots/shot9.PNG)&nbsp;   _Always enabled:_  * **Launch a shell command** (with parameters, customizable on preferences) ![icon](https://raw.githubusercontent.com/dzsessona/QuickOpener-NetBeans/master/QuickOpener/qoscreenshots/launch.png)&nbsp;  * **FileSystem browser on any location** (favorites, customizable on preferences) ![icon](https://raw.githubusercontent.com/dzsessona/QuickOpener-NetBeans/master/QuickOpener/qoscreenshots/shot10.png)&nbsp;  * **Open a shell on any location** (favorites, customizable on preferences) ![icon](https://raw.githubusercontent.com/dzsessona/QuickOpener-NetBeans/master/QuickOpener/qoscreenshots/shot11.png)&nbsp;   <h2>Updates</h2>  <h3>1.1.0:</h3> <ul> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/66">Feature 66</a>]: Refactor run custom dialog: simplify, add keyboard-only support</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/33">Feature 33</a>]: Support for project folder replacement variable ${projectFolder}/${mainProjectFolder}</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/67">Feature 67</a>]: More patterns - see <a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/67">details</a></li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/30">Feature 30</a>]: Support favorites nodes</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/64">Feature 64</a>]: Support lookups with java.io.File</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/65">Bugfix 65</a>]: Missing mnemonics in dialogs and options</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/69">Bugfix 69</a>]: "Launch custom command..." should be available if there is no selected file</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/77">Bugfix 77</a>]: Fix GUI detection for Ubuntu 16.04</li> </ul>  <h3>1.0.4:</h3> <ul> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/63">Bugfix 63</a>]: File manager opens twice (KDE)</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/62">Bugfix 62</a>]: java.lang.NoSuchMethodError: java.lang.Process.waitFor running NB @ JDK7</li>  </ul>  <h3>1.0.3:</h3> <ul> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/56">Task 56</a>]: Provide a 8.1 version for the plugin center</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/60">Task 60</a>]: Include new version of oscommands</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/61">Task 61</a>]: Convert to maven-project for better maintainence</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/51">Bugfix 51</a>]: [Windows] Open in FileManager should select the file/dir</li> <li>[<a href="https://github.com/dzsessona/QuickOpener-NetBeans/issues/49">Bugfix 49</a>]: [Linux] Crash in options when running "LinuxUnknown" and KDE not detected</li>   </ul>  <p><a href="https://github.com/kinkadzs/QuickOpener-NetBeans/wiki/Home">The project is hosted on github, click here to report a bug or make a suggestion...     </a>Enjoy, Diego+Benno.     </p>
jboss/metadata	Metadata data for AS8. ========================
rmct/AutoReferee	# AutoReferee [![Travis Build Status for AutoReferee](https://secure.travis-ci.org/rmct/AutoReferee.png?branch=master)](http://travis-ci.org/#!/rmct/AutoReferee)  ## [Competitive Minecraft Made Easy!](http://rmct.github.io/AutoReferee)  [Follow us on dev.bukkit.org](http://dev.bukkit.org/server-mods/autoreferee/)!  Join us for support at [#autoreferee](http://webchat.freenode.net?channels=autoreferee) on freenode.  **AutoReferee**, in cooperation with [_RMCT_](http://www.reddit.com/r/mctourney) and [_Major League Mining_](http://www.majorleaguemining.net/): A plugin to automatically referee Race for Wool (and other competitive Minecraft) matches. Automatically handles maps, teams, regions, end conditions, match statistics, and much more.  **Mapmakers:** Once you have built your map and [configured it for use with Autoreferee](http://github.com/rmct/AutoReferee/wiki/How-to-Configure-a-Map), the plugin will place a special configuration file directly into your world folder for you to package and distribute with your map. The configuration file configures AutoReferee automatically when your map is loaded. Please refer to the documentation regarding this configuration file.  _Note:_ This plugin uses [WorldEdit](http://wiki.sk89q.com/wiki/WorldEdit) for map configuration. Server operators need not run WorldEdit on their server, but to configure a map for use with **AutoReferee**, WorldEdit is required.  [![Plugin Statistics via mcstats.org](http://api.mcstats.org/signature/AutoReferee.png)](http://mcstats.org/plugin/AutoReferee)  ---  If you like AutoReferee, [consider donating a few dollars to help pay for map serving and hosting](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=authorblues%40gmail%2ecom&lc=US&item_name=AutoReferee&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donate_LG%2egif%3aNonHosted). Money raised will go directly to sustaining this plugin.
sboesebeck/morphium	morphium ========  Morphium - Java Object Mapper and Caching Layer for MongoDB  Morphium is a POJO Object mapper for Accessing Mongodb. Some of the main Features of Morphium: - actively developed - used by a number of projects (including holidayinsider.com and holidays.hrs.de/, simplesystem.com) - Transparent access to MongoDB - Transparent declarative (Annotation based) caching - Annotation based definition of mappings - Cluster Awareness - Cache synchronization between cluster nodes - Asynchronous and Buffered write access - Messaging - fluent interface for querying mongodb - support for the MongoDB aggregator framework - support for complex queries - support inheritance and polymorphism - support for javax.validation annotations - lifecycle method of pojos - nearly every aspect of Morphium can be replaced by own implementation (e.g. Query-Object, CacheImplementation...) - ConfigManager helps storing app configurations in Mongo with efficient access to it (cached) - Support for References, including lazy loaded references - Support for partial updated objects (when writing, only the changes of the object are transferred)  - Almost any operation morphium provides is async capable. That means, if you pass it an `AsyncOperationListener` as argument, you won't get a batch now, but after the async operation finished via the callback  for questions and feature requests / bug reports also have a look at the google group morphium-discuss@googlegroups.com   Quick Start ===========  before accessing mongo via Morphium, you need to configure Morphium. this is done by preparing a MorphiumConfig Object: ```java   MorphiumConfig cfg = new MorphiumConfig();   cfg.setDatabase("testdb");   cfg.addHost("localhost", 27017); ```  you can also configure Morphium using properties: new MorphiumConfig(properties); or a json-String: MorphiumConfig cfg = MorphiumConfig.createFromJson(json);  After that, you just need to instantiate Morphium:  ```   Morphium m=new Morphium(cfg); ```  There are some convenience constructors available since V2.2.23 which make your life a bit easier:  ```    Morphium m=new Morphium("localhost","test-db"); ```  this creates a morphium instance with most default settings, connecting to localhost, standard port 27017 and using database `test-db`  if necessary, it's of course possible to specify the port to connect to:  ```   Morphium m=new Morphium("localhost:27019","test-db");   Morphium n=new Morphium("localhost",27020,"test-db"); ```  then you are good to go:  ```java   Query<MyEntity> q=m.createQueryFor(MyEntity.class).f("a_field").eq("a id");   List<MyEntity> lst=q.asList();   MyEntity ent=q.get();   ...   m.store(ent); ```  Defining an Entity is quite simple as well: ```java   @Entity(translateCamelCase = true)   @Cache   public class MyEntity {     @Id     private MorphiumId myId;         private String aField;        private EmbeddedObject emb;        @Reference     private MyEntity otherEntity;       ...   }    @Embedded   public class EmbeddedObject {    ...   } ```  All Entities need an ID field. This field can by any type you like. If it is not `ObjectID` or `MorphiumID, you need to create the id yourself.  - Before Morphium 3.x: If the type is `org.bson.types.ObjectId`, it will be created by mongo, if not - you need to take care of that. - Since Morphium 3: To encapsulate dependencies better, morphium is not relying on mongodb code directly. Hence in your code, you do not use `ObjectID` anymore, but MorphiumID.  These are more or less source compatible, migration should be easy. But the rule from abve applies here to, if you use MorphiumID, you do not need to create the ID, Mongo is doing it.  Only entities can be referenced, as you need an ID for that. The type of the ID is not important. You can also use Maps, Lists or Arrays, all may also include other Entities or Embedded types. `` ## Use enum instead of strings for queries As using strings to query your object might be a bit error prone, you also can use enums instead of field name strings: ```java    Query<MyEntity> q=m.createQueryFor(MyEntity.class).f(MyEntity.Fields.aField).eq("a id"); ``` of course, these enums need to be created. have a look at https://github.com/sboesebeck/intelliJGenPropertyEnumsPlugin for a plugin for generating those automatically in our example, the batch would look like this: ```java   @Entity(translateCamelCase = true)   @Cache   public class MyEntity {     @Id     private MorphiumId myId;         private String aField;        private EmbeddedObject emb;        @Reference     private MyEntity otherEntity;     ...     public enum Fields { myId, aField, emb, otherEntity }       }    @Embedded   public class EmbeddedObject {    ...   } ```   This is a very short glance at all the features of Morphium!  For more information take a closer look at the wiki.  Have fun,  Stephan  # morphium  Morphium - Java Object Mapper and Caching Layer for MongoDB  Morphium is a POJO Object mapper for Accessing Mongodb. Some of the main Features of Morphium: - actively developed - used by a number of projects (including holidayinsider.com and holidays.hrs.de/) - Transparent access to MongoDB - Transparent declarative (Annotation based) caching - Annotation based definition of mappings - Cluster Awareness - Cache synchronization between cluster nodes - Asynchronous and Buffered write access - Messaging - fluent interface for querying mongodb - support for the MongoDB aggregator framework - support for complex queries - support inheritance and polymorphism - support for javax.validation annotations - lifecycle method of pojos - nearly every aspect of Morphium can be replaced by own implementation (e.g. Query-Object, CacheImplementation...) - ConfigManager helps storing app configurations in Mongo with efficient access to it (cached) - Support for References, including lazy loaded references - Support for partial updated objects (when writing, only the changes of the object are transferred)  - Almost any operation morphium provides is async capable. That means, if you pass it an `AsyncOperationListener` as argument, you won't get a batch now, but after the async operation finished via the callback  for questions and feature requests / bug reports also have a look at the google group morphium-discuss@googlegroups.com   # Quick Start  before accessing mongo via Morphium, you need to configure Morphium. this is done by preparing a MorphiumConfig Object: ```java   MorphiumConfig cfg = new MorphiumConfig();   cfg.setDatabase("testdb");   cfg.addHost("localhost", 27017); ```  you can also configure Morphium using properties: new MorphiumConfig(properties); or a json-String: MorphiumConfig cfg = MorphiumConfig.createFromJson(json);  After that, you just need to instantiate Morphium:  ```   Morphium m=new Morphium(cfg); ```  There are some convenience constructors available since V2.2.23 which make your life a bit easier:  ```    Morphium m=new Morphium("localhost","test-db"); ```  this creates a morphium instance with most default settings, connecting to localhost, standard port 27017 and using database `test-db`  if necessary, it's of course possible to specify the port to connect to:  ```   Morphium m=new Morphium("localhost:27019","test-db");   Morphium n=new Morphium("localhost",27020,"test-db"); ```  then you are good to go:  ```java   Query<MyEntity> q=m.createQueryFor(MyEntity.class).f("a_field").eq("a id");   List<MyEntity> lst=q.asList();   MyEntity ent=q.get();   ...   m.store(ent); ```  Defining an Entity is quite simple as well:  ```java   @Entity(translateCamelCase = true)   @Cache   public class MyEntity {     @Id     private MorphiumId myId;         private String aField;        private EmbeddedObject emb;        @Reference     private MyEntity otherEntity;       ...   }    @Embedded   public class EmbeddedObject {    ...   } ```  All Entities need an ID field. This field can by any type you like. If it is not `ObjectID` or `MorphiumID, you need to create the id yourself.  - Before Morphium 3.x: If the type is `org.bson.types.ObjectId`, it will be created by mongo, if not - you need to take care of that. - Since Morphium 3: To encapsulate dependencies better, morphium is not relying on mongodb code directly. Hence in your code, you do not use `ObjectID` anymore, but MorphiumID.  These are more or less source compatible, migration should be easy. But the rule from abve applies here to, if you use MorphiumID, you do not need to create the ID, Mongo is doing it.  Only entities can be referenced, as you need an ID for that. The type of the ID is not important. You can also use Maps, Lists or Arrays, all may also include other Entities or Embedded types. `` ## Use enum instead of strings for queries As using strings to query your object might be a bit error prone, you also can use enums instead of field name strings: ```java    Query<MyEntity> q=m.createQueryFor(MyEntity.class).f(MyEntity.Fields.aField).eq("a id"); ``` of course, these enums need to be created. have a look at https://github.com/sboesebeck/intelliJGenPropertyEnumsPlugin for a plugin for generating those automatically in our example, the batch would look like this: ```java   @Entity(translateCamelCase = true)   @Cache   public class MyEntity {     @Id     private MorphiumId myId;         private String aField;        private EmbeddedObject emb;        @Reference     private MyEntity otherEntity;     ...     public enum Fields { myId, aField, emb, otherEntity }       }    @Embedded   public class EmbeddedObject {    ...   } ```   This is a very short glance at all the features of Morphium!  For more information take a closer look at the wiki.  Have fun,  Stephan   # Aggregation  ## Count Counting is quite easy, call countAll() on any given Query-instance. Please keep in Mind: countAll() does not take limit() or skip() into account, always returns the number of all possible matches to the query.  ## Distinct values Morphium supports distinct queries (since 1.4.07) and allows you to get a list of values existing in the query result. Have a look, how to use it: ```java List<UncachedObject> lst = new ArrayList<UncachedObject>();         for (int i = 0; i < 100; i++) {             UncachedObject uc = new UncachedObject();             uc.setCounter(i % 3);             uc.setValue("Value " + (i % 2));             lst.add(uc);         }         MorphiumSingleton.get().storeList(lst);          List values = MorphiumSingleton.get().distinct("counter", UncachedObject.class);         assert (values.size() == 3) : "Size wrong: " + values.size();         for (Object o : values) {             log.info("counter: " + o.toString());         }         values = MorphiumSingleton.get().distinct("value", UncachedObject.class);         assert (values.size() == 2) : "Size wrong: " + values.size();         for (Object o : values) {             log.info("Value: " + o.toString());         } ``` The output would look like: ``` 19:03:17,211 INFO  [main] DistinctGroupTest: counter: 0 19:03:17,211 INFO  [main] DistinctGroupTest: counter: 1 19:03:17,211 INFO  [main] DistinctGroupTest: counter: 2 19:03:17,213 INFO  [main] DistinctGroupTest: Value: Value 0 19:03:17,213 INFO  [main] DistinctGroupTest: Value: Value 1 ``` Here the call to "distinct" returns a list ov values, which (of course) might be of any type. Those values are not unmarshalled, which means, if this would contain a sub-document, you will get a list of BasicDBObject.  ## Group Function Morphium has support for MongoDB's group-Function (similar to SQL group by). Here is an example: ```java         HashMap<String, Object> initial = new HashMap<String, Object>();         initial.put("count", 0);         initial.put("sum", 0);         DBObject ret = MorphiumSingleton.get().group(MorphiumSingleton.get().createQueryFor(UncachedObject.class), initial,                 "data.count++; data.sum+=obj.counter;", "data.avg=data.sum/data.count;");         log.info("got DBObject: " + ret); ``` Producing output: ``` 19:05:19,517 INFO  [main] DistinctGroupTest: got DBObject: [ { "count" : 100.0 , "sum" : 5050.0 , "avg" : 50.5}] ``` ## some Explanations:  parameter query is the query to execute, might be an empty one for the whole collection initial is a Map containing the data object that is manipulated by the javascript code in 'jsReduce' and 'jsFinalize' jsReduce is the call to reduce the data to what is needed, here count all objects and sum up the values. If you did not define the reduce function yourself in the given string, a function (obj,data) { and the closing brackets are added jsFinalize: Javascript code to finalize the data, if no function is specified in the string, function(data) {and the closing brackets are added. Here: finalize calculates the avg of all values you might also add fields to group by. Those might either all be not prefixed or prefixed with a - to have it not grouped by that the return value is a list of DBObject, containing at least the values from the initialMap given in the call!  **Attention** this might cause heavy load on the mongo if the correct indices are missing.  # Support for mongodb aggregation framework With V2.2.0 10gen introduces the new aggregation framework to the mongodb. Very useful for calculating statistics and such. For mor informrmation see Mongodb Aggregation Docu.  Since V1.5.0 morphium has support for this fexible aggregation framework. Consider this the first basic API, there will be some improvements, as soon as we know in what direction to improve the API.  For now, have a look at this example: ```java public class Aggregation extends MongoTest {     @Test     public void aggregatorTest() throws Exception {         createUncachedObjects(1000);          Aggregator<UncachedObject, Aggregate> a = MorphiumSingleton.get().createAggregator(UncachedObject.class, Aggregate.class);         assert (a.getResultType() != null);         //reduce input amount of data by reducing columns         a = a.project("counter");         //filter it more         a = a.match(MorphiumSingleton.get().createQueryFor(UncachedObject.class).f("counter").gt(100));         //sorting, necessary for $first/$last         a = a.sort("counter");         //limit input data rows         a = a.limit(15);         //group by - in this case, calculate considerin ALL data, as _id does not change         a = a.group("all").avg("schnitt", "$counter").sum("summe", "$counter").sum("anz", 1).last("letzter", "$counter").first("erster", "$counter").end();         //project the result into the desired structure (rename fields and such)         a = a.project(new BasicDBObject("summe", 1).append("anzahl", "$anz").append("schnitt", 1).append("last", "$letzter").append("first", "$erster"));          List<DBObject> obj = a.toAggregationList();         for (DBObject o : obj) {             log.info("Object: " + o.toString());         }         List<Aggregate> lst = a.aggregate();         assert (lst.size() == 1) : "Size wrong: " + lst.size();         log.info("Summe:   " + lst.get(0).getSumme());         log.info("Schnitt: " + lst.get(0).getSchnitt());         log.info("Last:    " + lst.get(0).getLast());         log.info("First:   " + lst.get(0).getFirst());         log.info("Anzahl:  " + lst.get(0).getAnzahl());           assert (lst.get(0).getAnzahl() == 15) : "Anzahl found: " + lst.get(0).getAnzahl();      }      @Test     public void aggregationTestcompare() throws Exception {         log.info("Preparing data");         createUncachedObjects(100000);         log.info("done... starting");         long start = System.currentTimeMillis();         Query<UncachedObject> q = MorphiumSingleton.get().createQueryFor(UncachedObject.class);         HashMap<Integer, Integer> sum = new HashMap<Integer, Integer>();         HashMap<Integer, Integer> anz = new HashMap<Integer, Integer>();         q = q.sort("counter");          for (UncachedObject u : q.asList()) {             int v = u.getCounter() % 3;             if (sum.get(v) == null) {                 sum.put(v, u.getCounter());             } else {                 sum.put(v, sum.get(v).intValue() + v);             }             if (anz.get(v) == null) {                 anz.put(v, 1);             } else {                 anz.put(v, anz.get(v).intValue() + 1);             }          }         for (Integer v : sum.keySet()) {             log.info("ID: " + v);             log.info("  anz: " + anz.get(v));             log.info("  sum: " + sum.get(v));             log.info("  avg: " + (sum.get(v) / anz.get(v)));         }         long dur = System.currentTimeMillis() - start;          log.info("Query took " + dur + "ms");          log.info("Starting test with Aggregation:");         start = System.currentTimeMillis();         Aggregator<UncachedObject, Aggregate> a = MorphiumSingleton.get().createAggregator(UncachedObject.class, Aggregate.class);         assert (a.getResultType() != null);         BasicDBList params = new BasicDBList();         params.add("$counter");         params.add(3);         BasicDBObject db = new BasicDBObject("$mod", params);         a = a.sort("$counter");         a = a.group(db).sum("summe", "$counter").sum("anzahl", 1).avg("schnitt", "$counter").end();         List<DBObject> obj = a.toAggregationList();         List<Aggregate> lst = a.aggregate();         assert (lst.size() == 3);         for (Aggregate ag : lst) {             log.info("ID: " + ag.getTheGeneratedId());             log.info(" sum:" + ag.getSumme());             log.info(" anz:" + ag.getAnzahl());             log.info(" avg:" + ag.getSchnitt());         }         dur = System.currentTimeMillis() - start;         log.info("Aggregation took " + dur + "ms");     }      @Embedded     public static class Aggregate {         private double schnitt;         private long summe;         private int last;         private int first;         private int anzahl;          @Property(fieldName = "_id")         private String theGeneratedId;          public int getAnzahl() {             return anzahl;         }          public void setAnzahl(int anzahl) {             this.anzahl = anzahl;         }          public int getLast() {             return last;         }          public void setLast(int last) {             this.last = last;         }          public int getFirst() {             return first;         }          public void setFirst(int first) {             this.first = first;         }          public double getSchnitt() {             return schnitt;         }          public void setSchnitt(double schnitt) {             this.schnitt = schnitt;         }          public long getSumme() {             return summe;         }          public void setSumme(long summe) {             this.summe = summe;         }          public String getTheGeneratedId() {             return theGeneratedId;         }          public void setTheGeneratedId(String theGeneratedId) {             this.theGeneratedId = theGeneratedId;         }     }   } ``` the second test case compares performance using the "usual" approach, reading all data, doing calculations with the aggregation framework approach. On my machine, the aggregation framework is about 8 times faster and produces way less load on mongo.  # Annotation Inheritence By default, Java does not support the inheritence of Annotations. This is ok in most cases, but in the case of entities it's a bugger. I added inheritence to morphium to be able to build flexible data structures and store them to mongo.  ## Implementation Well, it's quite easy, actually ;-) The algorithm for getting the inherited annotations looks as follows (simplified)  Take the annotations from the current class, if found, return it Take the superclass, if superclass is "Object" return null if there is the annotation to look for, return it continue with step 1 This way, all annotations in the hierarchy are taken into account and the most recent one is taken. You can always change the annotations when subclassing, although you cannot "erase" them (which means, if you inherit from an entity, it's always an entity). For Example: ```java    @Entity     @NoCache    public class Person {       @Id       private ObjectId id;      ....    } ``` And the subclass: ```java    @Cache(writeCache=true, readCache=false)    public class Parent {       @Reference       private List<Person> parentFrom;       ...    } ``` Please keep in mind, that unless specified otherwise, the classname will be taken as the name for your collection. Also, be sure to store your classname in the collection (set polymorph=true in @Entity annotation) if you want to store them in one collection.  All writer implementation support asynchronous calls like ```java    public <T> void store(List<T> lst, AsyncOperationCallback<T> callback);  ``` if callback==null the method call should be synchronous... If callback!=null do the call to mongo asynchronous in background. Usually, you specify the default behavior in your class definition: ```java   @Entity   @AsyncWrites   public class EntityType {    ...   } ``` All write operations to this type will be asynchronous! (synchronous call is not possible in this case!).  Asynchronous calls are also possible for Queries, you can call q.asList(callback) if you want to have this query be executed in background.  # Difference asynchronous write / write buffer Asynchronous calls will be issued at once to the mongoDb but the calling thread will not have to wait. It will be executed in Background. the `@WriteBuffer` annotation specifies a write buffer for this type (you can specify the size etc if you like). All writes will be held temporarily in ram until time frame is reached or the number of objects in write buffer exceeds the maximum you specified (0 means no maximum). Attention if you shut down the Java VM during that time, those entries will be lost. Please only use that for logging or "not so important" data. specifying a write buffer four you entitiy is quite easy: ```java   @Entity   @WriteBuffer(size=1000, timeout=5000)   public class MyBufferedLog {   ....   } ``` This means, all write access to this type will be stored for 5 seconds or 1000 entries, whichever occurs first. If you want to specify a different behavior when the maximum number of entries is reached, you can specify a strategy:  `WRITE_NEW`: write newest entry (synchronous and not add to buffer)  `WRITE_OLD`: write some old entries (and remove from buffer)  `DEL_OLD`: delete old entries from buffer - oldest elements won't be written to Mongo!  `IGNORE_NEW`: just ignore incoming - newest elements WILL NOT BE WRITTEN!  `JUST_WARN`: increase buffer and warn about it   #Authentication   In Mongo until V 2.4 authentication and user privileges were not really existent. With 2.4, roles are introduces which might make it a bit more complicated to get things working.  Morphum and authentication Morphium supports authentication, of course, but only once. So usually you have an application user, which connects to database. Login to mongo is configured as follows: ```java     MorphiumConfig cfg=new Morpiumconfig(...);     ...     cfg.setMongoLogin("tst");     cfg.setMongoPassword("tst"); ``` This user usually needs to have read/Write access to the database. If you want your indices to be created automatically by you, this user also needs to have the role dbAdmin for the corresponding database. If you use morphium with a replicase of mongo nodes, morphium needs to be able to get access to local database and get the replicaset status. In order to do so, either the mongo user needs to get additional roles (clusterAdmin and read to local db), or you specify a special user for that task, which has excactly those roles. Morphium authenticates with that different user for accessing replicaSet status (and only for getting the replicaset status) and is convigured very similar to the normal login: ```java      cfg.setMongoAdminUser("adm");      cfg.setMongoAdminPwd("adm"); ```  # Corresponding MongoD Config You need to run your mongo nodes with -auth (or authenticat = true set in config) and if you run a replicaset, those nodes need to share a key file or kerberos authentication. (see http://docs.mongodb.org/manual/reference/user-privileges/) Let's assume, that all works for now. Now you need to specify the users. One way of doing that is the following: * add the user for mongo to your main database (in our case tst) * add an admin user for your own usage from shell to admin db (with all privileges) * add the clusterAdmin user to admin db as well, grant read access to local ```js     use admin     db.addUser({user:"adm",pwd:"adm",                        roles:["read","clusterAdmin"],                         otherDBRoles:{local:["read"]}                       })     db.addUser({user:"admin",pwd:"admin",                       roles:["dbAdminAnyDatabase",                                 "readWriteAnyDatabase",                                 "clusterAdmin",                                 "userAdminAnyDatabase"]                        })      use morphium_test     db.addUser({user:"tst",pwd:"tst",roles:["readWrite","dbAdmin"]}) ``` Here morphium_test is your application database morphium is connected to primarily. The admin db is a system database.  This is far away from being a complete guide, I hope this just gets you started with authentication....  # Common problem: timestamps on your data This is something quite common: you want to know, when your data was last changed and maybe who did it. Usually you keep a timestamp with your object and you need to make sure, that these timestamps are updated accordingly. Morphium does this automatically - just declare the annotations: ```java  @Entity     @NoCache     @LastAccess     @LastChange     @CreationTime     public static class TstObjLA {         @Id         private ObjectId id;          @LastAccess         private long lastAccess;          @LastChange         private long lastChange;          @CreationTime         private long creationTime;          private String value;          public long getLastAccess() {             return lastAccess;         }          public void setLastAccess(long lastAccess) {             this.lastAccess = lastAccess;         }          public long getLastChange() {             return lastChange;         }          public void setLastChange(long lastChange) {             this.lastChange = lastChange;         }          public long getCreationTime() {             return creationTime;         }          public void setCreationTime(long creationTime) {             this.creationTime = creationTime;         }          public String getValue() {             return value;         }          public void setValue(String value) {             this.value = value;         }     } ``` You might ask, why do we need to specify, that access time is to be stored for the class and the field. The reason is: Performance! In order to search for a certain annotation we need to read all fields of the whole hierarchy the of the corresponding object which is rather expensive. In this case, we only search for those access fields, if necessary. All those are stored as long - System.currentTimeMillies()  ## Explanation:  * @StoreLastAccess: Stores the last time, this object was read from db! Careful with that one... * @StoreCreationTime: Stores the creation timestamp * @StoreLastChange: Timestamp the last moment, this object was stored.  # Synchronizing Caches between nodes It's a common problem, especially in clustered environments. How to synchronize caches on the different nodes. Morphium offers a simple solutions for it: On every write operation, a Message is stored in the Message queue (see MessagingSystem) and all nodes will clear the cache for the corresponding type (which will result in re-read of objects from mongo - keep that in mind if you plan to have a hundred hosts on your network) This is easy to use, does not cause a lot of overhead. Unfortunately it cannot be more efficient hence the Cache in Morphium is organized by searches.  the Morphium cache Syncrhonizer does not issue messages for uncached entities or entities, where clearOnWriteis set to false. Configurations are always synced - if you need a host-local configuration, you need to name it uniquely (by adding the hostname or mac address or something). BUT: All configuration will be synchronized to all nodes...  Here is an example on how to use this: ```java     Messaging m=new Messaging(morphium,10000,true);     CacheSynchronizer cs=new CacheSynchronizer(m,morphium); ``` Actually this is all there is to do, as the CacheSynchronizer registers itself to both morphium and the messaging system.  # Change sinces 1.4.0 Now the Caching is specified by every entity in the @Cache annotation using one Enum called SyncCacheStrategy. Possible Values are: NONE (Default), CLEAR_TYPE_CACHE (clear cache of all queries on change) and UPDATE_ENTRY (updates the entry itself), REMOVE_ENTRY_FROM_TYPE_CACHE (removes all entries from cache, containing this element) ```java enum SyncCacheStrategy {NONE, CLEAR_TYPE_CACHE, REMOVE_ENTRY_FROM_TYPE_CACHE, UPDATE_ENTRY} ``` UPDATE_ENTRY only works when updating records, not on drop or remove or update (like inc, set, push...). For example, if UPDATE_ENTRY is set, and you drop the collection, type cache will be cleared.  **Attention:** UPDATE_ENTRY will result in dirty reads, as the Item itself is updated, but not the corresponding searches!  Meaning: assume you have a Query result cached, where you have all Users listed which have a certain role: ```java    Query<User> q=morphium.createQueryFor(User.class);    q=q.f("role").eq("Admin");    List<User> lst=q.asList(); ```  Let's further assume you got 3 Users as a result. Now imagine, one node on your cluster changes the role of one of the users to something different than "Admin". If you have a list of users that might be changed while you use them! Careful with that! More importantly: your cache holds a copy of that list of users for a certain amount of time. During that time you will get a dirty read. Meaning: you will get objects that actually might not be part of your query or you will not get that actually might (not so bad actually).  Better use REMOVE_ENTRY_FROM_TYPE_CACHE in that case, as it will keep everything in cache except your search results containing the updated element. Might also cause a dirty read (as the newly added elements might not be added to your results) but it keeps findings more or less correct.  As all these synchronizations are done by sending messages via the Morphium own messaging system (which means storing messages in DB), you should really consider just disabling cache in case of heavy updates as a read from Mongo might actually be lots faster then sync of caches.  Keep that in Mind!  # Change since 1.3.07 Since 1.3.07 you need to add a autoSync=true to your cache annotation, in order to have things synced. It tuned out, that automatic syncing is not always the best solution. So, you can still manually sync your caches.  # Manually Syncing the Caches The sync in Morphium can be controlled totally manually (since 1.3.07), just send your own Clear-Cache Message using the corresponding method in CacheSynchronizer. ```java    cs.sendClearMessage(CachedObject.class,"Manual delete"); ```   # Caching in Morphium  Caching is very important even for such a extremely fast database as mongo. There are two kinds of caches: read cache and write cache. #Write cache The WriteCache is just a buffer, where all things to write will be stored and eventually stored to database. This is done by adding the Annotation @WriteBuffer to the class: ```java @Entity  @WriteBuffer(size = 150, strategy = WriteBuffer.STRATEGY.DEL_OLD)     public static class BufferedBySizeDelOldObject extends UncachedObject {      } ``` In this case, the buffer has a maximum of 150 entries, and if the buffer has reached that maximum, the oldest entries will just be deleted from buffer and hence NOT be written! Possible strategies are: * WriteBuffer.STRATEGY.DEL_OLD: delete oldest entries from buffer - use with caution * WriteBuffer.STRATEGY.IGNORE_NEW: Do not write the new entry - just discard it. use with caution * WriteBuffer.STRATEGY.JUST_WARN: just log a warning message, but store data anyway * WriteBuffer.STRATEGY.WRITE_NEW: write the new entry synchronously and wait for it to be finished * WriteBuffer.STRATEGY.WRITE_OLD: write some old data NOW, wait for it to be finished, than queue new entries  That's it - rest is 100% transparent - just call ```morphium.store(entity);``` - the rest is done automatically  # Read Cache Read caches are defined on type level with the annotation @Cache. There you can specify, how your cache should operate: ```java @Cache(clearOnWrite = true, maxEntries = 20000, strategy = Cache.ClearStrategy.LRU, syncCache = Cache.SyncCacheStrategy.CLEAR_TYPE_CACHE, timeout = 5000) @Entity public class MyCachedEntity { ..... } ```  here a cache is defined, which has a maximum of 20000 entries. Those Entries have a lifetime of 5 seconds (timeout=5000). Which means, no element will stay longer than 5sec in cache. The strategy defines, what should happen, when you read additional object, and the cache is full: * Cache.ClearStartegy.LRU: remove least recently used elements from cache * Cache.ClearStrategy.FIFO:first in first out - depending time added to cache * Cahce.ClearStrategy.RANDOM: just remove some random entries With ```clearOnWrite=true``` set, the local cache will be erased any time you write an entity of this typte to database. This prevents dirty reads. If set to false, you might end up with stale data (for as long as the timeout value) but produce less stress on mongo and be probably a bit faster.  CacheSynchronization is something important in clustered or replicated environments. In this case, the Write event is propagated to all cluster members. See [Cache Syncrhonization](https://github.com/sboesebeck/morphium/wiki/Cache-Synchronization) for more details.  **Internals / Implementation details ** * Morphium uses the cache based on the search query, sort options and collection overrides given. This means  that there might be doublicate cache entries. In order to minimize the memory usage, Morphium also uses an ID-Cache. So all results are just added to this id cache and those ids are added as result to the query cache. the Caches are organized per type. This means, if your entity is not marked with @Cache, queries to this type won't be cached, even if you override the collection name.  * The cache is implemented completely unblocking and completely thread safe. There is almost no synchronized block in morphium.   # Complex Data structures    In the jUnit tests, morphium is tested to support those complex data structure, like lists of lists, lists of maps or maps of lists of entities. I think, you'll get the picture: ```java   public static class CMapListObject extends MapListObject {         private Map<String, List<EmbObj>> map1;         private Map<String, EmbObj> map2;         private Map<String, List<String>> map3;         private Map<String, List<EmbObj>> map4;          private Map<String, Map<String, String>> map5;         private Map<String, Map<String, EmbObj>> map5a;         private Map<String, List<Map<String, EmbObj>>> map6a;          private List<Map<String, String>> map7;         private List<List<Map<String, String>>> map7a;         .... ``` All those are tested in JUnit and can be stored and read accordingly...  # Simple queries most of your queries probably are simple ones. like searching for a special id or value. This is done rather simply with the query-Object: morphium.createQueryFor(MyEntity.class).f("field").eq(value) if you add more f(fields) to the query, they will be concatenated by a logical AND. so you can do something like: ```java     Query<UncachedObject> q=morphium.createQueryFor(UncachedObject.class);     q.f("counter").gt(10).f("counter").lt(20); This would result in a query like: "All Uncached Objects, where counter is greater than 10 and counter is less then 20". ``` # Or Queries in addition to those AND-queries you can add an unlimited list of queries to it, which will be concatenated by a logical OR. ```java    q.f("counter").lt(100).or(q.q().f("value").eq("Value 12"), q.q().f("value").eq("other")); This would create a query like: "all UncachedObjects where counter is less than 100 and (value is 'value 12' or value is 'other')" ``` the Method q() creates a new empty query for the same object. It's a convenience Method. Please be careful, never use your query Object in the parameter list of or - this would cause and endless loop! ATTENTION here!  This gives you the possibility to create rather complex queries, which should handle about 75% of all cases. Although you can also add some NOR-Queries as well. These are like "not or"-Queries.... ```java    q.f("counter").lt(100).nor(q.q().f("counter").eq(90), q.q().f("counter").eq(55)); this would result in a query like: "All query objects where counter is less than 100 and not (counter=90 or counter=55). ``` this adds another complexity level to the queries ;-)  If that's not enough, specify your own query You can also specify your own query object (BasicDBObject from MongoDB) in case of a very complex query. This is part of the Query-Object and can be used rather easily: ```java         BasicDBObject query=new BasicDBObject();         query=query.append("counter",new BasicDBObject("$lt",10));         Query<UncachedObject> q=MorphiumSingleton.get().createQueryFor(UncachedObject.class);         List<UncachedObject> lst=q.complexQuery(query); ``` Although, in this case the query is a very simple one (counter < 10), but I think you get the Idea....  # Limitations Well, the fluent query interface does have its limitations. So its not possible to have a certain number of or-concatenated queries (like (counter==14 or Counter <10) and (counter >50 or counter ==30)). I'm not sure, this is very legible... maybe we should replace f by field and a like... what do you think?  # Customization Morphium can be customized in many ways. The configuration contains several settings that might be useful for your own custom implementation. * AggregatorClass and AggregatorFactory - used for the aggregation framework. Take a closer look here [Aggregator Framework support](https://github.com/sboesebeck/morphium/wiki/Aggregator-Framework-Support) * ObjectMapper: If you want to implement your own mapping algorithm you'd set this to your implementation. Default is ```de.caluga.morphium.ObjectMapperImpl``` * Cache : you can define your own cache, if you want to. * Field: MongoFields are used during querying: query.f() returns a mongoField. Here all operators are implemented. If you want an additional operator to the already existing ones, you could to this here * IteratorClass: Used for Morphiums paged iterator. Can be customized as well * All Writer implementations (AsyncWriter, BufferedWriter, Writer): If you want to tailor the behavior of the writer to your needs, you can set your implementation to be used by morphium. Be careful: both bufferedWriter and AsyncWriter use the Writer * and of course: the query object itself can be replaced with your own implementation.  This makes Morphium very flexible and not only because of this feature used by a lot of huge projects.  # Index management in morphium Morphium is capable of managing all indices, you might need for your database. In Mongo, indices are very, very important and the lack of which might increase the execution time of simple queries a log (like times 100). You can specify your indexes at class and field level. But you always must mark your entity with Indexes. Indexes will be created by morphium if you write your first data to a non existent collection. This is done to prevent morphium from creating new indexes on huge collections. You can enforce indexing for a given type by calling `ensureIndexesFor` ```java morphium.ensureIndexesFor(MyEntity.class); ```  Indexes can be specified on class level, and for a property - see those examples: ```java @Entity @Index //just a marker public class MyEntity {    @Id String myId;    @Index    String name;        @Index(options={"unique:true"})    String uid; } ``` In this example, there are three Indices defined, one on the ID, one on name and one on uid. All of them are defined in natural order. If you want to define an index, which consists of several fields, you need to do that at class level: ```java @Entity @Index({"-name,uid","ts"}) //defining indices public class MyEntity {    @Id String myId;    String name;    long ts;        @Index(options={"unique:true"})    String uid; } ``` In this case, one unique index for uid is defined, one compound index for name and uid (where name is sorted in reverse order), one single index for the field ts and one index for the ID of this entity. As you see, it's possible to define most of the indices in one @Index-Annotation at class level. Its also possible to add additional options, like 2d-Indexes or text indexes: ```java  @Index("position:2d")  //defining a gespacial index ```  # Ensure Index in Morphium The easiest way to make sure that indices are in place is by calling the method morphium.ensureIndex(). This one creates one index on the given Collection (or Entity respectively). Usage is as follows: ```java    morphium.ensureIndex(UncachedObject.class,"-timestamp","name"); //will create on combined index    morphium.ensureIndex(CachedObject.class,CachedObject.Fields.counter); //will create on +1 index on this type    Map<String, Integer> complexIndex=new HashMap<String,Integer>(); //Make it a sorted map, if necessary!    complexIndex.put("counter",-1);    complexIndex.put("name",1);    complexIndex.put("_id",-1);    morphium.ensureIndex(MyEntity.class,complexIndex); ```  Be careful: Index creation might cause your Mongo to freeze for a while depending on the size of your collections.  # Automatic Index creation Morphium supports automatic index creation. But those indexes are only created, when the collection is about to be newly created (usually, this is what you want, especially in conjunction with the new NameProvider support). Just add the annotations to your type or field to have it indexed: ```java   @Cache(clearOnWrite=false,writeCache=true,readCache=false,overridable=false)   @Entity   @Index({"timestamp","timestamp,level","host_name,timestamp","hostname,level,-timestamp","position:2d"})   public class MyLog  {      private long timestamp;      @Index(decrement=false)      private String hostName;      private String level;      public List<Double> position;      @Id      private String id;      @Index(options={"unique:true"})      private String uniqueField;      //additional fields and methods      ....   } ``` This will create several indexes, if the collection is about to be created (and only then): * timestamp * timestamp and level * host_name and timestamp * hostname, level and timestamp (-1) * host_name * a 2D-Index for geospacial search on the field position * and a unique index for the unique_field  When you have added the Index-Annotation to your entities, you can also use the Method morphium.ensureIndex(MyEntity.class) to honor those calls. This will ensure indices now, no matter whether the collection exists or not (use with care!)   # Morphium's Lazy Loading of references morphium supports lazy loading of references. This is easy to use, just add `@Reference(lazyLoading=true)` to the reference you want to have lazy loaded. ```java @Entity public class MyEntity {    ....    @Reference(lazyLoading=true)    private UncachedObject myReference;  //will be loaded when first accessed    @Reference    private MyEntity ent; //will be loaded when this object is loaded - use with caution                          //this could cause an endless loop    private MyEntity embedded; //this object is not available on its own                               //its embedded as subobject in this one } ```    # how does it work When a reference is being lazy loaded, the corresponding field will be set with a Proxy for an instance of the correct type, where only the ObjectID is set. Any access to it will be catched by the proxy, and any method will cause the object to be read from DB and unmarshalled. Hence this object will only be loaded upon first access.  It should be noted that when using Object.toString(); for testing that the object will be loaded from the database and appear to not be lazy loaded. In order to test Lazy Loading you should load the base object with the lazy reference and access it directly and it will be null. Additionally the referenced object will be null until the references objects fields are accessed.   # Logging  Morphium used Log4J til V2.2.20, which caused some trouble in a multithreadded, heavy load environment.  Right now, there is a very lightweight implementation of a logger, that is being used for morphiums internal logging. You can use this Logger class, if you want, as it is very lightweight and easy to use. It is being configured either using MorphiumConfig, or using JVM Parameters:  - `-Dmorphium.log.level=5` would set maximum log output, e.g. DEBUG. Levels are: 0 - no logging, 1 - FATAL only, 2 - also errors, 3 - include Warnings, 4 - add info to the list, 5 - debug messages also - `-Dmorphium.log.synced=false` if set to true, the output is unbuffered, causing every message to be written instantanously (usually used with STDOUT as output) - `-Dmorphium.log.file=FILENAME` - filename might also be `-` or STDOUT (both meaning stadard output) or STDERR for standard error  You also can specify an suffix for those options, which represent the logger's name (usually a class or packagename). For example, `-Dmorphium.log.level.de.caluga.morphium.Morphium=0` would switch off all messages from this class, `-Dmorphium.log.level.de.caluga.morphium.aggregation=5` would switch on debugging for all messages coming from classes in that package.  Of course, you can configure the logger using MorphiumConfig, there are the following methods:  ```    setGlobalLogLevel(int level);    setGloablLogFile(String fileName);    setGlobalLogSynced(boolean sync);     setLogLevelForPrefix(String prfx);    setLogLevelForClass(Class cls);     setLogFileForPrefix(String prfx);    setLogFileForClass(Class cls);     setLogSyncedForPrefix(String prfx);    setLogSyncedForClass(Class cls); ```  All those settings might also be set using environment variables, dots replaced by underscores (in Unix like: export morphium_log_level=5)  *Attention*: This logger class is by design not really threadsafe, you should create an own instance for every thread. The Logger won't create any errors in multithreaded environments, but might create some strange output *Attention*: Also, keep in mind that every instance of `Logger` will hold an open link to outputfile.  The logger class is more or less compatible with Log4J (at least most of the methods are).  The configuration of the logger is only read when initialized! So if you change your config, it won't effect your logger instances.  ##LoggerDelegate##  Introduced with V2.2.23BETA3 you can define a LogDelegate to be used instead of a File, just add the Prefix `class:` to the filename, e.g. `-Dmorphium.log.file=class:de.caluga.morphium.Log4JLoggerDelegate` would use a simple Log4J implementation.  Of course you can define your own LoggerDelegate, just implement the interface `de.caluga.morphium.LoggerDelegate` and add your classname as file.  *Attention*: The log delegate will be instanciated with every new Logger instance!   # Mapping Objects from and to Strings ## Why strings? yes, the whole Mongo communication is based on BSON-Objects which is totally fine for performance. BSON Objects can very easily be cast to a "normal" Json string using the .toString() method. But unfortunately i did not find a way back... Sometimes it might be useful to be able to parse strings stored in files or even in Mongo -> endless possibilities ;-)  ## How does it work I added one new dependency to a "Simple JSon Parser" which does excactly, what the name states: simply parsing Json. In our case into BSon representation. This can than easily be put into the "normal" mapping algorithm to create the corresponding Java-Class. ```java     String json=morphium.getMapper().marshall(myEntity).toString();     MyEntity myEntity=morphium.getMapper().unmarshall(MyEntity.class,json) ``` ## Caveats When de-serializing your json string, you need to know, which class to use to de-serialize the data into. If you have several different objects stored in your json text, you probably should set @Embedded(polymorph=true) as annotation to your entity. (Polymorph objects cause morphium to store the classname) One other thing: if you don't want to use ids in your object, use the @Embedded annotation...  ATTENTION: right now, this string parsing can parse one Json object at a time, so you cannot have several objects stored in one string. If there is need for changing that, contact me...  # Morphium paged iterator  ## Description Problem is, when dealing with huge tables or lots of data, you'd probably include paging to your queries. You would read data in chunks of for example 100 objects to avoid memory overflows. This is now available by Morphium. The new MorphiumIterator works as Iterable or Iterator - whatever you like. It's included in the Query-interface and can be used very easily: ```java Query<Type> q=morphium.createQueryFor(Type.class); q=q.f("field").eq..... //whatever  for (Type t:q.asIterable()) {    //do something with t } ``` This creates an iterator, reading all objects from the query in chunks of 10... if you want to read them one by one, you only ned to give the chunk-size to the call: ```java for (Type t:q.asIterable(1)) {    //now reads every single Object from db } ``` # The Iterator You can also use the iterator as in the "good ol' days". ```java    Iterator<Type> it=q.asIterable(100);  //reads objects in chunks of 100    while (it.hasNext()) {     ... //do something    } ``` If you use the MorphiumIterator as the type it acutally is, you'd get even more information: ```java    MorphiumIterator<Type> it=q.asIterable(100);    it.next();    ....    long count=it.getCount(); //returns the number of objects to be read    int cursorPos=it.getCursor(); //where are we right now, how many did we read    it.ahead(5); //jump ahead 5 objects    it.back(4); //jump back     int bufferSize=it.getCurrentBufferSize(); //how many objects are currently stored in RAM    List<Type> lst=it.getCurrentBuffer(); //get the objects in RAM ``` **Attention**: the count is the number of objects matching the query at the instanciation of the iterator. This ensures, that the iterator terminates. The Query will be executed every time the buffer boundaries are reached. It might cause unexpected results, if the sort of the query is wrong.  For example: ```java    //created Uncached Objects with counter 1-100; value is always "v"    Query<UncachedObject> qu=morphium.createQueryFor(UncachedObject.class).sort("-counter");    for (UncachedObject u:qu.asIterable()) {        UncachedObject uc=new UncachedObject();             uc.setCounter(u.getCounter()+1);             uc.setValue("WRONG!");             MorphiumSingleton.get().store(uc);             log.info("Current Counter: "+u.getCounter()+" and Value: "+u.getValue());    } ``` The output is as follows: ``` 14:21:10,494 INFO  [main] IteratorTest: Current Counter: 100 and Value: v 14:21:10,529 INFO  [main] IteratorTest: Current Counter: 99 and Value: v 14:21:10,565 INFO  [main] IteratorTest: Current Counter: 98 and Value: v 14:21:10,610 INFO  [main] IteratorTest: Current Counter: 97 and Value: v 14:21:10,645 INFO  [main] IteratorTest: Current Counter: 96 and Value: v 14:21:10,680 INFO  [main] IteratorTest: Current Counter: 95 and Value: v 14:21:10,715 INFO  [main] IteratorTest: Current Counter: 94 and Value: v 14:21:10,751 INFO  [main] IteratorTest: Current Counter: 93 and Value: v 14:21:10,786 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:10,822 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:10,857 INFO  [main] IteratorTest: Current Counter: 96 and Value: WRONG! 14:21:10,892 INFO  [main] IteratorTest: Current Counter: 95 and Value: v 14:21:10,927 INFO  [main] IteratorTest: Current Counter: 95 and Value: WRONG! 14:21:10,963 INFO  [main] IteratorTest: Current Counter: 94 and Value: v 14:21:10,999 INFO  [main] IteratorTest: Current Counter: 94 and Value: WRONG! 14:21:11,035 INFO  [main] IteratorTest: Current Counter: 93 and Value: v 14:21:11,070 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,105 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:11,140 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:11,175 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:11,210 INFO  [main] IteratorTest: Current Counter: 94 and Value: WRONG! 14:21:11,245 INFO  [main] IteratorTest: Current Counter: 94 and Value: WRONG! 14:21:11,284 INFO  [main] IteratorTest: Current Counter: 93 and Value: v 14:21:11,328 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,361 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,397 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,432 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:11,467 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:11,502 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:11,538 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:11,572 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,607 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,642 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,677 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,713 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,748 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:11,783 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:11,819 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:11,853 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:11,889 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:11,923 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,958 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:11,993 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:12,028 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:12,063 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:12,098 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,133 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,168 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,203 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,239 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:12,273 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:12,308 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:12,344 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:12,379 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:12,413 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,448 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,487 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,521 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,557 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,592 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:12,626 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:12,662 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:12,697 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:12,733 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,769 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,804 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,839 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,874 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,910 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:12,945 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:12,980 INFO  [main] IteratorTest: Current Counter: 93 and Value: WRONG! 14:21:13,015 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:13,051 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,085 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,121 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,156 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,192 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,226 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,262 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,297 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:13,331 INFO  [main] IteratorTest: Current Counter: 92 and Value: v 14:21:13,367 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,403 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,446 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,485 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,520 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,556 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,592 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,627 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,662 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:13,697 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,733 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,768 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,805 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,841 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,875 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,911 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,946 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:13,982 INFO  [main] IteratorTest: Current Counter: 92 and Value: WRONG! 14:21:14,017 INFO  [main] IteratorTest: Current Counter: 91 and Value: v 14:21:14,017 INFO  [main] IteratorTest: Cleaning up... 14:21:14,088 INFO  [main] IteratorTest: done... ``` The first chunk is ok, but all that follow are not. Fortunately count did not change or in this case, the iterator would never stop. Hence, if your collection changes while you're iterating over it, you might get inexpected results. Writing to the same collection within the loop of the iterator is generally a bad idea...  ## Advanced Features  Since V2.2.5 the morphium iterator supports lookahead (prefetching). This means its not only possible to define a window size to step through your data, but also how many of those windows should be prefetched, while you step through the first one.  This works totally transparent for the user, its just a simple call to activate this feature:  ``` theQuery.asIterable(1000,5); //window size 1000, 5 windows prefetch ```  Since 2.2.5 the morphium iterator is also able to be used by multiple threads simultaneously. This means, several threads access the _same_ iterator. This might be useful for querying and alike. To use that, you only need to set setMultithreaddedAccess to true in the iterator itself:  ``` MorphiumIterator<MyEntity> it=theQuery.asIterable(1000,15) it.setMultithreaddedAccess(true); ```  *Attention*: Setting mutlithreaddedAccess to true will cause the iterator to be a bit slower as it has to do some things in a `synchronized` fashion.  # Partially Updatable Entities The idea behind partial updates is, that only the changes to an entity are transmitted to the database and will thus reduce the load on network and Mongodb itself. Partially updatable entities is the implementation in Morphium and can be used in several ways.  ## Application driven This is the easiest way - you already know, what fields you changed and maybe you even do not want to store fields, that you actually did change. In that case, call the updateUsingFields-Method: ```java    UncachedObject o....    o.setValue("A value");    o.setCounter(105);    Morphium.get().updateUsingFields(o,"value"); //does only send updates for Value to mongodb                                                                          //counter is ignored ``` updateUsingFields honors the lifecycle methods as well as caches (write cache or clear read_cache on write). take a look at some code from the corresponding junit-test for better understanding: ```java  UncachedObject o... //read from MongoDB             o.setValue("Updated!");             Morphium.get().updateUsingFields(o, "value");             log.info("uncached object altered... look for it");             Query<UncachedObject> c=Morphium.get().createQueryFor(UncachedObject.class);             UncachedObject fnd= (UncachedObject) c.f("_id").eq( o.getMongoId()).get();             assert(fnd.getValue().equals("Updated!")):"Value not changed? "+fnd.getValue(); ``` ## Application driven - 2nd way Implement the interface PartiallyUpdateablewhich ensures, that one method is implemented in your entity, returning all fields that should be persisted with the next save operation. ```java @Entity public class SimpleEntitiy  implements ParitallyUpdateable {       private String v1;       @Property(fieldName="name")       private String theName;       ....       private List<String> updateFields=new ArrayList<String>();       ....        public void setV1(String v) {             v1=v;             updateFields.add("v1");      }       public void setTheName(String n) {           theName=n;           updateFields.add("name");      }     List<String> getAlteredFields() {          return updateFields;     } ``` This should illustrate, how this works. When store with such an entity-object is called, morphium checks whether or not PartiallyUpdateable is implemented and calls getAlteredFields to do a partial update (by calling updateUsingFields)  ## Through proxy Morphium is able to create a transparent proxy for your entities, taking care of all this mentioned above. See here the code from the JUnit-Test: ```java         UncachedObject uo=new UncachedObject();         uo=Morphium.get().createPartiallyUpdateableEntity(uo);         assert(uo instanceof PartiallyUpdateable):"Created proxy incorrect";         uo.setValue("A TEST");         List<String> alteredFields = ((PartiallyUpdateable) uo).getAlteredFields();         for (String f:alteredFields) {             log.info("Field altered: "+f);         }         assert(alteredFields.contains("value")):"Field not set?"; ``` The Object created by createPartiallyUpdateableEntity is a proxy object which does intercept all set-Method calls.  BUT: to be sure, that the correct field is stored in your List of altered fields, the method should have a UpdatingField annotation attached, to specify the field, that this setter does alter. This is actually only necessary, if the field name differs from the setter-name (according to standard java best practices). Usually, if you have a setter called setTheValue, the field theValue is altered. Unless you changed the field name with a @Property-Annotation, you do not need to do anything here... By rule of thumb: If you have a @Property-Annotation to your fields, you should also add the corresponding @UpdatingField-Annotation to the setter...  Actually, this sounds more complicated as it is ;-)  ## using the @PartialUpdate Annotation The current version supports automate parial updates. You only have to add the @PartialUpdate Annotation to your class and off you go. Attention: there are some things you need to take care of:  when you use @PartialUpdate Annotation to your class definition, all instances created by reading this object from Mongo will be embedded in a proxy, which does take car of the partial updates for you (see above createPartiallyUpdateableEntity). This may cause problems with Reflection or .getClass()-Code When you create the object yourself, it's not partial updateable - you need to call createPartiallyUpdateableEntity manually  This mechanism relays on coding convention CamelCase, which means, setter need to be named after the property they set - for Example setName should set the property name and setLastName the property lastName (as usual) if you have a non-setter Method (like incSomething) which does modify a Property, you need to add the @PartialUpdate-Annotation to the method specifying the property, which will be changed after calling this, e.g. ```java @Entity @PartialUpdate class PUTest {     private int counter;     //getter do not change values - hence not interesting for PartialUpdates     public int getCounter() {        return counter;     }     //Setter Name follows coding convention -> will work fine     public void setCounter(int c) {         counter=c;     }      //as this modifies a field, but is not named according to the setter-rule, you need to specify      //the corresponding field    @PartialUpdate("counter")     public void inc() {         counter++;     } }      ``` you can add the annotation to any method you like, the specified field will be add to the modified filed list accordingly it's not possible yet to specify more than one field to be changed or add the annotation more than once this code is not (yet) tested in conjunction with lazy loading - it might cause problems here dereferencing entities  ## Limitations This code is rather new and not heavily tested yet, the code is implemented honoring maps, lists and embedded entities, but there is not Unit-Test for those cases jet. It does not work for embedded entities yet (and I'm not sure how to implement that feature - if you have ideas, just contribute).  # Polymorphism in Morphium  Morphium supports polymorphism. That means, you can store objects of different kind in one collection and process them with a single call. Please keep these things in mind when using polymorphsim in morphium: * you need to make sure that all types are stored in the same collection. Usually you would do that by providing an own name provider * all entities in the collection should have a common superclass. Otherwise there would be strange "ClassCastExceptions" * when accessing the collection, you should always use the supertype. otherwise data might be lost. And, if storing an update, the type might change.  lets get in more detail about the last item. Imaginge you stored Several objects in one collection, all of those of type Person. Now you add an Employee, which is as subclass of person, to this collection. When you access the collection as Employee, all your persons will read into Employee objects - which means, this might break constraints or might even fail.  here is a testcase for morphium, which shows how polymorphism might be used: ```java package de.caluga.test.mongo.suite;  import de.caluga.morphium.Morphium; import de.caluga.morphium.MorphiumSingleton; import de.caluga.morphium.NameProvider; import de.caluga.morphium.ObjectMapper; import de.caluga.morphium.annotations.Entity; import de.caluga.morphium.annotations.Id; import de.caluga.morphium.annotations.caching.NoCache; import org.bson.types.ObjectId; import org.junit.Test;  import java.util.List;  /**  * Created with IntelliJ IDEA.  * User: stephan  * Date: 27.11.13  * Time: 08:39  * Test polymorphism mechanism in Morphium  */ public class PolymorphismTest extends MongoTest {     @Test     public void polymorphTest() throws Exception {         MorphiumSingleton.get().dropCollection(PolyTest.class);         OtherSubClass p = new OtherSubClass();         p.setPoly("poly");         p.setOther("other");         MorphiumSingleton.get().store(p);          SubClass sb = new SubClass();         sb.setPoly("poly super");         sb.setSub("sub");         MorphiumSingleton.get().store(sb);          assert (MorphiumSingleton.get().createQueryFor(PolyTest.class).countAll() == 2);         List<PolyTest> lst = MorphiumSingleton.get().createQueryFor(PolyTest.class).asList();         for (PolyTest tst : lst) {             log.info("Class " + tst.getClass().toString());         }     }      public static class PolyNameProvider implements NameProvider {          @Override         public String getCollectionName(Class<?> type, ObjectMapper om, boolean translateCamelCase, boolean useFQN, String specifiedName, Morphium morphium) {             return "poly";         }     }      @Entity(polymorph = true, nameProvider = PolyNameProvider.class)     @NoCache     public static abstract class PolyTest {         private String poly;          @Id         private ObjectId id;          public String getPoly() {             return poly;         }          public void setPoly(String poly) {             this.poly = poly;         }     }      public static class SubClass extends PolyTest {         private String sub;          public String getSub() {             return sub;         }          public void setSub(String sub) {             this.sub = sub;         }     }      public static class OtherSubClass extends PolyTest {         private String other;          public String getOther() {             return other;         }          public void setOther(String o) {             this.other = o;         }     } } ```  #Name provider  mongo is really fast and stores a lot of date in no time. Sometimes it's hard then, to get this data out of mongo again, especially for logs this might be an issue (in our case, we had more than a 100 million entries in one collection). It might be a good idea to change the collection name upon some rule (by date, timestamp whatever you like). Morphium supports this using a strategy-pattern.  ```java public class DatedCollectionNameProvider implements NameProvider{     @Override     public String getCollectionName(Class<?> type, ObjectMapper om, boolean translateCamelCase, boolean useFQN, String specifiedName, Morphium morphium) {         SimpleDateFormat df=new SimpleDateFormat("yyyyMM");         String date=df.format(new Date());         String ret=null;         if (specifiedName!=null) {             ret=specifiedName+="_"+date;         } else {                 String name = type.getSimpleName();                 if (useFQN) {                     name=type.getName();                 }             if (translateCamelCase) {                 name=om.convertCamelCase(name);             }             ret=name+"_"+date;         }           return ret;     } }  ``` This would create a monthly named collection like "my_entity_201206". In order to use that name provider, just add it to your @Entity-Annotation:   ```java @Entity(nameProvider = DatedCollectionNameProvider.class) public class MyEntity { .... } ```  ## Performance The name provider instances themselves are cached for each type upon first use, so you actually might do as much work as possible in the constructor.  BUT: on every read or store of an object the corresponding name provider method `getCollectionName` is called, this might cause Performance drawbacks, if you logic in there is quite time consuming.   # Retries on network error ## Write Concern is not enough The write concern aka WriteSafety-Annotation in morphium is not enough for being on the safe side. the WriteSafety only makes sure, that, if all is ok, data is written to the amount of nodes, you want it to be written. You define the safety level more or less in an Application point of view. This does not affect networking outage or other problems. Hence, you can set several retry-Settings...  ## retry settings in Writers Morphium has 3 different types of writers: * the normal writer: supports asynchronous and snychronous writes * the async writer: forces asyncnhrounous writes * the buffered writer: stores write requests in a buffer and executes those on block  This has some implications, as the core of morphium is asynchrounous, we need to make sure, there are not too many pending writes. (the "pile" is determined by the maximum amount of connections to mongo - hence this is something you won't need to configure) This is where the retry settings for writers come in. When writing data, this data is either written synchronously or asynchonously. In the latter case, the requests tend to pile up on heavy load. And we need to handle the case, when this pile gets too high. This is the retry. When the pile of pending requests is too high, wait for a speicified amount of time and try again to queue the operation. If that fails for all retries - throw an exception.  ## Retry settings for Network errors As we have a really sh... network which causes problems more than once a day, I needed to come up with a solution for this as well. As our network does not fail for more than a couple of requests, the idea is to detect network problems and retry the operation after a certain amount of time. This setting is specified globally in morphium config:  ```java         morphium.getConfig().setRetriesOnNetworkError(10);         morphium.getConfig().setSleepBetweenNetworkErrorRetries(500); ```  This causes morphium to retry any operation on mongo 10 times (if a network related error occurs) and pause 500ms between each try. This includes, reads, writes, updates, index creation and aggregation. If the access failed after the (in this case) 10th try - rethrow the networking error to the caller.   # InfluxDB Driver for Morphium 3.x  This is an implementation of the `MorphiumDriver` interface in the [morphium](https://github.com/sboesebeck/morphium) project.   Morphium was built to be a mongodb abstraction layer and driver. With Version `3.0` a new Driver architecture was introduced, making morphium able to work with theoretically any kind of database.  InfluxDB is a time series db which is something very different to mongodb and does not have a complex ql like a full blown SQL-DB.  Of course, using this driver will have some drawbacks, you will not be able to do everything, you could do with the CLI or a native driver.  ## Usecases  - send timeseries data to influxdb in an efficient and "known" way utelizing all the good stuff, morphium does offer (bulk requests etc) - simple cluster support - do simple queries to influx, gathering high level information  what this driver will __not__ do:  - help you visualizing things - administration of influxdb   ## How To  quite simple. Instanciate Morphium as usual, just set a different driver:  ```java         MorphiumConfig cfg = new MorphiumConfig("graphite", 100, 1000, 10000);         cfg.setDriverClass(InfluxDbDriver.class.getName());         cfg.addHostToSeed("localhost:8086");         cfg.setDatabase("graphite");          cfg.setLogLevelForClass(InfluxDbDriver.class, 5);         cfg.setGlobalLogSynced(true);         Morphium m = new Morphium(cfg); ```  Sending Data do influx works same as if it was a mongodb:  ```java             EntTest e = new EntTest();             //fill with data ...                         m.store(e); ```  reading from influx works also similar to mongodb, with minor changes:  ```java   Query<EntTest> q=m.createQueryFor(EntTest.class).f("host").eq(hosts[0]).f("lfd").gt(12);         q.addProjection("reqtime","mean");         q.addProjection("ret_code","group by");         List<EntTest> lst=q.asList();         System.out.println("Got results:"+lst.size()); ```  you need to set the aggregation method as projection operator. Also the `group by` clause is modeled ther.  And one major _hack_: if you want to do queries using time constraints like `where time > now() - 10d` you need to query using `time()`as morphium needs to detect that this is not a field but a function you're calling:  ```java  q=m.createQueryFor(EntTest.class).f("host").eq(hosts[1]).f("lfd").gt(12).f("time()").gt("now() - 100d");         q.addProjection("reqtime","mean");         q.addProjection("ret_code","group by"); ```   # Text Search  Mongodb has since V3.x a built in text search functionality. This can be used in commandline, or using morphium:  ```java   @Test     public void textIndexTest() throws Exception {         morphium.dropCollection(Person.class);         try {             morphium.ensureIndicesFor(Person.class);          } catch (Exception e) {             log.info("Text search not enabled - test skipped");             return;         }         createData();         waitForWrites();         Query<Person> p = morphium.createQueryFor(Person.class);         List<Person> lst = p.text(Query.TextSearchLanguages.english, "hugo", "bruce").asList();         assert (lst.size() == 2) : "size is " + lst.size();         p = morphium.createQueryFor(Person.class);         lst = p.text(Query.TextSearchLanguages.english, false, false, "Hugo", "Bruce").asList();         assert (lst.size() == 2) : "size is " + lst.size();     }  ```  In this case, there is some Data begin created, which puts the name of some superheroes in a mongo. Searching for the text ist something different than searching via regular expressions, because Text Indexes are way more efficient in that case.  If you need more information on text indexes, have a look at Mongodbs documentation and take a look at the Tests for TextIndexes within the sourcecode of morphium.
Raxa/raxacore	raxacore ========  The raxacore is an OpenMRS module that hosts all the services that are required by the RaxaEMR on-top of the OpenMRS installation.  The module provides core REST services required for core RaxaEMR management RaxaEMR modules may have their own backend services provided by other OpenMRS modules This module should only depend on the OpenMRS core and webservices.rest module  The raxacore module provides the following services - PatientListService
jclouds/jclouds-karaf	# Karaf jclouds Integration  This project currently hosts a Karaf feature repository for easy installation of jclouds inside Apache Karaf. It also provides Managed Service Factories for creating Compute and BlobStore services. Last but not least it provides a rich command set for using jclouds from the Karaf shell.  There is also support for using Chef via the [jclouds Chef integration](https://jclouds.apache.org/guides/chef/).  Usage Instructions ================== The instructions will make use of Amazon EC2 and S3, but can be applied to any provider or api supported by jclouds.  On Karaf 4.0.x or later:  Install jclouds AWS Modules --------------------------- Install the feature and a provider for blobstore and compute service:      karaf@root()> feature:repo-add mvn:org.apache.jclouds.karaf/jclouds-karaf/2.1.0/xml/features     karaf@root()> feature:install jclouds-aws-s3     karaf@root()> feature:install jclouds-aws-ec2      # Install the features from jclouds-labs by adding the labs repo:     karaf@root()> feature:repo-add mvn:org.apache.jclouds.karaf/jclouds-karaf-labs/2.1.0/xml/features     karaf@root()> feature:install jclouds-cloudsigma2-mia  Install Karaf Commands:      karaf@root()> feature:install jclouds-commands  To see the list of available jclouds modules for Karaf      karaf@root()> feature:list | grep jclouds  Compute Service --------------- Jclouds Karaf provides Managed Service Factories for ComputeService. There are currently two ways of creating a compute service:  * **Using the jclouds:compute-service-create command** * **By manually creating the configuration**  **Using the jclouds:compute-service-create command**  The compute service command allows you to create a reusable compute service for a jclouds provider or api. To create a compute service for the EC2 provider:      karaf@root()> jclouds:compute-service-create --provider aws-ec2 --identity XXXXXX --credential XXXXXXX  and for creating a compute service using an api, for example openstack:      karaf@root()> jclouds:compute-service-create --api openstack-nova --endpoint XXXXXXXX --identity XXXXXX --credential XXXXXXX  Note that when using apis you usually need to also specify the endpoint too. The command also supports adding extra options as key/value pairs. For example:      karaf@root()> jclouds:compute-service-create --api openstack-nova --endpoint XXXXXXXX --identity XXXXXX --credential XXXXXXX --add-option jclouds.keystone.credential-type=passwordCredentials  To see the list of installed providers and apis or remove the service for one of the providers, you can use the jclouds:compute-service-list and jclouds-compute-service-remove commands.  **Using the Karaf config commands**  To create a compute service using the Karaf's integration with the configuration admin all that needs to be done is to create a configuration with factory pid: org.jclouds.compute.      karaf@root()> config:edit  org.jclouds.compute-ec2     karaf@root()> config:property-set provider aws-ec2     karaf@root()> config:property-set identity XXXXXXXXX     karaf@root()> config:property-set credential XXXXXXXXX     karaf@root()> config:property-set jclouds.ec2.ami-owners  XXXXXXXXX     karaf@root()> config:update  Use the compute service commands      karaf@root()> jclouds:node-create --imageId YOUR_IMAGE_ID --locationId YOUR_LOCATION_ID GROUPNAME     karaf@root()> jclouds:node-list.  If you don't want/need to specify specific image, you specify the os family and the os version      karaf@root()> jclouds:node-create --os-family OS_FAMILY --os-version OS_VERSION --locationId YOUR_LOCATION_ID GROUPNAME  **Note:** You can supply additional options to select hardware etc.  Run a script to a single node or a group of nodes:      karaf@root()> jclouds:group-runscript --script-url URL_OF_THE_SCRIPT GROUPNAME.     karaf@root()> jclouds:node-runscript --script-url URL_OF_THE_SCRIPT NODEID.  For simple commands you can just inline the command, for example to get the uptime of the node:      karaf@root()> jclouds:group-runscript --direct uptime GROUPNAME.     karaf@root()> jclouds:node-runscript --direct uptime NODEID.  Or you can use whatever command you want.  Shutdown all your nodes or the nodes of a specific group:      karaf@root()> jclouds:group-destroy GROUPNAME     karaf@root()> jclouds:node-destroy-all GROUPNAME   BlobStore --------- There are currently two ways of creating a service for blobstore service:  * **Using the jclouds:blobstore-service-create-command** * **By manually creating the configuration**  **Using the jclouds:blobstore-service-create command**  The compute service command allows you to create and reuse blobstore service for a jclouds provider or api. To create a compute service for the S3 provider:      karaf@root()> jclouds:blobstore-service-create --provider aws-s3 --identity XXXXXX --credential XXXXXXX  To see the list of installed providers and apis or remove the service for one of the providers, you can use the jclouds:blobstore-service-list and jclouds-blobstore-service-remove commands.  **Using the Karaf config commands** Create a sample blobstore service, by using the console:      karaf@root()> config:edit  org.jclouds.blobstore-s3     karaf@root()> config:property-set provider aws-s3     karaf@root()> config:property-set identity XXXXXXXXX     karaf@root()> config:property-set credential XXXXXXXXX     karaf@root()> config:update  You can use the shell commands to list, create, delete, read or write to a blob:      karaf@root()> jclouds:blobstore-write BUCKET_NAME BLOB_NAME payload     karaf@root()> jclouds:blobstore-read BUCKET_NAME BLOB_NAME  This works well for String payloads, but for binary payloads the user can use the url to be used as input or output for the commands:      karaf@root()> jclouds:blobstore-write BUCKET_NAME BLOB_NAME URL_POINTING_TO_THE_PAYLOAD.     karaf@root()> jclouds:blobstore-read BUCKET_NAME BLOB_NAME LOCAL_FILE_TO_STORE_THE_BLOB.  If the payload represents a URI the content of the URL will be written instead. You can bypass this by specifying the *--store-url* and store the url as a string.  BlobStore URL Handler --------------------- The commands above are useful when using the shell, but most of the time you will want the use of blobstore to be transparent. Jclouds Karaf also provides a url handler which will allow you to use blobstore by using URLs of the following format:  <b>blob:/PROVIDER/CONTAINER/BLOB</b>  Chef ---- You can install the chef api, with the following command:      karaf@root()> feature:install jclouds-chef-api  Managed Service Factories and commands are also provided for Chef. The managed service factory allows you to create a reusable service just by passing the configuration. To install the managed service factories and the chef commands, you need to install the jclouds-chef feature:      karaf@root()>feature:install jclouds-chef  Then you can create a chef service, using the chef:service-create command:      karaf@root()> chef:service-create  --api chef --client-name CLIENT --validator-name VALIDATOR --client-key-file CLIENT.pem --validator-key-file VALIDATOR.pem --endpoint ENDPOINT  **OPSCODE Chef Example:** The above command for opscode chef, with client iocanel and validator iocanel-validator, the command looks like:      karaf@root()> chef:service-create  --api chef --client-name iocanel --validator-name iocanel-validator --client-key-file /Users/iocanel/.chef/iocanel.pem --validator-key-file /Users/iocanel/.chef/iocanel-validator.pem --endpoint https://api.opscode.com/organizations/iocanel  Once the service has been create, you can list your cookbooks using:      karaf@root()> chef:cookbook-list  **Using the Chef Service with any Provider / Api:** Once you have created the chef service and have made sure a couple of cookbooks are uploaded. You can use chef with any other compute service in your system. In the example above it will be used with EC2:      karaf@root()> node-create --imageId eu-west-1/ami-c1aaabb5 --hardwareId m1.medium --adminAccess  karaf      [id]                 [location] [hardware] [group]   [status]     eu-west-1/i-bbb5eff0 eu-west-1c m1.medium  karafchef RUNNING      karaf@root()> chef:node-bootstrap  eu-west-1/i-bbb5eff0 java::openjdk  The above can be also performed in a single step using the --recipe option:      karaf@root()> node-create --imageId eu-west-1/ami-c1aaabb5 --hardwareId m1.medium --adminAccess --recipe chef/java::openjdk karaf   Using multiple services per provider/api ----------------------------------------  As of jclouds-karaf 1.5.0 you are able to register multiple compute and blobstore services per provider or api. The commands will allow you to specify which service to use (just specifying provider/api isn't enough since we have multiple services). To "name" the service, you can use the --name option in the service create commands. If no id is specified the provider/api name will be used instead.  For compute services:      jclouds:compute-service-create --name aws1 --provider aws-ec2 ...     jclouds:node-list --name aws1   This can be very useful when you want to configure either different accounts per provider/api or use different configuration options. A small example:      jclouds:compute-service-create --name aws-eu-west-1 --provider aws-ec2 --add-option jclouds.regions=eu-west-1     jclouds:compute-service-create --name aws-us-east-1 --provider aws-ec2 --add-option jclouds.regions=us-east-1  The available ids are now shown in the compute-service-list commands:      jclouds:compute-service-list      Compute Providers:     ------------------     [id]                     [type]       [service]     aws-ec2                  compute      [ aws-eu-west-1 aws-us-east-1 ]   To destroy one of the two available services:      jclouds:compute-service-destroy aws-us-east-1     jclouds:compute-service-list      Compute Providers:     ------------------     [id]                     [type]       [service]     aws-ec2                  compute      [ aws-eu-west-1 ]   Blobstore services work in a very similar manner:      jclouds:blobstore-service-create --name s3-1 --provider aws-s3 ...   Using environmental variables ----------------------------- When it comes to creating a service, you usually need to specify a provider/api, identity, credentials and endpoint. You can either specify them using the command options as shown above, or pull them from your environment, if the corresponding environmental variables are found. Supported variables:  For Compute Services:  * **JCLOUDS_COMPUTE_PROVIDER** The name of the compute provider. * **JCLOUDS_COMPUTE_API** The name of the compute api. * **JCLOUDS_COMPUTE_IDENTITY** The identity for accessing the compute provider. * **JCLOUDS_COMPUTE_CREDENTIAL** The credential for accessing the compute provider. * **JCLOUDS_COMPUTE_ENDPOINT** The endpoint (This is usually needed when using compute apis). * **JCLOUDS_USER** The username of that will be used for accessing compute instances. * **JCLOUDS_PASSWORD** The password that will be used for accessing compute instances.  For Blob Stores:  * **JCLOUDS_BLOBSTORE_PROVIDER** The name of the blobstore provider. * **JCLOUDS_BLOBSTORE_API** The name of the blobstore api. * **JCLOUDS_BLOBSTORE_IDENTITY** The identity for accessing the blobstore provider. * **JCLOUDS_BLOBSTORE_CREDENTIAL** The credential for accessing the blobstore provider. * **JCLOUDS_BLOBSTORE_ENDPOINT** The endpoint (This is usually needed when using blobstore apis).  For Chef:  * **JCLOUDS_CHEF_API** The name of the blobstore api. * **JCLOUDS_CHEF_CLIENT_NAME** The client name. * **JCLOUDS_CHEF_CLIENT_CREDENTIAL** The client credential. * **JCLOUDS_CHEF_CLIENT_KEY_FILE** The path of the client key file (can be used instead of the above). * **JCLOUDS_CHEF_VALIDATOR_NAME** The validator name. * **JCLOUDS_CHEF_VALIDATOR_CREDENTIAL** The validator credential. * **JCLOUDS_CHEF_VALIDATOR_KEY_FILE** The path of the validator key file (can be used instead of the above). * **JCLOUDS_CHEF_ENDPOINT** The endpoint (This is usually needed when using chef apis).   Configuring command output -------------------------- As of jclouds-karaf version 1.5.0-beta.11 jclouds-karaf commands support output customization. The customization features are:  * **Width calculation** The commands calculate the required column width and adjust the format accordingly. * **Configurable columns** Can add remove columns using configuration. * **Groovy value retrieval** The display content is configurable using groovy expressions. * **Configurable column alignment** You can configure for each column left or right alignment. * **Configurable sorting options** Configure ordering by column using ascending or descending order.  The configuration for all columns can be found inside the org.jclouds.shell pid. Each configuration key is prefixed using the command category (node, image, location, hardware etc). The suffix defines the configuration topic. For example hardware.headers defines the headers to be displayed by the hardware commands. In the following commands the hardware category will be used as example.  **Defining the command headers** To specify the headers of a command we need to place to specify the headers configuration as a semicolon separated list. For hardware:       hardware.headers=[id];[ram];[cpu];[cores]   **Defining the display data** Display data are configured as a comma separated list of expressions (using the scripting engine of your choice, default is groovy). The expressions will be evaluated on the object of interest (in our example the hardware object). To display the id field of the hardware object the expression to use is hardware.id. The reason for choosing groovy (as a default) for retrieving the data and not a simple expression language is that groovy is powerful and can be used for more complex expressions. For example the Hardware object contains a collection of Processors and each processor has a filed of cores. To display the sum of cores among processors, we can use the following expression: hardware.processors.sum{it.cores}.  You can change the scripting engine:      hardware.engine=groovy  Please note that if you don't specify the engine, then groovy will be assumed.  To specify the display data, now all you need to do is to provide the expressions:      hardware.expressions=hardware.id;hardware.ram;hardware.processors.sum{it.cores*it.speed};hardware.processors.sum{it.cores}  The configuration above will display the hardware id in the first column, the hardware ram in the second column, the sum of cores X speed per processor in the third column and finally the sum of cores for all processors in the last column.  **Defining the sort order** To specify the sort column, the sortBy option can be used to point to the header of the column of interest. For example hardware hardware.sortby=[cpu].  **Changing the delimiter** Most of the configuration options for the shell table are passed as delimited strings. What happens when you want to change the delimiter? By default the delimiter is the semicolon symbol, but for each command category you can specify the delimiter. For example:       hardware.delimiter=,     hardware.headers=[id],[ram],[cpu],[cores]   Using jclouds-karaf with the OBR features of Karaf -------------------------------------------------- There are cases were there are small discrepancies between the jclouds-karaf required bundles and the ones that are used in your project. Even though inside OSGi you can have multiple versions of a bundle, it often doesn't make sense for micro versions.  To avoid that you can install the obr feature of Karaf before installing jclouds-karaf. The obr feature among others provides the obr resolver, which will try to check if osgi package requirements are satisfied by existing bundles, before installing new bundles.  For example, assuming that a given version of jclouds-karaf is using jersey 1.11 and in your containers version 1.13 is already installed, the obr resolver will check if the 1.13 version can satisfy your needs and if so it will skip the installation of 1.11.  Code completion ---------------  Most of the commands support tab completion, in order to help the user easily complete node ids, images, locations, blob containers etc.   ## License  Copyright (C) 2009-2013 The Apache Software Foundation  Licensed under the Apache License, Version 2.0
Belphemur/AdminCmd	New official repository is at http://github.com/AdminCMD/AdminCMD/
heroku/heroku.jar	# Heroku JAR The Heroku JAR is a java artifact that provides a simple wrapper for the Heroku REST API. The Heroku REST API allows Heroku users to manage their accounts, applications, addons, and other aspects related to Heroku.  [![Build Status](https://travis-ci.org/heroku/heroku.jar.svg?branch=master)](https://travis-ci.org/heroku/heroku.jar) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.heroku.api/heroku-api/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.heroku.api/heroku-api)  ## Usage  ### Add Dependencies to your pom.xml  ```xml <dependency>     <groupId>com.heroku.api</groupId>     <artifactId>heroku-api</artifactId>     <version>0.25</version> </dependency> <dependency>     <groupId>com.heroku.api</groupId>     <artifactId>heroku-json-jackson</artifactId>     <version>0.25</version> </dependency> <dependency>     <groupId>com.heroku.api</groupId>     <artifactId>heroku-http-apache</artifactId>     <version>0.25</version> </dependency> ```  The artifacts are in Maven Central so you won't need to build them locally first if you don't want to.  ### Use HerokuAPI HerokuAPI contains all the methods necessary to interact with Heroku's REST API. HerokuAPI must be instantiated with an API key in order to authenticate and make API calls. Requests to the API typically take no arguments, or simple strings. Responses come in the form of read-only POJOs, Maps, or void.  ```java String apiKey = "..."; HerokuAPI api = new HerokuAPI(apiKey); App app = api.createApp(); ```   ### API Key and Authentication Heroku uses an API key for authentication. The API key can be found on the [account page](https://api.heroku.com/account). API keys can be regenerated at any time by the user. Only the current API key shown on the account page will work. The API key only changes when a user chooses to [regenerate](https://api.heroku.com/account) -- keys do not expire automatically.  [Basic Authentication](http://www.ietf.org/rfc/rfc2617.txt) over HTTPS is used for authentication. An empty username and an API key are used to construct the Authorization HTTP header. HerokuAPI constructed with an API key, will handle authentication for API requests.  When using API keys: * If they need to be stored, store them securely (e.g. encrypt the file or database column). * Catch RequestFailedException in case of an authorization failure.  ### Examples  #### Instantiate HerokuAPI with an API Key ```java String apiKey = "..."; HerokuAPI api = new HerokuAPI(apiKey); ```  #### Create an Application ```java HerokuAPI api = new HerokuAPI(apiKey); App app = api.createApp(); ```  #### Create a named application on the cedar stack ```java HerokuAPI api = new HerokuAPI(apiKey); App app = api.createApp(new App().on(Heroku.Stack.Cedar).named("MyApp")); ```  #### List applications ```java HerokuAPI api = new HerokuAPI(apiKey); List<App> apps = api.listApps(); for (App app : apps) {     System.out.println(app.getName()); } ```  #### Update config ```java HerokuAPI api = new HerokuAPI(apiKey); api.updateConfig("myExistingApp", new HashMap<String,String>(){{put("SOME_KEY", "SOMEVALUE")}}); ```  #### Get Config ```java HerokuAPI api = new HerokuAPI(apiKey); Map<String, String> config = api.listConfig("myExistingApp"); for (Map.Entry<String, String> var : config.entrySet()) {     System.out.println(var.getKey() + "=" + var.getValue()); } ```  #### Remove Config The removeConfig call expects a single config var name to be removed. ```java HerokuAPI api = new HerokuAPI(apiKey); Map<String, String> config = api.removeConfig("myExistingApp", "configVarToRemove"); ```  #### Overriding the User-Agent Header The default User-Agent header is recommended for most use cases.  If this library is being used as part of another library or application that wishes to set its own User-Agent header value, implement the [`com.heroku.api.http.UserAgentValueProvider`](https://github.com/heroku/heroku.jar/blob/master/heroku-api/src/main/java/com/heroku/api/http/UserAgentValueProvider.java) interface and create a provider-configuration file at `META-INF/services/com.heroku.api.http.UserAgentValueProvider` containing the fully-qualified name of your provider class. See [`java.util.ServiceLoader`](http://docs.oracle.com/javase/6/docs/api/java/util/ServiceLoader.html) for details.  To conform to [RFC 2616 Section 14.43](http://tools.ietf.org/html/rfc2616#section-14.43), consider prepending the value from the [`DEFAULT`](https://github.com/heroku/heroku.jar/blob/master/heroku-api/src/main/java/com/heroku/api/http/UserAgentValueProvider.java) provider with your own user agent.  ## Building Locally  1. Clone the repo:          `git clone git@github.com:heroku/heroku-jar.git`  2. Build and install the jars:      * Without running the tests:              mvn install -DskipTests      * Or run with tests:              export HEROKU_TEST_USERS=[\{\"username\":\"defaultuser@heroku.com\",\"password\":\"defaultUserPass\",\"apikey\":\"defaultUserAPIKey\",\"defaultuser\":\"true\"\},\{\"username\":\"secondUser@heroku.com\",\"password\":\"password\",\"apikey\":\"apiKey\"\}]             mvn install  ## Continuous Integration  Tests are run automatically by Travis CI for all pushes and pull requests to `heroku/heroku.jar` with the exception of [pull requests from forks that only run unit tests](http://docs.travis-ci.com/user/pull-requests/#Security-Restrictions-when-testing-Pull-Requests). Release versions must be manually published as explained [below](#release).  ## Release  To release a new version, run the release script:  ``` $ bash release.sh ```  ## Some Design Considerations  ### Minimal Dependencies  One main design goal was to impose as few dependencies as possible on users of this api. Since there are a wide range of target users for this library, from build tools to ide plugins to applications, imposing an http client implementation or a json parsing implementation for users who are likely already using (a different) one was undesireable.  To achieve this goal it was necessary to break down the structure of the project to allow users to configure the exceution of the api in a way that dosent conflict with any dependencies in their project. Default implementations for http clients and json parsing are available should the user of this api decide to use them.  As a conseqence, the dependency configuration for the api is slightly more verbose.  Instead of a short dependency declaration and inflexible implementation such as this, (maven style)      <!--Not an actually available maven dependency-->     <dependency>        <groupId>com.heroku.api</groupId>        <artifactId>heroku-api-impl-with-httpclient-and-gson</artifactId>        <version>0.1-SNAPSHOT</version>     </dependency>  We opted for a slightly more verbose dependency declaration an a flexible implementation, like this       <dependency>        <groupId>com.heroku.api</groupId>        <artifactId>heroku-api</artifactId>        <version>0.1-SNAPSHOT</version>     </dependency>         <dependency>        <groupId>com.heroku.api</groupId>        <artifactId>heroku-json-gson</artifactId>        <version>0.1-SNAPSHOT</version>     </dependency>     <dependency>        <groupId>com.heroku.api</groupId>        <artifactId>heroku-http-apache</artifactId>        <version>0.1-SNAPSHOT</version>     </dependency>  ### Flexible async model  Since we have decided to allow pluggable http client implementations, we also decided to allow asynchronous apis provided by the underlying httpclient implementations to surface themselves in the API. The com.heroku.api.connection.AsyncConnection interface allows implementations to parameterize the type of "Future" object they return from an async request. So for instance...  The provided implementation of Connection that uses apache httpclient `implements AsyncConnection<java.util.concurrent.Future>` and so calls to executeCommandAsync will return a `<T extends CommandResponse> java.util.concurrent.Future<T>`  The provided implementation of Connection that uses twitter finagle `implements AsyncConnection<com.twitter.util.Future>` and so calls to executeCommandAsync will return a `<T extends CommandResponse> com.twitter.util.Future<T>`, which has a much richer, composable api than the java.util.concurrent.Future api.
evanchooly/javabot	[![Build Status](https://travis-ci.org/evanchooly/javabot.svg?branch=master)](https://travis-ci.org/evanchooly/javabot)  javabot =======  factoid bot for irc channels   Building -------- To build and test Javabot, you'll need to do a few things.  1. Install MongoDB.    For Fedora, you can install it with     ```       sudo yum install mongodb-server    ```    On OS X, you can use    ```       brew install mongo    ```    If you're using [Docker](https://www.docker.com/), you can run a Docker image for MongoDB:    ```       docker pull mongo    ```     (Note that if you're on Windows, you may need to add network translation to handle port 27017; see `images/nat    .png` for how this would look in VirtualBox.)  1. Copy `javabot-sample.properties` to `javabot.properties` and update any properties as needed.  If you want to run the web application  as well, copy `javabot-sample.yml` to `javabot.yml` and adjust as necessary.   1. Start mongodb; one example command, which will locate the data files in the    current directory, is this:   ```     mongod --noauth --dbpath . ```    In Docker, you'd use: ```     docker run -d -p 27017:27017 --name mongodb mongo ``` 1. Build and test.  Developing ------  If you use IDEA, make sure you have "Use plugin registry" enabled in your Maven configuration.  Note also that IDEA may not pick up the generated java source as part of the build path; if `Sofia` does not resolve after running `mvn compile` at least once, then open up the javabot.iml file and add `<sourceFolder url="file://$MODULE_DIR$/src/main/java" isTestSource="false" />` under the `<content>` XML node. `<content>` should look like this:      <content url="file://$MODULE_DIR$">       <sourceFolder url="file://$MODULE_DIR$/src/main/kotlin" isTestSource="false" />       <sourceFolder url="file://$MODULE_DIR$/src/main/java" isTestSource="false" />       <sourceFolder url="file://$MODULE_DIR$/src/main/resources" type="java-resource" />       <sourceFolder url="file://$MODULE_DIR$/src/test/kotlin" isTestSource="true" />       <sourceFolder url="file://$MODULE_DIR$/src/test/resources" type="java-test-resource" />       <sourceFolder url="file://$MODULE_DIR$/src/main" isTestSource="false" />       <excludeFolder url="file://$MODULE_DIR$/target" />     </content>  To test the web application aspects, you need to copy the `javabot-sample.yml` to `javabot.yml`; this will put the web container on port 8081 by default.
erdincyilmazel/Cambridge	![Cambridge Template Engine](https://github.com/erdincyilmazel/Cambridge/raw/master/docs/site/img/logo.png)  # Simple is the keyword  Cambridge is a template engine for the java platform which can be used with your web framework of choice. It currently supports servlets, JAX-RS, Play Framework. (Spring MVC and Struts 2.0 support is in development.)  Pure markup syntax, performance, extensibility and simplicity are the key features of Cambridge. Designed by web developers for web developers to make your life easier.  ## Show me the code      <div id="loginBox" a:if="!loggedIn">        Username: <input type="text" name="username"/>        Password: <input type="password" name="password"/>        <input type="button" onclick="login();" value="Sign in"/>     </div>     <div a:else>        <div>Hello ${user.name}!</div>        <div>Here are your online friends:</div>         <ul>           <li a:foreach="user.onlineFriends" a:as="friend">${friend.name}</li>        </ul>     </div>  If you're wondering what the html code above is doing, here is what it tells Cambridge to do. Lets start with the `a:if` attribute in the first line. It basically says, the div #loginBox should only be rendered if the loggedIn property in our data model is false. That is how you would do if blocks in Cambridge. It uses the scope of your html tag for defining where the if logic should be applied. The next tag which immediatelly follows the div with id #loginBox has another attribute named `a:else` which says this div should be rendered if the condition in the first tag was not met.  If you look at the `${user.name}` syntax, that is how you write expressions in Cambridge. In this case it is printing the name property of a User object.  Finally if the user is already logged in, we want to display online friends of the current user as a bunch of `<li>` elements. You don't need to wrap the `<li>` tags with some looping code in cambridge. All you need to do is to define an `a:foreach` attribute which will iterate over the onlineFriends property of the user object and render a separate `<li>` for every online friend of `user`.  The attributes like `a:if`, `a:foreach` are called Behaviors in Cambridge. You can attach behaviors to your markup tags by adding these attributes which themselves will not be displayed in the rendered output.  ## Overview of features  ### Pure markup syntax You don't need to pollute your html/xml template file with any server side code, tons of external tags or scripts. Everything can be achieved by just adding custom attributes to your existing tags.  ### Template inheritance Cambridge template files can extend other template files and override certain parts of the parent template. This makes maintaining your templates much easier with project wide skeleton templates that you can re-use.  ### Extensibility Cambridge is designed from ground up with modularity and extensibility in mind. The internal API which is used to build all the built-in behaviors, functions, filters and custom tags is very available to anyone using Cambridge so you can easily add new features to Cambridge. Cambridge also allows you to plug-in your expression language of choice such as MVEL, OGNL or JEXL if you want more features then what the built-in expression language provides.  ### Performance Cambridge is optimized for web-scale. Our benchmarks show that Cambridge outperforms its alternatives like Freemarker, Velocity and Play Framework. Once parsed, Cambridge templates get converted to highly efficient reusable/immutable objects.  ### Rich web framework support Cambridge comes with out of the box support for many of the MVC style popular web frameworks.  ### Java API to manipulate your templates With Cambridge, you can start with 100% pure html templates without any custom tags or attributes and attach dynamic behavior to them by just using a Java API.  ## Join the mailing list and get support  [Support Mailing List](http://groups.google.com/group/cambridgetemplates)
jenkinsci/deploy-plugin	Jenkins Deploy Plugin =========================  This plugin permits Jenkins to Deploy into containers and application servers.  See [Deploy Plugin](https://wiki.jenkins-ci.org/display/JENKINS/Deploy+Plugin) for more information.  [![Build Status](https://jenkins.ci.cloudbees.com/buildStatus/icon?job=plugins/deploy-plugin)](https://jenkins.ci.cloudbees.com/job/plugins/job/deploy-plugin/)  License -------  	The MIT License (MIT)      Copyright (c) 2014 <copyright holders>      Permission is hereby granted, free of charge, to any person obtaining a copy     of this software and associated documentation files (the "Software"), to deal     in the Software without restriction, including without limitation the rights     to use, copy, modify, merge, publish, distribute, sublicense, and/or sell     copies of the Software, and to permit persons to whom the Software is     furnished to do so, subject to the following conditions:      The above copyright notice and this permission notice shall be included in     all copies or substantial portions of the Software.      THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR     IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE     AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,     OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN     THE SOFTWARE.
zauberlabs/gnip4j	## What is gnip4j?  Gnip4j is an Open Source (ASFL 2.0) library to access and process Activities (tweets) from the [Gnip API](http://support.gnip.com/) from the data from the Java language.   ## Features    * 100% Pure Java - works on any Java Platform version 1.8 or later     * Minimal dependencies:        * The [Jackson JSON library](http://jackson.codehaus.org/) is the only requiered dependecy       * Optionally if `slf4-api` is present in the classpath, logging is supported.    * Out-of-the-box gzip support    * Push model via Streaming (asynchronous processing)    * Error handling support (and exponential-backoff reconnections)    * Monitoring Support (JMX)    * Enterprise Data Collector Support    * Powertrack V1 and V2 support  ## How To use gnip4j  For more information refer to the Gnip4j Reference Guide. Available formats:      * [https://github.com/zauberlabs/gnip4j/wiki/Reference]   ## Maven Integration  Add to your dependency Managment:  ```xml      <dependencyManagement>        <dependencies>         <dependency>            <groupId>com.zaubersoftware.gnip4j</groupId>            <artifactId>gnip4j-core</artifactId>            <version>${gnip4j.version}</version>         </dependency>         <dependency>            <groupId>com.zaubersoftware.gnip4j</groupId>            <artifactId>gnip4j-http</artifactId>            <version>${gnip4j.version}</version>         </dependency>       </dependencies>      </dependencyManagement>      ...      <properties>          <gnip4j.version>2.0.0</gnip4j.version>      </properties> ```  and add to your project:   ```xml     <dependencies>        <dependency>         <groupId>com.zaubersoftware.gnip4j</groupId>         <artifactId>gnip4j-core</artifactId>        </dependency>     </dependencies> ```  If you need logging adding `slf4j` as a dependency.   ## Code Snippet  ### Consuming ten raw Activities from the stream     ```java     final AtomicInteger counter = new AtomicInteger();      final DefaultGnipFacade x = DefaultGnipFacade.createPowertrackV2(…);     final GnipStream stream = x.createPowertrackStream(String.class)             .withAccount("YourAccount")             .withType("prod")             .withUnmarshall(new StringUnmarshaller())             .withObserver(new StreamNotificationAdapter<String>() {                 @Override                 public void notify(final String activity, final GnipStream stream) {                     System.out.println(activity);                     if (i >= 10) {                         stream.close();                     }                 }             })             .build();     stream.await(); ```  ## How To Contribute  [Send pull requests](http://help.github.com/pull-requests/) using GitHub.  Had an Issue? Fill it in https://github.com/zauberlabs/gnip4j/issues .  ## Get in Contact  If you want to get in contact with us, or people interested in the project, please visit our group at http://groups.google.com/group/gnip4j  ## License  Copyright (c) 2011-2016 Zauber S.A. <http://www.zaubersoftware.com/>  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.  gnip4j includes portions of software from licenced by the Apache Fundation (package `com.zaubersoftware.gnip4j.api.stats.commonsio`).  ## Code Conventions and Housekeeping  None of these is essential for a pull request, but they will all help.  They can also be added after the original pull request but before a merge.  * Use the project code format conventions. Import `XXXX.xml` from the root of   the project if you are using Eclipse. Checkstyle configuration is available    at `XXXX.xml` * Make sure all new .java files to have a simple Javadoc class comment with at   least an @author tag identifying you, and preferably at least a paragraph on    what the class is for. * Add the ASF license header comment to all new .java files (copy from existing    files in the project) * Add yourself as an @author to the .java files that you modify substantially   (more than cosmetic changes). * A few unit tests would help a lot as well - someone has to do it. * If no-one else is using your branch, please rebase it against the current   master (or other target branch in the main project).
Kegbot/kegbot-android	Kegbot for Android ===================  Overview --------  This is the source code for the Kegbot Android application!  Main repository: https://github.com/Kegbot/kegbot-android  Home page: http://kegbot.org/   Developers: Quick Setup Instructions ------------------------------------  Bear with us as better develop documentation is coming!  In the meantime, here are quick and dirty steps:  - Clone the kegbot-android repo - Eclipse: Import -> Existing Projects into Workspace. - Import the projects (Kegtab, KegtabTest)  Patches Welcome! ----------------  Kegbot is open source; we'd love to have your patches to make it better.  If you're considering adding something major, do get in touch with us in the forums or on #freenode to talk about it first; it should make the pull request go faster.  License and Copyright ---------------------  All code is offered under the GPLv2 license, unless otherwise noted. Please see LICENSE.txt for the full license.  All code and documentation are Copyright 2003-2012 Mike Wakerly, unless otherwise noted.  The Kegbot name and logo are trademarks of the Kegbot project; please don't reuse them without our permission.
octo-technology/sonar-objective-c	SonarQube Plugin for Objective C ================================  This repository hosts the Objective-C plugin for [SonarQube](http://www.sonarqube.org/). This plugin enables to analyze and track the quality of iOS (iPhone, iPad) and MacOS developments.  This plugin is not supported by SonarSource. SonarSource offers a [commercial SonarSource Objective-C plugin](http://www.sonarsource.com/products/plugins/languages/objective-c/) as well. Both plugins do not offer the same functionalities/support.  The development of this plugin has always been done thanks to the community. If you wish to contribute, check the [Contributing](https://github.com/octo-technology/sonar-objective-c/wiki/Contributing) wiki page.  Find below an example of an iOS SonarQube dashboard: <p align="center">   <img src="sample/screen%20shot%20SonarQube%20dashboard.png" alt="Example iOS SonarQube dashboard" width="80%"/> </p>  ###Features  - [ ] Complexity - [ ] Design - [x] Documentation - [x] Duplications - [x] Issues - [x] Size - [x] Tests  For more details, see the list of [SonarQube metrics](https://github.com/octo-technology/sonar-objective-c/wiki/Features) implemented or pending.  ###Compatibility  - Use 0.3.x releases for SonarQube < 4.3 - Use 0.4.x releases for SonarQube >= 4.3 (4.x and 5.x)  ###Download  The latest version is the 0.4.0 and it's available [here](http://bit.ly/18A7OkE). The latest SonarQube 3.x release is the 0.3.1, and it's available [here](http://bit.ly/1fSwd5I).    You can also download the latest build of the plugin from [Cloudbees](https://rfelden.ci.cloudbees.com/job/sonar-objective-c/lastSuccessfulBuild/artifact/target/).   In the worst case, the Maven repository with all snapshots and releases is available here: http://repository-rfelden.forge.cloudbees.com/  ###Prerequisites  - a Mac with Xcode... - [SonarQube](http://docs.codehaus.org/display/SONAR/Setup+and+Upgrade) and [SonarQube Runner](http://docs.codehaus.org/display/SONAR/Installing+and+Configuring+SonarQube+Runner) installed ([HomeBrew](http://brew.sh) installed and ```brew install sonar-runner```) - [xctool](https://github.com/facebook/xctool) ([HomeBrew](http://brew.sh) installed and ```brew install xctool```). If you are using Xcode 6, make sure to update xctool (```brew upgrade xctool```) to a version > 0.2.2. - [OCLint](http://docs.oclint.org/en/dev/intro/installation.html) installed. Version 0.8.1 recommended  ([HomeBrew](http://brew.sh) installed and ```brew install https://gist.githubusercontent.com/TonyAnhTran/e1522b93853c5a456b74/raw/157549c7a77261e906fb88bc5606afd8bd727a73/oclint.rb```).  - [gcovr](http://gcovr.com) installed  ###Installation (once for all your Objective-C projects) - Install [the plugin](http://bit.ly/18A7OkE) through the Update Center (of SonarQube) or download it into the $SONARQUBE_HOME/extensions/plugins directory - Copy [run-sonar.sh](https://rawgithub.com/octo-technology/sonar-objective-c/master/src/main/shell/run-sonar.sh) somewhere in your PATH - Restart the SonarQube server.  ###Configuration (once per project) - Copy [sonar-project.properties](https://rawgithub.com/octo-technology/sonar-objective-c/master/sample/sonar-project.properties) in your Xcode project root folder (along your .xcodeproj file) - Edit the *sonar-project.properties* file to match your Xcode iOS/MacOS project  **The good news is that you don't have to modify your Xcode project to enable SonarQube!**. Ok, there might be one needed modification if you don't have a specific scheme for your test target, but that's all.  ###Analysis - Run the script ```run-sonar.sh``` in your Xcode project root folder - Enjoy or file an issue!  ###Update (once per plugin update) - Install the [latest plugin](http://bit.ly/18A7OkE) version - Copy [run-sonar.sh](https://rawgithub.com/octo-technology/sonar-objective-c/master/src/main/shell/run-sonar.sh) somewhere in your PATH  If you still have *run-sonar.sh* file in each of your project (not recommended), you will need to update all those files.  ###Credits * **Cyril Picat** * **Gilles Grousset** * **Denis Bregeon** * **François Helg** * **Romain Felden** * **Mete Balci**  ###History - v0.4.0 (2015/01): support for SonarQube >= 4.3 (4.x & 5.x) - v0.3.1 (2013/10): fix release - v0.3 (2013/10): added support for OCUnit tests and test coverage - v0.2 (2013/10): added OCLint checks as SonarQube violations - v0.0.1 (2012/09): v0 with basic metrics such as nb lines of code, nb lines of comment, nb of files, duplications  ###License  SonarQube Plugin for Objective C is released under the GNU LGPL 3 license:   http://www.gnu.org/licenses/lgpl.txt
Salaboy/emergency-service-drools-app	This application was built to show how to architect applications using BPM, a Rule Engine and Complex Event Processing. The aim of the application is to provide a generic architecture to implement flexible solution t hat uses the previous mentioned topics as basic principles.  The application is right now decoupled in 7 logical modules that are responsible for different activities. We tried to maintain as decoupled as possible each module to provide a flexible and pluggeable architecture. If you don't like the implementation of one of these modules, you can easily replace it for one that cover all your needs.  Each module is a functional block that allow the overall application to run and interact with different services. This allows you to customize each module configuration to run smoothly on your specific environment. Main Modules Introduction  The following sections describe each module responsibility:  Project Modules   Model  This module contains your domain objects that represent the information that your application is interested to handle. This module now contains some extra utility classes and some WorkItemHandlers that will be used by the core module. I'm planning to create a separate module with these technical assets, but for now they can live there for practical reasons. Core  The core module contains all the Domain Specific Services interfaces and implementations. This project can be expanded as a multi module pom project in the future for storing one single service per project, allowing more flexibility for deployments.  World UI  This module allows us to see what is happening in the real world. You need to think about this module as a view of our world that allows to generate interactions. For real implementations this module can be seen as external sources of events that will be directly plugged to the core module with external connectors  Task Lists UI  We currently have two different implementations of this module. This module represent the software that is being used inside the Emergency Services company. You will find in the project source code that a Standalone implementation and a Web Application provide the same functionality. Two more implementations are being created for demonstrating best practices and for mobile phones. Running the Application  We first need to download the source code using git from: https://github.com/Salaboy/emergency-service-drools-app  Once you get the source code in your machine go inside the project structure and run:  mvn clean install  This will build all the modules and allow you to start playing with the application. If you get problems with maven dependencies you need to add a new certificate to your JDK to allow maven download snapshots from Plug Tree (org) repositories hosted in cloudbees.com. I strongly recommend you doing this to get always the latest version of our projects.  "Make sure that java trusts the PlugTree repository by adding it's public key to the cacerts file. To do that, follow these steps:      Browse to https://repository-plugtree.forge.cloudbees.com/snapshot/     Select "Tools->Page Info" from the menu.     Select the "Security" icon and then click the "View Certificate" button.     Select the "Details" tab.     In the "Certificate Hierarchy" pane, select "*.forge.cloudbees.com" and then click the "Export..." button at the bottom of the dialog.     Save the certificate in a file (I used "publickey.forge.cloudbees.com") and select the "X.509 Certificate (PEM)" format.     Execute the following command (you may need to adjust the arguments for alias, keystore, and file based on your installation and where you saved the public key):  sudo keytool -import -alias mail.redhat.com -keystore /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/security/cacerts -file ~/publickey.forge.cloudbees.com  When prompted, the default keystore password is "changeit". " -> copied from the project wiki: https://github.com/Salaboy/emergency-service-drools-app/wiki/Fedora-14-install   The Run Module  We have developed a special module that facilitates running all the application modules. we are planing to create a graphical interface to start up all the modules but it is still work in progress. Right now you can start the application using maven and one terminal per module that you want to start up.  To start up the application there are are some dependency restrictions. We obviously need to start the application core before starting the UIs. If we are planing to use an external event generator (sensor) we need to bind it before generating emergencies in the world UI application. If we are not planning to use external sensors we can go ahead without running the sensors module.  Let's run the following modules: emergency-service-core, emergency-service-world-ui, emergency-service-tasklist-ui-web For starting the application you should open three different terminals/consoles in your computer and have maven installed.  From the first terminal, standing inside the run/ module you need to run: mvn -Pcore  Once core is started you can start world ui and task lists web ui.  From a different terminal you need to run: mvn -Pcitymap From a different terminal you need to run: mvn -Ptasklists From a different terminal you need to run: mvn -Pdashboard From a different terminal you need to run: mvn -Psensors Interacting with the application  Now that we have the main three modules running we can start interacting with the application. Let's create and emergency and let's try to solve it. The following video guides you through the Heart Attack Scenario.  Take a look at the following video that explains how to setup the application in your machine and run a simple use case:   Useful URLs  Git: http://git-scm.com/  Maven: http://maven.apache.org/download.html  Emergency Services Source Code: https://github.com/Salaboy/emergency-service-drools-app  To clone the source code (for having a local copy of the source code) run in your terminal:   git clone git://github.com/Salaboy/emergency-service-drools-app.git  Task Lists UI - Web Implementation: http://localhost:8081/emergency-services/
fbacchella/jrds	What is it? =============  Jrds is performance collector, much like cacti or munins. But it intends to be more easy to use and able to collect a high number of machines in a very short time. It's fully written in java and avoid call external process to increase performances. It uses [RRD4J](http://code.google.com/p/rrd4j/), a clone of [rrdtool](http://oss.oetiker.ch/rrdtool/|) written in java.  The site is [jrds.fr](http://jrds.fr)  How it works? =============  JRDS is a java web application, that can run in any servlet server like tomcat or resin. It can also run in a standalone mode, using jetty.  It uses threads to parallelize work. Each host is collected within the same thread and the number of simultaneous threads can be configured. It use only one thread for each host to avoid overload of a server.  It use mainly snmp to collect data, but can be easily extended. There is also some jdbc probes, a agent using RMI for the communication, and it can also parse XML data collected with HTTP. The currently available probes can be found [here](http://jrds.fr/sourcetype/start). Additional collectors can be used using external jars.
eccentricdevotion/TARDIS	# TARDIS  TARDIS is a CraftBukkit plugin that allows you to create a TARDIS that lets you time travel (teleport) to random locations. It adds a Whovian twist to the typical /sethome and /home commands.  **As a player, you can:**  * Create a TARDIS that is bigger on the inside. * Time travel to a random location. * Time travel to saved destinations. * Grow rooms in your TARDIS. * Take companions with you when you time travel. * Collect Artron Energy to power your TARDIS.
rwinch/spring-modern-templating	About =================  Source Code and materials for [A Tour of Modern Templating Frameworks with Spring MVC](https://www.youtube.com/watch?v=EdRNoQK7r7g&list=PLgGXSWYM2FpOgqA05ex4vdZno0wbwgJyz&index=19). To run the samples import the project into Spring Tool Suite using the Gradle Eclipse plugin.   Running the sample project ==================  The following provides information on setting up a development environment that can run the sample in [Spring Tool Suite 3.0.0+](http://www.springsource.org/sts). Other IDE's should work using Gradle's IDE support, but have not been tested.  * IDE Setup   * Install Spring Tool Suite 3.0.0+   * You will need the following plugins installed (can be found on the Extensions Page) 	* Gradle Eclipse * Importing the project into Spring Tool Suite   * File->Import...->Gradle Project
shane-k-j/jboss-trading	JBoss EAP 6  1) Add a management user (username=admin, password=password).  2) standalone.sh --server-config standalone-full.xml  3) mvn clean install -Plocalhost-remote
encomendaz/services	EncomendaZ Web Services =============================  Uma alternativa para acessar os serviços do EncomendaZ via Web Services (RESTful).  Rastreamento ------------  Este serviço expõe as informações contidas nas páginas HTML dos Correios. O parse é feito com auxílio do projeto Alfred Library (http://alfredlibrary.org).    ### Consulta (GET Method)  O serviço de rastreamento está disponível nesta URL:  http://services.encomendaz.net/tracking.json?id=PB882615209BR&start=1&end=6  Os parâmetros "start" e "end" são opcionais, mas o "id" é obrigatório e devem ser enviados via query.  Para resquisições JSONP, use esta URL:  http://services.encomendaz.net/tracking.json?id=PB882615209BR&start=1&end=6&callback=myJSONPCallback  Monitoramento ------------  Este serviço disponibiliza o cadastro, exclusão e consulta de monitoramento de encomendas dos Correios  ### Consulta (GET Method)  A consulta de monitoramentos está disponível nesta URL:  http://services.encomendaz.net/monitoring.json?clientId=cleverson.sacramento@gmail.com  O parâmetro "clientId" é obrigatório e deve corresponder ao e-mail do interessado no monitoramento da encomenda enviado via query.  Para resquisições JSONP, use esta URL:  http://services.encomendaz.net/monitoring.json?clientId=cleverson.sacramento@gmail.com&callback=myJSONPCallback  ### Inclusão (PUT Method)  Os parâmetros "clientId" e "trackId" são obrigatórios e devem corresponder ao e-mail do interessado (clientId) no monitoramento de uma determianda encomenda (trackId) e devem ser enviados via post.  ### Exclusão (DELETE Method)  http://services.encomendaz.net/monitoring.json?clientId=cleverson.sacramento@gmail.com&trackId=XX000000000XX  Os parâmetros "clientId" e "trackId" são obrigatórios e devem ser enviados via query.  Contribuição --------------  O projeto está de portas abertas para contribuição. Se quiser ajudar, experimente diversos códigos de rastreio e registre os bugs aqui:  https://github.com/encomendaz/services/issues  Se preferir submeter código-fonte, faça um fork e envie seu pull request. Este serviço é de graça!  Leia mais --------------  http://cleversonsacramento.com/2012/03/12/restful-web-services-dos-correios/
hank/litecoin-wallet	Welcome to _Litecoin Wallet_, a standalone Litecoin payment app for your Android device!  This project contains several sub-projects:   * __wallet__:      The Android app itself. This is probably what you're searching for.  * __market__:      App description and promo material for the Google Play app store.  * __integration-android__:      A tiny library for integrating Litecoin payments into your own Android app      (e.g. donations, in-app purchases).  * __sample-integration-android__:      A minimal example app to demonstrate integration of Litecoin payments into      your Android app.  You can build all sub-projects at once using Maven:  `mvn clean install`
nasa/MCT-Plugins	MCT-Plugins ===========  Each plug-in may be built in one of two ways. If building MCT from source, these may be included alongside other projects and packaged by making appropriate changes to pom.xml within an assembly. As a convenience, ant scripts are included to build as stand-alone plugins. An existing platform build (such as the evaluation version) is required and may be specified as a property "mct.dir", i.e.:  ant -Dmct.dir=/Applications/MCT   Ancestor View:  A plug-in for viewing a graph of referencing components within MCT. Select "Ancestor View" to see a graph indicating which components (such as collections) refer to this component, with more information further up the tree.   **Chronology:**  A set of interfaces used for communicating time-stamped information between plug-ins.    **Notebook:**  A plug-in for making and maintaining notes within MCT. Notes may be annotated with other objects, such as telemetry elements, by dragging and dropping them into the note's text field. Notes are also time-stamped, so they can be viewed in time-enabled views (such as timelines). Depends upon Chronology.   **Timeline:**  A plug-in for viewing time-stamped information (notes, events) in a graphical timeline. The "Timeline" view shows event sequences, such as notebook entries, horizontally in relation to their occurrence in time. These events may be reorganized using drag and drop if the event sequence permits changes. Depends upon Chronology.   **Earth View:**  A plug-in for viewing state vectors relative to the Earth. To view, create an "Orbit" object from the Create menu. You may set initial vectors (units are km and km/s respectively, and position is relative to Earth's center; orbits are approximated at an accelerated rate and are not physically accurate). The resulting collection of state vectors can be viewed as spatial coordinates using the "Orbit" view.  Contains a true-color image of the Earth, owned by NASA, from the Visible Earth catalog.   http://visibleearth.nasa.gov/view.php?id=73909  R. Stockli, E. Vermote, N. Saleous, R. Simmon and D. Herring (2005). The Blue Marble Next Generation - A true color earth dataset including seasonal dynamics from MODIS. Published by the NASA Earth Observatory. Corresponding author: rstockli@climate.gsfc.nasa.gov   **Quickstart Persistence:**  Provides a simple in-memory persistence service populated with a small number of components and displays. To use, the compiled jar should be placed in the resources/platform of an MCT installation, in lieu of databasePersistence-1.1.0.jar. Note that the example plugin may need to be moved from resources/plugins to resources/platform as well, as this quickstart persistence service is pre-populated with example telemetry components.   **SatelliteTracker:**  This plug-in allows users to create satellites in MCT and track their orbits, in real-time, through various views:  <UL>    <LI> Satellite Orbits in 3D via the Earth View plug-in.    <LI> Real-time locations on a 2D Mercator Projection (A new to MCT; created within this plug-in).    <LI> All of the views that come standard with the core-MCT distribution (i.e.: MultiColumn View, Plots over time, Alpha, etc.).  </UL>   Along with adding an interesting data-source to MCT, the true purpose of this plug-in is that the SatelliteTracker serves as a concrete example on how to write a plug-in for MCT.  As SatelliteTracker is a flagship example for plug-in development, comments have been added throughout the source-files to guide the developer on the design-style and requirements put-forth by MCT's structure (and similarly, a section on the Wiki concerning developing-with MCT references the source contained within SatelliteTracker).  Similar to EarthView, this plug-in contains two true-color image of the Earth (one with and one without snow), owned by NASA, from the Visible Earth catalog.  http://visibleearth.nasa.gov/view.php?id=73909  R. Stockli, E. Vermote, N. Saleous, R. Simmon and D. Herring (2005). The Blue Marble Next Generation - A true color earth dataset including seasonal dynamics from MODIS. Published by the NASA Earth Observatory. Corresponding author: rstockli@climate.gsfc.nasa.gov
cheptsov/SpringMVCApp	This file was created by IntelliJ IDEA 12.1 for binding GitHub repository
vert-x/vertx-maven	# Vert.x 2.x is **deprecated** - use instead https://github.com/vert-x3/vertx-maven-starter  ## vertx-maven  Maven infrastructure for Vertx.  * Contains    * Maven Archetype for Vertx    * Maven Plugin for Vertx       * Supports          * runmod          * pullInDeps  #### For more details refer inner directories.
zscott/MultiBitExchange	[![Ready Issues](https://badge.waffle.io/zscott/MultiBitExchange.png?label=ready)](https://waffle.io/zscott/MultiBitExchange)  [![Build Status](https://travis-ci.org/zscott/MultiBitExchange.png)](https://travis-ci.org/zscott/MultiBitExchange)  ## The Vision  MultiBit Exchange is an open source platform for building exchanges.  ## What is it?  A platform that consists of an efficient event-based matching engine with a REST API front-end that allows you to: * Register multiple currency pairs * Submit market and limit orders * Cancel orders * View the status of your orders * View market depth * Stream real-time changes to market depth * View open orders * View order history * View trade history * Stream real-time trades  MultiBit Exchanges is written by professional developers and is well tested and clean making it easier to understand and extend to: * Support additional analytics and real-time data streams * Support additional order types  ## What can I do with it?  There is an endless variety of types of exchanges that can be created using MultiBit Exchange. Here are a few ideas: * An exchange for trading fiat for electronic currencies such as Bitcoin, Litecoin, Dogecoin, and Namecoin. * An exchange for trading Bitcoin for cell phone minutes, gift cards, gold, etc. * A precious metal exchange: gold, silver, platinum, etc. * A traditional currency exchange * A test platform for experimenting with trading bots, experimental order types, etc. * An Inter-exchange arbitrage platform  ## What are the major characteristics of MultiBit Exchange?  MultiBit Exchange aims to be more that just a functioning exchange, but a well crafted codebase that is: * Tested * Neatly structured * High-throughput / Low-latency by leveraging event sourcing and the [LMAX Disruptor pattern](http://martinfowler.com/articles/lmax.html). * Extensible (not necessarily configurable or pluggable, but definitely malleable) * Strong audit trail. ALL orders, trades, and changes to markets are stored in an audit log. * Scalable by employing CQRS to keep frequently read data models seperate from core matching engine.  ## What development methodology is used?  > "Clean code always looks like it was written by someone who cares." > Michael Feathers  Great software doesn't just happen. It requires a disciplined approach. The following guiding principles are used to develop MultiBit Exchange:  * Focus on the core domain and use of Domain Driven Design principles * Create tests to guide development - Test Driven Development * Be disciplined and keep the code well-structured and layered * Don't re-invent the wheel, but leverage proven technology wherever possible  ## Architecture  MultiBit Exchange follows the [hexagonal architecture](http://alistair.cockburn.us/Hexagonal+architecture) which is also known as the [onion architecture](http://jeffreypalermo.com/blog/the-onion-architecture-part-1/). Each prescribes that abstractions should not depend upon details. Details should depend upon abstractions.  This decouples what the system does from how it does it which makes the core domain model cleaner and more focused and therefore easier to understand, test, and extend.  ## Initial goals of MultiBit Exchange  The initial focus is to release a robust Matching Engine. Following that, there are many directions MultiBit Exchange can be taken.  ## Future Ideas for MultiBit Exchange  MultiBit Exchange is relatively new, so focus is key, but I have a few ideas about where to go after the first release:  * Accounting & Inter-Broker settlement - include more support for accounting functions * Decentralization - not sure how yet, but being resistant to shutdown and DDoS attacks is important. * Enhance Performance - to be able to handle huge volumes without compromising on latency.  ## Standing on the Shoulders of Giants  Many thanks to all the hard work that was put into the many ideas, libraries, and systems that MultiBit Exchange is built on:  Domain Driven Design lies at the core of MultiBit Exchange. http://skillsmatter.com/expert-profile/home/eric-evans - Thank you Eric Evans.  google-guice - Guice is used for dependency injection throughout. https://code.google.com/p/google-guice/people/list  guava-libraries - Guava is used throughout and makes Java just a little nicer to work with. https://code.google.com/p/guava-libraries/people/list  Dropwizard - Dropwizard serves as the front-end for REST API and web interfaces. http://dropwizard.codahale.com/about/contributors/  LMAX Disruptor - The Disruptor pattern is used to help with speedy production and consumption of events. https://github.com/mikeb01  AXON Framework - The AXON Framework is used to help with CQRS. http://www.axonframework.org/  MongoDB - Much of the persistence is provided by MongoDB. http://www.mongodb.org/  Heroku - Used as a hosting environment during development. https://www.heroku.com/  ## Installing MongoDB  MongoDB is used as an Event Store and to persist CQRS [Read Models](http://martinfowler.com/bliki/CQRS.html). Follow the usual [MongoDB installation instructions](http://docs.mongodb.org/manual/installation/), such as  ``` $ brew update $ brew install mongo ```  Start MongoDB as a background process with  ``` $ mongod & ```  Then create the following collections through the Mongo CLI  ``` $ mongo > use mbexchange ```  ## Building and running  From the console you can do the following ``` $ cd <project root> $ mvn clean install $ java -jar target/web-develop-SNAPSHOT.jar server mbexchange-demo.yml ```  If startup was successful, the first thing you will need to do is create an exchange (a container for currency pairs).  ### Install a REST Client To interact with the REST API use a browser plugin such as [POSTMAN](https://chrome.google.com/webstore/detail/postman-rest-client/fdmmgilgnpjigdojojpjoooidkmcomcm?hl=en) which works well with Chrome.  ### Register an Exchange Be sure to include the following header ``` Content-type: application/json ```  The format of the JSON document for creating an exchange: ``` {   "identifier":"ex" } ```  Next, you can navigate to [localhost:8080/exchanges/ex/pairs](http://localhost:8080/exchanges/ex/pairs) to see a list of currency pairs in the 'ex' exchange. The list should be empty at this point.   To add a currency pair to the exchange POST a JSON document to [/exchanges/ex/pairs](http://localhost:8080/exchanges/ex/pairs)  Again, be sure to include the following header ``` Content-type: application/json ```  The CURRENT format of the JSON document for creating a currency pair: ``` {   "baseCurrency": "BaseCCY",   "counterCurrency": "CounterCCY" } ```   Navigate back to [localhost:8080/exchanges/ex/pairs](http://localhost:8080/exchanges/ex/pairs) to see the newly registered currency pair.  To submit a market order to the exchange POST a JSON document to [/exchanges/ex/orders](http://localhost:8080/exchanges/ex/orders)  Again, be sure to include the following header ``` Content-type: application/json ```  The format of the JSON document for specifying orders:  ```  {    "broker":"BrokerIdentifier",    "side":"Sell",    "qty":"80.33001",    "ticker":"BaseCCY/CounterCCY",    "price":"M"  }  ``` * "broker" can be any string currently. * "side" is case-insensitive and can be "Buy" or "Sell" * "qty" is a number between 0 and 10,000,000 with a maximum of 8 decimal places. * "ticker" must correspond to a previously added currency pair (see above) * "price" must be "M" for market orders or a number representing limit price for limit orders.  To submit a market order to the exchange POST a JSON document to [/exchanges/ex/orders](http://localhost:8080/exchanges/ex/orders)  Again, be sure to include the following header ``` Content-type: application/json ```  The format of the JSON document for specifying orders:  ```  {    "broker":"BrokerIdentifier",    "side":"Sell",    "qty":"80.33001",    "ticker":"BaseCCY/CounterCCY",    "price":"M"  }  ``` ## Where can I find complete API documentation? The documentation is a work in progress and can be found at: http://docs.zachscott.apiary.io/  ## How will I know if the server is working? On startup you should see the following: ``` INFO  [2014-01-25 18:12:40,007] com.yammer.dropwizard.cli.ServerCommand: Starting MultiBitExchangeApiWebService ___  ___      _ _   _______ _ _     _____         _                             |  \/  |     | | | (_) ___ (_) |   |  ___|       | |                            | .  . |_   _| | |_ _| |_/ /_| |_  | |____  _____| |__   __ _ _ __   __ _  ___  | |\/| | | | | | __| | ___ \ | __| |  __\ \/ / __| '_ \ / _` | '_ \ / _` |/ _ \ | |  | | |_| | | |_| | |_/ / | |_  | |___>  < (__| | | | (_| | | | | (_| |  __/ \_|  |_/\__,_|_|\__|_\____/|_|\__| \____/_/\_\___|_| |_|\__,_|_| |_|\__, |\___|                                                                      __/ |                                                                          |___/   INFO  [2014-01-25 18:12:40,183] com.yammer.dropwizard.config.Environment:       GET     /exchanges/{exchangeId}/securities (org.multibit.exchange.infrastructure.adaptor.api.resources.SecuritiesResource)     GET     /exchanges/{exchangeId}/securities/{ticker}/orderbook (org.multibit.exchange.infrastructure.adaptor.api.resources.SecuritiesResource)     POST    /exchanges/{exchangeId}/securities (org.multibit.exchange.infrastructure.adaptor.api.resources.SecuritiesResource)     POST    /exchanges (org.multibit.exchange.infrastructure.adaptor.api.resources.ExchangeResource)     POST    /exchanges/{exchangeId}/orders (org.multibit.exchange.infrastructure.adaptor.api.resources.ExchangeResource)  INFO  [2014-01-25 18:12:40,183] com.yammer.dropwizard.config.Environment: tasks =       POST    /tasks/gc (com.yammer.dropwizard.tasks.GarbageCollectionTask) ``` ## How was this startup banner generated? http://patorjk.com/software/taag/#p=display&f=Doom&t=MultiBit%20Exchange  ## Which branch? Use `master` for the latest production release. Use `develop` for the latest release candidate.  If you wish to contribute, please start with `develop`.  ## License  Copyright (c) 2014 Zach Scott, http://zach-scott.com/  This software consists of voluntary contributions made by many individuals (see AUTHORS.txt) For the exact contribution history, see the revision history and logs, available at https://github.com/zscott/MultiBitExchange  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
minnal/minnal	Minnal  [![Build Status](https://travis-ci.org/minnal/minnal.png)](https://travis-ci.org/minnal/minnal) [![Coverage Status](https://img.shields.io/coveralls/minnal/minnal.svg)](https://coveralls.io/r/minnal/minnal) ======  Minnal is a highly scalable and productive RESTful service framework that helps you eliminate boiler plate code and build services faster. It allows you to build services at lightning speed through its powerful instrumentation module. It generates the APIs for your resources at runtime and eliminates the need to write controller and data access layers for your service leaving you to focus on enriching your domain.  Visit [minnal.github.io/minnal](http://minnal.github.io/minnal) for more information.  ## How Minnal is different from other frameworks? Minnal draw parallels with the likes of RESTExpress and DropWizard and has been inspired by those frameworks. It differentiates itself from them by focusing on domain modeling and evolving APIs around them. As a application developer, you should be able to focus more on enriching the domain layer with Minnal.  ## Getting Started The [Getting Started](http://minnal.github.io/minnal/getting-started.html) page walks you through building a shopping cart application and in the process explains the core concepts of minnal.  ## Documentation The [User Documentation](http://minnal.github.io/minnal/manual/index.html) explains in detail about the various modules and functionalities provided by minnal. This page is continously updated on every release.  ## Examples The [Pet Clinic Application](https://github.com/minnal/minnal/tree/master/minnal-examples/minnal-examples-petclinic) serves as an example for simplicity and agility of minnal. This application is taken from [Spring Projects](https://github.com/spring-projects/spring-petclinic/) and ported to minnal.  The [Order Management Application](https://github.com/minnal/minnal/tree/master/minnal-examples/minnal-examples-oms) uses most of the functionalities of the instrumentation module and has a basic authentication setup.  ## License Minnal is licensed under [The Apache Software License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0.txt)
socrata/datasync	Socrata Datasync ================  Last updated: June 2, 2017  Looking for the latest release? Get it here: https://github.com/socrata/datasync/releases  ## General Information DataSync is an executable Java application which serves as a general solution to automate publishing data on the Socrata platform. It can be used through a easy-to-use graphical interface or as a command-line tool ('headless mode'). Whether you are a non-technical user, developer, or ETL specialist DataSync makes data publishing simple and reliable. DataSync takes a CSV or TSV file on a local machine or networked hard drive and publishes it to a Socrata dataset so that the Socrata dataset stays up-to-date. DataSync can also publish geospatial files such as zipped shapefiles, geoJSON, KML, and KMZ files. DataSync jobs can be integrated into an ETL process, scheduled using a tool such as the Windows Task Scheduler or Cron, or used to perform updates or create new datasets in batches. DataSync works on any platform that runs Java version 1.7 or higher (i.e. Windows, Mac, and Linux). This simple, yet powerful publishing tool lets you easily update Socrata datasets programmatically and automatically (scheduled), without writing a single line of code.  [Comprehensive DataSync Documentation](http://socrata.github.io/datasync/)  The Socrata University Class: [Socrata Introduction to Integration](http://socrata.wistia.com/medias/q4pwut6s56)  ### Standard Jobs Standard jobs can be set up to take a CSV data file from a local machine or networked folder and publish it to a specific dataset. A job can be automated easily using the Windows Task Scheduler or similar tool to run the job at specified intervals (i.e. once per day). ![standard job tab](http://i.imgur.com/byN0ibq.png?1)  ### GIS Jobs GIS jobs can be set up to handle geospatial datasets, such as zipped shapefiles, geoJSON, KML, or KMZ files and replace specific datasets on Socrata. The job can then be automated in a similar fashion to standard jobs.  ### Port Jobs Port jobs are used for moving data around that is already on the Socrata platform. Users that have publisher rights can make copies of datasets through this tool. Port jobs allow the copying of both dataset schemas (metadata and columns) and data (rows). ![port job tab](http://i.imgur.com/tMz2sQP.png?1)   ## Developers This repository is our development basecamp. If you find a bug or have questions, comments, or suggestions, you can contribute to our [issue tracker](https://github.com/socrata/datasync/issues).  ### Apache Maven DataSync uses Maven for building and package management. For more information: [What is Maven?](http://maven.apache.org/what-is-maven.html)  To build the project run: ``` mvn clean install ```  To compile the project into an executable JAR file (including all dependencies) run: ``` mvn clean compile -Dmaven.test.skip=true assembly:single ```  This puts the JAR file into the "target" directory inside the repo.  So to open DataSync, simply: ``` cd target java -jar DataSync-1.8.2-jar-with-dependencies.jar ```  ### Java SDK  DataSync can be used as a Java SDK, for detailed documentation refer to: [http://socrata.github.io/datasync/guides/datasync-library-sdk.html](http://socrata.github.io/datasync/guides/datasync-library-sdk.html)
jcabi/jcabi-jdbc	<img src="http://img.jcabi.com/logo-square.png" width="64px" height="64px" />  [![Managed by Zerocracy](http://www.0crat.com/badge/C3RUBL5H9.svg)](http://www.0crat.com/p/C3RUBL5H9) [![DevOps By Rultor.com](http://www.rultor.com/b/jcabi/jcabi-jdbc)](http://www.rultor.com/p/jcabi/jcabi-jdbc)  [![Build Status](https://travis-ci.org/jcabi/jcabi-jdbc.svg?branch=master)](https://travis-ci.org/jcabi/jcabi-jdbc) [![PDD status](http://www.0pdd.com/svg?name=jcabi/jcabi-jdbc)](http://www.0pdd.com/p?name=jcabi/jcabi-jdbc) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.jcabi/jcabi-jdbc/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.jcabi/jcabi-jdbc) [![Javadoc](https://javadoc-emblem.rhcloud.com/doc/com.jcabi/jcabi-jdbc/badge.svg)](http://www.javadoc.io/doc/com.jcabi/jcabi-jdbc) [![Coverage Status](https://coveralls.io/repos/jcabi/jcabi-jdbc/badge.svg?branch=__rultor&service=github)](https://coveralls.io/github/jcabi/jcabi-jdbc?branch=__rultor) [![Dependencies](https://www.versioneye.com/user/projects/561ac40ea193340f28001102/badge.svg?style=flat)](https://www.versioneye.com/user/projects/561ac40ea193340f28001102)  More details are here: [jdbc.jcabi.com](http://jdbc.jcabi.com/index.html). Also, read this blog post: [Fluent JDBC Decorator](http://www.yegor256.com/2014/08/18/fluent-jdbc-decorator.html).  `JdbcSession` is a convenient fluent wrapper around JDBC:  ```java import com.jcabi.jdbc.JdbcSession; public class Main {   public static void main(String[] args) {     String name = new JdbcSession(/* JDBC data source */)       .sql("SELECT name FROM foo WHERE id = ?")       .set(123)       .select(new SingleOutcome<String>(String.class));   } } ```  ## Questions?  If you have any questions about the framework, or something doesn't work as expected, please [submit an issue here](https://github.com/yegor256/jcabi/issues/new).  ## How to contribute?  Fork the repository, make changes, submit a pull request. We promise to review your changes same day and apply to the `master` branch, if they look correct.  Please run Maven build before submitting a pull request:  ``` $ mvn clean install -Pqulice ```  Please make sure that you're doing so under user account without administrative rights, otherwise the build will fail (postgresql instance needed for tests can't be launched under admin/root account).
Norconex/collector-http	Norconex HTTP Collector ========================  ![Norconex HTTP Collector Logo](https://www.norconex.com/collectors/img/collector-http.png)  Norconex HTTP Collector is a full-featured **web crawler** (or spider) that can manipulate and store collected data into a repositoriy of your choice (e.g. a search engine). It very flexible, powerful, easy to extend, and portable. Can be used command-line with file-based configuration on any OS, or can be embedded into Java applications using well documented APIs.  Visit the web site for binary downloads and documentation:  ### https://www.norconex.com/collectors/collector-http/
jcabi/jcabi-xml	<img src="http://img.jcabi.com/logo-square.png" width="64px" height="64px" />  [![Managed by Zerocracy](http://www.0crat.com/badge/C3RUBL5H9.svg)](http://www.0crat.com/p/C3RUBL5H9) [![DevOps By Rultor.com](http://www.rultor.com/b/jcabi/jcabi-xml)](http://www.rultor.com/p/jcabi/jcabi-xml)  [![Build Status](https://travis-ci.org/jcabi/jcabi-xml.svg?branch=master)](https://travis-ci.org/jcabi/jcabi-xml) [![PDD status](http://www.0pdd.com/svg?name=jcabi/jcabi-xml)](http://www.0pdd.com/p?name=jcabi/jcabi-xml) [![Build status](https://ci.appveyor.com/api/projects/status/323ak1323abk3x30/branch/master?svg=true)](https://ci.appveyor.com/project/yegor256/jcabi-xml/branch/master) [![Coverage Status](https://coveralls.io/repos/jcabi/jcabi-xml/badge.svg?branch=__rultor&service=github)](https://coveralls.io/github/jcabi/jcabi-xml?branch=__rultor) [![Javadoc](https://javadoc-emblem.rhcloud.com/doc/com.jcabi/jcabi-xml/badge.svg)](http://www.javadoc.io/doc/com.jcabi/jcabi-xml)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.jcabi/jcabi-xml/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.jcabi/jcabi-xml) [![Dependencies](https://www.versioneye.com/user/projects/561a9e86a193340f2f00115e/badge.svg?style=flat)](https://www.versioneye.com/user/projects/561a9e86a193340f2f00115e)  More details are here: [xml.jcabi.com](http://xml.jcabi.com/index.html). Also, read this blog post: [Java XML Parsing Made Easy](http://www.yegor256.com/2014/04/24/java-xml-parsing-and-traversing.html).  It's a simple wrapper around DOM that makes XML parsing and printing easy and simple, for example:  ```java XML xml = new XMLDocument("<orders><order id="4">Coffee to go</order></orders>"); String id = xml.xpath("//order/@id").get(0); String name = xml.xpath("//order[@id=4]/text()").get(0); System.out.println(xml.toString()); ```  ## Questions?  If you have any questions about the framework, or something doesn't work as expected, please [submit an issue here](https://github.com/jcabi/jcabi-xml/issues/new).  ## How to contribute?  Fork the repository, make changes, submit a pull request. We promise to review your changes same day and apply to the `master` branch, if they look correct.  Please run Maven build before submitting a pull request:  ``` $ mvn clean install -Pqulice ```
astefanutti/metrics-cdi	# CDI Extension for Metrics  [![Build Status][Travis badge]][Travis build] [![Coverage Status][Coveralls badge]][Coveralls build] [![Dependency Status][VersionEye badge]][VersionEye build] [![Maven Central][Maven Central badge]][Maven Central build]  [Travis badge]: https://travis-ci.org/astefanutti/metrics-cdi.svg [Travis build]: https://travis-ci.org/astefanutti/metrics-cdi [Coveralls badge]: https://coveralls.io/repos/astefanutti/metrics-cdi/badge.svg [Coveralls build]: https://coveralls.io/github/astefanutti/metrics-cdi [VersionEye badge]: https://www.versioneye.com/user/projects/52a633be632bacbded00001c/badge.svg [VersionEye build]: https://www.versioneye.com/user/projects/52a633be632bacbded00001c [Maven Central badge]: http://img.shields.io/maven-central/v/io.astefanutti.metrics.cdi/metrics-cdi.svg [Maven Central build]: http://repo1.maven.org/maven2/io/astefanutti/metrics/cdi/metrics-cdi/1.4.0/  [CDI][] portable extension for Dropwizard [Metrics][] compliant with [JSR 346: Contexts and Dependency Injection for Java<sup>TM</sup> EE 1.2][JSR 346 1.2].  [CDI]: http://www.cdi-spec.org/ [Metrics]: http://metrics.dropwizard.io/ [JSR 346]: https://jcp.org/en/jsr/detail?id=346 [JSR 346 1.1]: https://jcp.org/aboutJava/communityprocess/final/jsr346/index.html [JSR 346 1.2]: https://jcp.org/aboutJava/communityprocess/mrel/jsr346/index.html [CDI 1.1]: http://docs.jboss.org/cdi/spec/1.1/cdi-spec.html [CDI 1.2]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html  ## About  _Metrics CDI_ provides support for the [_Metrics_ annotations][Metrics annotations] in CDI enabled environments. It implements the contract specified by these annotations with the following level of functionality: + Intercept invocations of bean constructors, methods and public methods of bean classes annotated with [`@Counted`][], [`@ExceptionMetered`][], [`@Metered`][] and [`@Timed`][], + Create [`Gauge`][] and [`CachedGauge`][] instances for bean methods annotated with [`@Gauge`][] and [`@CachedGauge`][] respectively, + Inject [`Counter`][], [`Gauge`][], [`Histogram`][], [`Meter`][] and [`Timer`][] instances, + Register or retrieve the produced [`Metric`][] instances in the resolved [`MetricRegistry`][] bean, + Declare automatically a default [`MetricRegistry`][] bean if no one exists in the CDI container.  _Metrics CDI_ is compatible with _Metrics_ version `3.1.0`+.  [Metrics annotations]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/annotation/package-summary.html [`@CachedGauge`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/annotation/CachedGauge.html [`@Counted`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/annotation/Counted.html [`@ExceptionMetered`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/annotation/ExceptionMetered.html [`@Gauge`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/annotation/Gauge.html [`@Metered`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/annotation/Gauge.html [`@Timed`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/annotation/Timed.html [`CachedGauge`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/CachedGauge.html [`Counter`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/Counter.html [`Gauge`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/Gauge.html [`Histogram`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/Histogram.html [`Meter`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/Meter.html [`Metric`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/Metric.html [`Timer`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/Timer.html [`MetricRegistry`]: https://dropwizard.github.io/metrics/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html  ## Getting Started  #### Using Maven  Add the `metrics-cdi` library as a dependency:  ```xml <dependency>     <groupId>io.astefanutti.metrics.cdi</groupId>     <artifactId>metrics-cdi</artifactId>     <version>1.4.0</version> </dependency> ```  #### Required Dependencies  Besides depending on _Metrics_ (`metrics-core` and `metrics-annotation` modules), _Metrics CDI_ requires a CDI enabled environment running in Java 7 or greater.  #### Supported Containers  _Metrics CDI_ is currently successfully tested with the following containers:  | Container             | Version        | Environment                          | | --------------------- | -------------- | ------------------------------------ | | [Weld][]              | `2.4.3.Final`  | Java SE 7,8 / [CDI 1.2][JSR 346 1.2] | | [OpenWebBeans][]      | `1.7.3`        | Java SE 7,8 / [CDI 1.2][JSR 346 1.2] | | [Jetty][]             | `9.4.5`        | [Servlet 3.1][]                      | | [WildFly 8][WildFly]  | `8.2.1.Final`  | [Java EE 7][]                        | | [WildFly 9][WildFly]  | `9.0.2.Final`  | [Java EE 7][]                        | | [WildFly 10][WildFly] | `10.1.0.Final` | [Java EE 7][]                        |  WildFly 8.1 requires to be patched with Weld 2.2+ as documented in [Weld 2.2 on WildFly][].  [Weld]: http://weld.cdi-spec.org/ [OpenWebBeans]: http://openwebbeans.apache.org/ [Jetty]: http://www.eclipse.org/jetty/ [WildFly]: http://www.wildfly.org/ [Servlet 3.1]: https://jcp.org/en/jsr/detail?id=340 [Java EE 7]: https://jcp.org/en/jsr/detail?id=342 [Weld 2.2 on WildFly]: http://weld.cdi-spec.org/news/2014/04/15/weld-220-final/  ## Usage  _Metrics CDI_ activates the [_Metrics_ AOP Instrumentation](#metrics-aop-instrumentation) for beans annotated with [_Metrics_ annotations][Metrics annotations] and automatically registers the corresponding `Metric` instances in the [_Metrics_ registry][] resolved for the CDI application. The registration of these `Metric` instances happens each time such a bean gets instantiated. Besides, `Metric` instances can be retrieved from the _Metrics_ registry by declaring [metrics injection points](#metrics-injection).  The [metrics registration](#metrics-registration) mechanism can be used to customize the `Metric` instances that get registered. Besides, the [_Metrics_ registry resolution](#metrics-registry-resolution) mechanism can be used for the application to provide a custom [`MetricRegistry`] instance.  [_Metrics_ registry]: https://dropwizard.github.io/metrics/3.1.0/getting-started/#the-registry  #### Metrics AOP Instrumentation  _Metrics_ comes with the [`metrics-annotation`][] module that contains a set of annotations and provides a standard way to integrate _Metrics_ with frameworks supporting Aspect Oriented Programming (AOP). These annotations are supported by _Metrics CDI_ that implements their contract as documented in their Javadoc.  [`metrics-annotation`]: https://github.com/dropwizard/metrics/tree/master/metrics-annotation  For example, a method of a bean can be annotated so that its execution can be monitored using _Metrics_:  ```java import com.codahale.metrics.annotation.Timed;  class TimedMethodBean {      @Timed     void timedMethod() {         // Timer name => TimedMethodBean.timedMethod     } } ```  or the [bean class][] can be annotated directly so that all its public methods get monitored:  ```java import com.codahale.metrics.annotation.Metered;  @Metered public class MeteredClassBean {      public void meteredMethod() {         // Meter name => MeteredClassBean.meteredMethod     } } ```  or the [bean constructor][] can be annotated so that its instantiations get monitored:  ```java import com.codahale.metrics.annotation.Counted;  class CountedConstructorBean {      @Counted     CountedConstructorBean() {         // Counter name => CountedConstructorBean.CountedConstructorBean     } } ```  The `name` and `absolute` attributes available on every _Metrics_ annotation can be used to customize the name of the `Metric` instance that gets registered in the _Metrics_ registry. The default naming convention being the annotated member simple name relative to the declaring class fully qualified name as illustrated in the above examples.  [bean class]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#what_classes_are_beans [bean constructor]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#bean_constructors  #### Metrics Injection  `Metric` instances can be retrieved from the _Metrics_ registry by declaring an [injected field][], e.g.:  ```java import com.codahale.metrics.Timer;  import javax.inject.Inject;  class TimerBean {      @Inject     private Timer timer; // Timer name => TimerBean.Timer } ```  `Metric` instances can be injected similarly as parameters of any [initializer method][] or [bean constructor][], e.g.:  ```java import com.codahale.metrics.Timer;  import javax.inject.Inject;  class TimerBean {      private final Timer timer;      @Inject     private TimerBean(Timer timer) { // Timer name => TimerBean.Timer        this.timer = timer;     } } ```  In the above example, Java 8 with the `-parameters` compiler option activated is required to get access to injected parameter name. Indeed, access to parameter names at runtime has been introduced with [JEP-118][]. More information can be found in [Obtaining Names of Method Parameters][] from the Java tutorials. To work around that limitation for Java versions prior to Java 8, or to declare a specific name, the `@Metric` annotation can be used as documented hereafter.  [JEP-118]: http://openjdk.java.net/jeps/118 [Obtaining Names of Method Parameters]: http://docs.oracle.com/javase/tutorial/reflect/member/methodparameterreflection.html  In order to provide metadata for the `Metric` instantiation and resolution, the injection point can be annotated with the `@Metric` annotation, e.g., with an [injected field][]:  ```java import com.codahale.metrics.Timer; import com.codahale.metrics.annotation.Metric;  import javax.inject.Inject;  @Inject @Metric(name = "timerName", absolute = true) private Timer timer; // Timer name => timerName ```  or when using a [bean constructor][]:  ```java import com.codahale.metrics.Timer; import com.codahale.metrics.annotation.Metric;  import javax.inject.Inject;  class TimerBean {      private final Timer timer;      @Inject     private TimerBean(@Metric(name = "timerName", absolute = true) Timer timer) {         // Timer name => timerName         this.timer = timer;     } } ```  [injected field]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#injected_fields [initializer method]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#initializer_methods  #### Metrics Registration  While _Metrics CDI_ automatically registers `Metric` instances during the [_Metrics_ AOP instrumentation](#metrics-aop-instrumentation), it may be necessary for an application to explicitly provide the `Metric` instances to register. For example, to provide particular `Reservoir` implementations to [histograms][] or [timers][], e.g. with a [producer field][]:  ```java import com.codahale.metrics.SlidingTimeWindowReservoir; import com.codahale.metrics.Timer; import com.codahale.metrics.annotation.Metric; import com.codahale.metrics.annotation.Timed;  import javax.enterprise.inject.Produces;  class TimedMethodBean {      @Produces     @Metric(name = "customTimer") // Timer name => TimedMethodBean.customTimer     Timer Timer = new Timer(new SlidingTimeWindowReservoir(1L, TimeUnit.MINUTES));      @Timed(name = "customTimer")     void timedMethod() {         // Timer name => TimedMethodBean.customTimer     } } ```  Another use case is to register custom [gauges], e.g. with a [producer method][]:  ```java class CacheHitRatioBean {      @Inject     private Meter hits;      @Timed(name = "calls")     public void cachedMethod() {         if (hit) hits.mark();     }      @Produces     @Metric(name = "cache-hits")     private Gauge<Double> cacheHitRatioGauge(Meter hits, Timer calls) {         return () -> Ratio.of(hits.getCount(), calls.getCount()).getValue();     } } ```  [gauges]: https://dropwizard.github.io/metrics/3.1.0/manual/core/#gauges [histograms]: https://dropwizard.github.io/metrics/3.1.0/manual/core/#histograms [timers]: https://dropwizard.github.io/metrics/3.1.0/manual/core/#timers [producer field]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#producer_field [producer method]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#producer_method  #### Metrics Registry Resolution  _Metrics CDI_ automatically registers a `MetricRegistry` bean into the CDI container to register any `Metric` instances produced. That _default_ `MetricRegistry` bean can be injected using standard CDI [typesafe resolution][], for example, by declaring an [injected field][]:  ```java import com.codahale.metrics.MetricRegistry;  import javax.inject.Inject;  @Inject private MetricRegistry registry; ```  or by declaring a [bean constructor][]:  ```java import com.codahale.metrics.MetricRegistry;  import javax.inject.Inject;  class MetricRegistryBean {      private final MetricRegistry registry;      @Inject     private MetricRegistryBean(MetricRegistry registry) {         this.registry = registry;     } } ```  Otherwise, _Metrics CDI_ uses any `MetricRegistry` bean declared in the CDI container with the [built-in _default_ qualifier][] `@Default` so that a _custom_ `MetricRegistry` can be provided. For example, that _custom_ `MetricRegistry` can be declared with a [producer field][]:  ```java import com.codahale.metrics.MetricRegistry;  import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Produces;  @Produces @ApplicationScoped private final MetricRegistry registry = new MetricRegistry(); ```  or with a [producer method][]:  ```java import com.codahale.metrics.MetricRegistry;  import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Produces;  class MetricRegistryFactoryBean {      @Produces     @ApplicationScoped     private MetricRegistry metricRegistry() {         return new MetricRegistry();     } } ```  [typesafe resolution]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#typesafe_resolution [built-in _default_ qualifier]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#builtin_qualifiers   #### Metrics CDI Configuration  _Metrics CDI_ fires a `MetricsConfiguration` event at deployment time that can be used by the application to configure it, e.g.:  ```java import io.astefanutti.metrics.cdi.MetricsConfiguration;  import javax.enterprise.event.Observes;  class MetricsCdiConfiguration {      static void configure(@Observes MetricsConfiguration metrics) {         metrics.useAbsoluteName(true);     } } ```  Note that this event can only be used within the context of the observer method invocation. Any attempt to call one of its methods outside of that context will result in an `IllegalStateException` to be thrown.  ## Limitations  [CDI 1.2][] leverages on [Java Interceptors Specification 1.2][] to provide the ability to [associate interceptors to beans][Binding an interceptor to a bean] via _typesafe_ interceptor bindings. Interceptors are a mean to separate cross-cutting concerns from the business logic and _Metrics CDI_ is relying on interceptors to implement the support of _Metrics_ annotations in a CDI enabled environment.  [CDI 1.2][] sets additional restrictions about the type of bean to which an interceptor can be bound. From a _Metrics CDI_ end-user perspective, that implies that the managed beans to be monitored with _Metrics_ (i.e. having at least one member method annotated with one of the _Metrics_ annotations) must be _proxyable_ bean types, as defined in [Unproxyable bean types][], that are: > + Classes which don’t have a non-private constructor with no parameters, > + Classes which are declared `final`, > + Classes which have non-static, final methods with public, protected or default visibility, > + Primitive types, > + And array types.  [Java Interceptors Specification 1.2]: http://download.oracle.com/otndocs/jcp/interceptors-1_2-mrel2-eval-spec/ [Binding an interceptor to a bean]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#binding_interceptor_to_bean [Unproxyable bean types]: http://docs.jboss.org/cdi/spec/1.2/cdi-spec.html#unproxyable  ## License  Copyright © 2013-2017, Antonin Stefanutti  Published under Apache Software License 2.0, see LICENSE
mhidaka/EventApp	EventApp ========  ABC2013a用に作ったカンファレンス一覧アプリです。じんわりメンテしていく予定だよ https://play.google.com/store/apps/details?id=org.techbooster.app.abc2013autumn
yeokm1/docs-to-pdf-converter	Docs to PDF Converter =====================  **(I'm not maintaining this code as I neither have personal resources nor am I still using this project. I'll be happy to oblige if you have any pull requests or even if you wish to be a co-maintainer.)**  A standalone Java library/command line tool that converts DOC, DOCX, PPT, PPTX and ODT documents to pdf files. (Requires JRE 7)  The v1.7 release has not been updated for about 2 years although it seems quite reliable for me. In response to an [issue request](https://github.com/yeokm1/docs-to-pdf-converter/issues/1) to update the libraries, I have done so with the new v1.8. I now use Maven to managed the libraries in the pom.xml file.  I have not tested v1.8 much so if you face any issues, you can still use v1.7 in the Releases section.  # Table of content    + [Why?](#why)   + [Command Line Usage](#command-line-usage)   + [Parameters](#parameters)   + [Library Usage](#library-usage)   + [Caveats and technical details](#caveats-and-technical-details)     - [DOC](#doc)     - [DOCX](#docx)     - [PPT and PPTX](#ppt-and-pptx)     - [ODT](#odt)   + [Main Libraries](#main-libraries) * [Compiling the code](#compiling-the-code)  ### Why?   I wanted a simple program that can convert Microsoft Office documents to PDF but without dependencies like LibreOffice or expensive proprietary solutions. Seeing as how code and libraries to convert each individual format is scattered around the web, I decided to combine all those solutions into one single program. Along the way, I decided to add ODT support as well since I encountered the code too.   ### Command Line Usage:    ``` java -jar doc-converter.jar -type "type" -input "path" -output "path" -verbose java -jar doc-converter.jar -input test.doc java -jar doc-converter.jar -i test.ppt -o ~\output.pdf java -jar doc-converter.jar -i ~\no-extension-file -o ~\output.pdf -t docx ```  ### Parameters:   ``` -inputPath (-i, -in, -input) "path" : specifies a path for the input file   -outputPath (-o, -out, -output) "path" : specifies a path for the output PDF, use input file directory and name.pdf if not specified (Optional)  -type (-t) [DOC | DOCX | PPT | PPTX | ODT] : Specifies doc converter. Leave blank to let program infer via file  extension (Optional)  -verbose (-v) : To view intermediate processing messages. (Optional) ```  ### Library Usage:    1. Drop the jar into your lib folder and add to build path.   2. Choose the converter of your choice, they are named DocToPDFConverter, DocxToPDFConverter, PptToPDFConverter, PptxToPDFConverter and OdtToPDFConverter.   3. Instantiate with 4 parameters      - InputStream `inStream`: Document source stream to be converted      - OutputStream `outStream`: Document output stream      - boolean `showMessages`: Whether to show intermediate processing messages to Standard Out (stdout)      - boolean `closeStreamsWhenComplete`: Whether to close input and output streams when complete   4. Call the "convert()" method and wait.     ### Caveats and technical details:   This tool relies on Apache POI, xdocreport, docx4j and odfdom libraries. They are not 100% reliable and the output format may not always be what you desire.   #### DOC: Generally ok but takes some time to convert.. I notice that after conversion, the paragraph spacing tends to increase affecting your page layout. Conversion is done using docx4j to convert DOC to DOCX then to PDF.(Cannot use xdocreport once the DOCX data is obtained as the intermediate data structure is docx4j specific.)  #### DOCX: Very good results. Fast conversion too.  Conversion is done using xdocreport library as it seems faster and more accurate than docx4j.  #### PPT and PPTX: Resulting file is a PDF comprising of a PNG embedded in each page. Should be good enough for printing. This is the limitation of the Apache POI and docx4j libraries.  #### ODT: Quality and speed as good as DOCX. Conversion is done using odfdom of the Apache ODF Toolkit.  ### Main Libraries   Apache POI:  https://poi.apache.org/   xdocreport: http://code.google.com/p/xdocreport/   docx4j: http://www.docx4java.org/   odfdom: https://incubator.apache.org/odftoolkit/odfdom/   and others...    ## Compiling the code  I'm using Eclipse Mars IDE Java EE with the M2Eclipse plugin. Simply create a workspace and import my project into it. Let Maven do its work in downloading all the necessary dependencies. Once everything is downloaded, you should be able to run the MainClass.  More details can be found in the [Wiki section](https://github.com/yeokm1/docs-to-pdf-converter/wiki/Setting-up-your-IDE-to-compile-the-project).  The MIT License (MIT) Copyright (c) 2013-2014 Yeo Kheng Meng
kuali/kc	**Kuali Research** == Kuali Research (KC) for Research Administration is a comprehensive system to manage the complexities of research administration needs from the faculty researcher through grants administration to federal funding agencies. KC is using MIT’s proven COEUS system as its baseline design, updating its technical architecture for vendor independence and integration with other administration systems.  ---------- ##**Installation** **Prerequisites** [Maven 3.3.x][1] [Java 1.8.x][2] [Tomcat 7.x][3] [MySQL 5.6.x][4] [Git 2.4.x][5]  **Instructions** The Kuali Research application uses Apache Maven as it's build tool.  Inside of Kuali Research's Maven configuration is a list of dependencies.  Some of these dependencies may not be available in a publicly available Maven repository.  At the very least you will need to install the following projects into your maven repo.  These projects may have more than one submodule.  * [SchemaSpy](https://github.com/kuali/schemaspy) * [Kuali Research Rice](https://github.com/kuali/kc-rice) * [Kuali Research API](https://github.com/kuali/kc-api) * [Kuali Research S2S Gen](https://github.com/kuali/kc-s2sgen)   > **Build Order:** The projects required to install Kuali Research must be built in the following order:  1. SchemaSpy  2. Kuali Research Rice  3. Kuali Research API  4. Kuali Research S2s Generator  5. Kuali Research   **Step 1. clone all required projects** ``` cd workspace git clone https://github.com/kuali/schemaspy git clone https://github.com/kuali/kc-rice git clone https://github.com/kuali/kc-api git clone https://github.com/kuali/kc-s2sgen git clone https://github.com/kuali/kc ``` **Step 2. determine correct tag versions for dependent projects.** Manually search the pom.xml file in the root directory of the Kuali Research project, and find the version numbers for KC Rice, KC Api, Kc S2s Generator, and SchemaSpy  ``` <coeus-api-all.version>x.x.x.x</coeus-api-all.version> <coeus-s2sgen.version>x.x.x</coeus-s2sgen.version> ... <rice.version>x.x.x.x</rice.version> <schemaspy.version>x.x.x.x</schemaspy.version> ```  Then check out the correct tag before installing.  **Cross Project Build Instructions** We provide several maven build profiles that may be useful.  Some of these profiles are specific to a project while others are available in all projects. When a project specific profile is available, it will be documented in the build step.  The following are a list of profiles available in all projects:  > **GRM Profile:** When building Kuali Research Projects you should turn the grm maven profile off as it is on by default.  You can do this by sending the following system parameter grm.off on the command line.  ``` mvn clean install -Dgrm.off=true ```  > **Enforcer Profile:** When building Kuali Research Projects you can turn the enforcer profile on as it is off by default.  This will turn on the maven enforcer plugin and fail the build if certain project quality rules are violated.  ```  mvn clean install -Penforcer ```  > **jdeps Profile:** When building Kuali Research Projects you can turn the jdeps profile on as it is off by default.  This will turn on the jdk jdeps tool and fail the build if internal jdk apis are detected.  ```  mvn clean install -Pjdeps ```  > **Error Prone Profile:** When building Kuali Research Projects you can turn the error-prone profile on as it is off by default.  This will turn on the strict error prone compiler and fail the compile step if certain source code errors are detected.  ```  mvn clean install -Perror-prone ```  > **enforce-project-quality Property:** This property turns on the enforcer, jdeps, and error prone profiles.  These profiles are off by default. ```  mvn clean install -Denforce-project-quality ```  > **Disable Javascript Frontend Builds:** This property disables building javascript related artifacts and the api documentation. This is not recommended for a production environment, but can be preferred during development as it can speed up builds significantly. The javascript build is enabled by default. ``` mvn clean install -Dbuild-jsfrontend-node.off=true ```  > **Disable cleaning javascript artifacts:** This property disables cleaning up after the javascript build process. By default node, npm,  and node_modules are cleaned during mvn clean. By using this flag you can disable this step which can save significant time during a non-production build. This is not recommended for a production environment. ``` mvn clean install -Dclean-jsfrontend-node.off ```  All Kuali Research projects use standard maven conventions to build and install artifacts.  The following documents how to install source, javadoc, and primary artifacts for each maven projects.  > **Source and Javadoc jars:** When building Kuali Research Projects it may be helpful to also build source and javadoc jars.  These jars can be consumed by tools such as debuggers.  Note: due to changes in the javadoc tool in Java 8, you may need to execute the compile phase before attempting to create a javadoc jar.  ``` mvn clean compile source:jar javadoc:jar install ```  **Step 3: Build SchemaSpy** Check out the correct schemaspy version and run maven clean install. ``` cd schemaspy git checkout tags/schemaspy-xxxx.xx mvn clean compile source:jar javadoc:jar install -Dgrm.off=true ```  **Step 4: Build Kuali Research Rice** Checkout the correct version of Kuali Research rice and install. ``` cd ../kc-rice git checkout tags/rice-x.x.x.xxxxx.xx mvn clean compile source:jar javadoc:jar install -Dgrm.off=true  ```  Wait until coeus-api has installed successfully before moving to the next step.  **Step 5: Build Kuali Research Api** Checkout the correct version of Kuali Research api and install. ``` cd ../kc-api git checkout tags/coeus-api-xxxx.xx mvn clean compile source:jar javadoc:jar install -Dgrm.off=true  ```  Wait until coeus-api has installed successfully before moving to the next step.  **Step 6: Build Kuali Research S2sGen** Checkout the correct version of coeus-s2sgen and install ``` cd ../kc-s2sgen git checkout tags/coeus-s2sgen-xxxx.xx mvn clean compile source:jar javadoc:jar install -Dgrm.off=true  ```  Wait until coeus-s2sgen has installed successfully before moving to the next step.  **Step 7: Build Kuali Research** Installing Kuali Research  Install without Oracle support ``` cd ../kc mvn clean compile source:jar javadoc:jar install -Dgrm.off=true ```  > **Oracle Profile:** If using an Oracle database make sure oracle profile is used to insure Oracle specific jars are added to the classpath.  Application will fail to start up if the Oracle jar is not added. ``` mvn clean install -Dgrm.off=true -Poracle ```  > **Integration Tests:** This runs the integration tests.  This profile requires a properly configured integration test database and configuration. ``` mvn clean install -Dgrm.off=true -Pitests ```  > **Precompile jsps:** This precompiles the Kuali Research jsps for Tomcat 7.  This is useful to verify the absence of compile errors in jsps while the Kuali Research Application is being built.  Precompilation also helps the initial page load time for all jsps. ``` mvn clean install -Dgrm.off=true -Pprecompile-jsp-tomcat-7 ```  > **Include Mysql Driver:** This adds the mysql driver to the classpath and is on by default.  This profile can be turned off by sending the following system parameter grm.off on the command line. ``` mvn clean install -Dgrm.off=true -Pinclude-mysql-driver ```  > **Include Oracle Driver:** This adds the oracle driver to the classpath and is on by default.  This profile can be turned off by sending the following system parameter grm.off on the command line. ``` mvn clean install -Dgrm.off=true -Pinclude-oracle-driver ```  > **Dev Profile:**When developing with KC there are some features that are useful only for development purposes. In order to enable these features you should enable the dev profile. Currently the dev profile only provides the p6spy dependency. See the section below on Configuration for how to use this feature. ``` mvn clean install -Pdev ```  > **Node Clean:**When building our api documentation, our build process will download node.js and various node dependencies.  By default, these artifacts are deleted on every mvn clean execution.  You can avoid this clean step by sending the following system parameter clean-jsfrontend-node.off on the command line.  This is useful to speed up project builds by avoiding the installation node.js on subsequent clean install iterations.  ``` mvn clean install -Dclean-jsfrontend-node.off ```  > **System Dependent Requirements:** Kuali Research is now using some node and npm dependencies as part of its build process. These dependencies have all been designed to be downloaded, installed and run without any additional system level requirements, but there are some system specific requirements that cannot be managed by our build process. This seems to primarily affect Windows, but additional systems may be affected depending on local configuration. If you are seeing errors attempting to build Kuali Research that relate to node or npm, please see the failing node project's documentation for what might be expected to be installed on your system. For example, we currently have a dependency on a node project called drafter that builds our api documentation. From their [Windows specific documentation][10], drafter appears to require Visual Studio Express 2012 and Python 2.7. These types of dependencies are beyond our control, but we strive to make the build process as simple as possible.  **Step 8: Install Spring Instrumentation**  ***For Tomcat***:  	[Download][6] and install tomcat spring instrumentation in the tomcat lib directory     Configure the tomcat spring instrumenting classloader in the tomcat context.xml file      ``` <Context>     <!-- ...snip... -->     <Loader loaderClass="org.springframework.instrument.classloading.tomcat.TomcatInstrumentableClassLoader"/>     <!-- ...snip... --> </Context> ```   ***Otherwise***: 	[Download][7] and configure java agent in at the jvm level. ``` 	-javaagent:/Users/user/.m2/repository/org/springframework/spring-instrument/3.2.13.RELEASE/spring-instrument-3.2.13.RELEASE.jar ```   >**Note:** Application will fail to start up in spring instrumentation is not installed correctly.  **Step 9: Install Graphviz**  [Download][8] and install Graphviz.  >**Note:** In order for SchemaSpy to be fully functioning, graphviz must be installed and the dot binary accessible on the system path.  ##**Creating DB**  Kuali Research supports MySQL 5.6 and Oracle. We recommend MySQL though as that is the database we develop and support internally and are more easily able to respond to problems with that database.  If you choose to use Oracle, please be sure to use the latest Oracle driver version 12 or higher.  The version 12 driver will work correctly with older Oracle database servers such as version 11. The build profiles include-mysql-driver and include-oracle-driver can be used to include the database drivers in the application.  * [Database Installation Instructions](coeus-db/coeus-db-sql/src/main/resources/co/kuali/coeus/data/migration/sql/README.md)  **Configuration Information**  This section contains some useful information about configuring the Kuali Research Application.  This is not an exhaustive list of configuration options or a tutorial on configuration.  For more options see coeus-impl/src/main/resources/META-INF/kc-config-defaults.xml inside the Kuali Research Application source or the Kuali Rice public documentation. > **SchemaSpy:** The kc.schemaspy.enabled config param turns SchemaSpy on or off.  SchemaSpy is a great tool for visualizing the Kuali Research database and is generally useful on server instances.  It does create additional CPU upon application startup while gathering database information.  After information gathering is complete, CPU usage will go back to normal as all SchemaSpy information is cached as static content.  It is recommended to disable SchemaSpy on developer machines and during integration test runs.  ``` <param name="kc.schemaspy.enabled">false</param> ```  > **Monitoring** The kc.monitoring.enabled config param turns Monitoring on or off.  Monitoring is done through Java Melody and is great for learning about the runtime characteristics of the Kuali Research Application.  Java Melody has low overhead and in general can be left on. ``` <param name="kc.monitoring.enabled">false</param> ```  > **P6Spy** P6Spy can be a useful tool during development that will allow you to view sql statements that are generated and executed against the database in real time. In order to use it in KC you will need to enable the dev profile mentioned above as well as reconfigure your database connection string and driver similar to the below sample. All other kc-config.xml options should remain the same. Additionally you will need to configure the spy.properties file found in *coeus-webapp/src/main/resources/* to specify the correct original driver and potentially the appender method if StdOut is not sufficient. ``` <param name="datasource.url">jdbc:p6spy:mysql://localhost:3306/kcdev</param> <param name="datasource.driver.name">com.p6spy.engine.spy.P6SpyDriver</param> ```    [1]: http://maven.apache.org/download.cgi   [2]: http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html   [3]: https://tomcat.apache.org/download-70.cgi   [4]: http://dev.mysql.com/downloads/mysql/   [5]: http://git-scm.com/downloads   [6]: http://mvnrepository.com/artifact/org.springframework/spring-instrument-tomcat/3.2.13.RELEASE   [7]: http://mvnrepository.com/artifact/org.springframework/spring-instrument/3.2.13.RELEASE   [8]: http://www.graphviz.org/Download..php   [9]: https://github.com/google/error-prone   [10]: https://github.com/apiaryio/drafter/wiki/Building-on-Windows
mraible/boot-makeover	boot-makeover =============  [A Webapp Makeover with Spring 4 and Spring Boot](http://raibledesigns.com/rd/entry/a_webapp_makeover_with_spring)
thymeleaf/thymeleaf-spring	thymeleaf-spring  ----------------  This repository contains:     * thymeleaf-spring3: Thymeleaf integration package for Spring 3.x    * thymeleaf-spring4: Thymeleaf integration package for Spring 4.x    * thymeleaf-spring5: Thymeleaf integration package for Spring 5.x   To learn more and download latest version:        http://www.thymeleaf.org
orangesignal/orangesignal-csv	# [OrangeSignal CSV](http://orangesignal.github.io/orangesignal-csv/) [![Build Status](https://travis-ci.org/orangesignal/orangesignal-csv.png?branch=master)](https://travis-ci.org/orangesignal/orangesignal-csv)  OrangeSignal CSV is a very flexible csv (comma-separated values) read and write library for Java.    The binary distributions includes the following third party software:   [jLHA (LHA Library for Java)](http://homepage1.nifty.com/dangan/en/Content/Program/Java/jLHA/jLHA.html).  ## Prerequisites  * Java 1.6+   OrangeSignal CSV is compiled for Java 1.6  ## Installation  ### Maven users  If you are using Maven, simply copy the following dependency into your pom.xml file. The artifact is hosted at [Maven Central](http://search.maven.org/#search%7Cga%7C1%7Corangesignal-csv), and is standalone (no dependencies).  ```xml <dependency>     <groupId>com.orangesignal</groupId>     <artifactId>orangesignal-csv</artifactId>     <version>2.2.1</version> </dependency> ```  ## Examples  CSV entity class  ```java @CsvEntity(header = true) public class Customer {      @CsvColumn(name = "name")     public String name;      @CsvColumn(name = "age")     public Integer age;  } ```  example code   ```java CsvConfig cfg = new CsvConfig(',', '"', '"'); cfg.setNullString("NULL"); cfg.setIgnoreLeadingWhitespaces(true); cfg.setIgnoreTrailingWhitespaces(true); cfg.setIgnoreEmptyLines(true); cfg.setIgnoreLinePatterns(Pattern.compile("^#.*$")); cfg.setVariableColumns(false);  List<Customer> list = new CsvEntityManager()     .config(cfg)     .load(Customer.class)     .filter(new SimpleBeanFilter().in("name", "Smith", "Johnson").gt("age", 21))     .offset(10)     .limit(1000)     .order(BeanOrder.desc("age"))     .from(reader); ```  ## How to use  * [User guide](http://orangesignal.github.io/orangesignal-csv/userguide.html) * [Migration](http://orangesignal.github.io/orangesignal-csv/migration.html)  Sorry, it is japanese only for now.  ## License  * Licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).
HearthStats/HearthStats.net-Uploader	[![Build Status](https://travis-ci.org/HearthStats/HearthStats.net-Uploader.svg?branch=master)](https://travis-ci.org/HearthStats/HearthStats.net-Uploader)  [![Coverage Status](https://img.shields.io/coveralls/HearthStats/HearthStats.net-Uploader.svg)](https://coveralls.io/r/HearthStats/HearthStats.net-Uploader?branch=master)  [![Stories in Ready](https://badge.waffle.io/HearthStats/HearthStats.net-Uploader.png?label=Ready)](https://waffle.io/HearthStats/HearthStats.net-Uploader)  [![Gitter chat](https://badges.gitter.im/HearthStats.png)](https://gitter.im/HearthStats)    HearthStats Companion  =====================    ___We are now officially supporting HearthstoneDeckTracker as the official HearthStats uploader for Windows. You can download it here: http://www.hearthstats.net/uploader___    __This means this repo is no longer maintained!__    (Previously known as the 'HearthStats Uploader')    This is a Java based utility designed to run in the background and automatically  upload your win ratios and other statistics to [HearthStats.net](http://HearthStats.net)  while you play Hearthstone. This program uses screen grab analysis of your Hearthstone window  and does not do any packet sniffing, monitoring, or network modification of any kind.    This project is and always will be open source so that you can do your own builds   and see exactly what's happening within the program.       Features  --------------------    * Support for both Windows and OS X  * Automatically tracks constructed and arena matches  * Tracks your class and your opponent's class  * Tracks your rank level for ranked matches  * Tracks your opponent's name  * Tracks number of rounds played and match duration  * Add notes to your matches directly from HearthStats Companion    See the [Development Status](https://github.com/HearthStats/HearthStats.net-Uploader/wiki/Development-Status) wiki page for more detail about what is currently supported.      Running Beta Builds  --------------------    This project is under HEAVY construction, but you can still run beta builds  to help test things out or just see how things are going. Check out the project's  [milestones](https://github.com/HearthStats/HearthStats.net-Uploader/issues/milestones)   to see how things are progressing.    * Make sure you have [Java 7](http://java.com/en/download/manual_java7.jsp) installed  * Download the __[latest release](https://hearthstats.net/uploader)__ of the HearthStats Companion  * Extract the downloaded zip file to any folder    * On **Windows**, run HearthStats.exe in the folder where you extracted the zip file    * On **Mac OS X**, run HearthStats in that folder, or from the Applications folder if you prefer  * A window should open called "HearthStats Companion"  * Start your Hearthstone client     * On **Windows**, put Hearthstone in **windowed mode** (see [issue #17](https://github.com/HearthStats/HearthStats.net-Uploader/issues/17))    * On **Mac OS X**, you can run Hearthstone in **windowed mode** or **full-screen**  * Look for notifications in the corner of your screen that indicate event detection    * Notifications will only appear in windowed mode, not in full-screen mode  * [Give us feedback and ideas!](http://help.hearthstats.net/forums/257732-hearthstats-companion)        Known Issues  -------------    * On Windows, Hearthstone must be running in __windowed mode__ for now (see [issue #17](https://github.com/HearthStats/HearthStats.net-Uploader/issues/17))  * On 64-bit Windows you need to install [Visual C++ Redistributable for Visual Studio 2012](http://www.microsoft.com/en-us/download/details.aspx?id=30679) if you don't have it already       Please see the full [list of known issues](https://github.com/HearthStats/HearthStats.net-Uploader/issues)  as well.    Contributing to this Project  ----------------------------    There are several things you can do to help this project out:    * Check out and participate in [the reddit thread](http://www.reddit.com/r/hearthstone/comments/1wa4rc/auto_uploader_for_hearthstatsnet_need_help_testing/)  * Download and test early builds, making sure to [report any issues you find](http://help.hearthstats.net/forums/257732-hearthstats-companion)  * Fork this repository if you can hack, and create a pull request if you come up with things to contribute.  * [Buy Jerome a cup of coffee via PayPal](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=F9XNSXLZNP9QQ) for his work on this app  * Donate to HearthStats.net's founder Jeff through the site's [about us page](http://hearthstats.net/aboutus)
jpmml/jpmml-model	JPMML-Model [![Build Status](https://travis-ci.org/jpmml/jpmml-model.png?branch=master)](https://travis-ci.org/jpmml/jpmml-model) ===========  Java Class Model API for Predictive Model Markup Language (PMML).  # Features #  * Full support for PMML 3.0, 3.1, 3.2, 4.0, 4.1, 4.2 and 4.3 schemas:   * Schema version annotations.   * Extension elements, attributes, enum values. * Enhanced API:   * Class hierarchy.   * Marker interfaces for common traits.   * Value constructors.   * Method chaining-friendly setter methods.   * Optional SAX Locator information. * [Visitor pattern](http://en.wikipedia.org/wiki/Visitor_pattern):   * Validation agents.   * Optimization and transformation agents. * Supported platforms:   * Java SE and EE.   * Android.   * Google Web Toolkit (GWT). * Supported JAXB runtimes:   * [GlassFish Metro](https://metro.java.net)   * [EclipseLink MOXy](https://www.eclipse.org/eclipselink)  # Prerequisites #  * Java 1.7 or newer.  # Installation #  JPMML-Model library JAR files (together with accompanying Java source and Javadocs JAR files) are released via [Maven Central Repository](http://repo1.maven.org/maven2/org/jpmml/).  The current version is **1.3.8** (10 September, 2017).  ```xml <dependency> 	<groupId>org.jpmml</groupId> 	<artifactId>pmml-model</artifactId> 	<version>1.3.8</version> </dependency> ```  # Usage #  The class model consists of two types of classes. There is a small number of manually crafted classes that are used for structuring the class hierarchy. They are permanently stored in the Java sources directory `/pmml-model/src/main/java`. Additionally, there is a much greater number of automatically generated classes that represent actual PMML elements. They can be found in the generated Java sources directory `/pmml-model/target/generated-sources/xjc` after a successful build operation.  All class model classes descend from class `org.dmg.pmml.PMMLObject`. Additional class hierarchy levels, if any, represent common behaviour and/or features. For example, all model classes descend from class `org.dmg.pmml.Model`.  The class model should be self-explanatory. The application developer is advised to consult with the latest [PMML specification](http://dmg.org/pmml/v4-3/GeneralStructure.html) about the specifics of individual PMML elements and attributes.  ### Unmarshalling ###  Load any PMML schema version 3.X or 4.X document into live `org.dmg.pmml.PMML` instance:  ```java public PMML load(InputStream is) throws SAXException, JAXBException {   return org.jpmml.model.PMMLUtil.unmarshal(is); } ```  **Important**: It is the responsibility of the application developer to ensure that the XML document does not contain malicious content (eg. XEE and XXE attacks).  ### Applying visitors ###  Delete SAX Locator information from the class model:  ```java public void optimize(PMML pmml){   LocatorNullifier nullifier = new LocatorNullifier();   nullifier.applyTo(pmml); } ```  ### Marshalling ###  Store live `org.dmg.pmml.PMML` instance into PMML schema version 4.3 document:  ```java public void store(PMML pmml, OutputStream os) throws JAXBException {   org.jpmml.model.PMMLUtil.marshal(pmml, os); } ```  # Example applications #  Module `pmml-model-example` exemplifies the use of JPMML-Model library.  This module can be built using [Apache Maven](http://maven.apache.org/): ``` mvn clean install ```  The resulting uber-JAR file `target/example-1.3-SNAPSHOT.jar` contains the following command-line applications: * `org.jpmml.model.CopyExample` [(source)](https://github.com/jpmml/jpmml-model/blob/master/pmml-model-example/src/main/java/org/jpmml/model/CopyExample.java). Copies and transforms a PMML schema version 3.X or 4.X document to a PMML schema version 4.3 document. * `org.jpmml.model.ObfuscationExample` [(source)](https://github.com/jpmml/jpmml-model/blob/master/pmml-model-example/src/main/java/org/jpmml/model/ObfuscationExample.java). Obfuscates a PMML document by replacing field names with their MD5 hashes. * `org.jpmml.model.ValidationExample` [(source)](https://github.com/jpmml/jpmml-model/blob/master/pmml-model-example/src/main/java/org/jpmml/model/ValidationExample.java). Validates a PMML schema version 3.X or 4.X document against the built-in XML Schema Definition (XSD) resource.  Copying `input.pmml` to `output.pmml`; the class model is transformed by applying a list of visitors to it: ``` java -javaagent:../pmml-agent/target/pmml-agent-1.3-SNAPSHOT.jar -cp target/example-1.3-SNAPSHOT.jar org.jpmml.model.CopyExample --visitor-classes org.jpmml.model.visitors.DictionaryCleaner,org.jpmml.model.visitors.MiningSchemaCleaner --summary true --input input.pmml --output output.pmml ```  Checking the validity of `model.pmml`: ``` java -cp target/example-1.3-SNAPSHOT.jar org.jpmml.model.ValidationExample --input model.pmml ```  Getting help: ``` java -cp target/example-1.3-SNAPSHOT.jar <application class name> --help ```  It is possible to activate a specific Java XML Binding (JAXB) runtime by setting the value of the `javax.xml.bind.context.factory` system property. Use `com.sun.xml.bind.v2.ContextFactory` for activating a GlassFish Metro runtime, and `org.eclipse.persistence.jaxb.JAXBContextFactory` for activating an EclipseLink MOXy runtime.  For example: ``` java -Djavax.xml.bind.context.factory=org.eclipse.persistence.jaxb.JAXBContextFactory -cp target/example-1.3-SNAPSHOT.jar ... ```  # Support and Documentation #  Limited public support is available via the [JPMML mailing list](https://groups.google.com/forum/#!forum/jpmml).  The [Openscoring.io blog](http://openscoring.io/blog/) contains fully worked out examples about using JPMML-Model and JPMML-Evaluator libraries.  Recommended reading: * [Extending PMML documents with custom XML content](http://openscoring.io/blog/2015/05/15/jpmml_model_api_vendor_extensions/) * [Transforming and measuring the memory consumption of class model objects using the Java agent technology](http://openscoring.io/blog/2015/02/06/jpmml_model_api_transform_measure/) * [Converting PMML documents between different schema versions](http://openscoring.io/blog/2014/06/20/jpmml_model_api_import_export/)  # License #  JPMML-Model is licensed under the [BSD 3-Clause License](http://opensource.org/licenses/BSD-3-Clause).  # Additional information #  Please contact [info@openscoring.io](mailto:info@openscoring.io)
lycying/c2d-engine	c2d-engine ==========  the new demo apk can be downloaded here https://github.com/lycying/c2d-engine/blob/master/c2d-tests-android.apk?raw=true __________  move from https://code.google.com/p/c2d-engine/  c2d is a game engine, base on [https://github.com/libgdx/libgdx libgdx].  Aim to easy use of the the libgdx api .I design c2d at my free time so if you have any ideas please give me feedback .  #### Some of the characteristics C2d built a readable and expandable API .    * Resources and loading  - Alias Resource Manager any resources my be had one or more alias so you can easy change the resource but not the codes.  - Custom resources load , you can change the subfix and protected your resources easily.  - Custom game loadding screen * Event and Screen Switch  - EventManager ,fire the regiestered event anytime.  - TransitionScreen the animations of screen switch * Graphics  - Animation Sprite support loop and LoopWithTime? mode.  - Parallax Layer the Parallax Layer ,you can custom the layer's content via ParallaxLayerDrawable?  - Surfaces support opengles1.x and 2.x . * Pixmap HelperModifying textures using libGDX Pixmap in runtime * Box2d  - Load the world from json the json file is designed with c2d-tools   ### Demo Some games use c2d-engine.  Fool Dig https://play.google.com/store/apps/details?id=info.u250.digs  ![screenshort](https://lh3.ggpht.com/nLj_-YjGajv0_h4jhS_8hn05klB0QCjqjzfmoa95TABV-kgvFeR5Gaf1M5iyimAj1uE=h310)  My Dragon https://play.google.com/store/apps/details?id=com.joyboat6.iland  ![screenshort](https://lh3.ggpht.com/nVyf0i77YCnphOslQnTW4B865W_Ez0E2GEsYUAq76S2tHlnd3YeVpFmGL29AtUnL6A=h310)   ### Box2d editor  ![screenshort](https://raw.githubusercontent.com/lycying/c2d-engine/master/doc/c2d-box2d-editor-screenshot.png)  Be sure save it to disk first when u make a new scene.   Normal: * Mouse wheel zooming scene * Drag using Right mouse to move the scene. * Click the examples under the test scenario for testing. * Click the run test * Right-click on the list on the left to add the Fixture, Body etc. * Any change of the spinner or checkbox will be applied to the model.   Some Edit mode: * Circle editing mode:  - Click on the screen, move the mouse to resize the circle, tap the screen again to end * Box Edit mode:  - Click on the screen, and move the mouse to resize the box, tap the screen again to end * Polygon mode(This May Has Error If too many confusion Clicks ):  - The left mouse button click on the screen to add point.  - Right Click actor point to delete point.  - Click the ritht-bottom-vertices editor to  insert points  or delete point between specific points.  - Dragging the actor points to adjust the position of the vertex * Body edit mode:  - Right-click in the Right bottom Fixture list to add firmware  - Drag the body to move  - Drag the black area to rotate body  - Press "space" to enter joint mode * Joint edit Mode:  -  draw a straight line from object A to object B ,  select the joint mode at the pop-up menu  -  Press Space to enter the body  mode
naver/volley-extensions	# Introduction to Volley Extensions  This library provides assistance in developing Android applications using [Volley](https://android.googlesource.com/platform/frameworks/volley/).  It provides useful classes such as various requests, disk caches, memory caches, and custom views.  Volley Extensions has been divided into following pluggable sub-projects.   - [Volleyer](https://github.com/naver/volley-extensions/tree/master/volleyer) : Volleyer provides a very simple and much improved interface for developers to work with while using Volley. Also includes branch new features previously not in Volley.  - [Volley requests](https://github.com/naver/volley-extensions/tree/master/volley-requests) : Contains Request classes that parses json and xml using Jackson library and Simple xml library.  - [Volley caches](https://github.com/naver/volley-extensions/tree/master/volley-caches) : Contains improved DiskBasedCache and capable AUIL Cache for use in Volley.  - [Volley custom views](https://github.com/naver/volley-extensions/tree/master/volley-views) : Added user interaction capabilities to NetworkImageView.  Each sub-project includes guides and samples for installation/development.  # Volley mirror dependency This library depends on [Volley mirror project](https://github.com/mcxiaoke/android-volley). The mirror project keeps an up-to-date version of [Volley](https://android.googlesource.com/platform/frameworks/volley/) and automatically uploads to Maven Central.   The library will continue to keep this dependency until Volley provides an official package in Maven Central.  # License  	Copyright (C) 2014 Naver Corp.  	 	Licensed under the Apache License, Version 2.0 (the "License"); 	you may not use this file except in compliance with the License. 	You may obtain a copy of the License at  		http://www.apache.org/licenses/LICENSE-2.0  	Unless required by applicable law or agreed to in writing, software 	distributed under the License is distributed on an "AS IS" BASIS, 	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 	See the License for the specific language governing permissions and 	limitations under the License.
roikku/swift-explorer	Swift Explorer [![Build Status](https://roikku.ci.cloudbees.com/buildStatus/icon?job=swift-explorer)](https://roikku.ci.cloudbees.com/job/swift-explorer/) ========  User-friendly tool to manage files in an OpenStack Storage system (SWIFT). Please, visit [www.swiftexplorer.org](http://www.swiftexplorer.org) for further details.
zutherb/AppStash	# AppStash - Microservice Phone Shop Application  ## Overview  This application gives software architects and developers an example how a microservice web application can look like and it simulates a development cluster, which contains continuous integration infrastructure as well as all necessary nodes that are needed to run an online shop. Thus it will furthermore shown, how a distributed online shop can deployed with a multi deployment pipeline and how the distributed system can be monitored. The application is based on the following two online shop applications, which can be found on Github: - [AngularJS Phone Catalog](https://github.com/angular/angular-phonecat) - [MongoDB Pizza Shop](https://github.com/comsysto/mongodb-onlineshop)  Both project was combined to an new online shop that is indeed to sell mobile devices and implements the following use cases. An user is able to: - see different kinds of mobile devices catalogs (e.g. mobiles or tablets), - create a cart, - and order the created cart.  ![Use Case Online Shop](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/use_case_online_shop.png)  This use cases are implemented in the following two ways:  - A [Monolitic Webshop](https://github.com/zutherb/AppStash/#monolith-appserver), which is represented by a three layered   online shop based on [Apache Wicket](http://wicket.apache.org/), the [Spring Framework](http://projects.spring.io/spring-framework/)   and [Spring Data](http://projects.spring.io/spring-data/) that implements all given use cases, - and microservice architecture, which is based on a mix of the Monolitic Webshop and a [Microservice Catalog Frontend](https://github.com/zutherb/AppStash/#microservice-appserver)   as it is shown in the below deployment diagram. In this mix a so called Microservice Catalog Frontend provides the   use case that an user should be able to see the different mobile. Finally the Monolitic Webshop is used by the user   to create an order that means Monolitic Webshop represents a microservice on its own for this specific use case.   The Microservice Catalog Frontend is based on an [AngularJS](https://angularjs.org/) and [Typescript](http://www.typescriptlang.org/)   which access different kinds of [REST-Services](http://en.wikipedia.org/wiki/Representational_state_transfer) that are   implemented in [Scala](http://www.scala-lang.org/), [Spray](http://spray.io/), [Restx](http://restx.io/) and [Spring Boot](http://projects.spring.io/spring-boot/).  ![Deployment Diagram Online Shop](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/deployment_diagramm_online_shop.png)  ## Used technologies  For simplicity all services are supposed to run on a Java-VM at the moment.  * Web frontend   * Based on Angular JS and Typescript * Cart service   * Based on Spring boot and Groovy   * Redis backend via Spring Data * Product backend   * Based on Scala and Spray   * MongoDB backend via ReactiveMongo * Navigation backend   * Based on Restx with embedded Jetty   * MongoDB backend via Jongo  ## Presentations  Date       | Event                         | Title -----------|-------------------------------|---------------------------------------------------------------------------------------- 12.01.2015 | [JUGM](http://www.jugm.de/)   | [Next Generation IT - Qual oder Segen für den Entwickler](http://zutherb.github.io/AppStash/slides/01_jugm/) 23.01.2015 | [bobkonf](http://bobkonf.de/) [Video (german)](https://www.youtube.com/watch?v=G7CmsYNKP4A) | [Microservices und die Jagd nach mehr Konversion - Fluch oder Segen für den Entwickler](http://zutherb.github.io/AppStash/slides/02_bobkonf/) 26.01.2015 | [Microservices Meetup Munich](http://www.meetup.com/Microservices-Meetup-Munich/) [Video (german)](http://youtu.be/t6YfKMFvPvs)| [Microservices und die Jagd nach mehr Konversion](http://zutherb.github.io/AppStash/slides/03_microservice_usergroup_munich/) 24.02.2015 | [Microservices Meetup Berlin](http://www.meetup.com/Microservices-Meetup-Berlin/) | [Microservices & Conversion Hunting - Software architectures for changeableness](http://zutherb.github.io/AppStash/slides/04_microservice_usergroup_berlin/) 09.03.2015 | [Microservices Meetup Hamburg](http://www.meetup.com/Microservices-Meetup-Hamburg/) | [Microservices und die Jagd nach mehr Konversion](http://zutherb.github.io/AppStash/slides/05_microservice_usergroup_hamburg/) 25.03.2015 | [Javaland](http://javaland.eu/) | [Die Jagd nach mehr Konversion - Fluch oder Segen für den Entwickler](http://zutherb.github.io/AppStash/slides/06_javaland/) 15.04.2015 | [confess](https://www.regonline.com/builder/site/Default.aspx?EventID=1619724) | [Microservices and Conversion Hunting - How to build software architectures for changeableness](http://zutherb.github.io/AppStash/slides/07_confess/) 21.04.2015 | [Agile Softwarearchitektur Münster](http://www.meetup.com/Agile-Softwarearchitektur/) | [Microservices und die Jagd nach mehr Konversion](http://zutherb.github.io/AppStash/slides/08_agile_softwarearchitektur_muenster/) 23.06.2015 | [Devoxx Poland](http://devoxx.pl/) | [Microservices and Conversion Hunting - How to build software architectures for adaptability](http://zutherb.github.io/AppStash/slides/09_devoxx_pl/) 20.08.2015 | [IT Meetup Bali](http://www.meetup.com/IT-Meetup-Bali/events/224324564) | [Microservices and Conversion Hunting: Build Architectures for Changeability](http://zutherb.github.io/AppStash/slides/10_bali/) 14.09.2015 | [Microservices Meetup Hamburg](http://www.meetup.com/Microservices-Meetup-Hamburg/) | [Microservice-Deployment ganz einfach](http://zutherb.github.io/AppStash/slides/11_ljugm/) 15.09.2015 | [Lightweight Java User Group München](http://www.meetup.com/de/lightweight-java-user-group-munchen/) | [Microservice-Deployment ganz einfach](http://zutherb.github.io/AppStash/slides/11_ljugm/) 30.09.2015 | [code.talks](http://www.codetalks.de/) | [Microservices und die Jagd nach mehr Konversion](http://zutherb.github.io/AppStash/slides/12_codetalks) 06.10.2015 | [Java Forum Nord](http://www.java-forum-nord.de/) | [Microservices und die Jagd nach mehr Konversion](http://zutherb.github.io/AppStash/slides/13_java_forum_nord) 29.10.2015 | [JavaOne](https://www.oracle.com/javaone/index.html) | [Microservices and Conversion Hunting: Build Architectures for Changeability](http://zutherb.github.io/AppStash/slides/14_javaone) 03.11.2015 | [W-Jax](https://jax.de/wjax2015/) | [Von Null auf Hundert mit Microservices](http://zutherb.github.io/AppStash/slides/15_wjax) 28.01.2016 | [DevOps Karlsruhe Meetup](http://www.meetup.com/de-DE/DevOps-Karlsruhe-Meetup/) | [Von Null auf Hundert mit Microservices](http://zutherb.github.io/AppStash/slides/16_karlsruhe)  ## Articles  Title | Language ------|----------------- [Microservices und die Jagd nach mehr Konversion – das Heilmittel für erkrankte IT-Architekturen?](http://bernd-zuther.de/wp-content/uploads/2015/02/02-2015-Java-aktuell-Bernd-Zut-her-Microservices-und-die-Jagd-nach-mehr-Konversion-das-Heilmittel-fu%CC%88r-erkrankte-IT-Architekturen.pdf) | German [Microservice-Deployment ganz einfach mit Giant Swarm](https://blog.codecentric.de/2015/05/microservice-deployment-ganz-einfach-mit-giant-swarm/) | German [Microservice-Deployment ganz einfach mit Kubernetes](https://blog.codecentric.de/2015/05/microservice-deployment-ganz-einfach-mit-kubernetes/) | German [Microservice-Deployment ganz einfach mit Docker Compose](https://blog.codecentric.de/2015/05/microservice-deployment-ganz-einfach-mit-docker-compose/) | German [Microservice-Deployment ganz einfach ohne Docker mit der Linux-Paketverwaltung](https://blog.codecentric.de/2015/05/microservice-deployment-ganz-einfach-ohne-docker-mit-der-linux-paketverwaltung/) | German [Canary-Releases mit der Very Awesome Microservices Platform (Vamp)](https://blog.codecentric.de/2015/10/canary-release-mit-der-awesome-microservices-platform-vamp/) | German [In 10 Minuten zum Kubernetes Cluster auf AWS](https://blog.codecentric.de/2015/12/in-10-minuten-zum-kubernetes-cluster-auf-aws/) | German  ## Directory Layout  The following directory layout shows only the important directories that are necessary to implement the given use cases in the [overview](https://github.com/zutherb/AppStash/#overview).      microservice/           --> all files of the microservice applications are located in this folder         frontend/           --> all microservice frontend applications are located in this folder             catalog/        --> an AngularJS frontend application that shows the product catalog and is used to create a cart is located in this directory             checkout/       --> all files that are needed to glue the checkout form of the monolithic to the microservice catalog frontend         service/            --> all business services are located in the folder             cart/           --> a spring boot cart rest service is located in the folder             navigation/     --> a java based restx navigation rest service is located in the folder             product/        --> a scala spray product rest service is located in the folder     monolithic/             --> all files of the monolithic application are located in this directory  ## Prerequisites  You need some dependencies to run the application cluster or to add add own services to the showcase application.  ###Running   You need at least 16 GB RAM to run the whole cluster that emulates a whole development environment like you can find it in the must professional software development projects. Furthermore you have to install the following software dependencies on your machine. If you want to run the shop applications with lower memory you has to uses [Docker](https://github.com/zutherb/AppStash/blob/master/compose/README.md) or [Kubernetes](https://github.com/zutherb/AppStash/blob/master/kubernetes/README.md).  The provisioning should work on other systems as well but is only tested on MacOSX 10.10. That is the reason because the whole instructions that are shown, are related to MacOSX.  #### Git  - Git [home](http://git-scm.com/) (download, documentation) is a distributed revision control system. - A good place to learn about setting up git is [here](https://help.github.com/articles/set-up-git) - You should install Git with [Homebrew](http://brew.sh/)  ``` brew install git ```  #### Vagrant  - Vagrant creates and configures lightweight, reproducible, and portable development environments. You do not have to learn much about Vagrant, but you should be able to install it and execute the following commandline: ```vagrant up``` - [Vagrant](https://www.vagrantup.com/) (download, documentation) - You should install Vagrant with [Homebrew](http://brew.sh/)  ``` brew tap caskroom/cask brew install brew-cask brew cask install virtualbox brew cask install vagrant brew cask install vagrant-manager ```  #### Ansible  - [Ansible](http://www.ansible.com/) (download, documentation) is a tool for automating infrastructure orchestration.   You must not know Ansible, but you have to install Ansible otherwise Vagrant is not able to create the virtual machines. - You should install Ansible with [Homebrew](http://brew.sh/)  ``` brew install ansible ```  ###Boot up the cluster  The only thing you have to do to run the whole microservice cluster is to execute the following commands:  ```bash git clone git@github.com:zutherb/AppStash.git appstash cd appstash/vagrant vagrant plugin install vagrant-cachier vagrant plugin install vagrant-hostsupdater vagrant up ```  Vagrant will provision each node in the cluster with all the necessary components (e.g. Monitoring, Build, Database, Debian repository and Application Server). The initial setup can take a few minutes.  ### Deploy on production servers  You have to execute the [Production Deployment Builds](http://ci.microservice.io:8080/view/Production%20Deployment/) on the [Jenkins CI Server](http://jenkins-ci.org/) after you have boot up the cluster. Otherwise you can not use the production urls that are given in the next section. Therefore you have to execute the following two builds:  - [Microservice Production Deployment](http://ci.microservice.io:8080/view/Production%20Deployment/job/shop-microservice-production-deployment/build?delay=0sec) - [Monolith Production Deployment](http://ci.microservice.io:8080/view/Production%20Deployment/job/shop-monolitic-production-deployment/build?delay=0sec)  Please check if all builds are green sometimes the catalog ui build fails and must be re run untill it is green.   ![CI-Node](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/production-deployment.png)  After you have complete this, the cluster is fully installed and you can start to work with it.  ## Workings with the application cluster  The Cluster contains of the following nodes:  Vargrant-Name | IP            | Hostname           | Application                 | Forward --------------|---------------|--------------------|-----------------------------|-------------------------------------------------------------------- buildserver   | 10.211.55.200 | ci-node            | Jenkins                     | http://ci.microservice.io:8080/ reposerver    | 10.211.55.201 | ci-repo            | Artifact Repository (NGINX) | dbserver      | 10.211.55.202 | mongodb-node       | MongoDB                     | localhost:27017 dbserver      | 10.211.55.202 | redis-node         | Redis                       | localhost:6379 appserver1    | 10.211.55.101 | app-server-node-1  | Legacy Shop                 | http://test.monolith.io:8080/shop/ appserver1    | 10.211.55.101 | app-server-node-1  | Probe                       | http://test.monolith.io:8080/probe/ (admin / topsecret) appserver2    | 10.211.55.102 | app-server-node-2  | Legacy Shop                 | http://shop.monolith.io:8080/shop/ appserver2    | 10.211.55.102 | app-server-node-2  | Probe                       | http://shop.monolith.io:8080/probe/ (admin / topsecret) appserver3    | 10.211.55.103 | app-server-node-3  | Microservice Shop           | http://test-shop.microservice.io/ appserver3    | 10.211.55.104 | app-server-node-4  | Microservice Shop           | http://shop.microservice.io/ elasticsearch | 10.211.55.100 | monitoring-node    | Kibana                      | http://monitoring.microservice.io/ elasticsearch | 10.211.55.100 | monitoring-node    | Nagios                      | http://monitoring.microservice.io/nagios3/ (nagiosadmin / admin123) elasticsearch | 10.211.55.100 | monitoring-node    | Icinga                      | http://monitoring.microservice.io/icinga/ (icingaadmin / admin123)  ###CI-Node  A Jenkins build server is running on the CI-Node. Jenkins is an open source continuous integration tool written in Java that provides a continuous integration services for software development which supports diffent SCM tools. Furthermore Jenkins can execute different build scripts like [Gradle](http://gradle.org/) as well as arbitrary shell scripts and Windows batch commands.  You can reach the jenkins that builds and deploy the monolith and microservice application under the following url http://ci.microservice.io:8080/.  ![CI-Node](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/ci-node.png)  ###Monolith Appserver  The monolith online shop is deployed on the Monolith Appserver which is a reference implementation for the given use cases in the [Overview](https://github.com/zutherb/AppStash/#overview). You can reach the online shop under the following url http://shop.monolith.io:8080/shop/ .  ![Monolith Appserver](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/monolith-appserver.png)  Furthermore you can reach the [PSI Probe](https://code.google.com/p/psi-probe/) monitoring and log analysis services under the following url http://shop.monolith.io:8080/probe/. The user credentials are admin / topsecret.  PSI Probe is a community-driven fork of Lambda Probe, which is intended to replace the Tomcat Manager and should make it easier to manage and monitor an instance of Apache Tomcat. PSI Probe does not require any changes to an existing app and it provides many features through a web-accessible interface that becomes available simply by deploying it to your server. These features include:  - Requests: Monitor traffic in real-time, even on a per-application basis. - Sessions: Browse/search attributes, view last IP, expire, estimate size. - Logs: View contents, download, change levels at runtime. - Threads: View execution stack, kill. - JVM: Memory usage charts, advise GC.  ![Probe](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/probe.png) ![Probe](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/probe-log.png)  Moreover you access the performance monitor [JETM](http://jetm.void.fm/) under the following url http://shop.monolith.io:8080/shop/performance/ which is a small and free library that is included in the monolith online shop, that helps locating performance problems in existing Java applications. JETM enables developers to track down performance issues on demand, either programmatic or declarative with minimal impact on application performance, even in production.  ![JETM Overview](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/performance-overview.png) ![JETM Request view](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/performance-request-view.png)  [JMX](http://en.wikipedia.org/wiki/Java_Management_Extensions) is a natural way to have access to technical management, e.g. for tuning, statistics, log levels and so on. Unfortunately, it lacks a lightweight tool to expose mbeans and to browse them securely on any application and environment without heavy infrastructure setup. [JMiniX](https://code.google.com/p/jminix/) provides such a feature. You can reach JMiniX under the following url http://shop.monolith.io:8080/shop/jmx/.  ![JMiniX](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/jminix.png)  ###Microservice Appserver  The microservice based online shop is deployed on the microservice appserver which is a reference implementation for the given use cases in the [Overview](https://github.com/zutherb/AppStash/#overview). You can reach the online shop under the following url http://shop.microservice.io/ .  ![Microservice Appserver](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/microservice-appserver.png)  The microservice based online shop consists of two frontend parts as you can see in the deployment diagram in the [overview section](https://github.com/zutherb/AppStash/#overview). The first part is an AngularJS Catalog Frontend that makes it possible to see a catalog for mobiles as well as a catalog for tablets. Furthermore a user is able to create a cart. If a user wants to order a created cart there is same clue logic in the [monolithic web application](https://github.com/zutherb/AppStash/#monolith-appserver) that a cart which was created in the AngularJS Catalog Frontend can be order with the checkout of the Wicket online shop on the monolith appserver.  ###Monitoring Server  Monitoring a monolithic web application is no major pain as you can see in the [monolith appserver section](https://github.com/zutherb/AppStash/#monolith-appserver). A distributed web application, like it is shown in the [microservice appserver section](https://github.com/zutherb/AppStash/#microservice-appserver), is not so easy to monitor. In this small example there is a [Ngnix](http://nginx.org/) web server that logs all request that comes into it. The Ngnix deliveries a AngularJS Catalog Frontend that represents a A single-page application (SPA). A SPA is a web application that fits on a single web page with the goal of providing a more fluid user experience akin to a desktop application. In addition to the SPA there are the three rest services a cart, product and a navigation service which are need for the different uses cases and are implemented in the programming languages Groovy, Scala and Java. This services must be alive that an user can see the products or create a cart. Furthermore there is a legacy JEE web application which is deployed in a [Tomcat Webserver](http://tomcat.apache.org/). An user can order its cart with that legacy JEE web application.  Icinga is an open source network and computer system monitoring application. It was originally created as a fork of the Nagios system monitoring application. Icinga is attempting to get past perceived short-comings in Nagios development process, as well as adding new features such as a modern Web 2.0 style user interface, additional database connectors, and a REST API that lets administrators integrate numerous extensions without complicated modification of the Icinga core.  ![Icinga Status Map](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/icinga-status-map.png) ![Icinga Status Report](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/icinga-status.png)  Kibana is a browser based analytics and search interface for Elasticsearch that was developed primarily to view Logstash event data. Logstash is a tool that can be used to collect, process and forward events and log messages. Collection is accomplished via number of configurable input plugins including raw socket/packet communication, file tailing and several message bus clients. Once an input plugin has collected data it can be processed by any number of filters which modify and annotate the event data. Finally events are routed to output plugins which can forward the events to a variety of external programs including Elasticsearch, local files and several message bus implementations.  ![Kibana](https://raw.githubusercontent.com/zutherb/AppStash/master/external/images/kibana.png)  ## Contact  If you have any questions or remarks, please don't hesitate to contact me. For feature requests or general feedback, you can also use the [issue tracker](https://github.com/zutherb/AppStash/issues)  [Bernd Zuther](mailto:bernd.zuther@me.com)  ## Licensing  This work is open source, and is licensed under the Apache License, Version 2.0.
apache/incubator-blur	README file for Apache Blur ===========================  Apache Blur is a NoSQL data store built on top of Lucene, Hadoop, Thrift,  and Zookeeper.  Tables consist of a series of shards (Lucene indexes) that are distributed across a cluster of commodity servers.  Mail Lists =========  User list blur-user@incubator.apache.org Dev list blur-dev@incubator.apache.org  Website =======  http://incubator.apache.org/blur/  Documentation =============  http://incubator.apache.org/blur/docs/  The docs are also located in the docs/ directory in the binary  artifact, use the index.html as a starting point.
Netflix-Skunkworks/WSPerfLab	# WSPerfLab =========  Project for testing web-service implementations.  The intent of the test design is to simulate behavior typical of real-world web service applications.  This includes things such as:  - parallel network execution to backend services - multiple (5 in this case) backend services - conditional request flows (C and D require data from A, E requires data from B) - JSON deserialization and serialization - work such as math, iteration, string manipulation (not just proxying a stream of bytes)  # Structure  The test setup will consist of the 3 layers:  ```ws-backend-mock <- ws-impls <- ws-client```  The [ws-backend-mock](ws-backend-mock) is a simple Java backend app accepting request arguments to affect its response size and latency.  The various [test implementations](ws-impls) are intended to each implement the same logic with different architectures, languages and frameworks.  The [ws-client](ws-client) will drive the traffic and capture performance metrics.  Metrics to be captured are:  - client-side end-to-end latency for entire trip including network - response payload size  # Test Implementations  Information about test cases and implementation requirements can be found in the [ws-impls README](ws-impls)</a>.   # Statistics and Report
yoavst/quickapps	# Quick Circle Apps  [![Join the chat at https://gitter.im/yoavst/quickapps](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/yoavst/quickapps?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  **Quick Circle Apps** is an Android application made by Yoav Sternberg. The application provide modules for the Quick Circle Case.  [![Get it on Google Play](http://www.android.com/images/brand/get_it_on_play_logo_small.png)](https://play.google.com/store/apps/details?id=com.yoavst.quickapps)  ## How it works? The app uses LG Quick Circle SDK and QSlide SDK. The app was built using Kotlin, A Statically typed programming language targeting the JVM and JavaScript, which was developed by JetBrains.  * Torch - Enable/Disable camera flash. * Music -Register `NotificationListenerService` that implements `RemoteController.OnClientUpdateListener`. * Notifications - Register another `NotificationListenerService`. * Calendar - Reading events data from `CalendarContract.Events`. * Toggles - Each toggles use its permissions to change the state. * Stopwatch - Uses `TimerTask` that run every 10 milliseconds to update the clock. * Calculator - Evaluate the math string using EvalEx library. * News - Use Feedly Cloud Api to receive the newest 20 articles from the user feed. * Compass - Use Compass sensor. * Dialer - call `ACTION_CALL` intent. The on-call screen is LG's one. * Magic 8 ball - Magic... * Recorder - use `MediaRecorder` to recorded audio. * Barcode Scanner - using zxing library. * Dice - a poor paid man that roll dices for us.  License -------      Copyright 2014 Yoav Sternberg      Quick Circle Apps is free software: you can redistribute it and/or modify     it under the terms of the GNU General Public License as published by     the Free Software Foundation, either version 3 of the License, or     (at your option) any later version.      Quick Circle Apps is distributed in the hope that it will be useful,     but WITHOUT ANY WARRANTY; without even the implied warranty of     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     GNU General Public License for more details.      You should have received a copy of the GNU General Public License     along with Quick Circle Apps. If not, see <http://www.gnu.org/licenses/>.  ---
Skarafaz/mercury	# Mercury-SSH Commander  Simple Android app that sends pre-configured commands to remote servers via SSH.  <a href="https://play.google.com/store/apps/details?id=it.skarafaz.mercury&utm_source=global_co&utm_medium=prtnr&utm_content=Mar2515&utm_campaign=PartBadge&pcampaignid=MKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1"><img alt="Get it on Google Play" src="https://play.google.com/intl/en_us/badges/images/generic/en-play-badge.png" width=200/></a>  ## Usage  Take a look at the [wiki](https://github.com/Skarafaz/mercury/wiki) to learn how to configure and use Mercury-SSH.  ## Limitations  This app is intended to be used as a simple remote so interactive commands and output handling are not supported.  ## License  This software is released under GPLv2 License, please refer to the attached LICENSE file for details.
vert-x3/vertx-rx	# Rx extension for Vert.x  [![Build Status](https://vertx.ci.cloudbees.com/buildStatus/icon?job=vert.x3-rx)](https://vertx.ci.cloudbees.com/view/vert.x-3/job/vert.x3-rx/)  Vert.x module adding support for Reactive Extensions (Rx) using the Rx libraries. This allows Vert.x developers to use the Rx type-safe composable API to build Vert.x verticles. This module provides helpers for adapting Vert.x stream and future constructs to Rx observables.  ## RxJava  See https://github.com/vert-x3/vertx-rx/tree/master/rx-java/src/main/asciidoc/java/index.adoc  ## RxJS  See https://github.com/vert-x3/vertx-rx/tree/master/rx-js/src/main/asciidoc/js/index.adoc  ## RxGroovy  See https://github.com/vert-x3/vertx-rx/tree/master/rx-groovy/src/main/asciidoc/groovy/index.adoc
rlf/uSkyBlock	# uSkyBlock  This is a continually updated and custom version of Talabrek's Ultimate SkyBlock plugin.  We are on [Spigot](https://www.spigotmc.org/resources/uskyblock.2280/) and [Bukkit](http://dev.bukkit.org/bukkit-plugins/uskyblock/)  Currently [Open Issues](https://github.com/rlf/uSkyBlock/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aopen%20-label%3A%22T%20ready%20for%20test%22%20-label%3A%22T%20tested%20awaiting%20reporter%22)  # Installation  This version depends on the following plugins:  * Bukkit/Spigot 1.9-R0.1-SNAPSHOT * Vault 1.5.x * WorldEdit 6.0.0-SNAPSHOT * WorldGuard 6.0.0-SNAPSHOT  ## Releases  [![Build Status](https://api.travis-ci.org/rlf/uSkyBlock.svg)](https://travis-ci.org/rlf/uSkyBlock) [![Average time to resolve an issue](http://isitmaintained.com/badge/resolution/rlf/uSkyBlock.svg)](http://isitmaintained.com/project/rlf/uSkyBlock "Average time to resolve an issue") [![Percentage of issues still open](http://isitmaintained.com/badge/open/rlf/uSkyBlock.svg)](http://isitmaintained.com/project/rlf/uSkyBlock "Percentage of issues still open")  https://github.com/rlf/uSkyBlock/releases  Pre-releases will end in a date-stamp, and is considered **unsafe** for production servers.  Releases have a clean version number, has been tested, and should be safe for production servers.  ### Bukkit/Spigot 1.7.9/10 Releases  We provide pre-compiled versions (no support) [here](http://rlf.github.io/uSkyBlock):  * [2.4.9 for Bukkit 1.7.10](http://rlf.github.io/uSkyBlock/releases/bukkit-1.7.10/uSkyBlock-2.4.9.jar)  ## Config-files  *Note*: Config files might change quite a bit, and upon activation, the plugin will try to merge the existing ones with the new ones. A backup is kept under the `uSkyBlock/backup` folder.  Please make sure, that the config files are as you expect them to be, before using the plugin or releasing it to "the public".  ## Building/Compiling  See (https://github.com/rlf/uSkyBlock/wiki/Building)  # API uSkyBlock has an API (since v2.0.1-RC1.65).  To use it, simply drop the api-jar to the classpath of your own plugin, and write some code along these lines: ```java Plugin plugin = Bukkit.getPluginManager().getPlugin("uSkyBlock"); if (plugin instanceof uSkyBlockAPI && plugin.isEnabled()) {   uSkyBlockAPI usb = (uSkyBlockAPI) plugin;   player.sendMessage(String.format(     "\u00a79Your island score is \u00a74%5.2f!",      usb.getIslandLevel(player)   )); } ``` For further details regarding the API, visit the Wiki page: https://github.com/rlf/uSkyBlock/wiki/uSkyBlock-API  ## Contributing  Fork-away, and create pull-requests - we review and accept almost any changes.  But *please* conform with the (https://github.com/rlf/uSkyBlock/wiki/Coding-Guidelines)  ## License  TL;DR - This is licensed under GPLv3  ### Explanation / History Originally the uSkyBlock was a continuation of the skySMP plugin, which was licensed under GPLv3 (see http://dev.bukkit.org/bukkit-plugins/skysmp/).  Talabrek intended to share the code with the public, but simply didn't have the time available to do so.  Unfortunately, he had registered the plugin as `All rights reserved` on Bukkit, meaning the bukkit staff put the plugin under moderation - further increasing the work-load required to share the plugin.  Those trying to get hold on Talabrek, had a hard time, and eventually multiple developers got their hands on different versions of the uSkyBlock plugin, and tried to continue the work in various channels (wesley27 and wolfwork comes to mind).  On the very last day of 2014, we received the following e-mail from Talabrek:  > Recently, now that a stable 1.8 and the future of spigot is looking hopeful, I have gotten back to work on the plugin. There is much to be done though, and I just don't have the time to do it, so I finally decided to make it available for the public to work on. This is when I noticed the work you and others have done on the plugin. > > I don't have the time and energy to devote to actively developing this plugin anymore, but it is like a pet project to me so I would still like to have a role in it's development. You are making the best effort that I have seen, so I would like for you to continue. > > If you are interested, I can make my current code available to you (it's much different than what you currently have now, but some parts might be useful). > > -Talabrek  So, with Talabreks blessing, this repository will try to consolidate the many different "branches" out there.  ## References  * [GPLv3](http://www.gnu.org/copyleft/gpl.html) - [tl;dr Legal](https://www.tldrlegal.com/l/gpl-3.0) * Dutchy1001s guides and tutorials on uSkyBlock - [http://debocraft.x10.mx/skyblock/](http://debocraft.x10.mx/skyblock/)
apache/tomcat80	## Welcome to Apache Tomcat!  ### What Is It?  The Apache Tomcat® software is an open source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket technologies. The Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket specifications are developed under the [Java Community Process](http://jcp.org/en/introduction/overview).  The Apache Tomcat software is developed in an open and participatory environment and released under the [Apache License version 2](http://www.apache.org/licenses/). The Apache Tomcat project is intended to be a collaboration of the best-of-breed developers from around the world. We invite you to participate in this open development project. To learn more about getting involved, [click here](http://tomcat.apache.org/getinvolved.html) or keep reading.  Apache Tomcat software powers numerous large-scale, mission-critical web applications across a diverse range of industries and organizations. Some of these users and their stories are listed on the [PoweredBy wiki page](http://wiki.apache.org/tomcat/PoweredBy).  Apache Tomcat, Tomcat, Apache, the Apache feather, and the Apache Tomcat project logo are trademarks of the Apache Software Foundation.  ### The Latest Version  The current latest version in this branch (trunk) can be found on the [Tomcat 9.0](https://tomcat.apache.org/download-90.cgi) page.  ### Documentation  The documentation available as of the date of this release is included in the docs webapp which ships with tomcat. You can access that webapp by starting tomcat and visiting http://localhost:8080/docs/ in your browser. The most up-to-date documentation can be found at http://tomcat.apache.org/tomcat-9.0-doc/.  ### Installation  Please see [RUNNING.txt](RUNNING.txt) for more info.  ### Licensing  Please see [LICENSE](LICENSE) for more info.  ### Support and Mailing List Information  * Free community support is available through the [tomcat-users](http://tomcat.apache.org/lists.html#tomcat-users) email list and a dedicated [IRC channel](http://tomcat.apache.org/irc.html) (#tomcat on Freenode).  * If you want freely available support for running Apache Tomcat, please see the resources page [here](http://tomcat.apache.org/findhelp.html).  * If you want to be informed about new code releases, bug fixes, security fixes, general news and information about Apache Tomcat, please subscribe to the [tomcat-announce](http://tomcat.apache.org/lists.html#tomcat-announce) email list.  * If you have a concrete bug report for Apache Tomcat, please see the instructions for reporting a bug [here](http://tomcat.apache.org/bugreport.html).  ### Contributing  Please see [CONTRIBUTING](CONTRIBUTING.md) for more info.
yandex-qatools/postgresql-embedded	# Embedded PostgreSQL Server [![Maven Central](https://maven-badges.herokuapp.com/maven-central/ru.yandex.qatools.embed/postgresql-embedded/badge.svg?style=flat)](https://maven-badges.herokuapp.com/maven-central/ru.yandex.qatools.embed/postgresql-embedded) [![Build status](https://travis-ci.org/yandex-qatools/postgresql-embedded.svg?branch=master)](https://travis-ci.org/yandex-qatools/postgresql-embedded/) [![Windows build status](https://ci.appveyor.com/api/projects/status/00ov87k6fe2euwvo?svg=true)](https://ci.appveyor.com/project/smecsia/postgresql-embedded)  Embedded PostgreSQL server provides a platform neutral way for running postgres binaries in unittests. This library is based on [Flapdoodle OSS's embed process](https://github.com/flapdoodle-oss/de.flapdoodle.embed.process).   ## Motivation  * It's much easier than installing specific version manually * You can choose the version right from the code * You can start your development environment with the PostgreSQL embedded with the single command  ### Maven  Add the following dependency to your pom.xml: ```xml <dependency>     <groupId>ru.yandex.qatools.embed</groupId>     <artifactId>postgresql-embedded</artifactId>     <version>2.4</version> </dependency> ``` ### Gradle  Add a line to build.gradle: ```groovy compile 'ru.yandex.qatools.embed:postgresql-embedded:2.4' ```  ## Howto  Here is the example of how to launch and use the embedded PostgreSQL instance ```java // starting Postgres final EmbeddedPostgres postgres = new EmbeddedPostgres(V9_6); // predefined data directory // final EmbeddedPostgres postgres = new EmbeddedPostgres(V9_6, "/path/to/predefined/data/directory"); final String url = postgres.start("localhost", 5432, "dbName", "userName", "password");  // connecting to a running Postgres and feeding up the database final Connection conn = DriverManager.getConnection(url); conn.createStatement().execute("CREATE TABLE films (code char(5));"); conn.createStatement().execute("INSERT INTO films VALUES ('movie');");  // ... or you can execute SQL files... //postgres.getProcess().importFromFile(new File("someFile.sql")) // ... or even SQL files with PSQL variables in them... //postgres.getProcess().importFromFileWithArgs(new File("someFile.sql"), "-v", "tblName=someTable") // ... or even restore database from dump file //postgres.getProcess().restoreFromFile(new File("src/test/resources/test.binary_dump"))  // performing some assertions final Statement statement = conn.createStatement(); assertThat(statement.execute("SELECT * FROM films;"), is(true)); assertThat(statement.getResultSet().next(), is(true)); assertThat(statement.getResultSet().getString("code"), is("movie"));  // close db connection conn.close(); // stop Postgres postgres.stop(); ```  ### How to avoid archive extraction on every run  You can specify the cached artifact store to avoid archives downloading and extraction (in case if a directory remains on every run). ```java final EmbeddedPostgres postgres = new EmbeddedPostgres(); postgres.start(cachedRuntimeConfig("/path/to/my/extracted/postgres")); ```  ### How to configure logging  Just configure your own `slf4j` appenders. Here is the example of typical `src/test/resources/log4j.properties` file:  ```java # suppress inspection "UnusedProperty" for whole file log4j.rootLogger=DEBUG, stdout  # reduce logging for postgresql-embedded log4j.logger.ru.yandex.qatools.embed=INFO log4j.logger.de.flapdoodle.embed=INFO  # Direct log messages to stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target=System.out log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n log4j.throwableRenderer=org.apache.log4j.EnhancedThrowableRenderer ```  ### How to use your custom version of PostgreSQL  Pass the required `IVersion` interface implementation as a first argument of the `EmbeddedPostgres` object:  ```java final EmbeddedPostgres postgres = new EmbeddedPostgres(() -> (IS_OS_WINDOWS) ? "9.6.2-2" : "9.6.2-1"); ```  ### Known issues * A lot of issues have been reported for this library under Windows. Please try to use the suggested way of start up and use the cached artifact storage (to avoid extraction of the archive as extraction is extremely slow under Windows):  ```java postgres.start(cachedRuntimeConfig("C:\\Users\\vasya\\pgembedded-installation")); ```  * PostgreSQL server is known to not start under the privileged user (which means you cannot start it under root/Administrator of your system):    > `initdb must be run as the user that will own the server process, because the server needs to have access to the files and directories that initdb creates. Since the server cannot be run as root, you must not run initdb as root either. (It will in fact refuse to do so.)`    ([link](http://www.postgresql.org/docs/9.5/static/app-initdb.html)).         However some users have launched it successfully on Windows under Administrator, so you can try anyway.     ### Supported Versions  Versions: 9.6.2, 9.5.5, 9.4.10, any custom  Platforms: Linux, Windows and MacOSX supported
Netflix/spectator	# Spectator  Simple library for instrumenting code to record dimensional time series.  ## Requirements  * Java 8 or higher. * Java 7 or higher for spectator 0.27.x or earlier.  ## Documentation  * [Wiki](http://netflix.github.io/spectator/en/latest/) * [Javadoc](http://netflix.github.io/spectator/en/latest/javadoc/spectator-api/) * [![Build Status](https://travis-ci.org/Netflix/spectator.svg)](https://travis-ci.org/Netflix/spectator/builds)  ## Dependencies  To instrument your code you need to depend on the api library. This provides the minimal interfaces for you to code against and build test cases. The only dependency is slf4j.  ``` com.netflix.spectator:spectator-api:0.58.0 ```  If running at Netflix with the standard platform, see the [Netflix Integration](http://netflix.github.io/spectator/en/latest/intro/netflix/) page on the wiki.  ## Instrumenting Code  Suppose we have a server and we want to keep track of:  * Number of requests received with dimensions for breaking down by status code, country, and   the exception type if the request fails in an unexpected way. * Latency for handling requests. * Summary of the response sizes. * Current number of active connections on the server.  Here is some sample code that does that:  ```java // In the application initialization setup a registry Registry registry = new DefaultRegistry(); Server s = new Server(registry);  public class Server {   private final Registry registry;   private final Id requestCountId;   private final Timer requestLatency;   private final DistributionSummary responseSizes;    @Inject   public Server(Registry registry) {     this.registry = registry;      // Create a base id for the request count. The id will get refined with     // additional dimensions when we receive a request.     requestCountId = registry.createId("server.requestCount");      // Create a timer for tracking the latency. The reference can be held onto     // to avoid additional lookup cost in critical paths.     requestLatency = registry.timer("server.requestLatency");      // Create a distribution summary meter for tracking the response sizes.     responseSizes = registry.distributionSummary("server.responseSizes");      // Gauge type that can be sampled. In this case it will invoke the     // specified method via reflection to get the value. The registry will     // keep a weak reference to the object passed in so that registration will     // not prevent garbage collection of the server object.     registry.methodValue("server.numConnections", this, "getNumConnections");   }    public Response handle(Request req) {     final long s = System.nanoTime();     requestLatency.record(() -> {       try {         Response res = doSomething(req);          // Update the counter id with dimensions based on the request. The         // counter will then be looked up in the registry which should be         // fairly cheap, such as lookup of id object in a ConcurrentHashMap.         // However, it is more expensive than having a local variable set         // to the counter.         final Id cntId = requestCountId           .withTag("country", req.country())           .withTag("status", res.status());         registry.counter(cntId).increment();          responseSizes.record(res.body().size());          return res;       } catch (Exception e) {         final Id cntId = requestCountId           .withTag("country", req.country())           .withTag("status", "exception")           .withTag("error", e.getClass().getSimpleName());         registry.counter(cntId).increment();         throw e;       }     });   }    public int getNumConnections() {     // however we determine the current number of connections on the server   } } ```
OWASP/java-html-sanitizer	# OWASP Java HTML Sanitizer [<img src="https://travis-ci.org/OWASP/java-html-sanitizer.svg">](https://travis-ci.org/OWASP/java-html-sanitizer)  A fast and easy to configure HTML Sanitizer written in Java which lets you include HTML authored by third-parties in your web application while protecting against XSS.  The existing dependencies are on guava and JSR 305.  The other jars are only needed by the test suite.  The JSR 305 dependency is a compile-only dependency, only needed for annotations.   This code was written with security best practices in mind, has an extensive test suite, and has undergone [adversarial security review](docs/attack_review_ground_rules.md).  ----  [Getting Started](docs/getting_started.md) includes instructions on how to get started with or without Maven.  You can use [prepackaged policies](https://rawgit.com/OWASP/java-html-sanitizer/master/distrib/javadoc/org/owasp/html/Sanitizers.html):  ```Java PolicyFactory policy = Sanitizers.FORMATTING.and(Sanitizers.LINKS); String safeHTML = policy.sanitize(untrustedHTML); ```  or the [tests](https://github.com/OWASP/java-html-sanitizer/blob/master/src/test/java/org/owasp/html/HtmlPolicyBuilderTest.java) show how to configure your own [policy](https://rawgit.com/OWASP/java-html-sanitizer/master/distrib/javadoc/org/owasp/html/HtmlPolicyBuilder.html):  ```Java PolicyFactory policy = new HtmlPolicyBuilder()     .allowElements("a")     .allowUrlProtocols("https")     .allowAttributes("href").onElements("a")     .requireRelNofollowOnLinks()     .toFactory(); String safeHTML = policy.sanitize(untrustedHTML); ```  or you can write [custom policies](https://rawgit.com/OWASP/java-html-sanitizer/master/distrib/javadoc/org/owasp/html/ElementPolicy.html) to do things like changing `h1`s to `div`s with a certain class:  ```Java PolicyFactory policy = new HtmlPolicyBuilder()     .allowElements("p")     .allowElements(         new ElementPolicy() {           public String apply(String elementName, List<String> attrs) {             attrs.add("class");             attrs.add("header-" + elementName);             return "div";           }         }, "h1", "h2", "h3", "h4", "h5", "h6")     .toFactory(); String safeHTML = policy.sanitize(untrustedHTML); ```  ``` Please note that the elements "a", "font", "img", "input" and "span" need to be explicitly whitelisted  using the `allowWithoutAttributes()` method if you want them to be allowed through the filter when  these elements do not include any attributes. ``` ----  Subscribe to the [mailing list](http://groups.google.com/group/owasp-java-html-sanitizer-support) to be notified of known [Vulnerabilities](docs/vulnerabilities.md). If you wish to report a vulnerability, please see [AttackReviewGroundRules](docs/attack_review_ground_rules.md).  ----  [Thanks to everyone who has helped with criticism and code](docs/credits.md)
maxiee/HeartBeat	#心动 HeartBeat  ![icon](icon.png)  心动——记录生活中的心动瞬间。  HeartBeat - Record your emotional moments.  ##截图 ScreenShots  ![screenshot](screenshot.png)  ##下载 Download  <a href="https://play.google.com/store/apps/details?id=com.maxiee.heartbeat"><img src="http://www.android.com/images/brand/get_it_on_play_logo_large.png"/></a>  [酷市场下载地址 CoolMarket download](http://coolapk.com/apk/com.maxiee.heartbeat)  ##应用说明  心动是一款生活日记工具，您可以用它记录生活中的心动瞬间，添加感想。回忆这些瞬间，感悟生活，添加新的感想，面对真实的自我，更好地面对未来。  你可以把它当做是心灵的日记本，心灵的随笔录，甚至是心灵的照相机。  记录下生活中的每一个心动瞬间，添加您的感悟。  除此之外，您还可以使用标签对事件分类，从而更好地梳理生活。标签还具有纪念日功能，统计您对生活做出的努力。  您也可以为事件添加照片，留下美好回忆。  为了帮助您更好地提高生活质量，心动设立了心路功能，能够帮您从对所有事件进行统计，从而掌握自己的心态变化。  心动特色功能：  1. 事件列表：浏览生活中的大小事。  2. 今日：浏览当日事件，发表感想，记录新事件。  3. 事件详情：独特的时间轴效果，串起你的思绪。  4. 标签：给事件可添加多个标签，按标签分类浏览事件，方便你更好地组织心动事件。  5. 标签云：生活重心在哪里，在标签云中一览便知。  6. 纪念日：点击标签，即可浏览此标签下的事件，以及起始信息。  7. 心路：对事件以及感想进行统计，反映近来状态变化以及自身心态。  8. 搜索：搜索记录下的事件。  9. 手势密码：添加手势密码，防止他人看到您的隐私。  10. 备份功能：将数据备份至外部储存或云服务。并可以导入恢复。  心动为一款开源 APP， GitHub 项目主页：https://github.com/maxiee/HeartBeat  为了保护您的隐私，心动不使用网络访问权限，没有联网操作，您可以放心地记录自己的生活。  ##Introduction  HeartBeat is a life journal APP. You can record your emotional moment and add thoughts with it. Though recalling the moments, you can add new thoughts. By facing the true self, you may In the face of the future better.  You can view HeartBeat as a diary of your mind, a journal of your heart, and even an camera of your spirit.  Record every emotional moment in your life, add your thoughts.  Beyond that. you can add labels to events in order to classify them. There is also a memorial day in the label detail page.  You can add an image to every event, to leave a good memory.  To help you improve your quality of life, HeartBeat designs a function called Journey of the heart. It shows a statistics by analyzing your events and thoughts, to help you understand your mind-changing.  Features:  1. Event list: Look through your events.  2. Today: View your events today, add new event.  3. Event Detail: Contains a wonderful time axis, organizing your thoughts.  4. Label: Each event can add several labels. Through classifying events by labels, you can organize your life better.  5. Label cloud: Help you find life center.  6. Memorial day: Click a label, shows the memorial day information and the relative events under this label.  7. Journey of the heart: It shows a statistics by analyzing your events and thoughts, to help you understand your mind-changing.  8. Search: search your events.  9. Guesture password: Protect from others seeing your privacy by adding a guesture password.  10. Back up: Back up your data to external storage or cloud services and restore by import it.  HeartBeat is an open source APP, GitHub homepage:  https://github.com/maxiee/HeartBeat  In order to protect your privacy, HeartBeat doesn't use Internet access permission. There is no network connection. You can safely record your life.  ##License  ``` Copyright (c) 2015 Maxiee  Licensed under the Apache License, Version 2.0 (the "License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at        http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```  ##联系方式 Contact Me  新浪微博 Sina Weibo：[@Maxiee_Bayesian](http://weibo.com/maxiee)  邮箱 Email：maxieewong@gmail.com  ##关于我 About me  [我的简历 resume in Chinese](http://maxiee.github.io/static/html/resume.html)，欢迎与我联系！
hengyunabc/xdiamond	﻿  ## 简介  全局配置中心，存储应用的配置项，解决配置混乱分散的问题。名字来源于淘宝的开源项目[diamond](http://code.taobao.org/p/diamond/src/ "")，前面加上一个字母X以示区别。  [wiki](https://github.com/hengyunabc/xdiamond/wiki "")  [设计思路](https://github.com/hengyunabc/xdiamond/wiki/%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF "")  * 交流QQ群 450548123  ## 在线演示 地址：[xdiamond.coding.io](http://xdiamond.coding.io/ "")，登陆**选择standard**，用户名密码：admin/admin   ## 特性  * 所见即所得，在管理界面上看到的所有配置即项目运行时的配置 * 支持groupId，artifactId，version，profile四个维度以应对复杂环境 * 支持公共组件的配置继承，client jar包配置继承 * 配置修改实时通知客户端 * 和spring集成，使用简单 * **完善的权限系统** * 集成LDAP登陆，支持同步LDAP组/用户 * 支持http RESTful api获取配置 * 以secret key防止非法获取配置 * 丰富的metrics, connection统计  ## 使用技术  * netty * mybatis * spring mvc * shiro * ehcache * dropwizard metrics * crash shell * swagger * angularjs * bootstrap * bower   ## 工作原理  * 每个项目有groupId，artifactId，version，然后在不同的环境里对应不同的profile，比如：test, dev, product。  * 应用在启动时，通过网络连接到xdiamond配置中心，获取到最新的配置。如果没有获取到，从本地备份读取最后拉取的配置。  * 在Spring初始化时，把配置转为Properties，应用可以通过````${}````表达式或者````@Value````来获取配置。  * 如果配置有更新，可以通过Listener来通知应用。  每个项目都有一个base的profile，所有的profile都会继承base的配置。在base可以放一些公共的配置，比如某个服务的端口。  对于使用者，xdiamond提供的是一个Properties对象。用户可以结合Spring等来使用。  ## 界面截图 * 项目管理： ![xdiamond-project.png](img/xdiamond-project.png "") * Profile管理： ![xdiamond-profile.png](img/xdiamond-profile.png "") * Config管理： ![xdiamond-config.png](img/xdiamond-config.png "") * 项目依赖关系图： ![xdiamond-dependencygraphic.png](img/xdiamond-dependencygraphic.png "") * Metrics信息： ![xdiamond-metric.png](img/xdiamond-metric.png "") * Connection信息： ![xdiamond-connection.png](img/xdiamond-connection.png "")  ## 本地开发环境 * git clone 代码 * 运行xdiamond server：  ```bash git clone https://github.com/hengyunabc/xdiamond.git --depth=1 cd xdiamond/xdiamond-server mvn tomcat7:run -DskipTests ``` 然后访问 http://localhost:8080/xdiamond-server ，用admin/admin, standard登录  * 执行client例子代码：  ```bash cd xdiamond-client-example/ mvn exec:java -Dexec.mainClass="io.github.xdiamond.example.ClientExampleMain" ``` 默认是获取product环境的配置，如果想获取dev环境的配置，则可以执行： ```bash mvn exec:exec -Dexec.executable="java" -Dexec.args="-Dxdiamond.project.profile=dev -classpath %classpath io.github.xdiamond.example.ClientExampleMain" ``` * 演示结果  client启动时会打印获取到的配置，另外在web界面上connections里可以看到当前连接的客户端信息。  ## maven dependency  maven依赖已经发布到中央仓库：http://search.maven.org/#search%7Cga%7C1%7Cxdiamond  ```xml <dependency>     <groupId>io.github.hengyunabc.xdiamond</groupId>     <artifactId>xdiamond-client</artifactId>     <version>1.0.4</version> </dependency> ```
pkliang/gankmaku	Gankmaku =========    Project created for learning how to apply the **Android-CleanArchitecture** with [Reactive Programming](https://gist.github.com/staltz/868e7e9bc2a7b8c1f754) and Android good practices to develop a **MVP (Model View Presenter)** application using [gank.io/api](http://gank.io/api) as the domain of the application.   Motivation ---- After studied these two projects:  - [Android-CleanArchitecture](https://github.com/android10/Android-CleanArchitecture) - [Mosby](https://github.com/sockeqwe/mosby)  I decided to make my own implenentation using these two projects as reference.   Some Libraries used in this project ----  - [Dagger 2](https://github.com/google/dagger) - [Retrofit](http://square.github.io/retrofit) - [ButterKnife](https://github.com/JakeWharton/butterknife) - [Rxjava](https://github.com/ReactiveX/RxJava) - [Glide](https://github.com/bumptech/glide) - [Gradle Retrolambda Plugin](https://github.com/evant/gradle-retrolambda) - [RecyclerViewPager](https://github.com/lsjwzh/RecyclerViewPager)  Screenshot ---- ![screenshot](./art/gankmaku.gif "Screenshot")  [Download apk](./app/gankmaku.apk)  Developed by --- Liang Song - <pkliang@gmail.com>  License ---- ``` Copyright 2015 Liang Song  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```
JMaNGOS/JMaNGOS	= JMaNGOS -- General information =    Copyright (C) JMaNGOS (http://jmangos.org)      JMaNGOS is free software; you can redistribute it and/or modify    it under the terms of the GNU General Public License as published by    the Free Software Foundation; either version 2 of the License, or    (at your option) any later version.      This program is distributed in the hope that it will be useful,    but WITHOUT ANY WARRANTY; without even the implied warranty of    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the    GNU General Public License for more details.      You should have received a copy of the GNU General Public License    along with this program; if not, write to the Free Software    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA    JMaNGOS is a MMORPG Framework based mostly on Java. It is completely   open source, and is community supported.  JMaNGOS in not MaNGOS on Java. It's a new way.  If you wish to contribute ideas or code please visit   our site linked below or make pull requests to our github repo at   https://github.com/JMaNGOS/JMaNGOS    For further information on the JMaNGOS project, please visit our  project website at http://jmangos.org    To get nightly builds of the master branch of JMaNGOS, please visit  http://jenkins.jmangos.org    Documentation including installation instructions can be found inside  the Wiki website at https://github.com/JMaNGOS/JMaNGOS/wiki    SQL files to create the database can be found in the Sql directory. Files  to update your database from an older revision/version can be found in the  Install/sql/update directory.
fomkin/adt-maven-plugin	[![Build Status](https://secure.travis-ci.org/yelbota/adt-maven-plugin.png?branch=master)](http://travis-ci.org/yelbota/adt-maven-plugin)  Build Adobe AIR applications with your Maven! ================================================  General purpose ---------------  Some time ago, Adobe released AIR for mobile devices. There was a question: how to package AIR-application automatically? Flexmojos allows you to build only \*.air packages, so I have created the plugin which could work with platform dependent AIR SDK and additionally build packages for mobile devices. ***This plugin is not replacement of Flexmojos***. You still need Flexmojos to build SWF.  Current status (1.0.7) ----------------------  * Building AIR, APK, IPA packages * Native desktop packages (DMG on Mac OSX, EXE on Windows) * Adobe Native Extensions (ANE) support * Run custom adt command * Simple configuration * No need installing SDK. Plugin downloads it as dependency    Plans --------------------------------------------  * Install to device mojo * Linux SDK artifact working over wine (just for fun :)  Quick start -----------------------------------------------  First, make sure that your project has `swf` packaging.      <packaging>swf</packaging>  Add repository with plugin and SDK artifacts into your POM      <pluginRepositories>         <pluginRepository>             <id>yelbota-dropbox-repo</id>             <url>http://dl.dropbox.com/u/36020926/maven/</url>             <snapshots><enabled>false</enabled></snapshots>             <releases><enabled>true</enabled></releases>         </pluginRepository>     </pluginRepositories>      <repositories>         <repository>             <id>yelbota-dropbox-repo</id>             <url>http://dl.dropbox.com/u/36020926/maven/</url>             <snapshots><enabled>false</enabled></snapshots>             <releases><enabled>true</enabled></releases>         </repository>     </repositories>  Add `adt-maven-plugin` into plugins section      <plugin>              <groupId>com.yelbota.plugins</groupId>         <artifactId>adt-maven-plugin</artifactId>         <version>1.0.7</version>                  <executions>           <execution>             <goals>                 <goal>package</goal>             </goals>           </execution>         </executions>                  <configuration>                      <sdkVersion>3.5</sdkVersion>                          <target>ipa-debug</target>             <keystore>certificate.p12</keystore>             <storepass>******</storepass>             <tsa>none</tsa>              <!-- Required for ipa* targets -->             <provisioningProfile>myapp.mobileprovision</provisioningProfile>                          <!--                   Optional. Application descriptor. By default is                   src/main/resources/application-descriptor.xml             -->             <descriptor>src/main/flex/Project-app.xml</descriptor>                          <!--                   Optional. Replaces versionNumber in application descriptor. Useful                  for CI. 0.0.0 by default.              -->             <versionNumber>${build.number}</versionNumber>                          <!--                   Optional. Replaces versionLabel in application descriptor.                   ${project.version} by default.              -->             <versionLabel>${project.version}</versionLabel>                          <!--                   By default includes lookedup in target/classes directory. Usualy                  maven-resources-plugin copy here content of src/main/resources.                  You can change this behaviour by setting <includesRoot> property.              -->             <includes>                 <include>icons</include>             </includes>                          <!--                   Optional. Plugin home directory. For example "${user.home}/.adt" allows to keep SDK always unpacked for many projects.                  ${project.build.directory} by default.              -->             <pluginHome></pluginHome>              <!-- (iOS only, AIR 3.4 and higher) Enables the telemetry-based ActionScript sampler in iOS                   applications. Using this flag lets you profile the application with Adobe Scout. Although                  Scout can profile any Flash platform content, enabling detailed telemetry gives you                   deep insight into ActionScript function timing, DisplayList, Stage3D rendering and more.                  Note that using this flag will have a slight performance impact, so do not use it for                  production applications.             -->             <sampler>false</sampler>         </configuration>     </plugin>  You can configure signing with `build.adt.keystore`, `build.adt.storepass` and `build.adt.mobileprovision` properties.      mvn package -Dbuild.adt.keystore=certificate.p12 -Dbuild.adt.storepass=******  If you want to use your own SDK package, place it into plugin dependencies. Be aware, that AIR SDK is platform dependent.      <plugin>         <groupId>com.yelbota.plugins</groupId>         <artifactId>adt-maven-plugin</artifactId>         <version>1.0.7</version>         <dependencies>             <dependency>                 <groupId>com.adobe.air</groupId>                 <artifactId>air-sdk</artifactId>                 <version>3.5</version>                 <type>zip</type>                 <classifier>${os.family}</classifier>             </dependency>         </dependencies>         ...     </plugin>  AIR Native Extensions support -----------------------------------------------      ANE support designed in true maven style. Just deploy your extension to maven repository and add dependency. You don't need to include `<extensions>` section in application descriptor. It will be done automatically.      <dependency>         <groupId>com.adobe.extensions</groupId>         <artifactId>vibration</artifactId>         <version>1.0</version>         <type>ane</type>     </dependency>  Note that Flexmojos 4.x doesn't support ANE dependencies, so you need to use Flexmojos 6.x.  Run custom command -----------------------------------------------  You can run custom ADT command using `command` goal.       <plugin>         <groupId>com.yelbota.plugins</groupId>         <artifactId>adt-maven-plugin</artifactId>         <version>1.0.7</version>         <configuration>             <sdkVersion>3.5</sdkVersion>         </configuration>         <executions>             <execution>                 <goals>                     <goal>command</goal>                 </goals>                 <configuration>                     <arguments>-certificate -cn cert 1024-RSA ${project.build.directory}/cert.p12 111</arguments>                 </configuration>             </execution>         </executions>     </plugin>  Examples -----------------------------------------------  [https://github.com/yelbota/adt-maven-plugin/tree/master/src/it](https://github.com/yelbota/adt-maven-plugin/tree/master/src/it)  Foreign resources -----------------------------------------------  * [Apache Maven](http://maven.apache.org) * [Flexmojos](http://flexmojos.net) * [Building Adobe AIR Applications](http://help.adobe.com/en_US/air/build/air_buildingapps.pdf)
tacitknowledge/autopatch	AutoPatch =========  AutoPatch automates the application of changes to persistent storage.  AutoPatch was born from the needs of using an agile development process while working on systems that have persistent storage. Without AutoPatch, developers usually can't afford the maintenance headache of their own database, and DBAs are required just to apply changes to all of the various environments a serious development effort requires.  The very application of database changes becomes an inefficient, error-prone, expensive process, all conspiring to discourage any refactoring that touches the model, or being a bottleneck when model changes are made.  AutoPatch solves this problem, completely.  With AutoPatch, an agile development process that requires a database change looks like this:  * Developer alters the model, which requires a change to the database * Developer possibly consults a DBA, and develops a SQL patch against   their personal database that implements the alteration * Developer commits the patch to source control at the same time as they   commit their dependent code * Other developers' and environments' databases are automatically updated   by AutoPatch the next time the new source is run  This represents streamlined environment maintenance, allowing developers to cheaply have their own databases and all databases to stay in synch with massively lower costs and no environment skew.  Requirements ------------  * Java 6. That's it.   Where do I get AutoPatch? ------------------------- AutoPatch is open source and is hosted at [Github](http://github.com/tacitknowledge/autopatch).  The documentation for AutoPatch is on the [AutoPatch Wiki](https://github.com/tacitknowledge/autopatch/wiki)  You can include AutoPatch in your Maven project via:      <dependency>       <groupId>com.tacitknowledge</groupId>       <artifactId>autopatch</artifactId>       <version>1.4.2</version>     </dependency>  Help ====  If you have a question about AutoPatch feel free to post it up to our new [AutoPatch Google Group](http://groups.google.com/group/autopatch-users/).
apache/chukwa	#Apache Chukwa Project  <img src="http://chukwa.apache.org/images/chukwa_logo_small.jpg" align="right" width="300" />  Chukwa is an open source data collection system for monitoring large distributed systems.  Chukwa is built on top of the Hadoop Distributed File System (HDFS) and Map/Reduce  framework and inherits Hadoop’s scalability and robustness. Chukwa also includes a  ﬂexible and powerful toolkit for displaying, monitoring and analyzing results to  make the best use of the collected data.   ##Overview  Log processing was one of the original purposes of MapReduce. Unfortunately, using  Hadoop MapReduce to monitor Hadoop can be inefficient. Batch processing nature of  Hadoop MapReduce prevents the system to provide real time status of the cluster.  We started this journey at beginning of 2008, and a lot of Hadoop components have  been built to improve overall reliability of the system and improve realtimeness of  monitoring. We have adopted HBase to facilitate lower latency of random reads and  using in memory updates and write ahead logs to improve the reliability for root  cause analysis.  Logs are generated incrementally across many machines, but Hadoop MapReduce works  best on a small number of large files. Merging the reduced output of multiple runs  may require additional mapreduce jobs. This creates some overhead for data management  on Hadoop.  Chukwa is a Hadoop subproject devoted to bridging that gap between logs processing  and Hadoop ecosystem. Chukwa is a scalable distributed monitoring and analysis system,  particularly logs from Hadoop and other distributed systems.  The Chukwa Documentation provides the information you need to get started using  Chukwa. <a href="http://chukwa.apache.org/docs/r0.6.0/design.html">Architecture and  Design document</a> provides high level view of Chukwa design.  If you're trying to set up a Chukwa cluster from scratch,  <a href="http://chukwa.apache.org/docs/r0.6.0/user.html">User Guide</a> describes the  setup and deploy procedure.  If you want to configure the Chukwa agent process, to control what's collected, you  should read the <a href="http://chukwa.apache.org/docs/r0.6.0/agent.html">Agent Guide</a>.  There is also a <a href="http://chukwa.apache.org/docs/r0.6.0/pipeline.html">Pipeline Guide</a>  describing configuration parameters for ETL processes for the data pipeline.  And if you want to develop Chukwa to monitor other data sources,  <a href="http://chukwa.apache.org/docs/r0.6.0/programming.html"Programming Guide</a>  maybe handy to learn about Chukwa programming API.  If you have more questions, you can ask on the  <a href="http://chukwa.apache.org/mail-lists.html">Chukwa mailing lists</a>  ##Bulding Chukwa  To build Chukwa from source you require <a href="http://maven.apache.org">Apache Maven</a>: ``` mvn clean package ``` To check that things are ok, run  ``` mvn test ``` tests should take and run successfully after roughly fifteen minutes.  ##Running Cukwa  Users should definately begin with the  <a href="http://chukwa.apache.org/docs/r0.6.0/Quick_Start_Guide.html"> Chukwa Quick Start Guide</a>  If you're impatient, the following is the 30-second explanation:  The minimum you need to run Chukwa are agents on each machine you're  monitoring, and a collector to write the collected data to HDFS.  The basic command to start an agent is bin/chukwa agent.    If you want to start a bunch of agents, you can use the bin/start-agents.sh script. This just uses ssh to start agents on a list of machines, given in conf/agents. It's exactly parallel to Hadoop's start-hdfs and start-mapred scripts.    There are stop scripts that do the exact opposite of the start commands.
RestOpenGov/RestOpenGov	Atención: El proyecto se encuentra deprecado. Consultas al slack de #nardoz http://www.nardoz.com/  RestOpenGov ===========  El proyecto RestOpenGov surge inicialmente para proveer acceso programático a la información que el Gobierno de la Ciudad de Buenos Aires expone a través de http://data.buenosaires.gob.ar/.  Luego de una primera iteración el objetivo de RestOpenGov se ha vuelto más general, y se propone proveer una API pública de tipo REST, que permita acceder de una manera estándar a información que los gobiernos de diversos países y ciudades expongan a partir de fuentes heterogéneas de datos.  Por lo tanto, RestOpenGov estará compuesto por una serie de proyectos que interactuarán para lograr este fin.  ## Proyectos  #### [RestOpenGov Crawler](https://github.com/RestOpenGov/RestOpenGov/tree/master/crawler) Es el encargado de acceder periódicamente a los diversos endpoints publicados por los gobiernos, extraer la información que allí publican, procesarla, indexarla y almacenarla en un servidor elasticsearch para su posterior consulta.  #### [RestBA](https://github.com/RestOpenGov/RestOpenGov/tree/master/RestBA) API java que brinda acceso programático a la información expuesta por el Gobierno de la Ciudad de Buenos, accediendo a RestOpenGov. Permite a los desarrolladores acceder de manera simple y type-safe a la información expuesta.  ## Aplicaciones de ejemplo  #### [RestOpenGov.js](https://github.com/RestOpenGov/RestOpenGov/tree/master/RestOpenGov.js) Es un simple cliente de RestOpenGov escrito en Javascript. Permite explorar los datos y realizar búsquedas.  #### [openBafici](https://openbafici-rog.rhcloud.com/)  Una aplicación web mobile, desarrollada con restOpenGov, Play Framework 2.0 y Scala, desplegada en Openshift, para que puedas consultar toda la información del BAFICI desde tu celular. (fork me at [github](https://github.com/RestOpenGov/RestOpenGov/tree/master/openBafici))  #### [playDemo](https://playdemo-rog.rhcloud.com/)  Tutorial paso a paso que que muestra cómo utilizar el servicio de restOpenGov, creando una aplicación Play 2.0 desde cero y poniéndola en línea en Openshift. Consultá el [tutorial](https://github.com/RestOpenGov/RestOpenGov/blob/master/playdemo/README.md)  ## Primeros pasos Para comenzar a utilizar una instalación de RestOpenGov hemos preparado [este tutorial](https://github.com/RestOpenGov/RestOpenGov/wiki/Primeros-pasos).  ## Comunidad * [Wiki](https://github.com/RestOpenGov/RestOpenGov/wiki) * [Mailing List](http://groups.google.com/group/restopengov) * [Issue Tracking](https://github.com/RestOpenGov/RestOpenGov/issues) * [Seguinos en twitter](https://twitter.com/#!/RestOpenGov)  ## Autores * Nicolás Melendez ([@nfmelendez](http://twitter.com/nfmelendez)) * Alan Reid ([@alan_reid](http://twitter.com/alan_reid)) * Sebastián Scarano ([@develsas](http://twitter.com/develsas)) * Marcos Della Pittima ([@mdellapittima](http://twitter.com/mdellapittima)) * Pablo Paladino ([@palamago](http://twitter.com/palamago)) * Walter J. Franck ([@wfranck](http://twitter.com/wfranck))  ## Licencia Este software es distribuído bajo la licencia Apache 2.0: http://www.apache.org/licenses/LICENSE-2.0
stephenh/joist	Joist is a ORM based on code generation.  The goal is to provide Rails-like "empty domain objects" in an ORM that is simple, pleasant to use, and, if needed, scales nicely to really large schemas.  See [joist.ws](http://joist.ws) for more information.  Building ========  For Eclipse, the current `.classpath`/`.project` files assumed you've installed the [Gradle STS plugin](https://github.com/spring-projects/eclipse-integration-gradle/).  From the command line:      gradle install  Note that `install` is required to, like multi-project Maven setups, install the jars from upstream projects into your local Ivy repository for the downstream projects to fetch.  Note: The tests for the features project will fail until you create a local database schema by running the `FeaturesCycle.launch` target in Eclipse. (There is not currently a Buildr target to do this.)  Todo ====  * Composite columns (e.g. TimePoint with both time+zone), if needed * Don't muck with system properties * Repo interfaces   * Implement stub that copies values (iterates Alias, `toJdbcValue`, `ArrayList<Object>`)   * Only one commit/flush at a time, serialized transaction isolation, leverage op locks * Configuration option (global, per-collection) to disable collection ticking   * ...maybe remove/solve annoyance of cross-collection stomp? * Document PostgreSQL/MySQL no fsync settings for faster tests
jesperfj/force-rest-api	# Force.com REST API Connector  Lightweight library for building Force.com apps with OAuth authentication and data access through the Force.com REST API.  # Usage  Releases are published on Maven Central. Include in your project with:      <dependency>         <groupId>com.frejo</groupId>         <artifactId>force-rest-api</artifactId>         <version>0.0.38</version>     </dependency>  ## Build and link locally      $ git clone https://github.com/jesperfj/force-rest-api.git     $ cd force-rest-api     $ mvn install -DskipTests  The version number is never updated in SCM. So builds will always produce a module with version 0-SNAPSHOT. Add it as a dependency to your local builds with:      <dependency>         <groupId>com.frejo</groupId>         <artifactId>force-rest-api</artifactId>         <version>0-SNAPSHOT</version>     </dependency>  To check out the source code for a particular version found in Maven Central, use the corresponding git tag, e.g:       $ git clone https://github.com/jesperfj/force-rest-api.git      $ cd force-rest-api      $ git checkout force-rest-api-0.0.28  ## Authentication and Instantiation  ### API versions  Force.com API updates its API version with every Salesforce release (3 times per year). The new version is supposed to always be backwards compatible, so in theory it is safe to always use the latest API version. However `force-rest-api` is designed to be conservative. The API version used may change with new versions of the library, but for a given version of the library, the version will always be `ApiVersion.DEFAULT_VERSION` unless you explicitly set it to something different. You set the API version when you instantiate an `ApiConfig`:      ApiConfig mycfg = new ApiConfig().setApiVersionString("v99.0");  You can also use the `ApiVersion` enum to set the version:      ApiConfig mycfg = new ApiConfig().setApiVersion(ApiVersion.V38);  But the enum may not always have the version you need and there is no particular benefit to using it compared to using a simple String.  ### Username / Password Authentication  Authenticate using just login and password:      ForceApi api = new ForceApi(new ApiConfig()         .setUsername("user@domain.com")         .setPassword("password"));  ### OAuth Username/Password Authentication Flow  As [documented here](https://help.salesforce.com/help/doc/en/remoteaccess_oauth_username_password_flow.htm)      ForceApi api = new ForceApi(new ApiConfig()         .setUsername("user@domain.com")         .setPassword("password")         .setClientId("longclientidalphanumstring")         .setClientSecret("notsolongnumeric"));  ### OAuth Web Server Flow  As [documented here](https://help.salesforce.com/help/doc/en/remoteaccess_oauth_web_server_flow.htm)      String url = Auth.startOAuthWebServerFlow(new AuthorizationRequest()     	.apiConfig(new ApiConfig()     		.setClientId("longclientidalphanumstring")     		.setRedirectURI("https://myapp.mydomain.com/oauth"))     	.state("mystate"));      // redirect browser to url     // Browser will get redirected back to your app after user authentication at     // https://myapp.mydomain.com/oauth with a code parameter. Now do:  	ApiSession s = Auth.completeOAuthWebServerFlow(new AuthorizationResponse() 		.apiConfig(new ApiConfig() 			.setClientId("longclientidalphanumstring") 			.setClientSecret("notsolongnumeric") 			.setRedirectURI("https://myapp.mydomain.com/oauth")) 		.code("alphanumericstringpassedbackinbrowserrequest"));          ForceApi api = new ForceApi(s.getApiConfig(),s);   ### Instantiate with existing accessToken and endpoint  If you already have an access token and endpoint (e.g. from a cookie), you can pass an ApiSession instance to ForceApi:      ApiConfig c = new ApiConfig()         .setRefreshToken("refreshtoken")         .setClientId("longclientidalphanumstring")         .setClientSecret("notsolongnumeric"),          ApiSession s = new ApiSession() 	    .setApiConfig(c) 	    .setAccessToken("accessToken") 	    .setApiEndpoint("apiEndpoint");          ForceApi api = new ForceApi(c,s);  ## CRUD and Query Operations  ### Get an SObject      Account res = api.getSObject("Account", "001D000000INjVe").as(Account.class);  This assumes you have an Account class defined with proper Jackson deserialization annotations. For example:      import org.codehaus.jackson.annotate.JsonIgnoreProperties;     import org.codehaus.jackson.annotate.JsonProperty;      @JsonIgnoreProperties(ignoreUnknown=true)     public class Account {      	@JsonProperty(value="Id")     	String id;     	@JsonProperty(value="Name")     	String name;     	@JsonProperty(value="AnnualRevenue")     	private Double annualRevenue;     	@JsonProperty(value="externalId__c")     	String externalId;	 	     	public String getId() { return id; }     	public void setId(String id) { this.id = id; }     	public String getName() { return name; }     	public void setName(String name) { this.name = name; }     	public Double getAnnualRevenue() { return annualRevenue; }     	public void setAnnualRevenue(Double value) { annualRevenue = value; }     	public String getExternalId() { return externalId; }     	public void setExternalId(String externalId) { this.externalId = externalId; }     }  ### Create SObject      Account a = new Account();     a.setName("Test account");     String id = api.createSObject("account", a);  ### Update SObject      a.setName("Updated Test Account");     api.updateSObject("account", id, a);  ### Create or Update SObject      a = new Account();     a.setName("Perhaps existing account");     a.setAnnualRevenue(3141592.65);     api.createOrUpdateSObject("account", "externalId__c", "1234", a);  ### Delete an SObject      api.deleteSObject("account", id);  ### Query SObjects      QueryResult<Account> res = api.query("SELECT id FROM Account WHERE name LIKE 'Test account%'", Account.class);  ## Working with API versions  You can inspect supported API versions and get more detailed info for each version using `SupportedVersions`:      SupportedVersions versions = api.getSupportedVersions();     System.out.println(versions.oldest());          // prints v20.0     System.out.println(versions.contains("v25.0")); // prints true  The set of supported versions may vary based on where your organization is located. New versions are introduced 3 times a year and are rolled out gradually. During the rollout period, some organizations will have the latest version while others will not. The oldest supported version for REST API is v20.0. Salesforce API versions go further back than v20.0, but REST API does not support those older versions.  There is a direct mapping between season/year and version numbers. You can translate between season/year and version number in this way:      ExtendedApiVersion v = new ExtendedApiversion(ExtendedApiVersion.Season.SPRING, 2012);     System.out.println(v.getVersionString());       // prints v21.0  `ExtendedApiVersion` is called "Extended" because it goes beyond what `ApiVersion` offers and can represent more details about an API version, e.g. its season, year and URL base.  ## Run Tests  This project has a mix of unit tests and integration tests that hit the actual API. To make the integration tests work copy `src/test/resources/test.properties.sample` to `src/test/resources/test.properties` and replace the properties in the file with actual values.  ### Login and password  Add your Force.com developer org login and password. Needless to say, don't use credentials for a production org containing sensitive data. If you don't have a developer org, [sign up for one](http://www.developerforce.com/events/regular/registration.php?d=70130000000EjHb). It's free.  ### Client ID and Secret  Once you have signed up for an org, navigate to the Remote Access Setup:  * Click on "Admin User" drop-down in upper-right * Select Setup * In the left-side navigation pane, under "App Setup", click on "Develop" * Select "Remote Access"  Now create a new Remote Access Application:  * Click on "New" * Choose any name for your application * Choose any callback URL (you'll need to set this properly when web server flow is supported) * Choose some contact email * Click "Save" * Copy "Consumer Key" to the property "clientId" in test.properties * Click on "Click to reveal" and copy "Consumer Secret" to "clientSecret" in test.properties  ### Add `externalId__c` to Account SObject  Use the Force.com Web UI to add a custom field called `externalId__c` and mark it as an external ID field:  * (sorry, you have to figure out how to do this yourself. Will add instructions or automate it later)  ### Create a second user for IP restrictions test  To test IP restrictions failure handling you need additional test setup:  * Go to Manage Users --> Profiles and create a new profile based on "Standard Platform User". Call it "IP Restricted User" * Set Login IP Ranges for the new profile to something obscure like 1.1.1.1-1.1.1.1. Hit save and confirm that it's ok even though your user is not logged in from this range. * Create a new user and reset password * Log in as the new user and generate a security token * Set username and password (with token appended) in test.properties. * Log back in with the admin user and go to Manage Users --> Profiles  ### Run Tests  Now run tests with      $ mvn test  You will see some log messages that look like errors or warnings. That's expected and does not indicate test failures.  ### Interactive end-to-end OAuth handshake Test  This test is not run as part of the test suite because it requires manual intervention. Run it like this:      mvn -Dtest=com.force.api.EndToEndOAuthFlowExample test  # Cutting a Release  This project now uses [Alex Fontaine's](http://axelfontaine.com/blog/final-nail.html) release process because the release plugin is a pretty insane piece of software that should never exist. The pom.xml version number checked into SCM is always `0-SNAPSHOT`. Mapping releases back to source code now relies on git tags only.  The project is set up to release to Maven Central. If you have forked it and want to deploy your own version, you will need to update groupId and set up your own Sonatype credentials and GPG. Assuming this is all correctly set up. Here's how you cut a new release:  First ensure all your code is checked in (with `git status` or the like). Then run tests one extra time and also test javadoc generation since it's easy to introduce errors in javadoc comments that will break the deploy:      $ mvn test javadoc:javadoc  Now find the latest version number with `git tag` (or in Maven central depending on what you trust most). Bump the version number to that plus one:      $ mvn versions:set -DnewVersion=<new-version>  For example:      $ mvn versions:set -DnewVersion=0.0.50  This will update pom.xml locally to the new version and leaving it uncommitted (which is what you want). Now run      $ mvn scm:tag  This tags the local and remote repository with the full module name, e.g. force-rest-api-0.0.50. Now deploy:      $ mvn clean deploy -DperformRelease  When you're done, reset the local version change to pom.xml with:      $ mvn versions:revert  Just as a validation, try to push local changes including tags:      $ git push origin master --tags  There should be nothing to push. If something is messed up, delete the tags in Github and in your local repo and start over.  # Release History  ## 0.0.38  * Introduces ability to use a custom Jackson ObjectMapper. This can be used to support JodaTime for example. It also allows developers to choose how null values should be treated during serialization and deserialization. Before, null values were always ignored which is not always what you want. The custom ObjectMapper is set on [ApiConfig](src/main/java/com/force/api/ApiConfig.java). It will be used everywhere in [ForceApi](src/main/java/com/force/api/ForceApi.java) and [ResourceRepresentation](src/main/java/com/force/api/ResourceRepresentation.java), but not in the [Auth](src/main/java/com/force/api/Auth.java) class. * Allows any 2.x jackson-databind version 2.5 or newer. Tests have only been run with 2.5.0 and 2.9.1.  ## 0.0.37  * Remove specific response code checks from generic REST api calls. Different resources may return different response codes on the same verb, e.g. POST to chatter resources returns 201, but POST to `/process/approvals/` return 200. The library already checks the bounds of the response code and throws an exception if it is not between 200 and 299. The strict check on response codes is considered a bug introduced in 0.0.35 and fixed with this release.  ## 0.0.36  * Introduced [SessionRefreshListener](src/main/java/com/force/api/SessionRefreshListener.java) so you can register a listener and be notified when ForceApi refreshes the access token. See the [test](src/test/java/com/force/api/SessionRefreshTest.java) for sample code.  ## 0.0.35  * Introduced generic REST api calls `get`, `delete`, `post`, `put` and `patch` on [ForceApi][forceapi] for any arbitrary path. This allows force-rest-api to be used for the many non-sObject resources exposed in Force.com REST API. See [ChatterTest](src/test/java/com/force/api/chatter/ChatterTest.java) for an example. * Added `getSession()` convenience method on [ForceApi][forceapi] as requested by several people. It took me a little while to become comfortable with it. * Added `curlHelper()` convenience method on [ForceApi][forceapi] to easily print a curl command string with valid access token for debugging purposes.  ## 0.0.34  * Introduced `ForceApi.getSupportedVersions` and friends to enabled more advanced version handling. Thanks to @cswendrowski for the contributions. See "Working with API versions" in this README.  ## 0.0.33  * Update to Salesforce API v39  ## 0.0.32  * Add explicit authentication error handling. Addresses issue #32  ## 0.0.31  * Update to v37 * Add queryAll  ## 0.0.30  * Fix NullPointerException in ApiConfig.setForceURL. Thanks [steventamm](https://github.com/steventamm).  ## 0.0.29  * Update to Force.com API version 36  ## 0.0.28  * No feature changes * Project now configured to release to Maven Central * No longer uses maven-release-plugin * Version number in source code is always 0-SNAPSHOT * Use git tags to map from Maven Central version to corresponding source code  ## 0.0.23  * Upgrade to Jackson 2. Thanks to [emckissick](https://github.com/emckissick) for the pull request.  ## 0.0.22  * Include Javadoc in release jars  ## 0.0.21  * Made various fixes to get tests passing again after a long period of inactivity * end-to-end oauth test has been renamed to exlude it from test suite. Run it manually instead. It no longer uses HtmlUnit but instead requires manual intervention * ApiVersion is now up to date up to v33.0. * API version can now be set as a string. Setting it as an ApiVersion enum has been deprecated. There doesn't seem to be much value in strongly typing the api version.  ## 0.0.20  * [thysmichels](https://github.com/thysmichels) noticed that Spring 14 broke this library because [Identity.java](src/main/java/com/force/api/Identity.java) was set to strictly map to the underlying JSON resource. This class now uses `ignoreUnknown=true` so it should be more robust to changes.  ## 0.0.19  * [ryanbrainard](https://github.com/ryanbrainard) added QueryMore and various other enhancements to make it work better with [RichSObjects](https://github.com/ryanbrainard/richsobjects) * [Burn0ut07](https://github.com/Burn0ut07) added PickListEntry support to DescribeSObject  ## 0.0.18  * Some relationship queries work now. See QueryTest for an example. * Tested with Jackson 1.9.7  ## 0.0.17  * Modified deserialization of query results to better supper queries that return graphs of records.  ## 0.0.16  * Added more testing, including an end-to-end oauth flow test using HtmlUnit * Scope is now an enum  ## 0.0.15  * ApiSession now serializable, so it can be cached in Memcached and similar  ## 0.0.14  * Fixed bug in DescribeSObject. Had inlineHelpText as boolean instead of String  ## 0.0.13  * More complete DescribeSObject. Can now be used to generate Java classes. An example can be found in the tests based on [PojoCodeGenerator](https://github.com/forcedotcom/wsc/blob/master/src/main/java/com/sforce/rest/tools/PojoCodeGenerator.java)  ## 0.0.12  * 0.0.11 broke describeSObject. Fixed now and added test  ## 0.0.11  0.0.10 was botched. Missed a checkin  ## 0.0.10  * Basic exceptions * Some internal refactorings * First attempt at session renewal  ## 0.0.9  * Minimalistic Describe  ## 0.0.8  * Added revoke support ([read more](http://blogs.developerforce.com/developer-relations/2011/11/revoking-oauth-2-0-access-tokens-and-refresh-tokens.html)) * Refactored refreshToken out of ApiConfig  ## 0.0.7  * Added support for OAuth refresh_token flow * Added a bit more debug info to createSObject * Should work with Jackson 1.9.1 and 1.9.2. Both are accepted in the version range  ## 0.0.6  * Tested with Winter '12, API version 23 * Requires (and explicitly declares dependency on) Jackson 1.9.1. Not tested with other Jackson versions. * Basic CRUD and query functionality for SObjects * OAuth functionality that covers all Force.com options * Only happy path tested, almost no error scenarios or edge cases covered except for some sporadic debug output * Focused on typed access. But you must build SObject classes manually for now (or use builders available elsewhere)  # Project Goals:  * Make it as thin as possible   * Status: Both ForceApi and Auth classes are very thin wrappers on top of the APIs. * Few or no dependencies   * Status: Currently only depends on Jackson. Could consider supporting gson as well for added flexibility * Other projects will handle generation of typed SObject classes and it should work here * Automatic session renewal   * Status: Added in 0.0.10 and testable in 0.0.12. Waiting for feedback to see if it works. * Pluggable JSON kit   * Status: Not yet. This is currently low priority * Make sure it's Spring friendly. [This solution](http://stackoverflow.com/questions/2901166/how-to-make-spring-accept-fluent-non-void-setters) may be necessary.   * Status: No Spring work has been done yet * Consider adding newrelic hooks.  # License  [BSD 2-clause license](http://opensource.org/licenses/bsd-license.php)  # Author  Jesper Joergensen  [forceapi]: src/main/java/com/force/api/ForceApi.java
socialsignin/spring-social-security-demo	Spring Social Security Demo ===========================  Simple Hello World Webapp demonstrating the <a href="https://github.com/socialsignin/spring-social-security"> spring-social-security</a> module.  Resources in the application are protected using provider specific roles such has ROLE_USER_TWITTER or ROLE_USER_FACEBOOK, or simply by ROLE_USER.  Spring Security is configured with the SpringSocialSecurityAuthenticationFilter which ensures that users attempting to access a protected resource are prompted to connect with the relevant SaaS provider in order to authenticate.    Once authenticated, users confirm their chosen username, a account is created for them, and they can access the protected resource.  Once a user has an account, they can login to the system any time by simply reconnecting with any of the providers they have previously connected with the app previously.  Local user account creation is implemented using the default persistence of Spring-Social-Security where local account details are stored within the UsersConnectionRepository itself, users are effictively stored as connections to the "springSocialSecurity" provider.  This local account creation strategy can be overridden and the local accounts can be persisted using your own domain model by providing custom implementations of a couple of the components from Spring Social Security - see the forked demo at https://github.com/michaellavelle/spring-social-security-demo for an illustration of this.  Running the demo ----------------  /src/main/resources/environment.properties must be populated with Twitter consumer key and secret and Facebook clientId and secret for this application to run.   The return url of the Facebook client account must also be configured to be the connection url for this application - http://localhost:8080/ . As Twitter allows any return url by default, no such requirement is needed for the Twitter client account.  Twitter and Facebook are two arbitrary spring-social providers - alternative providers can be used instead - they must simply be registered in place of the Twitter/Facebook connection factory classes in SpringSocialSecurityDemoWebappConfig.  This webapp consists of a basic implementation of Spring Social framework, configured with an in-memory datasource for persistence of UserConnection data.   This in-memory datasource (configured in spring-config.xml) can be replaced with custom datasource as necessary.  The PostContruct method in SpringSocialSecurityDemoWebappConfig can be removed if the in-memory database is replaced.  To get started , clone the spring-social-security-demo project.  Once the Twitter and Facebook client details have been populated in the environment.properties file, and the Facebook client account has been set up with a return url of  http://localhost:8080 the application can be started using the in-built Jetty plugin:  mvn jetty:run  from the base directory of the spring-social-security-demo project.  Access http://localhost:8080/ in your web browser.  The application has two primary pages, the public home page ( http://localhost:8080/ ) and a protected resource ( http://localhost:8080/protected ).      Spring Security is configured in the spring-config.xml file to treat the protected url as a protected resource and delegates to spring-social-security for authentication via the springSocialSecurityAuthenticationFilter bean.  Users are then asked to login via spring-social, and once they have authenticated with Twitter they are redirected back to the application and locally logged in.  Application overview --------------------  The bulk of this application sets up the environment for Spring Social and Spring Security, with the spring-social-security bridge between these two frameworks being configured with a few lines of configuration:  ```    <!-- Start Import of Spring Social Security -->  	<!-- Scan classpath for components, including our Social Security Configuration  		class --> 	<context:component-scan 		base-package="org.socialsignin.springsocial.security" />     <!-- End Import of Spring Social Security --> ``` ```   <!-- configuration of spring security -->  <!-- Note the springSocialSecurityAuthenticationFilter is registered in place of the FORM_LOGIN_FILTER, and the entry point for protected resources is defined as the springSocialSecurityEntryPoint -->  	<security:http use-expressions="true" 		entry-point-ref="springSocialSecurityEntryPoint" xmlns="http://www.springframework.org/schema/security">  		<intercept-url pattern="/protected/**" access="hasRole('ROLE_USER')" /> 		<intercept-url pattern="/oauthconnect.jsp" access="hasRole('ROLE_USER')" /> 		  		<security:logout logout-url="/logout" />  		<anonymous /> 		<security:custom-filter position="FORM_LOGIN_FILTER" 			ref="springSocialSecurityAuthenticationFilter" />  	</security:http> 	 	<bean id="springSocialSecurityEntryPoint"   		class="org.springframework.security.web.authentication.LoginUrlAuthenticationEntryPoint">  		<property name="loginFormUrl" value="/oauthlogin.jsp"/> 	</bean> 	  <!-- end configuration of spring security -->  <!-- Configuration of spring social -->  <!-- Note the postSignInUrl is set to /authenticate, the signUp url is set to /signup and a provider specific connect interceptor is registered for each post-login connect provider -->  	<bean class="org.springframework.social.connect.web.ProviderSignInController"> 		<constructor-arg value="${application.secureUrl}" /> 		<property name="signUpUrl" value="/signup" /> 		<property name="applicationUrl" value="${application.secureUrl}" /> 		<property name="postSignInUrl" value="/authenticate" /> 		<!-- relies on by-type autowiring for the other constructor-args --> 	</bean>  	<bean class="org.springframework.social.connect.web.ConnectController"> 		<!-- relies on by-type autowiring for the constructor-args --> 		<property name="applicationUrl" value="${application.secureUrl}" /> 		<property name="interceptors"> 			<list>     	    	<ref bean="facebookConnectInterceptor" /> 			</list> 		</property> 	</bean>   <!-- End configuration of spring social -->  ```  The only additional code which is needed for this spring-social-security demo is the FacebookConnectInterceptor, needed because the Spring-Social framework requires API-specific connect interceptors to be registered before they can be called.  This interceptor is registered with the ConnectController as above.
I-TECH/OpenEMRConnect	# OpenEMRConnect Interconnect Electronic Medical Records systems to each other and to other systems  **This Repository is now defunct as of June 15, 2015. Each of the modules it contains have been moved to their own individual repositories under their respective names.**
metsci/glimpse	# Glimpse  Glimpse is a library for building dynamic, interactive Java applications for visualizing Big Data.  ### Getting Started  There's lots of information available at http://glimpse.metsci.com including:  * [Video](http://metsci.github.com/glimpse/videos.html) * [Web Start Demo](http://metsci.github.com/glimpse/webstart.html) * [Screenshots](http://metsci.github.com/glimpse/screenshots.html) * [Getting Started Guide](http://metsci.github.com/glimpse/guide.html) * [Discussion Forums](https://groups.google.com/forum/?fromgroups#!forum/metsci-glimpse) * [Blog](http://metsci.github.com/glimpse/blog.html) * [Javadoc](http://glimpse.metsci.com/apidocs/1.3.x/index.html)  ### License  Glimpse is licenced under the [BSD 3-Clause License](https://github.com/metsci/glimpse/blob/master/LICENSE).  ### Credits  Built by [Metron, Inc.](http://www.metsci.com). Want to work on projects like this? [We're hiring](http://www.metsci.com/Default.aspx?tabid=164)!
stormpath/stormpath-shiro	[![Build Status](https://api.travis-ci.org/stormpath/stormpath-shiro.png?branch=master)](https://travis-ci.org/stormpath/stormpath-shiro) [![Maven Central](https://img.shields.io/maven-central/v/com.stormpath.shiro/stormpath-shiro-core.svg)]() [![codecov](https://codecov.io/gh/stormpath/stormpath-shiro/branch/master/graph/badge.svg)](https://codecov.io/gh/stormpath/stormpath-shiro) [![license](https://img.shields.io/github/license/stormpath/stormpath-shiro.svg)]()  #Stormpath is Joining Okta We are incredibly excited to announce that [Stormpath is joining forces with Okta](https://stormpath.com/blog/stormpaths-new-path?utm_source=github&utm_medium=readme&utm-campaign=okta-announcement). Please visit [the Migration FAQs](https://stormpath.com/oktaplusstormpath?utm_source=github&utm_medium=readme&utm-campaign=okta-announcement) for a detailed look at what this means for Stormpath users.  We're available to answer all questions at [support@stormpath.com](mailto:support@stormpath.com).  # Apache Shiro plugin for Stormpath #  Copyright &copy; 2013-2016 Stormpath, Inc. and contributors. This project is open-source via the [Apache 2.0 License](http://www.apache.org/licenses/LICENSE-2.0).  The `stormpath-shiro` plugin allows an [Apache Shiro](http://shiro.apache.org)-enabled application use the [Stormpath](http://www.stormpath.com) User Management & Authentication service for all authentication and access control needs.  Pairing Shiro with Stormpath gives you a full application security system complete with immediate user account support, authentication, account registration and password reset workflows, password security and more - with little to no coding on your part.    Usage documentation [is in the wiki](https://github.com/stormpath/stormpath-shiro/wiki).  ### Build Instructions ###  This project requires Maven 3 to build.    #### Basic Build ####  Run the following from a command prompt:  `mvn install`  #### Build with Docs ####  Install sphinx: `pip install sphinx` or using virtualenv: ```bash pushd extensions/servlet/docs virtualenv docs source docs/bin/activate pip install -r requirements.txt popd ```  Then run: `mvn install -Pdocs -Psphinx-docs`  #### Run the TCK ####  Clone the https://github.com/stormpath/stormpath-framework-tck project and run `mvn clean install -P\!run-ITs` Then run `mvn install -Prun-TCK`   ## Change Log  ### 0.8.0 RC1 - Upgraded Shiro dependency to latest release candidate: 1.4.0-RC2 - Upgraded Stormpath SDK dependency to latest released version: 1.1.4 - Moved Shiro specific code to the Apache Shiro Repository - Moved default configuration to [stormpath-shiro.ini](https://github.com/stormpath/stormpath-shiro/blob/master/extensions/servlet/src/main/resources/com/stormpath/shiro/servlet/config/stormpath-shiro.ini) - Default configuration now disables Shiro's session tracking (using stateless JWTs in place) - Added JAX-RS example - Added Dropwizard + AngularJS example  ### 0.7.2 - Fixed validation problem in web-fragment.xml - Upgraded Stormpath SDK dependency to latest released version: 1.1.1  ### 0.7.1 - Corrected Spring test dependencies scope - Spring modules now correctly set the `EventBus` instance on the `SecurityManager`   ### 0.7.0 - Upgraded Shiro dependency to latest stable release of 1.3.2 - Upgraded Stormpath SDK dependency to latest released version: 1.0.4 - Added `stormpath-shiro-servlet-plugin` to automatically configure Shiro to use a Stormpath realm and use Stormpath login UI out of the box - Added spring-boot starters for both web and non-web applications, see `stormpath-shiro-spring-boot-starter` and `stormpath-shiro-spring-boot-web-starter` - Added examples for core, servlet, spring-boot, and spring-boot-web extensions.  ### 0.6.0  - Upgraded Shiro dependency to latest stable release of 1.2.3 - Upgraded Stormpath SDK dependency to latest released version: 1.0.RC2 - [Issue 6](https://github.com/stormpath/stormpath-shiro/issues/6): Fixed bug that prevented Authentication data to be removed from cache after a successful logout.  ### 0.5.0  - Upgraded Stormpath SDK dependency to latest stable release of 0.9.1 - Added Permission support!  It is now possible to assign Shiro permissions to Stormpath Accounts or Groups by leveraging Stormpath's newly released [CustomData](http://docs.stormpath.com/rest/product-guide/#custom-data) feature.  You can add and remove permission to an Account or Group by modifying that account or group's CustomData resource.  For example:  ```java Account account = getAccount(); //lookup account  //edit the permisssions assigned to the Account: new CustomDataPermissionsEditor(account.getCustomData())     .append("user:1234:edit")     .append("document:*")     .remove("printer:*:print");      //persist the account's permission changes: account.save(); ```  The same `CustomDataPermissionsEditor` can be used to assign permissions to Groups as well, and assumes 'transitive association': any permissions assigned to a Group are also 'inherited' to the Accounts in the Group.  In other words, an account's total assigned permissions are any permissions assigned directly to the account, plus, all of the permissions assigned to any Group that contains the account.  The `CustomDataPermissionsEditor` will save the permissions as a JSON list in the CustomData resource, under the default `apacheShiroPermissions` field name, for example:  ```json {     ... any other of your own custom data properties ...,      "apacheShiroPermissions": [         "perm1",         "perm2",         ...,         "permN"     ] } ``` If you would like to change the default field name, you can call the `setFieldName` method:  ```java new CustomDataPermissionsEditor(account.getCustomData())     .setFieldName("whateverYouWantHere")     .append("user:1234:edit")     .append("document:*")     .remove("printer:*:print"); ```  But you'll also need to update your `ApplicationRealm`'s configuration to reflect the new name so it can function - the realm reads the same `CustomData` field, so they must be identical to ensure both read and write scenarios access the same field.  For example, if using `shiro.ini`:      stormpathRealm.groupPermissionResolver.customDataFieldName = whateverYouWantHere     stormpathRealm.accountPermissionResolver.customDataFieldName = whateverYouWantHere  - The `ApplicationRealm` implementation now has a default `groupPermissionResolver` and `accountPermissionResolver` properties that leverage respective group or account `CustomData` to support permissions as described above.  Prior to this 0.5.0 release, there were no default implementations of these properties - you had to implement the interfaces yourself to support permissions.  Now Permissions are built in by default (although you could still provide your own custom implementations if you have custom needs of course).  ### 0.4.0  - Upgraded Stormpath SDK dependency to latest stable release of 0.8.1 - Added CacheManager/Cache bridging support.  This allows the Stormpath SDK to use the same caching mechanism that you're already using for Shiro, simplifying cache configuration/setup.  For example:  ```ini [main]  cacheManager = my.shiro.CacheManagerImplementation securityManager.cacheManager = $cacheManager  # Stormpath integration: stormpathClient = com.stormpath.shiro.client.ClientFactory # etc... stormpathClient.cacheManager = $cacheManager ```  If for some reason you *don't* want the Stormpath SDK to use Shiro's caching mechanism, you can configure the `stormpathCacheManager` property (instead of the expected Shiro-specific `cacheManager` property), which accepts a `com.stormpath.sdk.cache.CacheManager` instance instead:  ```ini # ... stormpathCacheManager = my.com.stormpath.sdk.cache.CacheManagerImplementation # etc... stormpathClient.stormpathCacheManager = $stormpathCacheManager ``` But note this approach requires you to set-up/configure two separate caching mechanisms.  See ClientFactory `setCacheManager` and `setStormpathCacheManager` JavaDoc for more.  ### 0.3.1  0.3.1 is a minor dependency fix: the Stormpath Java SDK dependency has been upgraded to reflect its latest 0.8.0 release.  This is the only change - no additional features/changes have been made otherwise.
rolandkrueger/vaadin-by-example	Welcome to _vaadin-by-example_ ==============================  __Learn Vaadin by working example projects.__ - - - - - - - - - - - - - - - - - - - - - - - - - - -   The goal of this project is to provide learners of the  [Vaadin](http://www.vaadin.com/ "Vaadin") toolkit with working example projects for various problems which one might encounter while coding with Vaadin. The idea behind this is to complement articles and tutorials on Vaadin topics with code examples that will work out of the box.  Oftentimes, tutorials only provide code snippets to illustrate how the things they describe will work. These snippets only focus on the described problem. Beginners often have difficulties transferring these excerpts into a working piece of code.  This project aims at fixing that. It will provide working examples for tutorials that can be found either in the example project itself or at some given location in the web. To make understanding the concepts easier, the examples contain as much code as necessary and as litte code as possible.  Licensing --------- Since the main intention of this project is to offer people example code that actually works, this code should also be usable as a template for copy & pasting parts of the examples into own projects. Therefore, the licensing for the examples' source code should be as unrestrictive as possible. People shall be able to use parts of the code as they see fit without having to bother with the requisites of more or less restrictive software licenses. The examples should therefore be licensed under unrestrictive licenses, such as the MIT license or similar.  Contribute! -----------  Contributions to this project are welcome! This is not planned as a one-man-show, so go ahead and fork this project. There are some things to take into consideration, though, when contributing example projects.  * __Licensing__ You should use a license that is as unrestrictive as possible. See the section above about licensing for an explanation of the reason for that. Each example should contain the license text in a text file named 'LICENSE'.  * __Example Size__ Example projects should be kept as concise as possible so as to not distract learners too much from the core intention of the example code.  * __Working Out Of The Box__ Examples should be runnable without requiring a complex setup. Ideally, they should be accompanied with portable build mechanisms, such as a Maven pom.xml making the example quickly accessible with a simple _mvn jetty:run_.  * __No JAR Dependencies__ Examples should not contain the necessary dependencies as binary JAR files. These should be obtainable by users through other channels. Again, using Apache Maven or Ivy builds will facilitate that.  * __Accompanying Tutorials__ An example project should not stand all by itself. Every example should be accompanied by one or more tutorials that illustrate the background of the code. These tutorials could be contained directly in the example. Another option would be to provide a hyperlink to the location of the tutorial on the web in some read-me file. Besides that link, such a read-me file should contain an abstract of the respective tutorial.  * __Project Naming__ Names for example projects should be chosen such that the example's intention becomes clear already from the name. That said, refrain from generic project names such as 'HelloWorld' or 'VaadinExample'.  Disclaimer ---------- The name Vaadin and related trademarks/trade names are the property of Vaadin Ltd. and may not be used other than stated in [Vaadin's Terms of Service](https://vaadin.com/terms-of-service). Copyright to the Vaadin Framework is owned by Vaadin Ltd.
airlift/airship	airship
seratch/junithelper	# JUnit Helper  ## What's this?  "JUnit Helper" helps JUnit users to cover all tests for public, protected and package local methods by saving much labor to start writing new test cases.   Currently, command line script, Maven plugin and Eclipse IDE plugin are provided. Eclipse plugin also has an advantage of making it easier to go back and forth between developing class and test class by "Alt + 8 "(test->dev) and "Alt + 9"(dev->test).  ## JUnit testing without dull routine  We developers have to do two non-essential dull routine in JUnit testing.  First, it takes no small cost to prepare a new test case. A general procedure to prepare a JUnit test case might be like this - (1) Creating package and test class (2) Defining test methods (3) Finding out how to instantiate (4) Preparing required args, (5) Starting writing logic at last.  JUnit Helper automates (1) - (4) of above. Developers can focus on writing logic.  And second, it is a hard job to ascertain whether the test case aimed to add already exists in a huge test class that might not be maintained for a long time.   JUnit Helper deals with the problem by using a naming convention of test methods that is based on method signature. For example, "public void doSomething(String arg)" -> "@Test public void doSomething_A$String() throws Exception". The naming rule is customizable to meet some needs.  ## Overview  If you want to test the following method:  ```java public SearchResult search(SearchCondition condition) {   return null; } ```  Carry out whichever operation:  * Command Line Script (for Windows, Mac OS X, UNIX)  ```sh ./junithelper(.bat) make src/main/java ```  * Maven  ```sh mvn junithelper:make ```  * Eclipse plugin  Select Java file and "Alt + 9"  JUnit Helper outputs the following test method:  ```java @Test public void search_A$SearchCondition() throws Exception {   // TODO auto-generated by JUnit Helper.   Sample target = new Sample();   SearchCondition condition = null;   SearchResult actual = target.search(condition);   SearchResult expected = null;   assertThat(actual, is(equalTo(expected)); } ```  JUnit 3.x style is also available:  ```java public void test_search_A$SearchCondition() throws Exception {   // TODO auto-generated by JUnit Helper.   ... ```  It will be provided without compile errors in most cases. For example, if the args contain primitives or arrays,  ```java public SearchResult search(String[] array, List<String> list, long primitive, Version version) {   return null; } ```  like this:  ```java @Test public void search_A$StringArray$List$long$Version() throws Exception {   // TODO auto-generated by JUnit Helper.   Sample target = new Sample();   String[] array = new String[] {};   List<String> list = new ArrayList<String>();   long primitive = 0L;   Version version = null;   SearchResult actual = target.search(array, list, primitive, version);   SearchResult expected = null;   assertThat(actual, is(equalTo(expected)); } ```  "search_A$SearchCondition" or "search_A$StringArray$List$long$Version" is the part to specify which method it tests. Delimiters and some other rules are customizable.  If you want to have several cases for same method, it's a standard way to discriminate them by method name suffix.  ```java @Test public void search_A$SearchCondition_null() throws Exception {   // TODO auto-generated by JUnit Helper.   ... }  @Test public void search_A$SearchCondition_empty() throws Exception {   // TODO auto-generated by JUnit Helper.   ... }  @Test public void search_A$SearchCondition_paging() throws Exception {   // TODO auto-generated by JUnit Helper.   ... } ```  If you add a new method,   ```java public List<String> addedMethod(InputStream is) {   return null; } ```  JUnit Helper also adds a test method by make command or "Alt + 9" on Eclipse:  ```java @Test public void addedMethod_A$InputStream() throws Exception {   // TODO auto-generated by JUnit Helper.   ... ```  ## With mock object frameworks  If you want, JUnit Helper is also able to generate template test code with mock object frameworks:  ### Mockito  ```java import static org.hamcrest.CoreMatchers.*; import static org.junit.Assert.*; import static org.mockito.BDDMockito.*;  @Test public void search_A$SearchCondition() throws Exception {   // TODO auto-generated by JUnit Helper.   Sample target = new Sample();   // given   SearchCondition condition = mock(SearchCondition.class);   // e.g. : given(mocked.called()).willReturn(1);   // when   SearchResult actual = target.search(condition);   // then   // e.g. : verify(mocked).called();   SearchResult expected = null;   assertThat(actual, is(equalTo(expected)); } ```  ### JMock2  ```java import static org.hamcrest.CoreMatchers.*; import static org.junit.Assert.*; import org.jmock.Mockery; import org.jmock.Expectations; import org.jmock.lib.legacy.ClassImposteriser;  @Test public void search_A$SearchCondition() throws Exception {   // TODO auto-generated by JUnit Helper.   Mockery context = new Mockery(){{     setImposteriser(ClassImposteriser.INSTANCE);   }};   Sample target = new Sample();   final SearchCondition condition = context.mock(SearchCondition.class);   context.checking(new Expectations(){{     // e.g. : allowing(mocked).called(); will(returnValue(1));   }});   SearchResult actual = target.search(condition);   SearchResult expected = null;   assertThat(actual, is(equalTo(expected)); } ```  ### EasyMock  ```java import static org.hamcrest.CoreMatchers.*; import static org.junit.Assert.*; import org.easymock.classextension.EasyMock; import org.easymock.classextension.IMocksControl;  @Test public void search_A$SearchCondition() throws Exception {   // TODO auto-generated by JUnit Helper.   IMocksControl mocks = EasyMock.createControl();   Sample target = new Sample();   SearchCondition condition = mocks.createMock(SearchCondition.class);   // e.g. : EasyMock.expect(mocked.called()).andReturn(1);   mocks.replay();   SearchResult actual = target.search(condition);   mocks.verify();   SearchResult expected = null;   assertThat(actual, is(equalTo(expected)); } ```  ### JMockit  ```java import static org.hamcrest.CoreMatchers.*; import static org.junit.Assert.*; import mockit.Mocked; import mockit.Expectations;  @Mocked  SearchCondition search_A$SearchCondition_condition;  @Test public void search_A$SearchCondition() throws Exception {   // TODO auto-generated by JUnit Helper.   Sample target = new Sample();   SearchCondition condition = this.search_A$SearchCondition_condition;   new Expectations(){{     // e.g. : mocked.get(anyString); returns(200);   }};   SearchResult actual = target.search(condition);   SearchResult expected = null;   assertThat(actual, is(equalTo(expected)); } ```  # Eclipse Plugin  ## How to install  * Update site   http://junithelper.org/eclipse/plugins/site.xml  * Eclipse marketplace   http://marketplace.eclipse.org/content/junit-helper  ## How to use  It is very simple, only two shortcut-commands.The following commands are available on Java Editor, Package Explorer and Navigator.  ### Commands  * Alt + 8   Open target class from test when selecting test class, open test target class.  * Alt + 9  ![new_test_case](http://junithelper.org/img/new_test_case.png)   Open test class or create test class if it does not exist when selecting test target class.  If some methods that have no tests existed, the minimal test methods will be generated automatically.  ![junit4](http://junithelper.org/img/junit4.png)   JUnit 3.x style is also available:  ![junit3](http://junithelper.org/img/junit3.png)  * Alt + 3   Change current test case to JUnit 3.x style.  * Alt + 4   Change current test case to JUnit 4.x style.  ### Preference  * Window > Preferences > JUnit Helper  ![preference](http://junithelper.org/img/preferences_1.10.png)   If you need a project-specific configuration, put "junithelper-config.properties" in the project root directory. It will be put above global configuration in Eclipse.  ``` # language:en/ja language:en outputFileEncoding:UTF-8 directoryPathOfProductSourceCode:src/main/java directoryPathOfTestSourceCode:src/test/java # lineBreakPolicy:forceCRLF/forceLF/forceNewFileCRLF/forceNewFileLF lineBreakPolicy=forceNewFileCRLF useSoftTabs=false softTabSize=4 # junitVersion:version3/version4 junitVersion:version4 testCaseClassNameToExtend:junit.framework.TestCase isTemplateImplementationRequired:true target.isAccessorExcluded:true target.isExceptionPatternRequired:true target.isPackageLocalMethodRequired:true target.isProtectedMethodRequired:true target.isPublicMethodRequired:true # target.regexpCsvForExclusion:com/.example/..+.autogenerated/..+,com/.example/.NoNeedToTest target.regexpCsvForExclusion: testMethodName.isArgsRequired:true testMethodName.isReturnRequired:false testMethodName.basicDelimiter:_ testMethodName.argsAreaPrefix:A testMethodName.argsAreaDelimiter:$ testMethodName.returnAreaPrefix:R testMethodName.returnAreaDelimiter:$ testMethodName.exceptionAreaPrefix:T testMethodName.exceptionAreaDelimiter:$ # mockObjectFramework:Mockito/JMock2/JMockit/EasyMock mockObjectFramework: # testingPatternExplicitComment:ArrangeActAssert/GivenWhenThen testingPatternExplicitComment: isExtensionEnabled:true extensionConfigXML:junithelper-extension.xml ```  # Command Line Script  ## How to install  * Zip archive (junithelper-core-X.X.X.zip)   https://github.com/seratch/junithelper/downloads  ## How to use  ```sh $ ./tools/junithelper.bat   _    /   _  ._/_/_/_  /_  _  _ (_//_// // / / //_'//_//_'/                    / JUnit Helper version 1.12   Usage:   junithelper [command] [arg1] [arg2]  Commands:   junithelper make [baseDir/targetJavaFile]   junithelper force3 [baseDir/targetJavaFile]   junithelper force4 [baseDir/targetJavaFile]  JVM Options:   -Djunithelper.configProperties=[filepath]  $ ```  * make command  This command creates test cases or methods for the classes under specified package recursively.  ```sh $ ./tools/junithelper.bat make ./src/main/java/org/junithelper/core/util/   _    /   _  ._/_/_/_  /_  _  _ (_//_// // / / //_'//_//_'/                    / JUnit Helper version 1.12     Target: C:/workspace/junithelper-core/./src/main/java/org/junithelper/core/util/IOUtil.java   Target: C:/workspace/junithelper-core/./src/main/java/org/junithelper/core/util/ObjectUtil.java   Target: C:/workspace/junithelper-core/./src/main/java/org/junithelper/core/util/PrimitiveTypeUtil.java   Target: C:/workspace/junithelper-core/./src/main/java/org/junithelper/core/util/Stdout.java   Target: C:/workspace/junithelper-core/./src/main/java/org/junithelper/core/util/ThreadUtil.java  Are you sure?(y/n) y   Modified: C:/workspace/junithelper-core/./src/test/java/org/junithelper/core/util/IOUtilTest.java   Modified: C:/workspace/junithelper-core/./src/test/java/org/junithelper/core/util/ObjectUtilTest.java   Modified: C:/workspace/junithelper-core/./src/test/java/org/junithelper/core/util/PrimitiveTypeUtilTest.java   Created: C:/workspace/junithelper-core/./src/test/java/org/junithelper/core/util/StdoutTest.java   Modified: C:/workspace/junithelper-core/./src/test/java/org/junithelper/core/util/ThreadUtilTest.java  $ ```  * force3 command  This command forces test cases JUnit 3.x style for the classes under specified package recursively.  * force4 command  This command forces test cases JUnit 4.x style for the classes under specified package recursively.  ## Configuration  You can change the following Configurations by editing "junithelper-config.properties". Maven plugin's Configuration also uses same property names.  ``` # language:en/ja language:en outputFileEncoding:UTF-8 directoryPathOfProductSourceCode:src/main/java directoryPathOfTestSourceCode:src/test/java # lineBreakPolicy:forceCRLF/forceLF/forceNewFileCRLF/forceNewFileLF lineBreakPolicy=forceNewFileCRLF useSoftTabs=false softTabSize=4 # junitVersion:version3/version4 junitVersion:version4 testCaseClassNameToExtend:junit.framework.TestCase isTemplateImplementationRequired:true target.isAccessorExcluded:true target.isExceptionPatternRequired:true target.isPackageLocalMethodRequired:true target.isProtectedMethodRequired:true target.isPublicMethodRequired:true # target.regexpCsvForExclusion:com/.example/..+.autogenerated/..+,com/.example/.NoNeedToTest target.regexpCsvForExclusion: testMethodName.isArgsRequired:true testMethodName.isReturnRequired:false testMethodName.basicDelimiter:_ testMethodName.argsAreaPrefix:A testMethodName.argsAreaDelimiter:$ testMethodName.returnAreaPrefix:R testMethodName.returnAreaDelimiter:$ testMethodName.exceptionAreaPrefix:T testMethodName.exceptionAreaDelimiter:$ # mockObjectFramework:Mockito/JMock2/JMockit/EasyMock mockObjectFramework: # testingPatternExplicitComment:ArrangeActAssert/GivenWhenThen testingPatternExplicitComment: isExtensionEnabled:true extensionConfigXML:junithelper-extension.xml ```  # Maven Plugin  ## How to install   * pom.xml  ```xml <build>     <plugins>         ...         <plugin>             <groupId>org.junithelper</groupId>             <artifactId>maven-junithelper-plugin</artifactId>         </plugin>         ...     </plugins> </build> ```  Following is an example of configuration:  ```xml <build>     <plugins>         ...         <plugin>             <groupId>org.junithelper</groupId>             <artifactId>maven-junithelper-plugin</artifactId>             <configuration>                 <!-- language:en/ja -->                 <language>en</language>                 <outputFileEncoding>UTF-8</outputFileEncoding>                 <!-- lineBreakPolicy:forceCRLF/forceLF/forceNewFileCRLF/forceNewFileLF -->                 <lineBreakPolicy>forceNewFileCRLF</lineBreakPolicy>                 <useSoftTabs>false</useSoftTabs>                 <softTabSize>4</softTabSize>                 <directoryPathOfProductSourceCode>src/main/java</directoryPathOfProductSourceCode>                 <directoryPathOfTestSourceCode>src/test/java</directoryPathOfTestSourceCode>                 <!-- junitVersion:version3/version4 -->                 <junitVersion>version4</junitVersion>                 <testCaseClassNameToExtend>junit.framework.TestCase</testCaseClassNameToExtend>                 <isTemplateImplementationRequired>true</isTemplateImplementationRequired>                 <target_isAccessorExcluded>true</target_isAccessorExcluded>                 <target_isExceptionPatternRequired>true</target_isExceptionPatternRequired>                 <target_isPackageLocalMethodRequired>true</target_isPackageLocalMethodRequired>                 <target_isProtectedMethodRequired>true</target_isProtectedMethodRequired>                 <target_isPublicMethodRequired>true</target_isPublicMethodRequired>                 <target_regexpCsvForExclusion></target_regexpCsvForExclusion>                 <testMethodName_isArgsRequired>true</testMethodName_isArgsRequired>                 <testMethodName_isReturnRequired>false</testMethodName_isReturnRequired>                 <testMethodName_basicDelimiter>_</testMethodName_basicDelimiter>                 <testMethodName_argsAreaPrefix>A</testMethodName_argsAreaPrefix>                 <testMethodName_argsAreaDelimiter>$</testMethodName_argsAreaDelimiter>                 <testMethodName_returnAreaPrefix>R</testMethodName_returnAreaPrefix>                 <testMethodName_returnAreaDelimiter>$</testMethodName_returnAreaDelimiter>                 <testMethodName_exceptionAreaPrefix>T</testMethodName_exceptionAreaPrefix>                 <testMethodName_exceptionAreaDelimiter>$</testMethodName_exceptionAreaDelimiter>                 <!-- mockObjectFramework:Mockito/JMock2/JMockit/EasyMock -->                 <mockObjectFramework></mockObjectFramework>                 <!-- testingPatternExplicitComment:ArrangeActAssert/GivenWhenThen -->                 <testingPatternExplicitComment></testingPatternExplicitComment>                 <isExtensionEnabled>true</isExtensionEnabled>                 <extensionConfigXML>junithelper-extension.xml</extensionConfigXML>             </configuration>         </plugin>         ...     </plugins> </build> ```  ## How to use  ### Goals  * `junithelper:make`   This goal is used to execute adding or updating tests. It is possible to specify targets by adding "target" option, for example "mvn junithelper:make -Dtarget=src/main/java/snippet/" or "mvn junithelper:make -Dtarget=src/main/java/snippet/Sample.java".  * `junithepler:force3`   This goal is used to execute converting tests to JUnit 3.x style. "-Dtarget=***" is also availale.  * `junithelper:force4`   This goal is used to execute converting tests to JUnit 4.x style.  "-Dtarget=***" is also availale.  ```sh $ mvn junithelper:make [INFO] Scanning for projects... [INFO] ------------------------------------------------------------------------ [INFO] Building maven-junithelper-plugin Maven Mojo [INFO]    task-segment: [junithelper:make] [INFO] ------------------------------------------------------------------------  [INFO] [junithelper:make {execution: default-cli}]   _    /   _  ._/_/_/_  /_  _  _ (_//_// // / / //_'//_//_'/                    / JUnit Helper version 1.12     Target: /Users/seratch/IdeaProjects/sample/src/main/java/sample/SampleBean.java  Are you sure?(y/n)  ```  # JUnit Helper Extension  ## What's this?   You can customize JUnit Helper's template generation.  ## How to use   * Eclipse Plugin  Put junithelper-extension.xml in the project root directory. If you also put junithelper-config.properties, it is required to change "isExtensionEnabled" to true.  * Command Line Script  Change "isExtensionEnabled" to true in junithelper-config.properties and edit junithelper-extension.xml.  * Maven plugin  Add configuration in pom.xml and put junithelper-extension.xml in the project root directory.  ```xml <plugin>     <groupId>org.junithelper</groupId>     <artifactId>maven-junithelper-plugin</artifactId>     <configuration>         <isExtensionEnabled>true</isExtensionEnabled>         <extensionConfigXML>junithelper-extension.xml</extensionConfigXML>     </configuration> </plugin> ```  ## junithelper-extension.xml   * instantiation  Customize the default way to instantiate.  * arg  Add customized parameter-range testing patterns to each type of argument.  * return  Add customized assertions to each type of return value.  ```xml <?xml version="1.0" encoding="UTF-8"?> <junithelper-extension>    <!--      =============================      * Instantiation *     =============================     [NOTE] Mock objects or extension's Arg Patterns are given preference.     [NOTE] {instance} will be replaced to arg variable      ****** <XML> ******       <instantiation class="com.example.Worker">         <import>com.example.WorkerFactory</import>         <assign>WorkerFactory.getNewWorker()</assign>       </instantiation>       <instantiation class="java.util.Calendar">         <assign>Calendar.getInstance()</assign>         <post-assign>{instance}.add(Calendar.DATE, -1);</post-assign>       </instantiation>      ****** <Target> ******       public void putCalendar(Calendar cal) {}      ****** <Test> ******       @Test        public void putCalendar_A$() throws Exception {         // TODO auto-generated by JUnit Helper.         Worker target = WorkerFactory.getNewWorker();         Calendar cal = Calendar.getIntance();         cal.add(Calendar.DATE, -1);         target.putCalendar(cal);       }   -->   <instantiation class="java.util.Calendar">     <assign>Calendar.getInstance()</assign>     <post-assign>{instance}.add(Calendar.DATE, -1)</post-assign>   </instantiation>   <instantiation class="java.io.InputStream">     <import>java.io.ByteArrayInputStream</import>     <assign>new ByteArrayInputStream(new byte[] {})</assign>   </instantiation>    <!--      =============================      * Arg Patterns *     =============================     [NOTE] {arg} will be replaced to arg variable      ****** <XML> ******       <arg class="int" >         <pattern name="minus1"><assign>-1</assign></pattern>         <pattern name="random">           <import>java.util.Random</import>           <pre-assign>System.out.println("Before");</pre-assign>           <assign>new Random().nextInt(10)</assign>           <post-assign>System.out.println("After");</post-assign>         </pattern>       </arg>      ****** <Target> ******       public void increment(int i) {}      ****** <Test> ******       @Test        public void increment_A$int_intIsMinus1() throws Exception {         // TODO auto-generated by JUnit Helper.         Sample target = new Sample();         int i = -1;         target.increment(i);       }       @Test        public void increment_A$int_intIsRandom() throws Exception {         // TODO auto-generated by JUnit Helper.         Sample target = new Sample();         System.out.println("Before");         int i = new Random().nextInt(10);         System.out.println("After");         target.increment(i);       }   -->   <!-- Primitive types -->   <arg class="int" >     <pattern name="minus1"><assign>-1</assign></pattern>     <pattern name="0"><assign>0</assign></pattern>     <pattern name="1"><assign>1</assign></pattern>     <pattern name="2"><assign>2</assign></pattern>     <pattern name="random">       <import>java.util.Random</import>       <assign>new Random().nextInt(10)</assign>     </pattern>   </arg>   <arg class="long" >     <pattern name="minus1L"><assign>-1L</assign></pattern>     <pattern name="0L"><assign>0L</assign></pattern>     <pattern name="1L"><assign>1L</assign></pattern>     <pattern name="2L"><assign>2L</assign></pattern>   </arg>   <arg class="double" >     <pattern name="minus1_0D"><assign>-1.0D</assign></pattern>     <pattern name="0_0D"><assign>0.0D</assign></pattern>     <pattern name="0_5D"><assign>0.5D</assign></pattern>     <pattern name="1_0D"><assign>1.0D</assign></pattern>   </arg>   <arg class="boolean" >     <pattern name="true"><assign>true</assign></pattern>     <pattern name="false"><assign>false</assign></pattern>   </arg>      <!-- Primitive wrapper types -->   <arg class="java.lang.Integer" >     <pattern name="null"><assign>null</assign></pattern>     <pattern name="minus1"><assign>-1</assign></pattern>     <pattern name="0"><assign>0</assign></pattern>     <pattern name="1"><assign>1</assign></pattern>     <pattern name="2"><assign>2</assign></pattern>   </arg>   <arg class="java.lang.Long" >     <pattern name="null"><assign>null</assign></pattern>     <pattern name="minus1L"><assign>-1L</assign></pattern>     <pattern name="0L"><assign>0L</assign></pattern>     <pattern name="1L"><assign>1L</assign></pattern>     <pattern name="2L"><assign>2L</assign></pattern>   </arg>   <arg class="java.lang.Double" >     <pattern name="null"><assign>null</assign></pattern>     <pattern name="minus1_0D"><assign>-1.0D</assign></pattern>     <pattern name="0_0D"><assign>0.0D</assign></pattern>     <pattern name="0_5D"><assign>0.5D</assign></pattern>     <pattern name="1_0D"><assign>1.0D</assign></pattern>   </arg>   <arg class="java.lang.Boolean" >     <pattern name="null"><assign>null</assign></pattern>     <pattern name="true"><assign>true</assign></pattern>     <pattern name="false"><assign>false</assign></pattern>   </arg>   <arg class="java.lang.String" >     <pattern name="null"><assign>null</assign></pattern>     <pattern name="empty"><assign>""</assign></pattern>     <pattern name="2"><assign>"2"</assign></pattern>   </arg>    <!-- Date time -->   <arg class="java.util.Date">     <pattern name="null"><assign>null</assign></pattern>     <pattern name="now"><assign>new Date()</assign></pattern>   </arg>   <arg class="java.util.Calendar" >     <pattern name="null"><assign>null</assign></pattern>     <pattern name="now"><assign>Calendar.getInstance();</assign></pattern>     <pattern name="Date02_29">       <assign>Calendar.getInstance();</assign>       <post-assign>         {arg}.set(Calendar.YEAR, 2000);         {arg}.set(Calendar.MONTH, 2 - 1);         {arg}.set(Calendar.DATE, 29);         {arg}.set(Calendar.HOUR_OF_DAY, 0);         {arg}.set(Calendar.MINUTE, 0);         {arg}.set(Calendar.SECOND, 0);         {arg}.set(Calendar.MILLISECOND, 0);       </post-assign>     </pattern>     <pattern name="Date12_31">       <assign>Calendar.getInstance();</assign>       <post-assign>         {arg}.set(Calendar.YEAR, 1999);         {arg}.set(Calendar.MONTH, 12 - 1);         {arg}.set(Calendar.DATE, 31);         {arg}.set(Calendar.HOUR_OF_DAY, 0);         {arg}.set(Calendar.MINUTE, 0);         {arg}.set(Calendar.SECOND, 0);         {arg}.set(Calendar.MILLISECOND, 0);       </post-assign>     </pattern>     <pattern name="Date01_01">       <assign>Calendar.getInstance();</assign>       <post-assign>         {arg}.set(Calendar.YEAR, 2000);         {arg}.set(Calendar.MONTH, 1 - 1);         {arg}.set(Calendar.DATE, 1);         {arg}.set(Calendar.HOUR_OF_DAY, 0);         {arg}.set(Calendar.MINUTE, 0);         {arg}.set(Calendar.SECOND, 0);         {arg}.set(Calendar.MILLISECOND, 0);       </post-assign>     </pattern>     <pattern name="Time23_59_59">       <assign>Calendar.getInstance();</assign>       <post-assign>{arg}.set(Calendar.YEAR, 1995);         {arg}.set(Calendar.MONTH, 5 - 1);         {arg}.set(Calendar.DATE, 24);         {arg}.set(Calendar.HOUR_OF_DAY, 23);         {arg}.set(Calendar.MINUTE, 59);         {arg}.set(Calendar.SECOND, 59);         {arg}.set(Calendar.MILLISECOND, 0);       </post-assign>     </pattern>     <pattern name="Time00_00_00">       <assign>Calendar.getInstance();</assign>       <post-assign>         {arg}.set(Calendar.YEAR, 1995);         {arg}.set(Calendar.MONTH, 5 - 1);         {arg}.set(Calendar.DATE, 25);         {arg}.set(Calendar.HOUR_OF_DAY, 0);         {arg}.set(Calendar.MINUTE, 0);         {arg}.set(Calendar.SECOND, 0);         {arg}.set(Calendar.MILLISECOND, 0);       </post-assign>     </pattern>   </arg>   <arg class="org.joda.time.DateTime">     <import>org.joda.time.format.DateTimeFormat</import>     <pattern name="null"><assign>null</assign></pattern>     <pattern name="JavaBirthday">       <assign>         DateTimeFormat.forPattern("yyyyMMddHHmmss").parseDateTime("19950525000000")       </assign>     </pattern>   </arg>    <!--      =============================      * Assertions *     =============================      ****** <XML> ******       <return class="int">         <import>static org.hamcrest.Matchers.*</import>         <import>static org.junit.Assert.*</import>         <assert>assertThat(actual, is(greaterThanOrEqualTo(0)));</assert>         <assert>assertThat(actual, is(lessThanOrEqualTo(Integer.MAX_VALUE)));</assert>       </return>      ****** <Target> ******       public int increment() {         this.i += 1;         return this.i;       }      ****** <Test> ******       @Test        public void increment_A$() throws Exception {         // TODO auto-generated by JUnit Helper.         Sample target = new Sample();         int actual = target.increment();         assertThat(actual, is(greaterThanOrEqualTo(0)));         assertThat(actual, is(lessThanOrEqualTo(Integer.MAX_VALUE)));       }    -->   <!-- JUnit 3.x -->   <!--    <return class="int">     <assert>assertTrue(actual >= 0);</assert>   </return>    -->   <!-- JUnit 4.x -->   <return class="int">     <import>static org.hamcrest.Matchers.*</import>     <import>static org.junit.Assert.*</import>     <assert>assertThat(actual, is(greaterThanOrEqualTo(0)));</assert>     <assert>assertThat(actual, is(lessThanOrEqualTo(Integer.MAX_VALUE)));</assert>   </return>   <return class="java.lang.Integer">     <import>static org.hamcrest.Matchers.*</import>     <import>static org.junit.Assert.*</import>     <assert>assertThat(actual, is(greaterThanOrEqualTo(0)));</assert>     <assert>assertThat(actual, is(lessThanOrEqualTo(Integer.MAX_VALUE)));</assert>   </return>  </junithelper-extension> ```  ## JUnit Helper User Group  https://groups.google.com/group/junit-helper-group
elaatifi/orika	We've recently moved to a Github Organization repo https://github.com/orika-mapper/orika =========================================================================================   Orika ! [![Build Status](https://secure.travis-ci.org/elaatifi/orika.png)](http://travis-ci.org/elaatifi/orika) ------------------------------------------------------------------------------------------   Orika is a Java Bean mapping framework that recursively copies (among other capabilities) data from one object to another. It can be very useful when developing multi-layered applications.
PeterKnego/LeanEngine-Server	LeanEngine Server =================  This is the server part of LeanEngine project To get started visit http://www.lean-engine.com  What is LeanEngine ------------------  LeanEngine is an open source project aiming make the development of cloud-enabled mobile applications really simple, fast and fun. It lets you focus on developing your mobile app while it handles all the boring and complicated stuff. Stuff like:  * logins (Facebook, Google, Twitter, OpenID) * syncing data (store and retrieve data to and from cloud) * security * push over multiple platforms (coming soon)  All this you can achieve with using LeanEngine libraries and a few lines of code on your clients. LeanEngine server runs on Google App Engine.   Stay updated ------------  Twitter: http://twitter.com/#!/leanengine Google Plus: https://plus.google.com/u/0/115510332327304386091   License -------  Example server application is licensed under BSD-style license.  Server library is licensed under LGPL v3.  For more info see License.txt in root directory.
cukespace/cukespace	Cukes in Space! ===============  Cukes in Space! allows you to deploy and run Cucumber features in the application server of your choice using the Arquillian test framework.  ## Supported application servers:  The following application servers are supported. The artifact ```cukespace-core``` is required for all servers, and additional dependencies have been listed for each.  # Quickstart  This quickstart assumes you're already very familiar with [Arquillian][] and [Cucumber-JVM][], and that you've set up your Maven.  [Arquillian]: http://www.arquillian.org/ [Cucumber-JVM]: http://www.github.com/cucumber/cucumber-jvm [JBoss repository]: https://community.jboss.org/wiki/MavenGettingStarted-Users  ## Installation  Before you start writing features, you'll want to pull down the source for Cukes in Space! and install it to your local repository using the following command:  ```mvn install```  ## Project Setup  You'll want at least the following dependency in your pom.xml:  ```xml <dependency>     <groupId>com.github.cukespace</groupId>     <artifactId>cukespace-core</artifactId>     <version>{VERSION}</version>     <scope>test</scope> </dependency> ```  ## Creating Features  All you have to do is to replace Arquillian runner by ```CukeSpace```, create the test deployment, and write your steps in your test class:  ```java package my.features;  import cucumber.runtime.arquillian.junit.Cucumber; import my.features.domain.Belly; import my.features.glue.BellySteps;  @RunWith(CukeSpace.class) public class CukesInBellyTest {     @Deployment     public static Archive<?> createDeployment() {         return ShrinkWrap.create(WebArchive.class)             .addAsWebInfResource(EmptyAsset.INSTANCE, "beans.xml")             .addAsResource("my/features/cukes.feature")             .addClass(Belly.class)             .addClass(BellySteps.class)             .addClass(CukesInBellyFeature.class);     }          @EJB     private CukeService service;      @Inject     private CukeLocator cukeLocator;      @When("^I persist my cuke$")     public void persistCuke() {         this.service.persist(this.cukeLocator.findCuke());     } } ```  Arquillian will then package up all the necessary dependencies along with your test deployment and execute the feature in the application server. Your step definitions will also be serviced by Arquillian's awesome test enrichers, so your steps will have access to any resource supported by the Arquillian container you choose to use:  ```java // clip @EJB private CukeService service;  @Resource private Connection connection;  @PersistenceContext private EntityManager entityManager;  @Inject private CukeLocator cukeLocator;  @When("^I persist my cuke$") public void persistCuke() {     this.entityManager.persist(this.cukeLocator.findCuke()); } // clip ```   ### Functional UI Testing with Arquillian Drone  [This guide][] will help you get started with using the Arquillian Drone extension for functional testing.  [This guide]: http://arquillian.org/guides/functional_testing_using_drone/  To create features for functional UI testing, you first want to add all necessary Drone dependencies to your project's POM, then mark your deployment as untestable and inject a webdriver:  ```java // clip @Drone DefaultSelenium browser;  @Deployment(testable = false) public static Archive<?> createDeployment() {     return ShrinkWrap.create(WebArchive.class)         .addAsWebInfResource(EmptyAsset.INSTANCE, "beans.xml")         .addAsWebInfResource(new StringAsset("<faces-config version=\"2.0\"/>"), "faces-config.xml")         .addAsWebResource(new File("src/main/webapp/belly.xhtml"), "belly.xhtml")         .addClass(Belly.class)         .addClass(BellyController.class); } // clip ```  You can then access your Drone from any step definition.  ```java public class IrresistibleButtonSteps {     @Drone     private DefaultSelenium browser;          @When("^I click on an irresistible button$")     public void click() {         this.browser.click("id=irresistible-button");     } } ```  Be sure to remember to inject the webdriver into your test fixture, or you won't be able to inject it into any of your step definitions. You'll know when you've forgotten because you'll get the following error:  ``` java.lang.IllegalArgumentException: Drone Test context should not be null ```  ### Externalize some common features/steps  ### Configuration #### Arquillian.xml  Cuke in Space! comes with an arquillian extension. We already saw the reporting configuration but you can go a bit further. Here are the complete properties:      <extension qualifier="cucumber">         <property name="report">true</property>         <property name="reportDirectory">target/cucumber-report</property>         <property name="options">--tags @foo</property>         <property name="featureHome">/home/test/features</property>         <property name="tempDir">target/custom/features</property>     </extension>  | Property name    | Value                                                                                     | | ---------------- |:-----------------------------------------------------------------------------------------:| | report           | boolean to activate the reporting                                                         | | reportDirectory | where to store the report on the filesystem                                               | | options          | cucumber options used when `cucumber.api.junit.Cucumber.Options` is not on the test class | | featureHome      | where to look for features (base path)                                                    | | tempDir          | where custom loaders dump their resources                                                 | | objectFactory | if default (plain new instance) implementation doesn't work for you add the object factory jar you use in the deployment and configure the class there (tip: "cdi" is a value value)|  Important: objectFactory support is mainly for the getInstance() lookup, it is not recommanded to reuse cucumber-jvm implementation which would start/stop a container. This role is handled by arquillian already in CukeSpace and shouldn't be duplicated - can fail if you start the same container twice.  #### Reporting sample configuration  Cukespace supports some basic reporting in html format.  To activate it simply configure the cucumber arquillian extension in the file arquillian.xml :        <extension qualifier="cucumber">         <property name="report">true</property>         <property name="reportDirectory">target/cucumber-report</property>       </extension>  The report file will be then logged. For instance:       INFO - Cucumber report available at /home/rmannibucau/dev/cukespacetest/target/cucumber-report/feature-overview.html    #### Annotations  Cuke in Space! API annotations are in the package `cucumber.runtime.arquillian.api`.  #### Features  If you want to reuse some feature in multiple test you can specify it through @Features:  ```java @Features("org/foo/bar/scenarii.feature") // can support multiple features too @RunWith(CukeSpace.class) public class MyFeatureTest {     .... } ```  #### Steps  If you want to reuse some step classes you can using the annotation @Glues:  ```java @Glues(MySteps.class) // can support multiple steps classes too @RunWith(CukeSpace.class) public class MyFeatureTest {     .... } ```  ##### @Tags  `@Tags` let you filter features by tag.  ```java @Tags("@myTag") // can support multiple tags too @RunWith(CukeSpace.class) public class MyFeatureTest {     .... } ```  Note: you can also set tags adding them on the test JVM, for the client `"-Dcucumber.options=-t @myTag"`, for the server check your arquillian adapter, for TomEE for instance you can use: `"-Dtomee.properties=-Dcucumber.options=-t @myTag"`.  #### @Cucumber.Options  `@cucumber.api.junit.Cucumber.Options` from cucumber junit api (not in Cuke in Space! directly) is supporting in compatibility mode out of the box.  It is used to create `cucumber.runtime.RuntimeOptions` when running cucumber and `features` and `tags` attributes are supported.
inovex/zax	ZAX ===  ZAX is a mobile frontend for the Zabbix enterprise monitoring system. It is free and has no limits.  This app shows you not only active triggers, events and latest item data, but has a few more features:  Features * view current problems * view events * view latest Items * view screens * mark events as acknowledged * history for items (graphs) * push notifications - trigger updates in realtime (using Pubnub) - for setup instructions, see http://inovex.github.io/zax/#howto_push * HTTP auth for proxy * two homescreen widgets * Zabbix 2.x support  For more information and screenshots, see our homepage: http://inovex.github.io/zax/  This app uses the following libraries: * ActionBarSherlock (http://actionbarsherlock.com/) * ViewPagerIndicator (http://viewpagerindicator.com/) * NineOldAndroids (http://nineoldandroids.com/) * OrmLite (http://ormlite.com/) * Jackson Core (https://github.com/FasterXML/jackson-core) * GraphView for Android (https://github.com/jjoe64/GraphView) * Pubnub (http://www.pubnub.com/) A big thank you to all developers of these great products!
georchestra/georchestra	# geOrchestra  [![Build Status](https://travis-ci.org/georchestra/georchestra.svg?branch=16.12)](https://travis-ci.org/georchestra/georchestra) [![Codacy Badge](https://api.codacy.com/project/badge/grade/a879ac64588d4357ab72e79cd8026f99)](https://www.codacy.com/app/georchestra/georchestra)  geOrchestra is a complete **Spatial Data Infrastructure** solution.  It features a **metadata catalog** (GeoNetwork 3.0.4), an **OGC server** (GeoServer 2.8.3 and GeoWebCache 1.8.1) with fine-grained access control (based on GeoFence), an **advanced viewer and editor**, an **extractor** and **many more** (security and auth system based on proxy/CAS/LDAP, analytics, admin UIs, ...)   ## Releases  A new release is published every 6 months and is supported during 12 months.  Stable versions are named by their release date, eg 16.12 (latest stable) was published in december 2016.  Have a look at the [release notes](RELEASE_NOTES.md) for more information.   ## Install  Depending on your goals and skills, there are several ways to install geOrchestra:   * a [docker composition](https://github.com/georchestra/docker/blob/master/docker-compose.yml), which pulls pre-built images from [docker hub](https://hub.docker.com/u/georchestra/), is perfect for a quick start. Provided you have a good download speed and recent machine (8Gb required), you'll be up and running within 10 minutes. Read [how to run geOrchestra on Docker](https://github.com/georchestra/docker/blob/master/README.md) here.  * a contributed [ansible playbook](https://github.com/georchestra/ansible) allows you to spin an instance in a few minutes. This is probably the easiest way to create a small production server, since it takes care of installing the middleware, fetching the webapps and configuring them.  * generic [debian (or yum) packages](https://packages.georchestra.org/) are perfect to create complex architectures, but you'll have to [install and configure the middleware](docs/setup.md) first.  * you could also use the [generic wars](http://packages.georchestra.org/bot/wars/16.12/) with their "[datadir](https://github.com/georchestra/datadir)", as an alternate method. The above packages provide both.  * finally, [building from the sources](docs/build.md) is the most flexible solution, since it allows you to customize the webapps very deeply. You get custom WAR files, packages or docker images that you can [deploy](docs/deploy.md) to dev, test, or production servers.   If you opt for the middleware setup by yourself, there are several [optimizations](docs/optimizations.md), [good practices](docs/good_practices.md) and [tutorials](docs/tutorials.md) that are worth reading.  Note that the minimum system requirement is 2 cores and 4Gb RAM, but we recommend at least 4 cores and 8 Gb RAM for a production instance. More RAM is of course better !   ## Community  If you need more information, please ask on the [geOrchestra mailing list](https://groups.google.com/forum/#!forum/georchestra).   For help setting up your instance, or for dev-related questions, use the [#georchestra](https://kiwiirc.com/client/irc.freenode.net/georchestra) IRC channel or the [dev/tech list](https://groups.google.com/forum/#!forum/georchestra-dev).   ## More  Additional information can be found in the [georchestra.org](http://www.georchestra.org/) website and in the following links:  * [catalog](https://github.com/georchestra/geonetwork/blob/georchestra-gn3-15.12/README.md): standard GeoNetwork with a light customization,   * [viewer](mapfishapp/README.md) (aka mapfishapp): a robust, OGC-compliant webgis with editing capabilities,  * [extractor](extractorapp/README.md) (aka extractorapp): able to create zips from data served through OGC web services and send an email when your extraction is done,   * [geoserver](http://geoserver.org/): the reference implementation for many OGC web services,  * [geowebcache](http://geowebcache.org/): a fast and easy to use tile cache,  * [geofence](https://github.com/georchestra/geofence/blob/georchestra/georchestra.md): optional, advanced OGC web services security,  * [simple catalog](catalogapp/README.md) (aka catalogapp): a very lightweight UI to query CSW services,  * [analytics](analytics/README.md): admin-oriented module, a front-end to the [ogc-server-statistics](ogc-server-statistics/README.md) and [downloadform](downloadform/README.md) modules,  * [ldapadmin](ldapadmin/README.md): also an admin-oriented module, to manage users and groups,  * [header](header/README.md): the common header which is used by all modules,  * [epsg-extension](epsg-extension/README.md): a plugin to override the geotools srs definitions.  * [atlas](atlas/README.md): a server-side component to print multi-page PDF with one geographic feature per page.
paymill/paymill-java	![PAYMILL icon](https://static.paymill.com/r/335f99eb3914d517bf392beb1adaf7cccef786b6/img/logo-download_Light.png) # paymill-java  Java wrapper for PAYMILL API  [![Build Status](https://travis-ci.org/paymill/paymill-java.png?branch=master)](https://travis-ci.org/paymill/paymill-java)  ## Getting started  - If you are not familiar with PAYMILL, start with the [documentation](https://developers.paymill.com). - Install the latest release. - Check the API [reference](https://developers.paymill.com/API/). - Check the full JavaDoc [documentation](http://paymill.github.io/paymill-java/). - Check the tests.   ## Installation  - Releases are available in [maven central](http://search.maven.org/#artifactdetails|com.paymill|paymill-java|5.1.6|jar) and in this [repository](https://github.com/paymill/paymill-java/releases/tag/v5.1.3).  ```xml <dependency>   <groupId>com.paymill</groupId>   <artifactId>paymill-java</artifactId>   <version>5.1.6</version> </dependency> ```   ## What's new  We have released version 5, which follows version 2.1 of the PAYMILL's REST API. This version is not backwards compatible with version 4, altough changes are minor. We also added some [examples](/samples/) , how to use an alternative http client and how to  deal with incoming webhooks.  ## Usage  Initialize the library by providing your api key: ```java   PaymillContext paymillContext = new PaymillContext( "<YOUR PRIVATE API KEY>" ); ``` PaymillContecxt loads the context of PAYMILL for a single account, by providing a merchants private key. It creates 8 services, which represents the PAYMILL API:  * ChecksumService  * ClientService  * OfferService  * PaymentService  * PreauthorizationService  * RefundService  * SubscriptionService  * TransactionService  * WebhookService  These services should not be created directly. They have to be obtained by the context's accessors.  To run the tests: ```   mvn -DapiKey=<YOUR_PRIVATE_API_KEY> test ```  ### Using services   In all cases, you'll use the predefined service classes to access the PAYMILL API.  To fetch a service instance, call *service name* accessor from paymillContext, like ```java   ClientService clientService = paymillContext.getClientService(); ``` Every service instance provides basic methods for CRUD functionality.  ### Creating objects  Every service provides instance factory methods for creation. They are very different for every service, because every object can be created in a different way. The common pattern is ```java   xxxService.createXXX( params... ); ``` For example: client can be created with two optional parameters: *email* and *description*. So we have four possible methods to create the client: * clientService.create() - creates a client without email and description * clientService.createWithEmail( "john.rambo@paymill.com" ) - creates a client with email * clientService.createWithDescription( "CRM Id: fake_34212" ) - creates a client with description * clientService.createWithEmailAndDescription( "john.rambo@paymill.com", "CRM Id: fake_34212" ) - creates a client with email and description  ### Retrieving objects  You can retrieve an object by using the get() method with an object id: ```java   Client client = clientService.get( "client_12345" ); ``` or with the instance itself, which also refreshes it: ```java   clientService.get( client ); ``` This method throws an ApiException if there is no client under the given id.  *Important*: If you use a nested object (e.g. ` paymet = transaction.getClient().getPayments().get(0) ` ) you should always "refresh", as the nested object will contain only the id, and all other properties will be null.  ### Retrieving lists  To retrieve a list you may simply use the list() method: ```java   PaymillList<Client> clients = clientService.list(); ``` You may provide a filter and order to list method: ```java   PaymillList<Client> clients =     clientService.list(       Client.createFilter().byEmail( "john.rambo@paymill.com" ),       Client.createOrder().byCreatedAt().desc()     ); ``` This will load only clients with email john.rambo@paymill.com, order descending by creation date.  ### Updating objects  In order to update an object simply call a service's update() method: ```java   clientServive.update( client ); ``` The update method also refreshes the the given instance. For example: If you changed the value of 'createdAt' locally and  pass the instance to the update() method, it will be refreshed with the data from PAYMILL. Because 'createdAt' is not updateable field your change will be lost.  ### Deleting objects  You may delete objects by calling the service's delete() method with an object instance or object id. ```java   clientService.delete( "client_12345" ); ``` or ```java   clientService.delete( client ); ```  ### Using an alternative http client  Since version 5.0.0 the wrapper supports alternative http clients. To use one, you need to take these two steps:  1. Exclude the jersey dependecy from your pom like this: ```xml   		<dependency> 			<groupId>com.paymill</groupId> 			<artifactId>paymill-java</artifactId> 			<version>latestversion</version> 			<exclusions> 				<exclusion> 					<groupId>org.glassfish.jersey.core</groupId> 					<artifactId>jersey-client</artifactId> 				</exclusion> 			</exclusions> 		</dependency> ``` 2. Implement the HttpClient interface and create a PaymillContext with it.  We have an [example](/samples/jerseyOneHttp) with Jersey 1.X, the client used prior the 5.X release of the wrapper.  ## Spring integration  This example is suitable if you use this wrapper for a single account.  Defines the PAYMILL context in Spring context.  ```xml <bean id="paymillContext" class="com.paymill.context.PaymillContext">   <constructor-arg value="<YOUR PRIVATE API KEY>" /> </bean> ```  Defines custom Controller, which uses PAYMILL ClientService internaly. Note that the setter receives *paymillContext*. ```xml <bean id="clientController" class="com.yourpackage.ClientController">   <property name="clientService" ref="paymillContext" /> </bean> ```  The ClientController class itself. Note that the clientService property is set by getting the ClientService form the paymillContext.  ```java public class ClientController {   private ClientService clientService;    public void setClientService( PaymillContext paymillContext ) {     this.clientService = paymillContext.getClientService();   } } ``` ## Scala and Groovy integration  The wrapper can be used easily in Scala and Groovy projects. Note, that it depends on 3rd party libraries, thus usage with a dependecy managment tool like maven is recommended.  ### Scala example:  ```scala import com.paymill.context.PaymillContext  object HelloPaymill {   def main(args: Array[String]) {     val context = new PaymillContext("<YOUR PRIVATE API KEY>")     val client = context.getClientService().createWithEmail("lovely-client@example.com")     println(client.getId())   } } ```  ### Groovy example:  ```groovy import com.paymill.context.PaymillContext  class HelloPaymill {      public static void main(String[] args) {         def context = new PaymillContext("<YOUR PRIVATE API KEY>")         def client = context.getClientService().createWithEmail("lovely-client@example.com")         println(client.getId())     } }  ```  ## Dealing with webhooks  Take a look at the Webhook [sample](/samples/webhookresolver). It contains an example json deserializer, as well as hints, how to receive webhooks in your app.  ## Older API versions  The wrapper supports only the latest version of the PAYMILL Rest API (v2.1). Latest stable releases for older API versions:  * API v2.0 : 3.2.0 in [maven central](http://search.maven.org/#artifactdetails|com.paymill|paymill-java|3.2.0|jar) and in this [repository](https://github.com/paymill/paymill-java/releases/tag/v3.2.0).  ## Changelog  ### 5.1.6 * fix: Smaller issues with Javadoc Comments  ### 5.1.3 * fix: [#65](https://github.com/paymill/paymill-java/issues/65) pass on parameter 'description' in TransactionService.createWithPaymentAndClient, thanks to [@rethab](https://github.com/rethab)  ### 5.1.2 * fix: [#64](https://github.com/paymill/paymill-java/issues/64) Unboxing null always throws NullPointerException, thanks to [@vladaspasic](https://github.com/vladaspasic)  ### 5.1.1 * fix: [#64](https://github.com/paymill/paymill-java/issues/64) Unboxing null always throws NullPointerException, thanks to [@vladaspasic](https://github.com/vladaspasic)  ### 5.1.0  * fix tests: Token is now generated for each request * update: projcet dependencies * add SEPA mandate_reference for direct debit transaction and subscription * include internal objects [ShoppingCartItem](https://github.com/paymill/paymill-java/blob/master/src/main/java/com/paymill/models/ShoppingCartItem.java) and [Address](https://github.com/paymill/paymill-java/blob/master/src/main/java/com/paymill/models/Address.java) * add ChecksumService to create checksums for transactions that are started client-side, e.g. PayPal checkout.  ### 5.0.0  * Minor, but incompatible changes in the interface * fix: [#57](https://github.com/paymill/paymill-java/issues/57) removed final keywords from some classes to allow proper mocking, thanks to [@dobermai](https://github.com/dobermai) * fix: [#59](https://github.com/paymill/paymill-java/issues/59) added missing subscription status, thanks to @schaebo * improved deserialization and fixed test suite to work with arbitary accounts * improvement: [#51](https://github.com/paymill/paymill-java/issues/51) isolated jersey dependecy, alternative frameworks can now be used for http, thanks to @basoko * fix: [#50](https://github.com/paymill/paymill-java/issues/50) fixed interval deserialization, thanks to @basoko * improvement: [#53](https://github.com/paymill/paymill-java/issues/53) added missing payment object fields * switched to jersey 2, thanks to Dmitry. * updated webhook event types * included internal objects * update project dependencies  ### 4.0.1 * improvement: [#54](https://github.com/paymill/paymill-java/issues/54) now is possible to end the trial  ### 4.0.0 * Works with version 2.1 of PAYMILL's REST API. * update project dependencies  ### 3.2.0 * improvement: remove workaround for subscriptions without offer. * improvement: [#23](https://github.com/paymill/paymill-java/issues/23) now preauthorizations can be created with description. * fix: after update of the offer the PAYMILL API does not returns 0 days trial period any more. * update project dependencies  ### 3.1.2 * fix: [#47](https://github.com/paymill/paymill-java/issues/47) TransacionService list with filter for createdAt doesn't work. * fix: typo in Webhook.EventType.TRANSACTION_FAILED * improvement: missing enumeration value will not cause parsing error, but will be mapped to UNDEFINED * update project dependencies  ### 3.1.1 * fix in *createWithOfferPaymentAndClient*: now *trialStart* represents the timestamp in correct format.  ### 3.1.0 * Allow update of an offer for a subscription * update project dependencies  ### 3.0.5 * internal improvements * [#42](https://github.com/paymill/paymill-java/issues/42) add currency to fee * update project dependencies  ### 3.0.4 * [#38](https://github.com/paymill/paymill-java/issues/38) explicit dependency to *jersey-core*  ### 3.0.3 * update project dependencies * Ability to set HTTP connection timeout in milliseconds to PaymillContext constructor (infinity by default) * [#39](https://github.com/paymill/paymill-java/issues/39) fix deserialization of Subscription nextCaptureAt  ### 3.0.2 * fix: remove **_id** in names for reference types in filter parameters for list methods( eg. *client_id* to *client* )  ### 3.0.1 * Add "chargeback" value to Transaction.Status enum * Add workaround for subscription without offer. Now the lib will not throw parsing error.  ### 3.0.0 * Improved implementation internally, now most of the configuration uses annotations and reflection. * Package name changed from de.paymill to com.paymill. - Removed singleton Paymill object and replaced with a PaymillContext object. + All services are now directly accessible from PaymillContext. + Improved list handling. Now the list method returns a PaymillList, which also contains the dataCount. All models now contain methods to create filter and sort criteria. + Improved create methods. All create methods now work with arguments. Removed unnecessary fields in objects (e.g. token in Transaction). + Improved get, update and remove methods. Get and remove now work with both instances and IDs. Get and Update methods now work on the same instance. - Offer is no longer updateable for a subscription.  ### 2.6 First maven central  ## License  Copyright 2013 PAYMILL GmbH.  MIT License (enclosed)
tryggvil/eucalyptus	EUCALYPTUS: Elastic Utility Computing Architecture        for Linking Your Programs to Transiently Useful Systems  EUCALYPTUS is an open source service overlay that implements elastic computing using existing resources. The goal of EUCALYPTUS is to allow sites with existing clusters and server infrastructure to co-host an elastic computing service that is interface-compatible with Amazon's EC2.  Because EUCALYPTUS is designed to function as an overlay, it must be able to incorporate resources from different clusters or pools.  For example, EUCALYPTUS allows its administrator to set up a "cloud" that permit users to virtualized OS instances on a number of clusters transparently.  Enabling the necessary network interconnectivity in a way that is secure and portable is one novel feature of EUCALYPTUS.  Another stems from its ability to provide interface compatibility with the existing Amazon EC2 service.  EUCALYPTUS users can develop using their own local resources and then transition directly some or all of their functionality to EC2.  Finally, a key requirement of EUCALYPTUS is that it be able to serve as a research platform for elastic computing.  To this end, its design makes two significant contributions.  The first concerns the use of scarce network resources in a structured way.  A EUCALYPTUS allocation can function equally well in an environment in which all processors have externally routable IP addresses (e.g. Amazon's current environment) as well as one in which only a certain "head instance" is externally routable (as is the case with most academic research clusters today).  Secondly, EUCALYPTUS leverages the extensive Linux packaging and deployment support that is currently available while requiring minimal modification to the existing installed OS base.  Specifically, the target resources need only run a standard Xen-enabled kernel with Xen 3.1 or later hypervisor support.  All other functionality installs directly without need for kernel patching or module additions to the host OS domain.  For more information and complete documentation, please visit our website (http://open.eucalyptus.com).
emsixteeen/IterativeReduce	IterativeReduce - parallel iterative YARN framework for Hadoop
davidsowerby/krail	# krail  ![License](http://img.shields.io/:license-apache-blue.svg) [![Gitter](https://badges.gitter.im/davidsowerby/krail.svg)](https://gitter.im/davidsowerby/krail?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![Build Status](https://travis-ci.org/davidsowerby/krail.svg?branch=master)](https://travis-ci.org/davidsowerby/krail) [![Coverage Status](https://coveralls.io/repos/github/davidsowerby/krail/badge.svg?branch=master)](https://coveralls.io/github/davidsowerby/krail?branch=master)  Krail provides a framework for rapid Java web development by combining Vaadin, Guice, Apache Shiro, Apache Commons Configuration and others.  For more information, see the comprehensive [Tutorial](http://krail.readthedocs.org/en/master/), which also makes a reasonable demo.  (You can clone directly from the [Tutorial repo](https://github.com/davidsowerby/krail-tutorial))   This core library provides:  * Site navigation, using a sitemap configured by annotation or Guice * Authentication / Authorisation framework, including page access control * Vaadin Server Push (with option to disable it) * Event Bus * Extensive I18N support * User options * Application configuration through ini files, database etc * JSR 303 Validation (integrated with I18N) * User notifications  Additional libraries, integrated and configured through Guice, provide:  * JPA persistence - [krail-jpa](https://github.com/davidsowerby/krail-jpa), using [Apache Onami persist](http://onami.apache.org/persist/) and EclipseLink * Quartz scheduler - [krail-quartz](https://github.com/davidsowerby/krail-quartz), using, of course,  [Quartz Scheduler](http://www.quartz-scheduler.org/)   The [issues tracker](https://github.com/davidsowerby/krail/issues?milestone=7&state=open), [blog](http://rndjava.blogspot.co.uk/) and [Tutorial](http://krail.readthedocs.org/en/master/) provide more information.   ---  ## Notice  ---  Version 0.10.0.0 of Krail was a major refactoring.  See Release Notes.md for that release for details.  That was the last significant development that will be done under Vaadin 7.  This library, and some of its associated libraries, are undergoing a transition to Vaadin 8. The upgrade to Vaadin 8 affects [krail](https://github.com/davidsowerby/krail) and [krail-jpa](https://github.com/davidsowerby/krail-jpa), and for functional testing [krail-bench](https://github.com/davidsowerby/krail-bench) and [krail-testapp](https://github.com/davidsowerby/krail-testapp)  Normally the versions of these are independent, but to simplify the transition to Vaadin 8, are being brought temporarily into line:  | Krail version  |   Vaadin    |  |---------|------------| |0.12.x.x | version 7  | |0.13.x.x-v7compat | version 8, running version 7 compatibility  | |0.14.x.x | pure Vaadin 8   This is the current plan:  - Complete the refactoring - released 23rd Aug 2017 (0.10.0.0) - Update to latest Vaadin 7  - released 23rd Aug 2017 (0.10.0.0) - Upgrade to Vaadin 8 - It is hoped that this can be completed by around late September 2017 - Attempt a migration to [Eclipse Vert.x](http://vertx.io/) - that may or may not work!   ---   # Download <a href='https://bintray.com/dsowerby/maven/krail/view?source=watch' alt='Get automatic notifications about new "krail" versions'><img src='https://www.bintray.com/docs/images/bintray_badge_color.png'></a>  ## Gradle  ``` repositories { 	jcenter() } ```  ``` 'uk.q3c.krail:krail:0.10.0.0' ``` ## Maven  ``` <repository> 	<id>jcenter</id> 	<url>http://jcenter.bintray.com</url> </repository>  ```  ``` <dependency> 	<groupId>uk.q3c.krail</groupId> 	<artifactId>krail</artifactId> 	<version>0.10.0.0</version> </dependency> ``` ## Direct  [ ![Download](https://api.bintray.com/packages/dsowerby/maven/krail/images/download.svg) ](https://bintray.com/dsowerby/maven/krail/_latestVersion)  # Limitations  Would not work in a [clustered environment](https://github.com/davidsowerby/krail/issues/425)  # Status  23rd Aug 2017:  * Vaadin 7.7.10 is integrated with: * Guice 4.1.0 * Shiro 1.4.0, * MBassador (Event Bus) * Apache Commons Configuration * Guava cache   Krail is usable, though there is still work to ensure thread safety.  No major changes to the API expected.  Vaadin push is supported.  Tested on Tomcat 7 & 8   ## testApp  There is a [functional test application](https://github.com/davidsowerby/krail-testApp) which can also be used to explore functionality - though the [Tutorial](http://krail.readthedocs.org/en/latest/) may be better for that   # Project Build  Gradle is used (made a lot easier thanks to the [Gradle Vaadin plugin](https://github.com/johndevs/gradle-vaadin-plugin).  # Acknowledgements  Thanks to:   [Dirk Lietz](https://github.com/Odhrean) for his review and feedback for the Tutorial<br> [Mike Pilone](http://mikepilone.blogspot.co.uk/) for his blog post on Vaadin Shiro integration<br>   [ej technologies](http://www.ej-technologies.com/index.html) for an open source licence of [JProfiler](http://www.ej-technologies.com/products/jprofiler/overview.html)<br> [Vaadin](https://vaadin.com/home)<br> [Guice](https://github.com/google/guice)<br> [Apache Shiro](http://shiro.apache.org/)<br> [JUnit](http://junit.org/)<br> [Guava](https://github.com/google/guava) (cache and utilities)<br> [MBassador Event Bus](https://github.com/bennidi/mbassador)<br> [Flag Icons](http://www.icondrawer.com/)<br> [Apache Commons Configuration](http://commons.apache.org/proper/commons-configuration)<br> [Gradle](http://gradle.org/)<br> [Gradle Vaadin plugin](https://github.com/johndevs/gradle-vaadin-plugin)<br> [Gradle Docker Plugin](https://github.com/bmuschko/gradle-docker-plugin)<br> [Gradle Bintray Plugin](https://github.com/bintray/gradle-bintray-plugin)<br> [Bintray](https://bintray.com)<br> [Docker](https://www.docker.com/)<br> [Logback](http://logback.qos.ch/)<br> [slf4j](http://www.slf4j.org/)<br> [AssertJ](http://joel-costigliola.github.io/assertj/)<br> [Mycila](https://github.com/mycila)<br> [Mockito](https://github.com/mockito/mockito)<br> [spock](https://github.com/spockframework/spock) [FindBugs](http://findbugs.sourceforge.net/)
crazyhitty/Munch	#Munch A minimalistic, easy to use Rss reader application.  <a href="https://play.google.com/store/apps/details?id=com.crazyhitty.chdev.ks.munch">   <img alt="Get it on Google Play"        src="http://www.androiddocs.com/images/brand/en_app_rgb_wo_45.png" /> </a> [![Android Arsenal](https://img.shields.io/badge/Android%20Arsenal-Munch-brightgreen.svg?style=flat)](http://android-arsenal.com/details/3/2955)  #What is Munch ? Munch is an android app which enable the users to manage their Rss feeds. User can add new sources, manage them and view the article associated with the feeds.  #Permissions: * Internet * Access Network State * Read External Storage  These permissions are used to grab the data from the web and to know if user’s internet is available or not so that he can be notified regarding that matter. The Read External Storage permissions is used to retrieve the opml files from either SD card or internal storage. Also, this app will never snoop on your personal information.  #Application features: * Load Rss feeds quickly * Add Rss Sources * Manage Rss Sources * Archive feeds * Customizable settings * Ad free  #How does Munch work ? Munch uses jsoup xml parser to parse rss feeds and also load the web articles associated with those feeds. It also uses glide to lazy load article images(if available).  #Screenshots ![](https://lh3.googleusercontent.com/_nFUASUq-EEayxKAR5J2Pne94Fi_napfkaF8Ov1s7rPZuBH9kQBQtbK9L1F2FQa7YUg=h900-rw) ![](https://lh3.googleusercontent.com/625tiEodzuBVr5R_g8sVff8m-Z74EH1LdzRa6XufT94qcUCw13HaVdkUSIxbDWQ1hA=h900-rw) ![](https://lh3.googleusercontent.com/4GGI6N2Zdtg1-Fd1RuhEbJ3PWUEeR-ioyOt7XdUrelgAD6gJqnnaRot8PW0I-s39Cg=h900-rw) ![](https://lh3.googleusercontent.com/VaKLswqGi7QL7cAOE99ZIdIpsgGJFm8140AUUV5__8jrregugPM1-2nGPx4onUow2V_D=h900-rw) ![](https://lh3.googleusercontent.com/iMNYSHzW7q06dsfhs52XNHJOBVwt3YV5FV1WrRTK_VacHW_gHscDIBAMHnBogNkTWXvW=h900-rw) ![](https://lh3.googleusercontent.com/tkqDfYKtVhkKO4FgP3-nml3ctfprs2mqlHk0_AZCeYAmyxezOXKyYUBeGioyuWBcIGU=h900-rw)
fiveagency/Reedly	# README - Reedly #  This is the GitHub repo for the Reedly RSS reader app that showcases Clean Architecture.  To learn more about the Clean Architecture, take a look at this blog series:  [Start here - Part one](http://five.agency/android-architecture-part-1-every-new-beginning-is-hard/)  #### How do I build this on my machine? ####  In order to build the project:  1. git clone git@github.com:fiveagency/Reedly.git 2. Import the project into Android Studio by selecting settings.gradle file in the root of the project.  **NOTE:** You should use at least Android Studio 2.3.2.     ## LICENSE      Copyright 2017 Mihael Franceković - Five      Licensed under the Apache License, Version 2.0 (the "License");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an "AS IS" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License.
nilsbraden/ttrss-reader-fork	# TTRSS-Reader TTRSS-Reader (or ttrss-reader-fork) is a fork of the original Project from [ttrss-reader](https://code.google.com/p/ttrss-reader/). I took the sourcecode and continued the development because the original owner didn't reply to any messages back then (and has not done so as of today).  It is a client application for the RSS-Reader [Tiny Tiny RSS](http://tt-rss.org/), a PHP-based online feedreader which runs on your own webspace. It is accessible with any browser and even supports keyboard shortcuts through javascript. Download  Since google announced that the downloads from google-code will not be available after January 14th, 2014 I started to move all downloads to google-drive. The commentary with the current changelog will be removed in the process but the full changelog can be viewed in the Wiki.  [![play.google.com][3]][4]<br> [![QR-Code for play.google.com][5]][4]   * [**Download current Version**](https://github.com/nilsbraden/ttrss-reader-fork/releases)  * ([**Changelog**](https://github.com/nilsbraden/ttrss-reader-fork/wiki/Changelog))  # Features  * Use Swipe for Next/Previous Article  * Swipe-Area in bottom area of the Article-View to avoid swiping while scrolling  * Show or download images/videos/music attached to Articles  * Share URL via Twitter/SMS/Mail  * Mark everything (whole category or all categories) as read  * Navigate through articles with volume-buttons  * Read Labels  * Cache articles in local-database  * Cache images for offline browsing  * SSL-Supoprt with self-signed certificates  * Scheduled updates via Tasker/Locale  * Fast JSON-Parsing with stream-based parser GSON   See [Changelog](https://github.com/nilsbraden/ttrss-reader-fork/wiki/Changelog) for detailed information.  # Note regarding support and F-Droid Since [F-Droid](https://f-droid.org/repository/browse/?fdid=org.ttrssreader) is often several months late with the build I have to make this clear: I won't provide support for versions which are not exactly or at least near to the version which is stated above on this site ("Current Version"). Please provide the full version information from the About-box within the app when reporting an issue!  # Changelog Please follow this [link](https://github.com/nilsbraden/ttrss-reader-fork/wiki/Changelog)  # Contact Please feel free to contact me via E-Mail (ttrss /at/ nilsbraden.de) or via the Bugtracker (Issues). If you have questions specifically about Tiny Tiny RSS you may ask me or visit the [Tiny-Tiny-RSS-Forum](http://tt-rss.org/forum/) or #tt-rss at irc.tt-rss.org. If I don't answer fast enough or am not willing to implement your suggestion please keep in mind that this is only a hobby for me and I'm unwilling to implement every little feature just beacuse you suggested I do so. You may implement it and send me pull-requests so I can include the features but I can't guarantee I will do this for every thing you want to have.  # Donations If you would like to support my work and the further developement of TTRSS-reader you may submit a donation via Paypal or flattr me here:  * Paypal: [![paypal.com][1]][2]<br><br>  * Flattr: [![flattr.com][6]][7]  For more information you may want to visit [Donations](https://github.com/nilsbraden/ttrss-reader-fork/wiki/Donations).  # Screenshots ![Screenshot 1](http://nilsbraden.de/android/screenshots/small/device-2014-01-02-163718_1.png) ![Screenshot 2](http://nilsbraden.de/android/screenshots/small/device-2014-01-02-163709_1.png) ![Screenshot 3](http://nilsbraden.de/android/screenshots/small/device-2014-01-02-163755_1.png) ![Screenshot 4](http://nilsbraden.de/android/screenshots/small/device-2014-01-02-163740_1.png) ![Screenshot 5](http://nilsbraden.de/android/screenshots/small/Screenshot_2014-01-02-16-34-03_1.png) ![Screenshot 6](http://nilsbraden.de/android/screenshots/small/Screenshot_2014-01-02-16-34-16_1.png) ![Screenshot 7](http://nilsbraden.de/android/screenshots/small/Screenshot_2014-01-02-16-34-39_1.png) ![Screenshot 8](http://nilsbraden.de/android/screenshots/small/Screenshot_2014-01-02-16-35-11_1.png) ![Screenshot 9](http://nilsbraden.de/android/screenshots/small/Screenshot_2014-01-02-16-35-30_1.png)  [1]: https://nilsbraden.de/android/tt-rss/btn_donateCC_LG.gif [2]: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=SD4AN4APNW6B4 [3]: https://www.nilsbraden.de/android/tt-rss/60_avail_market_logo2.png [4]: https://play.google.com/store/apps/details?id=org.ttrssreader [5]: https://www.nilsbraden.de/android/tt-rss/qrcode.png [6]: https://www.nilsbraden.de/android/tt-rss/Flattr.svg.png [7]: http://flattr.com/thing/382465/TTRSS-Reader
yanghua/RssToMobiService	About  ======  RssToMobiService (rtms) is a service (you can treat it as a tool) helps you to fetch RSS and generate mobi file for your kindle device.   ------------  Dependency --------  - python env (pip) - redis - maven - one of Java's IDEs such as Eclipse, IntelliJ idea (optional)  > **NOTE** > > - python env (pip): two script will need python's env (fullTxt and kindlestrip) > - redis: used to store image's json object and cache processed entry desc > - maven: it is necessary that jar will be built with it > - Java's IDE: can help you edit some config item and code, it depends on yourself.  ### How to use > assume that you have installed those dependencies.  * enter your-local-repository  ``` cd RssToMobiService/src/main/resources/ ```  * modify some config file. ***mail.properties*** and ***feedlinks.txt*** must be modified generally. Check it one by one!  * run dispatch script  ``` (sudo) sh dispatch.sh ```  * return to project's root dir and build && generate jar file  ``` mvn assembly:assembly ```  * init python script's dependency packages  ``` (sudo) pip install feedparser (sudo) pip install readability-lxml ``` * start redis server (command see ***resources/redisMaintain.sh***)  * goto target dir && execute jar  ``` java -jar RssToMobiService-1.0-SNAPSHOT.jar ```  > **NOTE** > > - you must goto amazon's website to set your kindle's **receive-email** > - you should change the **feedlinks.txt**'s rss url > - these are apse some config items about mobi file you should pay attention to (**rtms.properites**)  ### What's more - you can make it as a deamon daily service. - optimize entry fetch/parse and image download   ### Q&A - [my blog about this repository](http://blog.csdn.net/yanghua_kobe/article/details/18950969) - [my email](mailto://yanghua1127@gmail.com) - [my weibo](http://weibo.com/yanghua1127)
DanielMichalski/spring-web-rss-channels	Spring Web RSS Channels ---------------------------------------------  This project is Spring MVC + JPA/Hibernate Web application.  It contains some useful configuration items:  - Spring MVC - Spring Data Repositories - JPA + Hibernate - Spring Security - Twitter Bootstrap - Apache Tiles - Autowired logger - Internationalization - JQuery + Ajax - JQuery validator - Sending mail using Spring + Java API - Spring profiles   How to run application --------------------------------------------- ```bash ## From base directory build app mvn clean install  ## Go to rss-web directory cd rss-web  ## Run tomcat7 server and deploy app mvn tomcat7:run-war  ## In Your browser go to URL address http://localhost:8081/rss-web/ ```  Video -----  [![Spring Web RSS Channels](http://img.youtube.com/vi/5fvERzlhdZU/0.jpg)](http://www.youtube.com/watch?v=5fvERzlhdZU "Spring Web RSS Channels")  Screens ---------------------------------------------  ![alt text](https://github.com/DanielMichalski/spring-web-rss-channels/blob/master/rss-web/src/main/resources/img/screen1.png "Screen 1")  ![alt text](https://github.com/DanielMichalski/spring-web-rss-channels/blob/master/rss-web/src/main/resources/img/screen2.png "Screen 2")  ![alt text](https://github.com/DanielMichalski/spring-web-rss-channels/blob/master/rss-web/src/main/resources/img/screen3.png "Screen 3")  ![alt text](https://github.com/DanielMichalski/spring-web-rss-channels/blob/master/rss-web/src/main/resources/img/screen4.png "Screen 4")  ![alt text](https://github.com/DanielMichalski/spring-web-rss-channels/blob/master/rss-web/src/main/resources/img/screen5.png "Screen 5")  ![alt text](https://github.com/DanielMichalski/spring-web-rss-channels/blob/master/rss-web/src/main/resources/img/screen6.png "Screen 6")
tulskiy/musique	musique is a 100% Java audio player that supports most popular formats      * MP3     * OGG Vorbis     * FLAC     * WavPack     * Monkey's Audio     * WAV, AU, AIFF  Also, musique can recode these formats into:      * OGG Vorbis     * Monkey's Audio     * WavPack     * WAV  There is a built in support for CUE files, gapless playback, SHOUTCast/IceCast radio, Last.fm Scrobbling  Building      To compile, run "ant", then run either musique.sh or musique.bat     to start the player     You also can open the project in IDE - there are configurations     for IntelliJ IDEA, NetBeans or Eclipse.     Main class is com.tulskiy.musque.system.Main      Requirements: Sun JDK 1.6, or OpenJDK 1.6   Shortcuts     Z, X, C, V, B - prev, play, pause, stop, next     Space - scroll to now playing     Q - add selected tracks to queue     Alt-Enter - tracks info / edit tags     Del - remove selected tracks from playlist     Ctrl T - new playlist     Ctrl S - save playlist     Ctrl F - Search     Ctrl P - properties     Ctrl W - closes window if tray is enabled, else quit program     Ctrl Q - quit program  GUI Tweaks      Tray Icons      On Gnome, tray icon is not transparent, so I have to draw background and gradient     myself to blend in. Here are some colors for standard Gnome/Ubuntu themes that will make the     tray icon seem transparent (written as Red, Green, Blue, can be set in     Properties->Color and Fonts):      Theme       | Background 1   | Background 2     ---------------------------------------------     Ambiance:     92, 91, 86     | 60, 59, 54     New Wave      58, 58, 58     | 42, 42, 42     Radiance      247, 244, 236  | 225, 215, 188     // other themes don't require gradient, so only one color     Clearlooks    237, 236, 235     Dust          52, 51, 48     Dust Sand     193, 189, 191     High contrast 0, 0, 51      GTK LaF:      On Ubuntu Maverick with GTK LaF and Ambiance theme, slider is too wide,     you can fix it by adding 'gui.thumbWidth: 15' to     $HOME/.musique/config     Tabs are too tall, this is JRE bug, it's been around for 5+ years, and is,     as many GUI bugs are, low priority      Double-click speed on Linux is faster than system settings, it might be     related to http://ubuntuforums.org/showthread.php?t=221642          create ~/.Xresources, and add         *multiClickTime: 400         Log Out and then Log back on          Substitute 400 with your prefered time (in milliseconds).      Mac OSX:          add -Xdock:name="Musique"         to musique.vmoptions to change program name in global menu   Performance Notes      The program should be run in client mode, not server, otherwise memory and CPU     usage will be too high.      I've tested the program in Windows 7 on Core 2 Duo 2.13 GHz, JRE 1.6u20 32-bit     and average CPU load was less than 1. Memory consumption was around 30-40 Mb      When I run the program in Ubuntu 10.04 32-bit and OpenJDK 1.6, memory consumption     is also around 30-40Mb, but the CPU load is ~4%.     However, deadbeef and rhytmbox use same amount of CPU so I guess it's because of     PulseAudio.      If you are running a 64-bit system and 64-bit JDK, it can not be run in client mode,     so it is better to install a 32-bit JRE. Then just change path to jre in musique.sh  Troubleshooting      On Linux, Sun JRE is using oss for sound output and the implementation is known to be     very buggy. Issues include conflicts with PulseAudio and Alsa - player throws LineUnavailableException     when some other program is playing audio, or, when the player is running, other programs can not     play sound.      So if you use PulseAudio, it is best to install OpenJDK, which has good     PulseAudio support (at least on Ubuntu). If you want to use Sun JDK instead of OpenJDK,     you can copy PulseAudio support from OpenJDK to Sun JDK using these instruction:      copy the following files from openjdk folder to java-sun folder     ./jre/lib/(i386, amd64)/libpulse-java.so     ./jre/ext/pulse-java.jar      Another solution is to use padsp or aoss to redirect sound to pulseaudio or alsa,     in this case, edit musique.sh. Find this line:      # uncomment to use OSS emulation, fixes sound problems with Sun JRE's     # DSP="padsp"      uncomment last line. Change it to aoss if you use alsa (you might need to install     the package first).  Title formatting help:      Everything is pretty much winamp/foobar2000 style:     Currently the following tags are supported:      %artist%, %title%, %album%, %year%, %genre%, %comment%, %length% - Don't need to explain these     %genres% - multi-valued version of %genre%     %albumArtist% - Album Artist (returns %artist% if album artist is not set)                     for MP3 I use album artist from TXXX     %fileName% - filename without extension     %file% - full file path     %trackNumber%, %trackTotal% - track number formatted as a two-digit number and total tracks     %track% - convenience method that returns raw track number as it is stored in tags     %disc%, %discTotal% - disc number and total discs     %codec% - returns codec (MPEG-1 Layer 3, FLAC, WavPack, Ogg Vorbis, Monkey's Audio, PCM)     %recordLabels%, %catalogNos% - set of record label and catalog number values      some technical info fields like     %bitrate%, %totalSamples%, %bps%, %channels%, %songIndex%, %sampleRate%,     %channelsAsString% (Mono or Stereo)     $playingTime() - playing time of current song, if any      You can also use some functions, but not many of them are implemented yet:     $if1(arg1, arg2, arg3) - returns arg2 if arg1 is not empty, and arg3 otherwise     $if3(arg1, arg2, ...) - returns first non-empty argument     $strcmp(arg1, arg2) - returns 1 if arg1=arg2, empty string otherwise     $isPlaying() - used to idicate that the song is currently playing, paused or in playback queue     $combine(arg1, arg2) - formatter for multi-valued fields, where arg1 is values and arg2 is format separator           '' - Regular strings are enclosed in single quotation marks     [arg1 arg2] - will return contents only if at least on of the args is not empty      Some sample usages:      $if1($strcmp('MPEG-1 Layer 3', %codec%),'this is mp3','hurray to free formats')         will compare field 'codec' with string 'mp3', and return 'this is mp3' if they are equal      [%artist% - ]%title% - returns 'Artist - Title' if artist field is not empty,         otherwise returns just 'Title'      $if3(%title%, %fileName%) - returns file name without extension if title is empty          $combine(%genres%, ', ') - returns genre list separated by comma (for example, "heavy metal, hard rock")
smedic/Android-YouTube-Background-Player	# Android-YouTube-Background-Player  YT player which plays only audio in background.  This is agains YT terms of service and could not be present on Google Play Store; therefore, I have developed it just for personal purposes.   Application provides mechanisms for searching videos and playlists on YT, as well as logging into a Google account in order to acquire private playlists, which normally cannot be accessed.  <img src="https://github.com/smedic/Android-YouTube-Background-Player/blob/master/raw/Screenshot_2016-03-24-10-21-27.png" alt="alt text" width="360" height="640"> <img src="https://github.com/smedic/Android-YouTube-Background-Player/blob/master/raw/Screenshot_2016-03-24-10-20-31.png" alt="alt text" width="360" height="640">
EasyDarwin/EasyPusher_Android	# EasyPusher_Android A simple, robust, low latency RTSP video&audio&screen stream pusher and recorder on android.精炼、稳定、高效的安卓前/后摄像头/手机桌面屏幕采集、编码、RTSP直播推送工具，充分秉承了RTP在即时通信领域中的技术特点，延时控制在300ms~500ms；  EasyPusher是EasyDarwin流媒体团队开发的一个推送流媒体音/视频流给开源流媒体服务器EasyDarwin的标准RTSP/RTP协议推送库，全平台支持(包括Windows/Linux(32 & 64)，ARM各平台，Android、IOS)，通过EasyPusher我们就可以避免接触到稍显复杂的RTSP/RTP/RTCP推送流程，只需要调用EasyPusher的几个API接口，就能轻松、稳定地把流媒体音视频数据推送给RTSP流媒体服务器进行转发和分发，EasyPusher经过长时间的企业用户检验，稳定性非常高;  ## 分支说明 ## - master分支是EasyPusher APP(https://fir.im/EasyPusher)的工程。如果需要验证Pusher的功能，可以使用这个工程进行编译运行，AS的版本无要求。 - library分支是EasyPusher library的工程，主要面向开发者，将pusher功能集成到现有APP的场景。library使用了android architecture component的一些特性，非常便于集成。（见：https://developer.android.com/topic/libraries/architecture/index.html）。该分支要求AS版本3.0-beta7及以上。  ## 版本下载 ##  - Android [https://fir.im/EasyPusher ](https://fir.im/EasyPusher "EasyPusher_Android")  ![EasyPusher_Android](http://www.easydarwin.org/skin/bs/images/app/EasyPusher_AN.png)  - iOS [https://itunes.apple.com/us/app/easypusher/id1211967057](https://itunes.apple.com/us/app/easypusher/id1211967057 "EasyPusher_iOS")  ![EasyPusher_iOS](http://www.easydarwin.org/skin/bs/images/app/EasyPusher_iOS.png)  ## 版本更新记录 ## ### v1.0.17.0218 (Build 10) ### 1. 支持本地录像，边推边录；  ### 1.1.16.1127 (Build 6) ### 1. 优化推送速度； 1. 增加软件编码； 1. 国内镜像：http://fir.im/EasyPusher；  ### 1.0.16.1007 (Build 5) ### 1. 增加屏幕录制、直播； 1. 增加升级功能；  ### 1.0.16.0423 (Build 3) ### 1. 支持基本的安卓Android前/后置摄像头RTSP直播推送； 1. 支持采集安卓Android手机麦克风声音直播；  ## 获取更多信息 ##  邮件：[support@easydarwin.org](mailto:support@easydarwin.org)   WEB：[www.EasyDarwin.org](http://www.easydarwin.org)  QQ交流群：[465901074](http://jq.qq.com/?_wv=1027&k=2G045mo "EasyPusher & EasyRTSPClient")  Copyright &copy; EasyDarwin.org 2012-2017  ![EasyDarwin](http://www.easydarwin.org/skin/easydarwin/images/wx_qrcode.jpg)
Arjun-sna/Android-AudioRecorder-App	# Android Audio Recorder App Application to record audio and save it locally in the device.  The application also visualizes the audio in wave format. It also supports recording in the background while you can continue using the device for other tasks.   <a href='https://play.google.com/store/apps/details?id=in.arjsna.audiorecorder' target='_blank'><img height='50' style='border:0px;height:50px;' src='https://cdn.rawgit.com/Arjun-sna/Arjun-sna.github.io/f8228c83/raw/GooglePlay.png' border='0' alt='GooglePlay Link' /></a>  <img src="https://arjun-sna.github.io/raw/audio_rec_1.png" width="250" />  <img src="https://arjun-sna.github.io/raw/audio_rec_2.png" width="250" />  <img src="https://arjun-sna.github.io/raw/audio_rec_3.png" width="250" /> <img src="https://arjun-sna.github.io/raw/audio_rec_4.png" width="250" /> <img src="https://arjun-sna.github.io/raw/audio_rec_6.png" width="250" />
atlasapi/atlas	Build and Run Atlas ===================  The Atlas source-code is hosted at [github.com/atlasapi](http://github.com/atlasapi) and is split across 5 projects:  * [github.com/atlasapi/atlas](http://github.com/atlasapi/atlas) * [github.com/atlasapi/atlas-persistence](http://github.com/atlasapi/atlas-persistence) * [github.com/atlasapi/atlas-model](http://github.com/atlasapi/atlas-model) * [github.com/atlasapi/atlas-feeds](http://github.com/atlasapi/atlas-feeds)  Additionally, [github.com/atlasapi/atlas-client](http://github.com/atlasapi/atlas-client) hosts the Java client library. [github.com/atlasapi/atlas](http://github.com/atlasapi/atlas) is the main entry point and you don't need the others unless you're interested in updating them too.  ## MongoDB  Atlas, and specifically atlas-persistence, uses [MongoDB](http://www.mongodb.org/) to store its indexed content. Atlas doesn't come packaged with MongoDB so you'll need to make sure it's installed - [Download MongoDB](http://www.mongodb.org/downloads).  During test runs, Atlas will run integration tests against a mongo running on port 8585. If one isn't running then it'll try and start one, assuming that mongod is available on its path, so either keep one running or make sure you've added mongo/bin to your path.  The running Atlas instance requires that MongoDB be running on its standard port:27107 and ZooKeeper/Kafka: running locally.  Please make sure you've kicked them off.  ## Maven  Atlas uses [Maven](http://maven.apache.org/) for all it's dependency and build management, so you'd better have mvn available on your path! We've included the MetaBroadcast public repo which houses all the dependencies that haven't been mavenised, and all our successful builds deploy to it so it has the latest atlas SNAPSHOTs available. This means you don't have to build the other atlas projects, if you don't want to.  It's worth noting that we don't current have a formal release process and everything's currently a SNAPSHOT release. We're sorry if this is a pain and we have every intention of creating some proper releases soon, when life has calmed down a bit.  ## Building and Running  So, to get everything built and ready:      mkdir /data                                      # Required for feed processing     git clone http://github.com/atlasapi/atlas.git     cd atlas     mvn clean install      This will download all the dependencies, compile the code and run the tests (make sure mongo's setup). To actually run the project locally:      mvn jetty:run -Dprocessing.config=true -Dupdaters.bbc.enabled=true -Djetty.port=8282 # Atlas processing     mvn jetty:run -Dprocessing.config=false                                              # Atlas front-end      This will startup Atlas locally using the lovely [Jetty](http://jetty.codehaus.org/jetty/).  Go to      http://localhost:8282/system/scheduler  and run "BBC Ion schedule update (today only)" to ingest the BBC schedule for today. When that completes, run "BBC Mongo Schedule repopulator" to generate the schedule.   Then go to      http://localhost:8080/3.0/schedule.json?from=now.minus.3h&to=now.plus.10h&channel=bbcone&publisher=bbc.co.uk   for some of today's BBC One schedule. You may need to modify the 'from' and 'to' parameters, depending on the time of day.  Enjoy!  ## Contributions  We welcome contributions to Atlas! If you'd like to get your hands dirty, please [fork the repositories](https://help.github.com/articles/fork-a-repo) and submit [pull requests](https://help.github.com/articles/using-pull-requests) with your changes.
feribg/audiogetter	# audiogetter  **This is a work in progress, currently it doesn't compile or run**  The purpose of this application is to allow the search and download of various audio sources such as youtube, vimeo etc. Furthermore  it's mobile optimized, which means it will only download the audio bytes from a youtube video, resulting in faster downloads and lower mobile data consumption.  Currently undergoing a major rewrite for a cleaner and better structured code, as well as moving to a zero-copy downlods which would improve performance significantly.
jenkinsci/email-ext-plugin	Please see https://wiki.jenkins-ci.org/display/JENKINS/Email-ext+plugin for information about the email-ext plugin.  [![Build Status](https://ci.jenkins.io/job/Plugins/job/email-ext-plugin/job/master/badge/icon)](https://ci.jenkins.io/job/Plugins/job/email-ext-plugin/job/master/)
elasticinbox/elasticinbox	ElasticInbox is reliable, distributed, scalable email store.  ## Overview  ElasticInbox provides highly available email store without a single point of  failure which can run on commodity hardware and scale linearly. ElasticInbox  can easily scale to millions of mailboxes, with hundreds of thousands messages  in each mailbox.  ## Requirements   * Java >= 1.6  * Apache Cassandra >= 1.1 (see http://cassandra.apache.org/)  ## Getting started  Please visit http://www.elasticinbox.com/ for more information.  ## Building from Source  To build and run from source you will need Maven 3 and Git:  ```bash % git clone git://github.com/elasticinbox/elasticinbox.git elasticinbox % cd elasticinbox % mvn clean install pax:provision -DskipITs ```  ## License  Licensed under the Modified BSD License.
ozimov/spring-boot-email-tools	# Spring Boot Email Tools A set of services and tools for sending emails in a **Spring Boot** application using plain text, html or a template engine to generate dynamic content.  **Source Website:** *[github.com/ozimov/spring-boot-email-tools](http://github.com/ozimov/spring-boot-email-tools/)*<br />  **Latest Release:** *0.6.3* <br /> **Latest Artifacts:** *it.ozimov:spring-boot-email-core*, *it.ozimov:spring-boot-freemarker-email*,     *it.ozimov:spring-boot-mustache-email*, *it.ozimov:spring-boot-pebble-email*, *it.ozimov:spring-boot-thymeleaf-email* <br /> **Continuous Integration:** <br /> [![Maven Central](https://maven-badges.herokuapp.com/maven-central/it.ozimov/spring-boot-email-core/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.github.ozimov/spring-boot-email-core) <br /> [![Build Status](https://travis-ci.org/ozimov/spring-boot-email-tools.svg?branch=master)](https://travis-ci.org/ozimov/spring-boot-email-tools) [![codecov.io](https://codecov.io/github/ozimov/spring-boot-email-tools/coverage.svg?branch=master)](https://codecov.io/github/ozimov/spring-boot-email-tools?branch=master) [![Codacy Badge](https://api.codacy.com/project/badge/grade/7a4364b93df6473fb18a597e900edceb)](https://www.codacy.com/app/roberto-trunfio/spring-boot-email-tools)  ![codecov.io](https://codecov.io/github/ozimov/spring-boot-email-tools/branch.svg?branch=master)   ## Background  The project relies on a templateless module `it.ozimov:spring-boot-email-core` that provides the core features (e.g. sending emails, scheduling and prioritizing, persistence). Since it is templateless, it  does not provide  any implementation of the service to be used to generate the body of the email via template engine.  If you want to use one of the template engines supported by this project (i.e. _Freemarker_, _Mustache_, _Pebble_ and _Thymeleaf_), you can use the dedicated templatefull module that is shipped with the core module. The standard naming for the templatefull module is `it.ozimov:spring-boot-{template_engine_name}-email` (where `{template_engine_name}` is for instance `pebble`).  ## Dependency Latest release is **`0.6.3`**. To use the core module, you can import the following dependency in Maven  ```xml <dependency>     <groupId>it.ozimov</groupId>     <artifactId>spring-boot-email-core</artifactId>     <version>0.6.3</version> </dependency> ```  To embed the module that includes the _Freemarker_ template engine, you can use the following Maven dependency:  ```xml <dependency>     <groupId>it.ozimov</groupId>     <artifactId>spring-boot-freemarker-email</artifactId>     <version>0.6.3</version> </dependency> ```  for _Mustache_:  ```xml <dependency>     <groupId>it.ozimov</groupId>     <artifactId>spring-boot-mustache-email</artifactId>     <version>0.6.3</version> </dependency> ```  for _Pebble_:  ```xml <dependency>     <groupId>it.ozimov</groupId>     <artifactId>spring-boot-pebble-email</artifactId>     <version>0.6.3</version> </dependency> ```  and for _Thymeleaf_:  ```xml <dependency>     <groupId>it.ozimov</groupId>     <artifactId>spring-boot-thymeleaf-email</artifactId>     <version>0.6.3</version> </dependency> ```  Remember that if you import the template-full module, the core module should not be required.   ## Usage In your main Spring Boot application, you need to add the annotation `@EnableEmailTools` to   enable support for all the services and controllers defined in the Spring Boot Email module, e.g.:  ```java @SpringBootApplication @EnableEmailTools public class MainApplication  {      public static void main(final String ... args) {         SpringApplication.run(MainApplication.class, args);     } } ```  in you `application.properties` set the configuration needed to send the emails, e.g. if you want to send the emails using a Gmail account you can set:  ```properties spring.mail.host=smtp.gmail.com spring.mail.port=587 spring.mail.username=name.surname@gmail.com spring.mail.password=V3ry_Str0ng_Password spring.mail.properties.mail.smtp.auth=true spring.mail.properties.mail.smtp.starttls.enable=true spring.mail.properties.mail.smtp.starttls.required=true ```  Plus, the additional properties must be added to prevent using the persistence layer ```properties spring.mail.scheduler.persistence.enabled=false spring.mail.scheduler.persistence.redis.embedded=false spring.mail.scheduler.persistence.redis.enabled=false ```  To send an email, use the ``EmailService`` in your Spring Boot application. E.g.   ```java @Autowired public EmailService emailService;  public void sendEmailWithoutTemplating(){    final Email email = DefaultEmail.builder()         .from(new InternetAddress("cicero@mala-tempora.currunt", "Marco Tullio Cicerone "))         .to(Lists.newArrayList(new InternetAddress("titus@de-rerum.natura", "Pomponius Attĭcus")))         .subject("Laelius de amicitia")         .body("Firmamentum autem stabilitatis constantiaeque eius, quam in amicitia quaerimus, fides est.")         .encoding("UTF-8").build();     emailService.send(email); } ```   The previous code will send a plain text message. To obtain some more dynamic fancy emails, you have two options: _i)_ the former and easier-to-use is to use a templatefull module (e.g. based on Freemarker); _ii)_ the latter (which requires some effort on your side) needs an an implementation of the interface **`it.ozimov.springboot.templating.mail.service.TemplateService`**.  The aforementioned interface requires a component that implements the following method  ```java String mergeTemplateIntoString(String templateReference, Map<String, Object> model)             throws IOException, TemplateException; ```  Assuming you opted for one of the previous options, just put the template in the required folder  (e.g. ``templates`` under ``resourses``) and try to execute the following code (it works with _Freemarker_):  ```java @Autowired public EmailService emailService;  public void sendEmailWithTemplating(){    Arrays.asList(new Cospirator("cassius@sic-semper.tyrannis", "Gaius Cassius Longinus"),             new Cospirator("brutus@sic-semper.tyrannis", "Marcus Iunius Brutus Caepio"))         .stream.forEach(tyrannicida -> {        final Email email = DefaultEmail.builder()             .from(new InternetAddress("divus.iulius@mala-tempora.currunt", "Gaius Iulius Caesar"))             .to(Lists.newArrayList(new InternetAddress(tyrannicida.getEmail(), tyrannicida.getName())))             .subject("Idus Martii")             .body("")//Empty body             .encoding("UTF-8").build();         //Defining the model object for the given Freemarker template         final Map<String, Object> modelObject = new HashMap<>();         modelObject.put("tyrannicida", tyrannicida.getName());         emailService.send(email, "idus_martii.ftl", modelObject);    }; }  private static class Cospirator {   private String email;   private String name;   public Cospirator(final String email, final String name){     this.email = email;     this.name = name;   }    //getters } ```  where the template ``idus_martii.ftl`` is a _Freemarker_ file like:  ```html <!doctype html> <html> 	<body> 		<p> 			Tu quoque, <em>${tyrannicida}</em>! 		</p> 	</body> </html> ```   The following example shows how to send and email that includes an inline image.   ```java @Autowired public EmailService emailService;  public void sendEmailWithTemplatingAndInlineImage(){        final Email email = DefaultEmail.builder()             .from(new InternetAddress("divus.iulius@mala-tempora.currunt", "Gaius Iulius Caesar"))             .to(Lists.newArrayList(new InternetAddress("brutus@sic-semper.tyrannis", "Marcus Iunius Brutus Caepio")))             .subject("Idus Martii")             .body("")//Empty body             .encoding("UTF-8").build();        //Defining the model object for the given Freemarker template        final Map<String, Object> modelObject = new HashMap<>();        final File imageFile = //load your picture here, e.g. "my_image.jpg"        modelObject.put("tyrannicida", tyrannicida.getName());         final InlinePicture inlinePicture = DefaultInlinePicture.builder()                                .file(imageFile)                                .imageType(ImageType.JPG)                                .templateName("my_image.jpg").build());         emailService.send(email, "idus_martii.ftl", modelObject, inlinePicture); } ```  where the template ``idus_martii.ftl`` is a Freemarker file like:  ```html <!doctype html> <html> 	<body> 		<p> 			<img src="my_image.jpg" /> 		</p> 	</body> </html> ```  be sure that the name provided in the ``InlinePicture`` matches with the one used in the template file path included, if any was set. This means that if in the template you have ``<img src="images/my_image.jpg" />`` then the definition has to be changed as follows:  ```java final InlinePicture inlinePicture = DefaultInlinePicture.builder()         .file(imageFile)         .imageType(ImageType.JPG)         .templateName("images/my_image.jpg").build()); ```  This is required to set the a proper content-id.  ## Email scheduling  The library supports email scheduling, but since version _0.6.3_ the scheduler is disabled by default. To enable  email scheduling, the following property has to be provided:   ```properties spring.mail.scheduler.enabled=true ```  Email can be set in different queues, from the one with  highest priority to the least important. Priority 1 is the highest.  To define the number of priority levels to be used in the scheduler,  just add in the `application.properties` the following line:  ```properties spring.mail.scheduler.priorityLevels=5 ```  If not provided, by default 10 priority levels are considered.  Scheduling an email is actually easy and the `EmailSchedulerService` allows to schedule an email with or without the use of a template engine.  In order to schedule a plain text email, just create your service (or controller) where you autowire  the service `EmailSchedulerService` and call a method `scheduleEmail` defined as in the following example  ```java @Service public void MyEmailSenderService {      @Autowired     private EmailSchedulerService EmailSchedulerService;               public void scheduleEmail() throws CannotSendEmailException {         final Email mimeEmail = DefaultEmail.builder()                                   .from(new InternetAddress("divus.iulius@mala-tempora.currunt", "Gaius Iulius Caesar"))                                   .to(Lists.newArrayList(new InternetAddress(tyrannicida.getEmail(), tyrannicida.getName())))                                   .subject("Idus Martii")                                   .body("Sic semper...")                                   .encoding("UTF-8")                                   .build();         final OffsetDateTime scheduledDateTime = OffsetDateTime.now().plusDays(1);         final int priorityLevel = 1;       EmailSchedulerService.schedule(mimeEmail, scheduledDateTime, priorityLevel);     } } ```  Here we go, by calling schedulerEmail() an email has been scheduled to be sent after one day. When scheduling emails, observe that **`OffsetDateTime` must be** used with **UTC**, so do not forget to convert it if you use a different zone offset.  To schedule an email with a template and inline images, just call a new method called `scheduleEmailWithTemplate()`  ```java @Service public void MyEmailWithTemplateSenderService {      @Autowired     private EmailSchedulerService EmailSchedulerService;               public void scheduleEmailWithTemplate() throws CannotSendEmailException {         final Email mimeEmail = DefaultEmail.builder()                                   .from(new InternetAddress("divus.iulius@mala-tempora.currunt", "Gaius Iulius Caesar"))                                   .to(Lists.newArrayList(new InternetAddress(tyrannicida.getEmail(), tyrannicida.getName())))                                   .subject("Idus Martii")                                   .body("")//Empty body                                   .encoding("UTF-8")                                   .build();        //Defining the model object for the given Freemarker template        final Map<String, Object> modelObject = new HashMap<>();        final File imageFile = //load your picture here, e.g. "my_image.jpg"        modelObject.put("tyrannicida", tyrannicida.getName());         final InlinePicture inlinePicture = DefaultInlinePicture.builder()                                .file(imageFile)                                .imageType(ImageType.JPG)                                .templateName("my_image.jpg").build();         final OffsetDateTime scheduledDateTime = OffsetDateTime.now().plusDays(1);         final int priorityLevel = 1;                EmailSchedulerService.schedule(mimeEmail, scheduledDateTime, priorityLevel,              "idus_martii.ftl", modelObject, inlinePicture);     }      } ```  ## Persistence Persistence has been introduced in version `0.4.0`. Persistence is mainly of interest if the scheduler is used, therefore it can be enabled only if the scheduler is enabled.  The persistence layer is optional, thus needs to be activated. The default implementation is fully based on embedded REDIS. To enable the default persistence layer just add the additional properties in your `application.properties` file:  ```properties spring.mail.scheduler.persistence.enabled=true spring.mail.scheduler.persistence.redis.enabled=true spring.mail.scheduler.persistence.redis.embedded=true spring.mail.scheduler.persistence.redis.host=localhost spring.mail.scheduler.persistence.redis.port=6381 spring.mail.scheduler.persistence.redis.settings= ```  I recommend to specify in the REDIS settings at least the `appendfilename` and `dir` properties, so that you know where the append file is placed and which name it uses. For instance do:  ```properties spring.mail.scheduler.persistence.redis.settings=appendfilename email_appendonly.aof, dir /Users/your_username/Downloads ```  By default we have the setting `appendonly yes` and `appendfsync everysec`. Feel free to override them or fine tune them  according with your needs.   Clearly, you can provide your own persistence layer by implementing the `PersistenceService` interface. You can also  use your REDIS implementation, but this will require extra coding on your side.   Observe that the persistence layer makes the emails being stored to be reloaded on application startup if not yet sent. In particular, the emails are loaded when scheduler is constructed.   ###Impact of the Persistence layer on the default priority-based scheduler The default scheduler is `PriorityQueueEmailSchedulerService`, which by default stores everything in memory. Clerarly, having thousands email being scheduled, storing everything in memory could drive to a potential `OutOfMemoryException`.  Enabling the persistence layer should allow to use REDIS for persisting scheduled emails. Anyway, you may want to customize the behavior of the scheduler when interacting with the persistence layer, you can use the following params:  ```properties spring.mail.scheduler.persistence.desiredBatchSize=200 spring.mail.scheduler.persistence.minKeptInMemory=100 spring.mail.scheduler.persistence.maxKeptInMemory=1000 ```  The first defines the maximum amount of emails being loaded from the persistence layer when a slot is available in the priority queues; the second amount is the wish for the minimum amount of emails available in memory: the third defines  the amount of emails to be kept in memory. Clearly, these two values impact the response time of the scheduler.  The less you store in memory, the more it takes to send the next email. The smaller is the batch size, the higher the times you interact with the persistence layer.  ## Customize email logging Very often, you want to log the email that you just sent or scheduled, but you would like to avoid a full  `toString` of the given email object. For instance, you may want to anonymize an email address, or to ignore custom headers. Here follows a list of properties you can use with some examples:  ```properties spring.mail.logging.enabled=true  spring.mail.logging.strategy.from=PLAIN_TEXT spring.mail.logging.strategy.replyTo=HIDDEN spring.mail.logging.strategy.to=FULL_TEXT_FROM_COMMERCIAL_AT, spring.mail.logging.strategy.cc=HIDDEN spring.mail.logging.strategy.bcc=HIDDEN spring.mail.logging.strategy.subject=PLAIN_TEXT spring.mail.logging.strategy.body=FIRST_DOZEN_THEN_STARS spring.mail.logging.strategy.attachments=HIDDEN spring.mail.logging.strategy.encoding=HIDDEN spring.mail.logging.strategy.locale=HIDDEN spring.mail.logging.strategy.sentAt=STANDARD_DATE_FORMAT_WITH_ZONE_ID spring.mail.logging.strategy.receiptTo=HIDDEN spring.mail.logging.strategy.depositionNotificationTo=HIDDEN spring.mail.logging.strategy.ignore.customHeaders=true spring.mail.logging.strategy.ignore.nullAndEmptyCollections=true ```  Allowed logging strategies are defined in the enum `it.ozimov.springboot.mail.logging.LoggingStrategy`. Do not pretend to apply a date-only strategy to an email address, or an email address-only strategy to  a text field. Usage should be straightforward.  ## Future plans  See open issues.  **Any contribution is welcome (and warmly encouraged).**   ## License  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.  ## How to open an issue  Are you experiencing an issue? Please, post a question on _StackOverflow_ or open an issue on GitHub.  Issues that are not reporting a minimal set of info to reproduce the bug will be closed with no further comments.  Information that should be provided for investigations: - version used - pom.xml - application.properties - exception stacktrace - Are the provided examples run with success?   ==============================================  [![forthebadge](http://forthebadge.com/images/badges/built-by-developers.svg)](http://forthebadge.com) [![forthebadge](http://forthebadge.com/images/badges/built-with-love.svg)](http://forthebadge.com) [![forthebadge](http://forthebadge.com/images/badges/pretty-risque.svg)](http://forthebadge.com) [![forthebadge](http://forthebadge.com/images/badges/makes-people-smile.svg)](http://forthebadge.com)
thymeleaf/thymeleafexamples-springmail	Thymeleaf 3 examples: Spring Mail =================================  This is an example application showing how to compose and send dynamic e-mails using Spring and Thymeleaf.  With Thymeleaf you can compose text and HTML emails easily.  To learn more about Thymeleaf and download the latest version visit      http://www.thymeleaf.org  In order to run the application to should configure your SMTP server correctly. You can do this by modifying the values on `src/main/resources/configuration.properties` and `src/main/resources/javamail.properties`  You can deploy the application any Java servlet container or executing the application  on an embedded Tomcat 7 with `mvn tomcat7:run` (the application will be at http://localhost:8080/springmail/).
jcabi/jcabi-email	<img src="http://img.jcabi.com/logo-square.svg" width="64px" height="64px" />  [![Managed by Zerocracy](http://www.0crat.com/badge/C3RUBL5H9.svg)](http://www.0crat.com/p/C3RUBL5H9) [![DevOps By Rultor.com](http://www.rultor.com/b/jcabi/jcabi-email)](http://www.rultor.com/p/jcabi/jcabi-email)  [![Build Status](https://travis-ci.org/jcabi/jcabi-email.svg?branch=master)](https://travis-ci.org/jcabi/jcabi-email) [![PDD status](http://www.0pdd.com/svg?name=jcabi/jcabi-email)](http://www.0pdd.com/p?name=jcabi/jcabi-email) [![Build status](https://ci.appveyor.com/api/projects/status/ueunlwvuxyiws57s/branch/master?svg=true)](https://ci.appveyor.com/project/yegor256/jcabi-email/branch/master) [![Javadoc](https://javadoc-emblem.rhcloud.com/doc/com.jcabi/jcabi-email/badge.svg)](http://www.javadoc.io/doc/com.jcabi/jcabi-email)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.jcabi/jcabi-email/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.jcabi/jcabi-email) [![Dependencies](https://www.versioneye.com/user/projects/561ac442a193340f2f0011cb/badge.svg?style=flat)](https://www.versioneye.com/user/projects/561ac442a193340f2f0011cb)  More details are here: [email.jcabi.com](http://email.jcabi.com/). This article explains how this library was designed: [How Immutability Helps](http://www.yegor256.com/2014/11/07/how-immutability-helps.html).  It is an object-oriented email sending SDK for Java:  ```java Postman postman = new Postman.Default(   new SMTP(     new Token("user", "password").access(       new Protocol.SMTP("smtp.gmail.com", 587)     )   ) ); postman.send(   new Envelope.MIME()     .with(new StSender("Yegor Bugayenko <yegor@jcabi.com>"))     .with(new StRecipient("Jeff Lebowski", "jeff@gmail.com"))     .with(new StSubject("dude, how are you?"))     .with(new StBCC("my-boss@jcabi.com"))     .with(new EnPlain("Hi, long time no see! :) Check my pic!"))     .with(       new EnBinary(         new File("/tmp/picture.gif"),         "my-picture.gif",         "image/gif"       )     ) ); ```  Make sure you have this dependencies:  ``` <dependency>   <groupId>javax.mail</groupId>   <artifactId>mailapi</artifactId>   <version>1.4.3</version>   <scope>provided</scope> </dependency> <dependency>   <groupId>javax.mail</groupId>   <artifactId>mail</artifactId>   <version>1.5.0-b01</version>   <scope>runtime</scope> </dependency> ```  ## Questions?  If you have any questions about the framework, or something doesn't work as expected, please [submit an issue here](https://github.com/jcabi/jcabi-email/issues/new).  ## How to contribute?  Fork the repository, make changes, submit a pull request. We promise to review your changes same day and apply to the `master` branch, if they look correct.  Please run Maven build before submitting a pull request:  ``` $ mvn clean install -Pqulice ```
codylerum/simple-email	Simple Email is a continuation of the Seam 3 Mail Module.  Has been refactored to com.outjected.mail packaging and CDI integration removed for simplicity.
opentable/otj-pg-embedded	OpenTable Embedded PostgreSQL Component =======================================  Allows embedding PostgreSQL into Java application code with no external dependencies.  Excellent for allowing you to unit test with a "real" Postgres without requiring end users to install and set up a database cluster.  [![Build Status](https://travis-ci.org/opentable/otj-pg-embedded.svg)](https://travis-ci.org/opentable/otj-pg-embedded)  ## Basic Usage  In your JUnit test just add:  ```java @Rule public SingleInstancePostgresRule pg = EmbeddedPostgresRules.singleInstance(); ```  This simply has JUnit manage an instance of EmbeddedPostgres (start, stop). You can then use this to get a DataSource with: `pg.getEmbeddedPostgres().getPostgresDatabase();`    Additionally you may use the [`EmbeddedPostgres`](src/main/java/com/opentable/db/postgres/embedded/EmbeddedPostgres.java) class directly by manually starting and stopping the instance; see [`EmbeddedPostgresTest`](src/test/java/com/opentable/db/postgres/embedded/EmbeddedPostgresTest.java) for an example.  Default username/password is: postgres/postgres and the default database is 'postgres'  ## Flyway Migrator  You can easily integrate Flyway database schema migration:  ```java @Rule public PreparedDbRule db =     EmbeddedPostgresRules.preparedDatabase(         FlywayPreparer.forClasspathLocation("db/my-db-schema")); ```  This will create an independent database for every test with the given schema loaded from the classpath. Database templates are used so the time cost is relatively small, given the superior isolation truly independent databases gives you.  ## Postgres version  The JAR file contains bundled version of Postgres. You can pass different Postgres version by implementing [`PgBinaryResolver`](src/main/java/com/opentable/db/postgres/embedded/PgBinaryResolver.java).  Example: ```java class ClasspathBinaryResolver implements PgBinaryResolver {     public InputStream getPgBinary(String system, String machineHardware) throws IOException {         ClassPathResource resource = new ClassPathResource(format("pgsql/postgresql-%s-%s.tbz", system, machineHardware));         return resource.getInputStream();     } }  EmbeddedPostgreSQL             .builder()             .setPgBinaryResolver(new ClasspathBinaryResolver())             .start();  ```   ---- Copyright (C) 2017 OpenTable, Inc
hypergrid-inc/dchq-docker-java-example	<figure> <img src="http://www.hypergrid.com/wp-content/themes/hypergrid/img/logo.png" alt="" /> </figure>   To run & manage the **28** Java application templates in this project on 13 different clouds and virtualization platforms (including vSphere, OpenStack, AWS, Rackspace, Microsoft Azure, Google Compute Engine, DigitalOcean, IBM SoftLayer, etc.), make sure that you either: -   **Sign Up for FREE on HyperForm SaaS (formerly DCHQ)** -- <http://dchq.io> (no credit card required), or -   **Download HyperForm On-Premise Standard Edition (formerly DCHQ) for FREE** -- <http://dchq.co/dchq-on-premise-download.html>  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ)    Customize & Run all the published Docker Java application templates and many other templates (including multi-tier LAMP, LAPP, & LAOP stacks, Mongo Replica Set Cluster, Drupal, Wordpress, MEAN.JS, etc.)   **Table of Contents**    - [DCHQ - Docker Java Example ](#dchq---docker-java-example-) - [A Step by Step Guide for Deploying & Managing a Java Application on Docker-based WebSphere, JBoss, Tomcat and Jetty Application Servers](#dchq---docker-java-example-) 	- [Background](#background) 	- [Configuring the web.xml and webapp-config.xml files in the Java application](#configuring-the-webxml-and-webapp-configxml-files-in-the-java-application) 	- [Using the liquibase bean to initialize the connected database](#using-the-liquibase-bean-to-initialize-the-connected-database) 	- [Building the YAML-based application templates that can re-used on any Linux host running anywhere](#building-the-yaml-based-application-templates-that-can-re-used-on-any-linux-host-running-anywhere) 		- [Plug-ins to configure Web Servers and Application Servers at Request Time and Post Provision](#plug-ins-to-configure-web-servers-and-application-servers-at-request-time--post-provision) 		- [cluster_size and host Parameters for HA Deployment Across Multiple Hosts](#cluster_size-and-host-parameters-for-ha-deployment-across-multiple-hosts) 		- [Environment Variable Bindings Across Images](#environment-variable-bindings-across-images) 		- [3-Tier Java (Nginx – Tomcat – MySQL)](#3-tier-java-nginx--tomcat--mysql) 		- [3-Tier Java (Nginx – Jetty – MySQL)](#3-tier-java-nginx--jetty--mysql) 		- [3-Tier Java (Nginx – JBoss – MySQL)](#3-tier-java-nginx--jboss--mysql) 		- [2-Tier Java (WebSphere – MySQL)](#2-tier-java-websphere--mysql) 		- [3-Tier Java (Nginx – Tomcat – PostgreSQL)](#3-tier-java-nginx--tomcat--postgresql) 		- [3-Tier Java (Nginx – Jetty – PostgreSQL)](#3-tier-java-nginx--jetty--postgresql) 		- [3-Tier Java (Nginx – JBoss – PostgreSQL)](#3-tier-java-nginx--jboss--postgresql) 		- [2-Tier Java (WebSphere – PostgreSQL)](#2-tier-java-websphere--postgresql) 		- [3-Tier Java (Nginx – Tomcat – Oracle-XE)](#3-tier-java-nginx--tomcat--oracle-xe) 		- [3-Tier Java (Nginx – Jetty – Oracle-XE)](#3-tier-java-nginx--jetty--oracle-xe) 		- [3-Tier Java (Nginx – JBoss – Oracle-XE)](#3-tier-java-nginx--jboss--oracle-xe) 		- [2-Tier Java (WebSphere – Oracle-XE)](#2-tier-java-websphere--oracle-xe) 		- [3-Tier Java (Nginx – Tomcat – MariaDB)](#3-tier-java-nginx--tomcat--mariadb) 		- [3-Tier Java (Nginx – Jetty – MariaDB)](#3-tier-java-nginx--jetty--mariadb) 		- [3-Tier Java (Nginx – JBoss – MariaDB)](#3-tier-java-nginx--jboss--mariadb) 		- [2-Tier Java (WebSphere – MariaDB)](#2-tier-java-websphere--mariadb) 		- [3-Tier Java (ApacheHTTP – Tomcat – MySQL)](#3-tier-java-apachehttp--tomcat--mysql) 		- [3-Tier Java (ApacheHTTP – Jetty – MySQL)](#3-tier-java-apachehttp--jetty--mysql) 		- [3-Tier Java (ApacheHTTP – JBoss – MySQL)](#3-tier-java-apachehttp--jboss--mysql) 		- [3-Tier Java (ApacheHTTP – Tomcat – PostgreSQL)](#3-tier-java-apachehttp--tomcat--postgresql) 		- [3-Tier Java (ApacheHTTP – Jetty – PostgreSQL)](#3-tier-java-apachehttp--jetty--postgresql) 		- [3-Tier Java (ApacheHTTP – JBoss – PostgreSQL)](#3-tier-java-apachehttp--jboss--postgresql) 		- [3-Tier Java (ApacheHTTP – Tomcat – Oracle-XE)](#3-tier-java-apachehttp--tomcat--oracle-xe) 		- [3-Tier Java (ApacheHTTP – Jetty – Oracle-XE)](#3-tier-java-apachehttp--jetty--oracle-xe) 		- [3-Tier Java (ApacheHTTP – JBoss – Oracle-XE)](#3-tier-java-apachehttp--jboss--oracle-xe) 		- [3-Tier Java (ApacheHTTP – Tomcat – MariaDB)](#3-tier-java-apachehttp--tomcat--mariadb) 		- [3-Tier Java (ApacheHTTP – Jetty – MariaDB)](#3-tier-java-apachehttp--jetty--mariadb) 		- [3-Tier Java (ApacheHTTP – JBoss – MariaDB)](#3-tier-java-apachehttp--jboss--mariadb) 		- [Invoking a plug-in to initialize the database separately on a 3-Tier Java (Nginx – Tomcat – MySQL)](#invoking-a-plug-in-to-initialize-the-database-separately-on-a-3-tier-java-nginx--tomcat--mysql) 		- [Using your script or deployment plan](#using-your-script-or-deployment-plan) 	- [Provisioning & Auto-Scaling the Underlying Infrastructure on Any Cloud](#provisioning--auto-scaling-the-underlying-infrastructure-on-any-cloud) 	- [Deploying the Multi-Tier Java Application on the Rackspace Cluster](#deploying-the-multi-tier-java-application-on-the-rackspace-cluster) 	- [Accessing The In-Browser Terminal For The Running Containers](#accessing-the-in-browser-terminal-for-the-running-containers) 	- [Monitoring the CPU, Memory & I/O Utilization of the Running Containers](#monitoring-the-cpu-memory--io-utilization-of-the-running-containers) 	- [Enabling the Continuous Delivery Workflow with Jenkins to Update the WAR File of the Running Application when a Build is Triggered](#enabling-the-continuous-delivery-workflow-with-jenkins-to-update-the-war-file-of-the-running-application-when-a-build-is-triggered) 	- [Scaling out the Tomcat Application Server Cluster](#scaling-out-the-tomcat-application-server-cluster) 	- [Conclusion](#conclusion)    DCHQ - Docker Java Example  =========================== <figure> <img src="/screenshots/0-Names%20Directory%20Java%20App.png" alt="" /> </figure>     A Step by Step Guide for Deploying & Managing a Java Application on Docker-based WebSphere, JBoss, Tomcat and Jetty Application Servers =======================================================================================================================================    Background ----------  Containerizing enterprise Java applications is still a challenge mostly because existing application composition frameworks do not address complex dependencies, external integrations or auto-scaling workflows post-provision. Moreover, the ephemeral design of containers meant that developers had to spin up new containers and re-create the complex dependencies & external integrations with every version update.  DCHQ, available in hosted and on-premise versions, addresses all of these challenges and simplifies the containerization of enterprise Java applications through an advance application composition framework that extends Docker Compose with cross-image environment variable bindings, extensible BASH script plug-ins that can be invoked at request time or post-provision, and application clustering for high availability across multiple hosts or regions with support for auto scaling.  Once an application is provisioned, a user can monitor the CPU, Memory, & I/O of the running containers, get notifications & alerts, and get access to application backups, automatic scale in/out workflows, and plug-in execution workflows to update running containers. Moreover, out-of-box workflows that facilitate Continuous Delivery with Jenkins allow developers to refresh the Java WAR file of a running application without disrupting the existing dependencies & integrations.  In previous blogs, we demonstrated the end-to-end deployment automation of various Java applications (like Pizza Shop and Movie Store apps) on multi-tier Docker-based application stacks across 13 different clouds & virtualization platforms. For full list of these blogs, you can visit this page: <http://dchq.co/docker-java-applications.html>  However many users were still confused on some of the fundamental aspects of application modeling. These questions include:  -   Where do these **environment variables** come from in your YAML-based application template?  -   How is the **database initialized with the proper schemas** needed from my Java application?  -   I already have a deployment plan for my WebLogic Application Server. Can I **run my own script** for deploying a Java application?  To address these questions, we created a sample “Names Directory” Java application in this GitHub project that can be deployed on these application stacks:  -   Apache HTTP Server (httpd) and Nginx (for load balancing)  -   WebSphere, JBoss, Tomcat and Jetty (as the application server)  -   MySQL, PostgreSQL, and Oracle (for the database)  In this project, we will provide a step-by-step guide for configuring, deploying and managing this Java application using different application stacks and on different cloud/virtual infrastructure.  We will cover:  -   Configuring the web.xml and webapp-config.xml files in the Java application  -   Using the liquibase bean to initialize the connected database  -   Building the YAML-based application templates that can re-used on any Linux host running anywhere  -   Provisioning & auto-scaling the underlying infrastructure on any cloud (with Rackspace being the example in this blog)  -   Deploying the multi-tier Java application on the Rackspace cluster  -   Monitoring the CPU, Memory & I/O of the Running Containers  -   Enabling the Continuous Delivery Workflow with Jenkins to update the WAR file of the running applications when a build is triggered  -   Scaling out the Application Server Cluster when the application is resource-constrained     Configuring the web.xml and webapp-config.xml files in the Java application ---------------------------------------------------------------------------  You can clone this sample “Names Directory” Java application from GitHub.  **git clone** <https://github.com/dchqinc/dchq-docker-java-example.git>  This is the most important step in “Dockerizing” your Java application. In order to leverage the environment variables you can pass when running containers, you will need to make sure that your application is configured in a way that will allow you to change certain properties at request time – like:  -   The database driver you would like to use  -   The database URL  -   The database credentials  -   Any other parameters that you would like to change at request time (e.g. the min/max connection pool size, idle timeout, etc.)  To achieve this, you will first need to configure **web.xml** to use the bootstrap Servlet to start up the Spring context.  <https://github.com/dchqinc/dchq-docker-java-example/blob/master/src/main/webapp/WEB-INF/web.xml>  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     <servlet>         <servlet-name>DispatcherServlet</servlet-name>         <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>         <init-param>             <param-name>contextConfigLocation</param-name>             <param-value>/WEB-INF/spring/webapp-config.xml</param-value>         </init-param>         <load-on-startup>1</load-on-startup>     </servlet>  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  You will notice that the **contextConfigLocation** is referencing **/WEB-INF/spring/webapp-config.xml**  Next, we will need to configure parameters in the **webapp-config.xml** file to reference host environment variables that will be passed at request time.  <https://github.com/dchqinc/dchq-docker-java-example/blob/master/src/main/webapp/WEB-INF/spring/webapp-config.xml>  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     <bean id="dataSource" class="snaq.db.DBPoolDataSource" destroy-method="release">         <property name="driverClassName" value="${database_driverClassName}"/>         <property name="url" value="${database_url}"/>         <property name="user" value="${database_username}"/>         <property name="password" value="${database_password}"/>         <property name="minPool" value="1"/>         <property name="maxPool" value="10"/>         <property name="maxSize" value="10"/>         <property name="idleTimeout" value="60"/>     </bean> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  You will notice that specific dataSource properties are referencing the following environment variables that will be passed on at request time:  -   **database_driverClassName**  -   **database_url**  -   **database_username**  -   **database_password**     Using the liquibase bean to initialize the connected database -------------------------------------------------------------  We typically recommend initializing the database schema as part of the Java application deployment itself. This way, you don’t have to worry about maintaining separate SQL files that need to be executed on the database separately.  However if you already have those SQL files and you still prefer executing them on the database separately – then DCHQ can help you automate this process through its plug-in framework. You can refer to this <a href=#invoking-a-plug-in-to-initialize-the-database-separately-on-a-3-tier-java-nginx--tomcat--mysql>section</a> for more information.  In order to include the SQL scripts for creating the database tables in the Java application, you will need to configure **webapp-config.xml** file to use **liquibase** bean that checks the dataSource and runs new statements from upgrade.sql. Liquibase tracks which changelog statements have run against each database.  <https://github.com/dchqinc/dchq-docker-java-example/blob/master/src/main/webapp/WEB-INF/spring/webapp-config.xml>  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     <bean id="liquibase" class="liquibase.integration.spring.SpringLiquibase">         <property name="dataSource" ref="dataSource" />         <property name="changeLog" value="/WEB-INF/upgrade/upgrade.sql" />     </bean> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Here’s the actual upgrade.sql file with the SQL statements for initializing the schema on the connected MySQL, PostgreSQL or Oracle database.  <https://github.com/dchqinc/dchq-docker-java-example/blob/master/src/main/webapp/WEB-INF/upgrade/upgrade.sql>  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --liquibase formatted sql  --changeset admin:1 dbms:mysql CREATE TABLE IF NOT EXISTS `NameDirectory` (     `id` INT(10) UNSIGNED NOT NULL AUTO_INCREMENT,     `firstName` VARCHAR(50) NOT NULL,     `lastName` VARCHAR(50) NOT NULL,     `createdTimestamp` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,     PRIMARY KEY (`id`) ) ENGINE=InnoDB;  --changeset admin:1 dbms:postgresql CREATE TABLE "NameDirectory" (     id SERIAL NOT NULL,     "firstName" VARCHAR(50) NOT NULL,     "lastName" VARCHAR(50) NOT NULL,     "createdTimestamp" TIMESTAMP(0) WITHOUT TIME ZONE DEFAULT timestamp 'now ()' NOT NULL,     PRIMARY KEY(id) )     WITH (oids = false);  --changeset admin:1 dbms:oracle CREATE TABLE NameDirectory (     id NUMBER(10) NOT NULL,     firstName VARCHAR2(50) NOT NULL,     lastName VARCHAR2(50) NOT NULL,     createdTimestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,     CONSTRAINT id_pk PRIMARY KEY (id) ); CREATE SEQUENCE NameDirectory_seq; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Building the YAML-based application templates that can re-used on any Linux host running anywhere -------------------------------------------------------------------------------------------------  Once logged in to DCHQ (either the hosted DCHQ.io or on-premise version), a user can navigate to **Manage** > **Templates** and then click on the **+** button to create a new **Docker Compose** template.  We have created **28 application templates** using the **official images from Docker Hub** for the same “Names Directory” Java application – but for different application servers and databases.  The templates include examples of the following application stacks (for the same Java application): -   **Apache HTTP Server (httpd) & Nginx** -- for load balancing -   **Tomcat, Jetty, WebSphere and JBoss** -- for the application servers -   **MySQL, MariaDB, PostgreSQL and Oracle XE** -- for the databases  ### Plug-ins to Configure Web Servers and Application Servers at **Request Time & Post-Provision**  Across all these application templates, you will notice that some of the containers are invoking BASH script plug-ins at request time in order to configure the container. These plug-ins can be executed post-provision as well.  These plug-ins can be created by navigating to **Manage > Plug-ins**. Once the BASH script is provided, the DCHQ agent will execute this script **inside the container**. A user can specify arguments that can be overridden at request time and post-provision. Anything preceded by the **$** sign is considered an argument -- for example, **$file_url** can be an argument that allows developers to specify the download URL for a WAR file. This can be overridden at request time and post-provision when a user wants to refresh the Java WAR file on a running container.  The plug-in ID needs to be provided when defining the YAML-based application template. For example, to invoke a BASH script plug-in for Nginx, we would reference the plug-in ID as follows: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  In the example templates, we are invoking 4 BASH script plug-ins.  **Nginx** is invoking a BASH script plug-in that injects container IP’s of the application servers in the default.conf file dynamically (or at request time). The plug-in ID is **0H1Nk**.  **Apache HTTP Server (httpd)** is invoking a BASH script plug-in that injects container IP’s of the application servers in the httpd.conf file dynamically (or at request time). The plug-in ID is **uazUi**.  The beauty of the Nginx and Apache HTTP Server (httpd) plug-ins is that they can be executed post-provision as part of the application server cluster scale in or scale out. This makes it possible to define auto-scale policies that automatically update the web server (or load balancer).  To get access to the Nginx and Apache HTTP Server (httpd) plug-ins under the EULA license, make sure you either: -   **Sign Up for FREE on DCHQ.io** -- <http://dchq.io> (no credit card required) -   **Download DCHQ On-Premise Standard Edition for FREE** -- <http://dchq.co/dchq-on-premise-download.html>  The application servers (Tomcat, Jetty, JBoss and WebSphere) are also invoking a BASH script plug-in to deploy the Java WAR file from the accessible GitHub URL.  <https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war>  **Tomcat, JBoss and Jetty** are invoking the exact same BASH script plug-in (plug-in ID: **oncXN**) – except the WAR file is getting deployed on different directories:  -   Tomcat – dir=/usr/local/tomcat/webapps/ROOT.war  -   Jetty – dir=/var/lib/jetty/webapps/ROOT.war  -   JBoss – dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war  The BASH script plug-in was created by navigating to **Manage** > **Plug-ins** and looks something like this:  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #!/bin/bash  # The sleep command is used to wait for Oracle to fully initialize sleep 10; rm –rf $delete_dir curl -L -o $dir $file_url ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  **$delete_dir**, **$dir** and **$file_url** are overrideable arguments that can be defined when creating the plug-ins or when requesting the application.  <figure> <img src="screenshots/0-Java%20WAR%20File%20Deployment%20Plug-in.png"  /> </figure>  **WebSphere** is invoking a different BASH script plug-in (plug-in ID: **rPuVb**) that will first execute init-server-env.sh and then deploy the Java WAR file from the accessible GitHub URL  <https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war>  <https://github.com/dchqinc/dchq-docker-java-example/blob/master/websphere-config/init-server-env.sh>  The Java WAR file will be deployed on this directory:  -   WebSphere– dir= /opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect.war  The BASH script plug-in was created by navigating to **Manage** > **Plug-ins** and looks something like this:  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #!/bin/bash  echo database_driverClassName=$database_driverClassName >> /opt/ibm/wlp/usr/servers/defaultServer/server.env echo database_url=$database_url >> /opt/ibm/wlp/usr/servers/defaultServer/server.env echo database_username=$database_username >> /opt/ibm/wlp/usr/servers/defaultServer/server.env echo database_password=$database_password >> /opt/ibm/wlp/usr/servers/defaultServer/server.env  rm -rf $delete_dir wget $file_url -O $dir ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  **$delete_dir**, **$dir** and **$file_url** are overrideable arguments that can be defined when creating the plug-ins or when requesting the application.  ### **cluster_size** and **host** parameters for HA deployment across multiple hosts  You will notice that the **cluster_size** parameter allows you to specify the number of containers to launch (with the same application dependencies).  The **host** parameter allows you to specify the host you would like to use for container deployments. This is possible if you have selected **Weave** as the networking layer when creating your clusters. That way you can ensure high-availability for your application server clusters across different hosts (or regions) and you can comply with affinity rules to ensure that the database runs on a separate host for example. Here are the values supported for the host parameter:  -   *host1, host2, host3*, etc. – selects a host randomly within a data-center (or cluster) for container deployments  -   *IP Address 1, IP Address 2, etc.* -- allows a user to specify the actual IP addresses to use for container deployments  -   *Hostname 1, Hostname 2, etc.* -- allows a user to specify the actual hostnames to use for container deployments  -   *Wildcards* (e.g. “db-*”, or “app-srv-*”) – to specify the wildcards to use within a hostname  ### Environment Variable Bindings Across Images  Additionally, a user can create cross-image environment variable bindings by making a reference to another image’s environment variable. In this case, we have made several bindings – including database_url=jdbc:mysql://{{MySQL|container_ip}}:3306/{{MySQL|MYSQL_DATABASE}} – in which the database container IP is resolved dynamically at request time and is used to ensure that the application servers can establish a connection with the database.  Here is a list of supported environment variable values:  -   **{{alphanumeric | 8}}** – creates a random 8-character alphanumeric string. This is most useful for creating random passwords.  -   **{{Image Name | ip}}** – allows you to enter the host IP address of a container as a value for an environment variable. This is most useful for allowing the middleware tier to establish a connection with the database.  -   **{{Image Name | container_ip}}** – allows you to enter the name of a container as a value for an environment variable. This is most useful for allowing the middleware tier to establish a secure connection with the database (without exposing the database port).  -   **{{Image Name | container_private_ip}}** – allows you to enter the internal IP of a container as a value for an environment variable. This is most useful for allowing the middleware tier to establish a secure connection with the database (without exposing the database port).  -   **{{Image Name | port_Port Number}}** – allows you to enter the Port number of a container as a value for an environment variable. This is most useful for allowing the middleware tier to establish a connection with the database. In this case, the port number specified needs to be the internal port number – i.e. not the external port that is allocated to the container. For example, {{PostgreSQL | port_5432}} will be translated to the actual external port that will allow the middleware tier to establish a connection with the database.  -   **{{Image Name | Environment Variable Name}}** – allows you to enter the value an image’s environment variable into another image’s environment variable. The use cases here are endless – as most multi-tier applications will have cross-image dependencies.     ### 3-Tier Java (Nginx – Tomcat – MySQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514af61cd74c28)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: tomcat:8.0.21-jre8   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MySQL|container_ip}}:3306/{{MySQL|MYSQL_DATABASE}}     - database_username={{MySQL|MYSQL_USER}}     - database_password={{MySQL|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/usr/local/tomcat/webapps/ROOT.war         - delete_dir=/usr/local/tomcat/webapps/ROOT MySQL:   image: mysql:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – Jetty – MySQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514b8ddc2d697d)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: jetty:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MySQL|container_ip}}:3306/{{MySQL|MYSQL_DATABASE}}     - database_username={{MySQL|MYSQL_USER}}     - database_password={{MySQL|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/var/lib/jetty/webapps/ROOT.war         - delete_dir=/var/lib/jetty/webapps/ROOT MySQL:   image: mysql:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – JBoss – MySQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514b9014df69b1)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: jboss/wildfly:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MySQL|container_ip}}:3306/{{MySQL|MYSQL_DATABASE}}     - database_username={{MySQL|MYSQL_USER}}     - database_password={{MySQL|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war         - delete_dir=/opt/jboss/wildfly/standalone/deployments/ROOT MySQL:   image: mysql:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 2-Tier Java (WebSphere – MySQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514b93e3ab6a77)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AppServer:   image: websphere-liberty:webProfile6   publish_all: true   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MySQL|container_ip}}:3306/{{MySQL|MYSQL_DATABASE}}     - database_username={{MySQL|MYSQL_USER}}     - database_password={{MySQL|MYSQL_ROOT_PASSWORD}}     - LICENSE=accept   plugins:     - !plugin       id: rPuVb       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect.war         - delete_dir=/opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect MySQL:   image: mysql:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – Tomcat – PostgreSQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514bbf6cb872db)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: tomcat:8.0.21-jre8   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=org.postgresql.Driver     - database_url=jdbc:postgresql://{{Postgres|container_ip}}:5432/{{Postgres|POSTGRES_DB}}     - database_username={{Postgres|POSTGRES_USER}}     - database_password={{Postgres|POSTGRES_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/usr/local/tomcat/webapps/ROOT.war         - delete_dir=/usr/local/tomcat/webapps/ROOT Postgres:   image: postgres:latest   host: host1   mem_min: 400m   environment:     - POSTGRES_USER=root     - POSTGRES_PASSWORD={{alphanumeric | 8}}     - POSTGRES_DB=names ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – Jetty – PostgreSQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514bc268ad7365)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: jetty:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=org.postgresql.Driver     - database_url=jdbc:postgresql://{{Postgres|container_ip}}:5432/{{Postgres|POSTGRES_DB}}     - database_username={{Postgres|POSTGRES_USER}}     - database_password={{Postgres|POSTGRES_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/var/lib/jetty/webapps/ROOT.war         - delete_dir=/var/lib/jetty/webapps/ROOT Postgres:   image: postgres:latest   host: host1   mem_min: 400m   environment:     - POSTGRES_USER=root     - POSTGRES_PASSWORD={{alphanumeric | 8}}     - POSTGRES_DB=names ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – JBoss – PostgreSQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514bc3373a7395)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: jboss/wildfly:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=org.postgresql.Driver     - database_url=jdbc:postgresql://{{Postgres|container_ip}}:5432/{{Postgres|POSTGRES_DB}}     - database_username={{Postgres|POSTGRES_USER}}     - database_password={{Postgres|POSTGRES_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war         - delete_dir=/opt/jboss/wildfly/standalone/deployments/ROOT Postgres:   image: postgres:latest   host: host1   mem_min: 400m   environment:     - POSTGRES_USER=root     - POSTGRES_PASSWORD={{alphanumeric | 8}}     - POSTGRES_DB=names ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 2-Tier Java (WebSphere – PostgreSQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514bc4a86c73e9)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AppServer:   image: websphere-liberty:webProfile6   publish_all: true   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=org.postgresql.Driver     - database_url=jdbc:postgresql://{{Postgres|container_ip}}:5432/{{Postgres|POSTGRES_DB}}     - database_username={{Postgres|POSTGRES_USER}}     - database_password={{Postgres|POSTGRES_PASSWORD}}     - LICENSE=accept   plugins:     - !plugin       id: rPuVb       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect.war         - delete_dir=/opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect Postgres:   image: postgres:latest   host: host1   mem_min: 400m   environment:     - POSTGRES_USER=root     - POSTGRES_PASSWORD={{alphanumeric | 8}}     - POSTGRES_DB=names ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – Tomcat – Oracle-XE)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514be51c9c7a67)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: tomcat:8.0.21-jre8   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=oracle.jdbc.OracleDriver     - database_url=jdbc:oracle:thin:@//{{Oracle|container_ip}}:1521/{{Oracle|sid}}     - database_username={{Oracle|username}}     - database_password={{Oracle|password}}     - TZ=UTC   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/usr/local/tomcat/webapps/ROOT.war         - delete_dir=/usr/local/tomcat/webapps/ROOT Oracle:   image: wnameless/oracle-xe-11g:latest   host: host1   mem_min: 400m   environment:     - username=system     - password=oracle     - sid=xe ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – Jetty – Oracle-XE)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514c2466530645)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: jetty:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=oracle.jdbc.OracleDriver     - database_url=jdbc:oracle:thin:@//{{Oracle|container_ip}}:1521/{{Oracle|sid}}     - database_username={{Oracle|username}}     - database_password={{Oracle|password}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/var/lib/jetty/webapps/ROOT.war         - delete_dir=/var/lib/jetty/webapps/ROOT Oracle:   image: wnameless/oracle-xe-11g:latest   host: host1   mem_min: 400m   environment:     - username=system     - password=oracle     - sid=xe ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – JBoss – Oracle-XE)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514c23b4c00610)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: jboss/wildfly:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=oracle.jdbc.OracleDriver     - database_url=jdbc:oracle:thin:@//{{Oracle|container_ip}}:1521/{{Oracle|sid}}     - database_username={{Oracle|username}}     - database_password={{Oracle|password}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war         - delete_dir=/opt/jboss/wildfly/standalone/deployments/ROOT Oracle:   image: wnameless/oracle-xe-11g:latest   host: host1   mem_min: 400m   environment:     - username=system     - password=oracle     - sid=xe ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 2-Tier Java (WebSphere – Oracle-XE)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101514c262d7e069b)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AppServer:   image: websphere-liberty:webProfile6   publish_all: true   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=oracle.jdbc.OracleDriver     - database_url=jdbc:oracle:thin:@//{{Oracle|container_ip}}:1521/{{Oracle|sid}}     - database_username={{Oracle|username}}     - database_password={{Oracle|password}}     - LICENSE=accept   plugins:     - !plugin       id: rPuVb       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect.war         - delete_dir=/opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect Oracle:   image: wnameless/oracle-xe-11g:latest   host: host1   mem_min: 400m   environment:     - username=system     - password=oracle     - sid=xe ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – Tomcat – MariaDB)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e1015151af25a8760d)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: tomcat:8.0.21-jre8   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MariaDB|container_ip}}:3306/{{MariaDB|MYSQL_DATABASE}}     - database_username={{MariaDB|MYSQL_USER}}     - database_password={{MariaDB|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/usr/local/tomcat/webapps/ROOT.war         - delete_dir=/usr/local/tomcat/webapps/ROOT MariaDB:   image: mariadb:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – Jetty – MariaDB)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e1015155d0fdc729bc)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: jetty:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MariaDB|container_ip}}:3306/{{MariaDB|MYSQL_DATABASE}}     - database_username={{MariaDB|MYSQL_USER}}     - database_password={{MariaDB|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/var/lib/jetty/webapps/ROOT.war         - delete_dir=/var/lib/jetty/webapps/ROOT MariaDB:   image: mariadb:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (Nginx – JBoss – MariaDB)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e1015155d1eca129f9)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LB:   image: nginx:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: 0H1Nk       restart: true       arguments:         - servers=server {{AppServer | container_ip}}:8080; AppServer:   image: jboss/wildfly:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MariaDB|container_ip}}:3306/{{MariaDB|MYSQL_DATABASE}}     - database_username={{MariaDB|MYSQL_USER}}     - database_password={{MariaDB|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war         - delete_dir=/opt/jboss/wildfly/standalone/deployments/ROOT MariaDB:   image: mariadb:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 2-Tier Java (WebSphere – MariaDB)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e1015155d1795129e6)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ AppServer:   image: websphere-liberty:webProfile6   publish_all: true   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MariaDB|container_ip}}:3306/{{MariaDB|MYSQL_DATABASE}}     - database_username={{MariaDB|MYSQL_USER}}     - database_password={{MariaDB|MYSQL_ROOT_PASSWORD}}     - LICENSE=accept   plugins:     - !plugin       id: rPuVb       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect.war         - delete_dir=/opt/ibm/wlp/usr/servers/defaultServer/dropins/dbconnect MariaDB:   image: mariadb:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – Tomcat – MySQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e10151562a98123993)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: tomcat:8.0.21-jre8   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MySQL|container_ip}}:3306/{{MySQL|MYSQL_DATABASE}}     - database_username={{MySQL|MYSQL_USER}}     - database_password={{MySQL|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/usr/local/tomcat/webapps/ROOT.war         - delete_dir=/usr/local/tomcat/webapps/ROOT MySQL:   image: mysql:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – Jetty – MySQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e10151563144c73aca)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: jetty:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MySQL|container_ip}}:3306/{{MySQL|MYSQL_DATABASE}}     - database_username={{MySQL|MYSQL_USER}}     - database_password={{MySQL|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/var/lib/jetty/webapps/ROOT.war         - delete_dir=/var/lib/jetty/webapps/ROOT MySQL:   image: mysql:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – JBoss – MySQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e10151562c890139f4)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: jboss/wildfly:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MySQL|container_ip}}:3306/{{MySQL|MYSQL_DATABASE}}     - database_username={{MySQL|MYSQL_USER}}     - database_password={{MySQL|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war         - delete_dir=/opt/jboss/wildfly/standalone/deployments/ROOT MySQL:   image: mysql:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – Tomcat – PostgreSQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e10151562ceb453a12)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: tomcat:8.0.21-jre8   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=org.postgresql.Driver     - database_url=jdbc:postgresql://{{Postgres|container_ip}}:5432/{{Postgres|POSTGRES_DB}}     - database_username={{Postgres|POSTGRES_USER}}     - database_password={{Postgres|POSTGRES_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/usr/local/tomcat/webapps/ROOT.war         - delete_dir=/usr/local/tomcat/webapps/ROOT Postgres:   image: postgres:latest   host: host1   mem_min: 400m   environment:     - POSTGRES_USER=root     - POSTGRES_PASSWORD={{alphanumeric | 8}}     - POSTGRES_DB=names ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – Jetty – PostgreSQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e10151562b5a9e39b5)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: jetty:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=org.postgresql.Driver     - database_url=jdbc:postgresql://{{Postgres|container_ip}}:5432/{{Postgres|POSTGRES_DB}}     - database_username={{Postgres|POSTGRES_USER}}     - database_password={{Postgres|POSTGRES_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/var/lib/jetty/webapps/ROOT.war         - delete_dir=/var/lib/jetty/webapps/ROOT Postgres:   image: postgres:latest   host: host1   mem_min: 400m   environment:     - POSTGRES_USER=root     - POSTGRES_PASSWORD={{alphanumeric | 8}}     - POSTGRES_DB=names ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – JBoss – PostgreSQL)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e10151563299393b0c)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: jboss/wildfly:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=org.postgresql.Driver     - database_url=jdbc:postgresql://{{Postgres|container_ip}}:5432/{{Postgres|POSTGRES_DB}}     - database_username={{Postgres|POSTGRES_USER}}     - database_password={{Postgres|POSTGRES_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war         - delete_dir=/opt/jboss/wildfly/standalone/deployments/ROOT Postgres:   image: postgres:latest   host: host1   mem_min: 400m   environment:     - POSTGRES_USER=root     - POSTGRES_PASSWORD={{alphanumeric | 8}}     - POSTGRES_DB=names ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – Tomcat – Oracle-XE)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101515634ee4b3b5a)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: tomcat:8.0.21-jre8   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=oracle.jdbc.OracleDriver     - database_url=jdbc:oracle:thin:@//{{Oracle|container_ip}}:1521/{{Oracle|sid}}     - database_username={{Oracle|username}}     - database_password={{Oracle|password}}     - TZ=UTC   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/usr/local/tomcat/webapps/ROOT.war         - delete_dir=/usr/local/tomcat/webapps/ROOT Oracle:   image: wnameless/oracle-xe-11g:latest   host: host1   mem_min: 400m   environment:     - username=system     - password=oracle     - sid=xe ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – Jetty – Oracle-XE)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e101515635760c3b72)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: jetty:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=oracle.jdbc.OracleDriver     - database_url=jdbc:oracle:thin:@//{{Oracle|container_ip}}:1521/{{Oracle|sid}}     - database_username={{Oracle|username}}     - database_password={{Oracle|password}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/var/lib/jetty/webapps/ROOT.war         - delete_dir=/var/lib/jetty/webapps/ROOT Oracle:   image: wnameless/oracle-xe-11g:latest   host: host1   mem_min: 400m   environment:     - username=system     - password=oracle     - sid=xe ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – JBoss – Oracle-XE)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e10151563602bc3b99)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: jboss/wildfly:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=oracle.jdbc.OracleDriver     - database_url=jdbc:oracle:thin:@//{{Oracle|container_ip}}:1521/{{Oracle|sid}}     - database_username={{Oracle|username}}     - database_password={{Oracle|password}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war         - delete_dir=/opt/jboss/wildfly/standalone/deployments/ROOT Oracle:   image: wnameless/oracle-xe-11g:latest   host: host1   mem_min: 400m   environment:     - username=system     - password=oracle     - sid=xe ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – Tomcat – MariaDB)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e1015154e61e5800c6)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: tomcat:8.0.21-jre8   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MariaDB|container_ip}}:3306/{{MariaDB|MYSQL_DATABASE}}     - database_username={{MariaDB|MYSQL_USER}}     - database_password={{MariaDB|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/usr/local/tomcat/webapps/ROOT.war         - delete_dir=/usr/local/tomcat/webapps/ROOT MariaDB:   image: mariadb:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – Jetty – MariaDB)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e10151562a1c033986)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: jetty:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MariaDB|container_ip}}:3306/{{MariaDB|MYSQL_DATABASE}}     - database_username={{MariaDB|MYSQL_USER}}     - database_password={{MariaDB|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/var/lib/jetty/webapps/ROOT.war         - delete_dir=/var/lib/jetty/webapps/ROOT MariaDB:   image: mariadb:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### 3-Tier Java (ApacheHTTP – JBoss – MariaDB)  [![Customize and Run](https://dl.dropboxusercontent.com/u/4090128/dchq-customize-and-run.png)](https://www.dchq.io/landing/products.html#/library?org=DCHQ&bl=2c91801a510732e1015156291f713964)  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ HTTP-LB:   image: httpd:latest   publish_all: true   mem_min: 50m   host: host1   plugins:     - !plugin       id: uazUi       restart: true       arguments:         - BalancerMembers=BalancerMember http://{{AppServer | container_ip}}:8080 AppServer:   image: jboss/wildfly:latest   mem_min: 600m   host: host1   cluster_size: 1   environment:     - database_driverClassName=com.mysql.jdbc.Driver     - database_url=jdbc:mysql://{{MariaDB|container_ip}}:3306/{{MariaDB|MYSQL_DATABASE}}     - database_username={{MariaDB|MYSQL_USER}}     - database_password={{MariaDB|MYSQL_ROOT_PASSWORD}}   plugins:     - !plugin       id: oncXN       restart: true       arguments:         - file_url=https://github.com/dchqinc/dchq-docker-java-example/raw/master/dbconnect.war         - dir=/opt/jboss/wildfly/standalone/deployments/ROOT.war         - delete_dir=/opt/jboss/wildfly/standalone/deployments/ROOT MariaDB:   image: mariadb:latest   host: host1   mem_min: 400m   environment:     - MYSQL_USER=root     - MYSQL_DATABASE=names     - MYSQL_ROOT_PASSWORD={{alphanumeric|8}} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     ### Invoking a plug-in to initialize the database separately on a 3-Tier Java (Nginx – Tomcat – MySQL)  We recommend initializing the database schema as part of the Java WAR file deployment itself. However if you still prefer executing the SQL files on the database separately – then DCHQ can help you automate this process through its plug-in framework.  In this example, MySQL in this 3-Tier application is invoking a BASH script plug-in to execute the upgrade.sql file. The BASH script plug-in was created by navigating to **Manage > Plug-ins** and looks something like this:  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #!/bin/bash  apt-get update apt-get -y install wget cd / wget $file_url /usr/bin/mysql -u $MYSQL_USER -p$MYSQL_ROOT_PASSWORD -h127.0.0.1 $MYSQL_DATABASE < /upgrade.sql ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  In this BASH script plug-in, **$MYSQL_USER**, **$MYSQL_ROOT_PASSWORD**, and **$MYSQL_DATABASE** are environment variables that are passed at request time.  **$file_url** is an overrideable argument that you can define when creating the plug-in or when requesting the application. For example, this could be the upgrade.sql file from GitHub:  <https://github.com/dchqinc/dchq-docker-java-example/raw/master/src/main/webapp/WEB-INF/upgrade/upgrade.sql>     ### Using your script or deployment plan  If you’re deploying your application on Oracle WebLogic Application Server and you would like to use your own deployment plan or custom Python script, then you can easily create a BASH script plug-in by navigating to **Manage** > **Plug-ins**. The plug-in for WebLogic may look something like this:  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #!/bin/bash  cd /oracle/fmwhome/wlst_custom/ wget $deploy-python-url wget $deploy_app-sh-url wget $war-file-url  chmod +x deploy_app.sh ./deploy_app.sh ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  **$deploy-python-url**, **$deploy_app-sh-url**, and **$war-file-url** are overrideable arguments that you can define when creating the plug-in or when requesting the application.  -   **$deploy-python-url**=(URL location for deploy.py script)  -   **$deploy_app-sh-url**=(URL location for deploy_app.sh file), and  -   **$war-file-url**=(URL location for the Java WAR file)     Provisioning & Auto-Scaling the Underlying Infrastructure on Any Cloud ----------------------------------------------------------------------  Once an application is saved, a user can register a Cloud Provider to automate the provisioning and auto-scaling of clusters on 12 different cloud end-points including VMware vSphere, OpenStack, CloudStack, Amazon Web Services, Rackspace, Microsoft Azure, DigitalOcean, IBM SoftLayer, Google Compute Engine, and many others.  First, a user can register a Cloud Provider for Rackspace (for example) by navigating to **Manage** > **Cloud Providers & Repos** and then clicking on the **+** button to select **Rackspace**. The Rackspace API Key needs to be provided – which can be retrieved from the Account Settings section of the Rackspace Cloud Control Panel.  <figure> <img src="screenshots/0-Rackspace%20Cloud%20Provider.png"  /> </figure>  A user can then create a cluster with an auto-scale policy to automatically spin up new Cloud Servers. This can be done by navigating to **Manage** > **Clusters** page and then clicking on the **+** button. You can select a capacity-based placement policy and then **Weave** as the networking layer in order to facilitate secure, password-protected cross-container communication across multiple hosts within a cluster. The **Auto-Scale Policy** for example, may set the maximum number of VM’s (or Cloud Servers) to 10.  <figure> <img src="screenshots/0-Rackspace%20Cluster.png"  /> </figure>  A user can now provision a number of Cloud Servers on the newly created cluster either through the UI-based workflow or by defining a simple YAML-based Machine Compose template that can be requested from the Self-Service Library.  **UI-based Workflow** – A user can request Rackspace Cloud Servers by navigating to **Manage** > **Hosts** and then clicking on the **+** button to select **Rackspace**. Once the Cloud Provider is selected, a user can select the region, size and image needed. Ports are opened by default on Rackspace Cloud Servers to accommodate some of the port requirements (e.g. 32000-59000 for Docker, 6783 for Weave, and 5672 for RabbitMQ). A Cluster is then selected and the number of Cloud Servers can be specified.  <figure> <img src="screenshots/0-Rackspace%20Cloud%20Server%20UI-based%20Request.png"  /> </figure>  **YAML-based Machine Compose Template** – A user can first create a Machine Compose template for Rackspace by navigating to **Manage** > **Templates** and then selecting **Machine Compose**.  Here’s the template for requesting a 4GB Cloud Server.  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Medium:   region: IAD   description: Rackspace small instance   instanceType: general1-4   image: IAD/5ed162cc-b4eb-4371-b24a-a0ae73376c73   count: 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  <figure> <img src="screenshots/0-Rackspace%20Cloud%20Server%20Machine%20Compose%20Template.png"  /> </figure>  The supported parameters for the Machine Compose template are summarized below:  -   **description**: Description of the blueprint/template  -   **instanceType**: Cloud provider specific value (e.g. general1-4)  -   **region**: Cloud provider specific value (e.g. IAD)  -   **image**: Mandatory - fully qualified image ID/name (e.g. IAD/5ed162cc-b4eb-4371-b24a-a0ae73376c73 or vSphere VM Template name)  -   **username**: Optional - only for vSphere VM Template username  -   **password**: Optional - only for vSphere VM Template encrypted password. You can encrypt the password using the endpoint <https://www.dchq.io/#/encrypt>  -   **network**: Optional – Cloud provider specific value (e.g. default)  -   **securityGroup**: Cloud provider specific value (e.g. dchq-security-group)  -   **keyPair**: Cloud provider specific value (e.g. private key)  -   **openPorts**: Optional - comma separated port values  -   **count**: Total no of VM's, defaults to 1.  Once the Machine Compose template is saved, a user can request this machine from the Self-Service **Library**. A user can click **Customize** and then select the **Cloud Provider** and **Cluster** to use for provisioning these Rackspace Cloud Servers.     Deploying the Multi-Tier Java Application on the Rackspace Cluster ------------------------------------------------------------------  Once the Cloud Servers are provisioned, a user can deploy a multi-tier, Docker-based Java applications on the new Cloud Servers. This can be done by navigating to the Self-Service Library and then clicking on Customize to request a multi-tier application.  A user can select an Environment Tag (like DEV or QE) and the Rackspace Cluster created before clicking on Run.  <figure> <img src="screenshots/0-Application%20Request.png"  /> </figure>  Accessing The In-Browser Terminal For The Running Containers ------------------------------------------------------------  A command prompt icon should be available next to the containers’ names on the Live Apps page. This allows users to enter the container using a secure communication protocol through the agent message queue. A white list of commands can be defined by the Tenant Admin to ensure that users do not make any harmful changes on the running containers.  For the Tomcat deployment for example, we used the command prompt to make sure that the Java WAR file was deployed under the /usr/local/tomcat/webapps/ directory.  <figure> <img src="screenshots/0-In-Browser%20Container%20Terminal.png"  /> </figure>  Monitoring the CPU, Memory & I/O Utilization of the Running Containers ----------------------------------------------------------------------  Once the application is up and running, our developers monitor the CPU, Memory, & I/O of the running containers to get alerts when these metrics exceed a pre-defined threshold. This is especially useful when our developers are performing functional & load testing.  A user can perform historical monitoring analysis and correlate issues to container updates or build deployments. This can be done by clicking on the **Actions** menu of the running application and then on **Monitoring**. A custom date range can be selected to view CPU, Memory and I/O historically.  <figure> <img src="screenshots/0-App%20Day-2%20Operations.png"  /> </figure>  <figure> <img src="screenshots/0-Containers%20Monitoring.png"  /> </figure>  Enabling the Continuous Delivery Workflow with Jenkins to Update the WAR File of the Running Application when a Build is Triggered ----------------------------------------------------------------------------------------------------------------------------------  For developers wishing to follow the “immutable” containers model by rebuilding Docker images containing the application code and spinning up new containers with every application update, DCHQ provides an automated build feature that allows developers to automatically create Docker images from Dockerfiles or private GitHub projects containing Dockerfiles.  However, many developers may wish to *update the running application server containers with the latest Java WAR file* instead. For that, DCHQ allows developers to enable a continuous delivery workflow with Jenkins. This can be done by clicking on the **Actions** menu of the running application and then selecting **Continuous Delivery**. A user can select a Jenkins instance that has already been registered with DCHQ, the actual Job on Jenkins that will produce the latest WAR file, and then a BASH script plug-in to grab this build and deploy it on a running application server. Once this policy is saved, DCHQ will grab the latest WAR file from Jenkins any time a build is triggered and deploy it on the running application server.  Developers, as a result will always have the latest Java WAR file deployed on their running containers in DEV/TEST environments.  <figure> <img src="screenshots/0-Continuous%20Delivery.png"  /> </figure>  Scaling out the Tomcat Application Server Cluster -------------------------------------------------  If the running application becomes resource constrained, a user can to scale out the application to meet the increasing load. Moreover, a user can schedule the scale out during business hours and the scale in during weekends for example.  To scale out the cluster of Tomcat servers from 1 to 2, a user can click on the **Actions** menu of the running application and then select **Scale Out**. A user can then specify the new size for the cluster and then click on **Run Now**.  <figure> <img src="screenshots/0-Scale%20Out.png"  /> </figure>  We then used the BASH plug-in to update Apache HTTP Server's httpd.conf file so that it’s aware of the new application server added. The BASH script plug-ins can also be scheduled to accommodate use cases like cleaning up logs or updating configurations at defined frequencies.   To execute a plug-in on a running container, a user can click on the **Actions** menu of the running application and then select **Plug-ins**. A user can then select the load balancer (Apache HTTP Server) container, search for the plug-in that needs to be executed, enable container restart using the toggle button. The default argument for this plug-in will dynamically resolve all the container IP’s of the running Tomcat servers and add them as part of the httpd.conf file.  <figure> <img src="screenshots/0-Plug-in%20Update%20Apache%20HTTP%20Server.png"  /> </figure>  An application time-line is available to track every change made to the application for auditing and diagnostics. This can be accessed from the expandable menu at the bottom of the page of a running application.  Alerts and notifications are available for when containers or hosts are down or when the CPU & Memory Utilization of either hosts or containers exceed a defined threshold.     Conclusion ----------  Containerizing enterprise Java applications is still a challenge mostly because existing application composition frameworks do not address complex dependencies, external integrations or auto-scaling workflows post-provision. Moreover, the ephemeral design of containers meant that developers had to spin up new containers and re-create the complex dependencies & external integrations with every version update.  DCHQ, available in hosted and on-premise versions, addresses all of these challenges and simplifies the containerization of enterprise Java applications through an advance application composition framework that facilitates cross-image environment variable bindings, extensible BASH script plug-ins that can be invoked at request time or post-provision, and application clustering for high availability across multiple hosts or regions with support for auto scaling.  Sign Up for FREE on <http://DCHQ.io> or download [DCHQ On-Premise](<http://dchq.co/dchq-on-premise.html>) to get access to out-of-box multi-tier Java application templates along with application lifecycle management functionality like monitoring, container updates, scale in/out and continuous delivery.
zalando-stups/java-sproc-wrapper	SProcWrapper ============  [![Build Status](https://travis-ci.org/zalando-stups/java-sproc-wrapper.svg)](https://travis-ci.org/zalando-stups/java-sproc-wrapper) [![Coverage Status](https://coveralls.io/repos/zalando-stups/java-sproc-wrapper/badge.svg)](https://coveralls.io/r/zalando-stups/java-sproc-wrapper) [![Javadoc](https://javadoc-emblem.rhcloud.com/doc/de.zalando/zalando-sprocwrapper/badge.svg)](http://www.javadoc.io/doc/de.zalando/zalando-sprocwrapper) [![Maven Central](https://img.shields.io/maven-central/v/de.zalando/zalando-sprocwrapper.svg)](https://maven-badges.herokuapp.com/maven-central/de.zalando/zalando-sprocwrapper) [![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/zalando/problem-spring-web/master/LICENSE)    Library to make PostgreSQL stored procedures(SProcs) available through simple Java "SProcService" interfaces including automatic object serialization and deserialization (using typemapper and convention-over-configuration).  Supports horizontal database sharding (partition/access logic lies within application), easy use of `pg_advisory_lock` through annotations to ensure single Java node execution, configurable statement timeouts per stored procedure, and PostgreSQL types including enums and hstore.  Usage ------------ To use SProcWrapper, add the following lines to your pom.xml:  ```xml <dependency>     <groupId>de.zalando</groupId>     <artifactId>zalando-sprocwrapper</artifactId>     <version>${zalando-sprocwrapper.version}</version> </dependency> ```  Type Mapping ------------  SProcWrapper provides an efficient and easy-to-use mechanism for translating values from database to Java objects and vice-versa. It allows us to map not only primitive types, but also complex types (Java domain objects).  Here are some examples!  Using basic types:  ```java @SProcService public interface CustomerSProcService {   @SProcCall   int registerCustomer(@SProcParam String name, @SProcParam String email); } ```  ```sql CREATE FUNCTION register_customer(p_name text, p_email text) RETURNS int AS $$   INSERT INTO z_data.customer(c_name, c_email)        VALUES (p_name, p_email)     RETURNING c_id $$ LANGUAGE 'sql' SECURITY DEFINER; ```  And a complex type:  ```java @SProcService public interface OrderSProcService {   @SProcCall   List<Order> findOrders(@SProcParam String email); } ```  ```sql CREATE FUNCTION find_orders(p_email text,   OUT order_id int,   OUT order_date timestamp,   OUT shipping_address order_address) RETURNS SETOF RECORD AS $$   SELECT o_id,          o_date,          ROW(oa_street, oa_city, oa_country)::order_address     FROM z_data.order     JOIN z_data.order_address       ON oa_order_id = o_id     JOIN z_data.customer       ON c_id = o_customer_id    WHERE c_email = p_email $$ LANGUAGE 'sql' SECURITY DEFINER; ```  Please check [unit/integration tests](src/test/java/de/zalando/sprocwrapper) for more examples.  The following table shows the mapping between a database type and a Java type:  | Database         | Java Type                                         | | ---------------- | ------------------------------------------------- | | smallint         | int                                               | | integer          | int                                               | | bigint           | long                                              | | decimal          | java.math.BigDecimal                              | | numeric          | java.math.BigDecimal                              | | real             | float                                             | | double precision | double                                            | | serial           | int                                               | | bigserial        | long                                              | | varchar          | java.lang.String                                  | | char             | char                                              | | text             | java.lang.String                                  | | timestamp        | java.sql.Timestamp                                | | timestamptz      | java.sql.Timestamp                                | | date             | java.sql.Timestamp                                | | time             | java.sql.Timestamp                                | | timetz           | java.sql.Timestamp                                | | boolean          | boolean                                           | | enum             | java.lang.Enum                                    | | array            | java.util.List / java.util.Set                    | | hstore           | java.util.Map<java.lang.String, java.lang.String> |  Note: SProcwrapper doesn't support functions returning arrays as a single output. If one wants to return a collection, please return a SETOF instead.  Prerequisites -------------   * Java 8  * To compile, one should use [Maven](http://maven.apache.org/) 2.1.0 or above  Dependencies ------------   * Spring Framework  * PostgreSQL JDBC driver ;)  * Google Guava  * and more see [pom.xml](pom.xml) for details  How to run integration tests ----------------------------  The provided helper script will start a PostgreSQL instance with Docker on port 5432 and run integration tests:      ./test.sh  You can use the provided Vagrant box to run the script in.  Known issues ------------  * PostgreSQL JDBC driver does not honor identical type names in different schemas, this may lead to issues if typemapper is used where types are present with equal name in more than one schema (this problem is solved now with the commit [3ca94e64d6322fa91c477200bfb3719deaeac153](https://github.com/pgjdbc/pgjdbc/commit/3ca94e64d6322fa91c477200bfb3719deaeac153) to [pgjdbc](https://github.com/pgjdbc/pgjdbc/) driver); * PostgreSQL domains are not supported as for now; * PostgreSQL `hstore` type is mapped from and to `Map<String,String>`, there is no way to use `Map` of different types for now; * Two different datasources with the same JDBC URL and different search paths might not work properly when we have types with the identical name; * SProcWrapper relies on the search path to resolve conflicting types with the same name (right now, we are not checking the schema). If one specifies the schema of the stored procedure's return type, SProcWrapper might end up using the wrong one, because it will use the `search_path` to resolve the conflict. For more info check test: `SimpleIT.testTypeLookupBugWithSchema`; * For integration with Spring's transaction management use the `TransactionAwareDataSourceProxy` as the data source injected into the data source provider.  Documentation -------------  You can find some more information about the SProcWrapper in our various Zalando Technology blog posts:  * http://tech.zalando.com/blog/goto-2013-why-zalando-trusts-in-postgresql/ * http://tech.zalando.com/blog/zalando-stored-procedure-wrapper-part-i/ * http://tech.zalando.com/blog/files/2013/04/jug_dortmund_april_2013.pdf  ## Contributing  See [contributing guideline](CONTRIBUTING.md).   License -------  MIT license. See [license file](LICENSE).
citusdata/podyn	# Podyn: DynamoDB to PostgreSQL replication  The `podyn` tool replicates DynamoDB tables to PostgreSQL tables, which can optionally be distributed using [Citus](https://www.citusdata.com/product). It can also keep the tables in sync by continuously streaming changes.  [Read the Blog post on Podyn](https://www.citusdata.com/blog/2017/09/22/dynamodb-to-postgres-replication/).  ## Building from source  To build a shaded JAR, run:  ``` git clone https://github.com/citusdata/podyn.git cd podyn mvn package ```  The JAR file will be at `target/podyn-1.0.jar`.  ## Running the JAR file  Once you've built the JAR, you can run it as follows.  ``` ./podyn --help usage: podyn  -c,--changes                    Continuously replicate changes  -d,--data                       Replicate the current data  -h,--help                       Show help  -lc,--lower-case-column-names   Use lower case column names  -m,--conversion-mode <arg>      Conversion mode, either columns or jsonb (default: columns)  -n,--num-connections <arg>      Database connection pool size (default 16)  -r,--scan-rate <arg>            Maximum reads/sec during scan (default 25)  -s,--schema                     Replicate the table schema  -t,--table <arg>                DynamoDB table name(s) to replicate  -u,--postgres-jdbc-url <arg>    PostgreSQL JDBC URL of the destination  -x,--citus                      Create distributed tables using Citus ```  When `--postgres-jdbc-url` is omitted, the SQL statements that would otherwise be sent to the database are sent to stdout. When `--table` is omitted, all DynamoDB tables in the region are replicated.  ## Replicate schema and data from DynamoDB  After [setting up your AWS credentials](http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default), you can replicate the schema and do an initial data load by running:  ``` export AWS_REGION=us-east-1  ./podyn --postgres-jdbc-url "jdbc:postgresql://host:5432/citus?sslmode=require&user=citus&password=pw" --schema --data --citus  Constructing table schema for table clicks Moving data for table clicks Adding new column to table clicks: ip text Adding new column to table clicks: object text ```  When `--schema` is specified, tables will be created in PostgreSQL as described in the *Schema conversion rules* section. If `--citus` is specified the tables will be distributed by the DynamoDB partition key. When the `--data` argument is specified, all the data in the DynamoDB table is scanned in batches and `COPY` is used to load the batch into postgres.  ## Stream changes from DynamoDB  After schema creation and the initial data load, you can continuously stream changes using:  ``` ./podyn --postgres-jdbc-url "jdbc:postgresql://host:5432/citus?sslmode=require&user=citus&password=pw" --changes --citus  Replicating changes for table clicks ... ```  The changes are processed in batches and new fields are added to the table as columns. The changes are translated into delete  or upsert statements that are sent to postgres over multiple connections (specified using `-n`) to achieve high throughput.  When running the command immediately after a data load, some changes that were made prior to the data load may be re-applied, causing the replicated database to temporarily regress. However, since the changes are applied in the same order they will eventually arrive at the current value. After loading a batch of changes into the database, a checkpoint is made. If the tool is restarted, it will continue from its last checkpoint. The checkpoints are stored in DynamoDB tables prefixed with `podyn_migration_`.   ## Schema conversion rules  Podyn currently has two ways of converting a DynamoDB schema to a PostgreSQL schema: `columns` and `jsonb`. The default is `columns`. To use the `jsonb` conversion mode add `-m jsonb` as an argument.  ### Columns mode  In the default schema conversion mode (`-m columns`), top-level keys in DynamoDB items are translated into columns. The initial schema is derived from the partition key, sort key and secondary indexes. When using Citus, the primary partition key becomes the distribution column.  DynamoDB types are mapped to PostgreSQL types according to the following table:  | DynamoDB type | PostgreSQL type | | ------------- | --------------- | | String        | text            | | Binary        | bytea           | | Numeric       | numeric         | | StringSet     | jsonb           | | NumberSet     | jsonb           | | BinarySet     | jsonb           | | Map           | jsonb           | | List          | jsonb           | | Boolean       | boolean         | | Null          | text            |  For example, a DynamoDB table named `clicks` has primary partition key: `sitename` (String), primary sort key: `time` (String), and a secondary index named `pageid-index` on: `pageid` (Numeric). The following statements will be sent to PostgreSQL:  ``` CREATE TABLE clicks (    sitename text NOT NULL,    "time" text NOT NULL,    pageid numeric NOT NULL,    PRIMARY KEY (sitename, "time") ); CREATE INDEX "pageid-index" ON clicks (pageid); SELECT create_distributed_table('clicks', 'sitename'); ```  A new column is added to the PostgreSQL table whenever a new key appears in an item during the initial data load or while replicating changes. For example, if `"ip" -> "198.51.100.3"` key is encountered in an item and the table does not have an `ip` column, then the following command is sent to the database:  ``` ALTER TABLE clicks ADD COLUMN ip text; ```  After replicating a single item, the table might look as follows.  ``` SELECT * FROM clicks; -[ RECORD 1 ]------------------------------------------------------------------------------------------------------------------------------------- sitename | citusdata.com time     | 2017-09-18 16:08:36.989788+02 pageid   | 347712 ip       | 198.51.100.3 object   | home_button ```  In DynamoDB, the same key can appear with different types as long as it's not part of the primary key or a secondary index. If Podyn encounters a key-value pair with a type that's not compatible with the column type, then a new column is added with the type name added as a suffix. For example, if an item appears which contains `"ip" -> false`, then Podyn would send the following command to PostgreSQL:  ``` ALTER TABLE clicks ADD COLUMN ip_boolean boolean; ```  If you have keys that have values of different types, or many different top-level keys, then you may want to consider using JSONB mode.  ### JSONB mode  In the JSONB conversion mode (`-m jsonb`), the initial schema is never changed. Instead, a single JSONB column is added that  contains the entire DynamoDB item in JSON format.  Constructing the schema in `jsonb` mode is done in the same way as in `columns` mode (partition key and secondary indexes become columns) except with an extra column named `data` with the `jsonb` type. In the `clicks` example, Podyn would send the following statements to PostgreSQL:  ``` CREATE TABLE clicks (    sitename text NOT NULL,    "time" text NOT NULL,    pageid numeric NOT NULL,    data jsonb,    PRIMARY KEY (sitename, "time") ); CREATE INDEX "pageid-index" ON clicks (pageid); SELECT create_distributed_table('clicks', 'sitename'); ```  During replication, the entire DynamoDB item is converted to JSON, including the fields that already appear as columns, for example:  ``` SELECT * FROM clicks; -[ RECORD 1 ]------------------------------------------------------------------------------------------------------------------------------------- sitename | citusdata.com time     | 2017-09-18 16:08:36.989788+02 pageid   | 347712 data     | {"ip": "198.51.100.3", "time": "2017-09-18 16:08:36.989788+02", "object": "home_button", "pageid": 347712, "sitename": "citusdata.com"} ```  The JSONB mode has an additional advantage that you can safely set up multiple Podyn instances for replicating changes. They will automatically divide the work and if a node fails, the other node(s) will automatically take over.
yandex-qatools/embedded-services	# Embedded services [![Maven Central](https://maven-badges.herokuapp.com/maven-central/ru.yandex.qatools.embed/embedded-services/badge.svg?style=flat)](https://maven-badges.herokuapp.com/maven-central/ru.yandex.qatools.embed/embedded-services) [![covarage](https://img.shields.io/sonar/http/sonar.qatools.ru/ru.yandex.qatools.embed:embedded-services/coverage.svg?style=flat)](http://sonar.qatools.ru/dashboard/index/887)  This project allows you to easily start your project with the embedded database (PostgreSQL, MongoDB) services and connect  them with the embedded ElasticSearch instance for full text indexing and search.  ## Why?  It's very easy to incorporate the embedded MongoDB/PostgreSQL within your test process.  ### Maven  Add the following dependency to your pom.xml: ```xml     <dependency>         <groupId>ru.yandex.qatools.embed</groupId>         <artifactId>embedded-services</artifactId>         <version>1.21</version>     </dependency> ``` ## How to run embedded MongoDB with ElasticSearch  ```java         // Starting the embedded services within temporary dir         MongoEmbeddedService mongo = new MongoEmbeddedService(                 "localhost:27017", "dbname", "username", "password", "localreplica"         );         mongo.start();         ElasticMongoIndexingService elastic = new ElasticMongoIndexingService(                 "localhost:27017", "dbname", "username", "password"         );         elastic.start();                  // Indexing collection `posts`         elastic.addToIndex("posts");                  // Searching within collection `posts` using Elastic (IndexingResult contains id of each post)         List<IndexingResult> posts = elastic.search("posts", "body:(lorem AND NOT ipsum)") ```  ## How to run embedded PostgreSQL with ElasticSearch  ```java         // Starting the embedded services within temporary dir         PostgresEmbeddedService postgres = new PostgresEmbeddedService(                 "localhost", 5429, "username", "password",  "dbname"         );         postgres.start();         ElasticPostgresIndexingService elastic = new ElasticPostgresIndexingService(                 Driver.class, "postgresql", "", "localhost", 5429, "username", "password", "dbname"         );         elastic.start();                  // Indexing table `posts`         elastic.addToIndex("posts");                  // Searching within table `posts` using Elastic (IndexingResult contains id of each post)         List<IndexingResult> posts = elastic.search("posts", "body:(lorem AND NOT ipsum)") ```
vert-x3/vertx-mysql-postgresql-client	= Vert.x MySQL PostgreSQL client  image:https://vertx.ci.cloudbees.com/buildStatus/icon?job=vert.x3-mysql-postgresql-client["Build Status",link="https://vertx.ci.cloudbees.com/view/vert.x-3/job/vert.x3-mysql-postgresql-client/"]   This Vert.x client uses the https://github.com/mauricio/postgresql-async[Mauricio Linhares driver] to support a fully async client for MySQL and PostgreSQL.  Please see the in source asciidoc documentation or the main documentation on the web-site for a full description:  * Web-site docs * link:src/main/asciidoc/java/index.adoc[Java in-source docs] * link:src/main/asciidoc/js/index.adoc[JavaScript in-source docs] * link:src/main/asciidoc/groovy/index.adoc[Groovy in-source docs]  == Requirements for running test suite  * A working PostgreSQL or MySQL server * For testing PostgreSQL: A `testdb` database on a local PostgreSQL install and a user called `vertx` with password `password` * For testing MySQL: A `testdb` database on a local MySQL install and a user called `vertx` with password `password`  Setting up test databases with Docker:  ---- docker run --rm --name vertx-postgres -e POSTGRES_USER=vertx -e POSTGRES_PASSWORD=password -e POSTGRES_DB=testdb -p 5432:5432 postgres ----  ---- docker run --rm --name vertx-mysql -e MYSQL_ROOT_PASSWORD=password -e MYSQL_USER=vertx -e MYSQL_PASSWORD=password -e MYSQL_DATABASE=testdb -p 3306:3306 mysql/mysql-server:5.6 ----
sroysf/webapp-springmvc-jpa-hibernate	## Purpose ##  Provide an integrated starter project for SpringMVC + Spring ORM, backed by Hibernate and PostgreSQL.   This project demonstrates a variety of useful functions right out of the box, and can be used as a starting point for a real web application. It makes extensive use of SpringMVC for the web tier. It also uses Spring to replace the direct access to the JPA entity manager API and  instead uses Spring dependency injection patterns to access data.  ## Technologies ##  * Maven * Hibernate * JPA annotations * PostgreSQL * JSTL * Servlets * Log4J via SLF4J * JUnit * Hibernate maven plugin * Spring MVC * Spring ORM  ## Features demonstrated ##  * JSTL page rendering * Implementation of REST API, automatically returning JSON serialization. * Parsing and controller routing in Spring MVC * File upload * Database access via injected DAO objects  ## Setup and run ##  Assuming you have setup your postgres database separately and have updated persistence.xml to point at it...  1. mvn clean install -DskipTests 2. mvn hibernate3:hbm2ddl 3. Use target/hibernate3/sql/schema.ddl to create your database schema 4. mvn -e exec:java -Dexec.mainClass=com.force.samples.util.DataLoadUtil  (adds some data to the database) 5. mvn tomcat:run 6. Point browser at [http://localhost:8080/webapp-springmvc-jpa-hibernate](http://localhost:8080/webapp-springmvc-jpa-hibernate)
hibernate/hibernate-ogm	# Hibernate OGM  *Version: 5.2.0.Beta1 - 12-10-2017*  ## Description  Hibernate OGM stores data in a NoSQL data grid using the Hibernate ORM engine.  The benefits are fairly obvious:  - write your model once using well known JPA annotations and select the right NoSQL data grid for your project  - Hibernate is familiar to many people  - you end up being able to use all the tools of the Hibernate ecosystem such as Hibernate Search or Hibernate Validator  Checkout <http://hibernate.org/ogm/> for more information.  ## Core datastores and contrib datastores  Hibernate OGM supports a large number of NoSQL datastores.  Some are included in this very repository:   * Infinispan  * MongoDB  * Neo4j  Others are in separate repositories, called contrib:   * [Cassandra](https://github.com/hibernate/hibernate-ogm-cassandra)  * [CouchDB](https://github.com/hibernate/hibernate-ogm-couchdb)  * [Ehcache](https://github.com/hibernate/hibernate-ogm-ehcache)  * [Redis](https://github.com/hibernate/hibernate-ogm-redis)  * [Ignite](https://github.com/hibernate/hibernate-ogm-ignite)  ## Useful pointers  Latest Documentation:   * Reference guide: <https://docs.jboss.org/hibernate/stable/ogm/reference/en-US/html_single/>  * Additional content: <http://community.jboss.org/en/hibernate/ogm>  Bug Reports:   * Hibernate JIRA (preferred): <https://hibernate.atlassian.net/projects/OGM/>  * Mailing list: <hibernate-dev@lists.jboss.org>  Support:   * The hibernate-ogm tag on Stackoverflow: <http://stackoverflow.com/questions/tagged/hibernate-ogm>  * Our forum: <https://forum.hibernate.org/viewforum.php?f=31>  ## Build instructions  The code is available on GitHub at <https://github.com/hibernate/hibernate-ogm>.  To run the full project build including tests for all backends, documentation etc. execute:      mvn clean install -s settings-example.xml  Note that for running the test suite against separately installed MongoDB or Neo4j servers their host name must be specified via an environment variable. See the sections below for the details.  To speed things up, there are several options for skipping parts of the build. To run the minimum project build without integration tests, documentation and distribution execute:      mvn clean install -DskipITs -DskipDocs -DskipDistro -s settings-example.xml  The following sections describe these options in more detail.  ### Importing sources in Eclipse  Import the project as any standard Maven project. This might trigger a dialog to automatically find and install additional m2e plugins: allow that.  Make sure that annotation processing is enabled in your project settings (see "Properties" - "Maven" - "Annotation Processing", the setting should be "Automatically configure JDT APT").  ### Integration tests  You can skip integration tests by specifying the `skipITs` property:      mvn clean install -DskipITs -s settings-example.xml  ### Documentation  The documentation is built by default as part of the project build. You can skip it by specifying the `skipDocs` property:      mvn clean install -DskipDocs -s settings-example.xml  If you just want to build the documentation, run it from the _documentation/manual_ subdirectory.  By default, the following command only builds the HTML version of the documentation:      mvn clean install -f documentation/manual/pom.xml -s settings-example.xml  If you also wish to generate the PDF version of the documentation, you need to use the `documentation-pdf` profile:      mvn clean install -f documentation/manual/pom.xml -s settings-example.xml -Pdocumentation-pdf  ### Distribution  The distribution bundle is built by default as part of the project build. You can skip it by specifying the `skipDistro` property:      mvn clean install -DskipDistro -s settings-example.xml  ### Integration tests  Integration tests can be run from the integrationtest module and the default behaviour is to download the WildFly application server, unpack the modules in it and run the tests using Arquillian.  #### WARNING Be careful when using on existing installation since the modules used by the build are going to be extracted into the server you want to run the test, changing the original setup.  ### MongoDB  For executing the tests in the _mongodb_ and _integrationtest/mongodb_ modules, by default the [embedmongo-maven-plugin](https://github.com/joelittlejohn/embedmongo-maven-plugin) is used which downloads the MongoDB distribution, extracts it, starts a _mongod_ process and shuts it down after test execution.  If required, you can configure the port to which the MongoDB instance binds to (by default 27018) and the target directory for the extracted binary (defaults to _${project.build.directory}/embeddedMongoDb/extracted_) like this:      mvn clean install -s settings-example.xml -DembeddedMongoDbTempDir=<my-temp-dir> -DembeddedMongoDbPort=<my-port>  To work with a separately installed MongoDB instance instead, specify the property `-DmongodbProvider=external`:      mvn clean install -s settings-example.xml -DmongodbProvider=external  This assumes MongoDB to be installed on `localhost`, using the default port and no authentication. If you work with different settings, configure the required properties in hibernate.properties (for the tests in _mongodb_) and/or the environment variables `MONGODB_HOSTNAME` `MONGODB_PORT` `MONGODB_USERNAME` `MONGODB_PASSWORD` (for the tests in _integrationtest/mongodb_) prior to running the tests:      export MONGODB_HOSTNAME=mongodb-machine     export MONGODB_PORT=1234     export MONGODB_USERNAME=someUsername     export MONGODB_PASSWORD=someP@ssw0rd     mvn clean install -s settings-example.xml -DmongodbProvider=external  ### Neo4j  For running the tests in the _neo4j_ and _integrationtest/neo4j_ modules, by default the embedded Neo4j configuration is used.  If you want to run the tests on a remote server, you need to specify the profile `neo4j-remote`      mvn clean install -s settings-example.xml -Pneo4j-remote  This assumes Neo4j to be installed on `localhost`, using the default port and no authentication. If you work with different settings, configure the required properties in hibernate.properties and/or the environment variables `NEO4J_HOSTNAME`, `NEO4J_PORT`, `NEO4J_USERNAME` and `NEO4J_PASSWORD` prior to running the tests:      export NEO4J_HOSTNAME=neo4j-machine     export NEO4J_PORT=1234     export NEO4J_USERNAME=someUsername     export NEO4J_PASSWORD=someP@ssw0rd  ## Notes  If you want to contribute, come to the <hibernate-dev@lists.jboss.org> mailing list or join us on #hibernate-dev on freenode (login required)  This software and its documentation are distributed under the terms of the FSF Lesser Gnu Public License (see license.txt).
dboissier/nosql4idea	# NoSql Plugin for IntelliJ IDEA version 0.1.0-SNAPSHOT  ## Description This plugin is a fork from [mongo4idea](https://github.com/dboissier/mongo4idea) and intends to integrate Redis and Couchbase databases. Please note that the Couchbase integration is experimental because I am not a strong user of this database.  ## Current status: EAP  * [Download the current SNAPSHOT for Idea 14](https://github.com/dboissier/nosql4idea/raw/master/snapshot/nosql4idea-0.1.0-SNAPSHOT-distribution.zip) * [Download the current SNAPSHOT for Idea 15](https://github.com/dboissier/nosql4idea/raw/master/snapshot/nosql4idea-0.1.0-SNAPSHOT-Idea15-distribution.zip)  ## Plugin Compatibility  This plugin is built with JDK 1.7 and idea 14.1 version.  The plugin has been tested with the following databases: * MongoDB 2.7 and 3.0 * Redis 2.8.21 * Couchbase Community 4.0.0  ## Installation    To install it : `Settings > Plugins > Install plugin from Disk`  ## Configuration  On the right, you will see a new tool window named NoSql Explorer  ![NoSqlBrowser](https://github.com/dboissier/nosql4idea/raw/master/doc/explorer.png)  * Click on the Wrench Icon from the toolbar and you will be redirected to the Plugin Settings. * You can specify the mongo and redis CLI paths at the top of the panel * To add a server, click on the **\[+\]** button and choose you database vendor  ![SettingsAddAServer](https://github.com/dboissier/nosql4idea/raw/master/doc/settings_add_a_server.png)  * Click on the **OK** button and enter the settings of your database  ![MongoSettings](https://github.com/dboissier/nosql4idea/raw/master/doc/mongo_settings.png)  * When all your dabatase are configured you should see then in the explorer panel   ![NoSqlBrowserWithServers](https://github.com/dboissier/nosql4idea/raw/master/doc/explorer_with_servers.png)  ## Viewing the Redis database content  Double click on the database icon from your redis server and the results will appear as a tab  ![RedisResults](https://github.com/dboissier/nosql4idea/raw/master/doc/redis_results.png)  You can filter the results (Currently, it runs a `KEYS <filter>` command. A `SCAN <filter>` will replace it in the future for optimization purpose.)  Like the **Properties editor**, you can group your data by prefix. Click on the corresponding icon and then click on the Elipsis icon to set you separator  ![RedisResultsGroupedByPrefix](https://github.com/dboissier/nosql4idea/raw/master/doc/redis_group_by_prefix.png)  ## Viewing the Couchbase database content   Double click on the database icon from your couchbase server and the results will appear as a tab  ![CouchbaseResults](https://github.com/dboissier/nosql4idea/raw/master/doc/couchbase_results.png)  **Important note** To get the results from each bucket, an **Index** must be created. Otherwise an error message is raised.  ## Roadmap  ### 0.1.0  * integrate Redis : view results with 'Group by prefix' feature like **properties editor** * integrate Couchbase : view results   ### 0.2.0  * delete, update and add features for Redis and Couchbase
helicalinsight/helicalinsight	## Helical Insight  > Community driven innovation via Helical Insight CE.  ## What is it  Helical Insight is world's first Open Source Business Intelligence framework which can help you derive insights out of your one or multiple datasources. Helical Insight is having all the features which you generally expect out of any BI tool (be it open source or proprietary). Being a framework, Helical Insight is highly extensible via APIs and SDK, thus features can be extended whenever required without compromising on your business requirement.   Helical Insight also comes with a unique Workflow rule engine, allowing you to call any functionality of Helical Insight or external functionality and thus empowering you to implement any sort of custom business process.  Use HTML skillset and Java skillset to add functionalities at the frontend and backend respectively.    See the [Quick start](https://helicalinsight.github.io/helicalinsight/#/quickstart) for more details.  ## Features  * New generation UI with one click access * Backend EFW method of reports, dashboards and other data analysis creation * User Role Management * Exporting to Multiple Formats * Email scheduling * Data Security * XML driven Workflow  * API Driven Framework * Community Support * Community Upgrades * Direct links to tutorials * Mobile & Cloud compatable * Cache for faster Performance * Compatible with All Modern Browsers   ## Supported Databases  We support all the JDBC4 complaint datbases, NoSQL, Big Data, RDBMS, Cloud db, Columnar database etc  #### RDBMS  * Mysql * PostgreSQL * SQL Server * Oracle * Firebird * Informix * Ingres * MariaDB * Presto * Progress * SQlite  #### NoSQL & Big Data  * Cassandra * Druid * HBase * MongoDb * Hive * NuoDB * Neo4j  #### Cloud  * Microsoft Azure SQL * Amazon RedShift Database * Google Cloud Sql  #### Flat Files  * CSV * TSV * JSON    ## How to build ?   Prerequisite:    To build Helical Insight Community Edition project you need   * `Maven 3` or higher version installed. * `Java 1.7` higher version installed. * Mysql database should be installed.  * Database with name `hice` to be created in the Mysql.  Steps:   1. Download the Helical Insight project from Helical Insight Github Page.   2. Find the hikaricp, tomcat-jdbc jar files in the resources folder of downloaded Helical Insight Project and then install it locally using your maven reporsitory using command.  		a. mvn install:install-file -Dfile={path/to/file} -DartifactId=HikariCP -Dversion=2.4.7-hi -Dpackaging=jar 		 		Example: mvn install:install-file -Dfile=E:\Helical\communityEdition\HikariCP-2.4.7-hi.jar -DgroupId=com.zaxxer -DartifactId=HikariCP -Dversion=2.4.7-hi -Dpackaging=jar 		 		b. mvn install:install-file -Dfile={path/to/file} -DgroupId=org.apache.tomcat -DartifactId=jdbc-pool -Dversion=7.0.65 -Dpackaging=jar 		 		Example: mvn install:install-file -Dfile=E:\Helical\communityEdition\tomcat-jdbc-7.0.65.jar -DgroupId=org.apache.tomcat -DartifactId=tomcat-jdbc -Dversion=7.0.65 -Dpackaging=jar  3. Change the variables in the `pom.xml` present in `hi-ce` folde for configuring HI Repository , Log Location and Database credentials of the `hice` database.  Where:  a) HI Reporsitory: This is the Helical Insight Report reporsitory, which contais all created reports and dashboards.  b) Log Location: On which Location Helical Insight Application log going to create.   c) hice Database: This database is going to store users/roles/profile information   ```text 		<systemDirectory>path/to/SystemDirectory</systemDirectory>                 <logLocation>path/to/log/folder</logLocation>                 <dbUser>database-user-name</dbUser>                 <dbPassword>database-password</dbPassword>                 <dbServer>database-server-host</dbServer>                 <dbPort>database-port</dbPort>                 <dbName>hice</dbName>          eg: 		<systemDirectory>E:/hi-repository</systemDirectory>  		<!--This is the path which points to the hi-repository folder present with the download-->                 <logLocation>E:/logs</logLocation> <!--log location-->                 <dbUser>hiuser</dbUser>                 <dbPassword>hiuser</dbPassword>                 <dbServer>localhost</dbServer>                 <dbPort>3306</dbPort>                 <dbName>hice</dbName> ```   4. Setting.xml configrations  a) Now open the Helical Insights setting.xml file present in below location  Location: E:\HDI UI\Comunity Edition\hi-repository\System\Admin\setting.xml   NOTE: Location of the setting.xml may be different based on the Helical Insight project location.  b) Find the `<efwSolution>` tag and change the value to  your `hi-repository` path.   c) Find the `<BaseUrl>` tag and change the value with your base url   Format for base url is:   http://<ip_address>:<port_no>/hi-ce/hi.html    Example: http://localhost:8080/hi-ce/hi.html   5. Run the command   Below command builds the Helical Insight Community Edition project and create the `hi-ce.war` file in hi-ce -> target folder.  Run command is also depends on the envirnment also.  Go to the Helical Insight project download location and run the following command  For `Dev` Envirnment  ```text   mvn clean package -Denv=dev ```  For `Production` Envirnment  ```text   mvn clean package -Denv=production ```  6. Now Deploy the application on any webserver like Apache tomcat and access the appliation using above mention url.  Example: http://localhost:8080/hi-ce/hi.html  ## Directly deploy You may also directly deploy  the `hi-ce.war` file in the application server from the `hi-ce/target` module You need `tomcat` or any other server. Please follow the instructions given [here](https://helicalinsight.github.io/helicalinsight/#/quickstart?id=manual-installation)   ## Application Screenshots  ![CommunityEdition](docs/_media/screens/login.png)   ![CommunityEdition](docs/_media/screens/welcome.png)   ![CommunityEdition](docs/_media/screens/data_sources.png)   ![CommunityEdition](docs/_media/screens/reports.png)   ![CommunityEdition](docs/_media/screens/file_browser.png)   #### Sample Report ![CommunityEdition](docs/_media/screens/SampleReport.gif)  #### Admin Page ![CommunityEdition](docs/_media/screens/admin.gif)     ## Examples  Check out the Demos to Helical Insight in use.  * [Industry specific demo](http://www.helicalinsight.com/industry-specific-demo/)   * [Job functions specific demo](http://www.helicalinsight.com/job-functions-specific-demo/)   * [Miscellaneous Use Cases](http://www.helicalinsight.com/miscellaneous-use-cases/)     ## Try Enterprise Edition  Helical Insight Enterprise Edition comes with many addon features like self service interface for reports and dashboards creation, multitenancy, machine learning and NLP, UI driven workflow etc.  Try Now [Download Free Trial.](http://www.helicalinsight.com/register/)    ## License  Copyright (c) Helical Insight. All rights reserved.  Licensed under the  Apache License.
marklogic/java-client-api	# The MarkLogic Java Client API  The API makes it easy to write, read, delete, and find documents in a [MarkLogic](http://developer.marklogic.com/) database.  For example:      // write a text, binary, XML, or JSON document from any source with ACID guarantees     documentManager.write(uri, new FileHandle()       .with(new File("file1234.json"))       .withFormat(JSON));      // read and directly parse to your preferred type, even your own POJOs!     JsonNode jsonDocContents = documentManager.readAs(uri, JsonNode.class);      // get matches super-fast using full-featured search     JsonNode results = queryManager.search(       new StructuredQueryBuilder().term("quick", "brown", "fox"),       new JacksonHandle()).get();  The Java API supports the following core features of the MarkLogic database:  *  Write and read binary, JSON, text, and XML documents. *  Query data structure trees, marked-up text, and all the hybrids in between those extremes. *  Project values, tuples, and triples from hierarchical documents and aggregate over them. *  Patch documents with partial updates. *  Match documents against alerting rules expressed as queries. *  Use Optimistic Locking to detect contention without creating locks on the server. *  Execute ACID modifications so the change either succeeds or throws an exception. *  Execute multi-statement transactions so changes to multiple documents succeed or fail together.  ### What's New in Java Client API 4.0.1  * Optic API - blends relational with NoSQL by providing joins and aggregates over documents   * is powered by the new row index and query optimizer   * uses row, triple, and/or lexicon lenses   * matches the functionality of the Optic API for XQuery and Javascript, but idiomatic for Java     developers * Data Movement SDK - move large amounts of data into, out of, or within a MarkLogic cluster   * WriteBatcher distributes writes across many threads and across the entire MarkLogic cluster   * QueryBatcher enables bulk processing or export of matches to a query by distributing the query     across many threads and batch processing to listeners   * Comes with ApplyTransformListener, DeleteListener, ExportListener, ExportToWriterListener, and     UrisToWriterListener   * With custom listeners you can easily and efficiently apply your business logic to batches of query     matches * Kerberos and Client Certificate Authentication * Geospatial double precision and queries on region indexes * Temporal document enhancements   * protect and wipe   * more control over version uris * Support for document metadata values  See also [CHANGELOG.md](CHANGELOG.md)  ### QuickStart  To use the API in your maven project, include the following in your pom.xml:      <dependency>         <groupId>com.marklogic</groupId>         <artifactId>java-client-api</artifactId>         <version>3.0.5</version>     </dependency>  For gradle projects, include the following:      dependencies {         compile group: 'com.marklogic', name: 'java-client-api', version: '3.0.5'     }  Read [The Java API in Five Minutes](http://developer.marklogic.com/try/java/index)  ### Learning More  The following resources document the Java API:  * [Java Application Developer's Guide](http://docs.marklogic.com/guide/java) * [JavaDoc](http://docs.marklogic.com/javadoc/client/index.html)  ### Installing  To use the Java API, either add Maven or Gradle dependency as explained above or download the jar and its dependencies:  http://developer.marklogic.com/products/java  Of course, you'll also need to install the database -- which you can do for free with the developer license:  https://developer.marklogic.com/free-developer  To obtain verified downloads signed with MarkLogic's PGP key, use maven tools or directly download the .jar and .asc files from [maven central](http://repo1.maven.org/maven2/com/marklogic/java-client-api/3.0.5/).  MarkLogic's pgp key ID is 48D4B86E and it is available from pgp.mit.edu by installing gnupg and running the command:      $ gpg --keyserver pgp.mit.edu --recv-key 48D4B86E  Files can be verified with the command:      $ gpg java-client-api-3.0.5.jar.asc   ### Building and Contributing  You can build the API in the same way as any Maven project on git:  1. Clone the java-client-api repository on your machine. 2. Choose the appropriate branch (usually develop) 3. Execute a Maven build in the directory containing the pom.xml file.  You might want to skip the tests until you have configured a test database and REST server:      $ mvn package -Dmaven.test.skip=true  See [CONTRIBUTING.md](CONTRIBUTING.md) for more on contributing to this github project.  ### Running JUnit Tests      $ mvn test-compile     $ mvn exec:java@test-server-init     $ mvn test  ## Support The MarkLogic Java Client API is maintained by MarkLogic Engineering and distributed under the [Apache 2.0 license](https://github.com/marklogic/java-client-api/blob/master/LICENSE). It is designed for use in production applications with MarkLogic Server. Everyone is encouraged to file bug reports, feature requests, and pull requests through GitHub. This input is critical and will be carefully considered, but we can’t promise a specific resolution or timeframe for any request. In addition, MarkLogic provides technical support for [release tags](https://github.com/marklogic/java-client-api/releases) of the Java Client API to licensed customers under the terms outlined in the [Support Handbook](http://www.marklogic.com/files/Mark_Logic_Support_Handbook.pdf). For more information or to sign up for support, visit [help.marklogic.com](http://help.marklogic.com).
swapnildipankar/java_jersey_spring_hibernate_maven	Archetype starter project to create RESTful WS using Java, Jersey WS, Spring, Hibernate, Maven, Liquibase. Sample code for integrating with Hibernate/MySQL and MongoDB/NoSQL.<br /><br /> __Note:__ _This is a skeletal archetype framework. Using the sample code, developers can send a sample CURL call (see section "Sample test CURL calls" below) to the running web service and see the changes propagated all the way to the databases. There are, currently, two webservice end points, /user and /purchase for demonstrating MySQL and MongoDB respectively. Developers will have to write the actual functionality to make use of the framework._  * __New:__ _Support for MongoDB_ * __Coming Up:__ _Support for Cassandra_ * __Coming Up:__ _Support for NuoDB_  #### Developers: * __Parth Parekh__ - parthparekh at gmail dot com * __Swapnil Dipankar__ - dswapnil at gmail dot com  #### Technologies used in the project * __Java__ * __Jersey WS Framework__ * __Spring Framework__ * __Hibernate__ * __Maven__ * __Liquibase__ * __MySQL__ * __Mongo DB__  #### Prerequisite packages __Note:__ _The project has been tested on several versions of Mac OSX and Ubuntu Linux, but can work on any other platform as long as the packages mentioned in this section are available on the platform. Ensure that the user has sudo access to the host_ * __OpenJDK SDK__ (run command _sudo apt-get install openjdk-6-jdk_, if not present) __Note:__ _Sun JDK can be used instead of OpenJDK without impacting the functionality of the framework_ * __Git__ (run command _sudo apt-get install git_ or _sudo apt-get install git-core_, if not present) * __Maven__ (run command _sudo apt-get install maven2_, if not present) * __Curl__ (run command _sudo apt-get install curl_, if not present) * __MySQL Server__ (run command _sudo apt-get install mysql-server_, if not present) * __MongoDB__ (run command _sudo apt-get install mongodb_, if not present) * __Vim (optional)__ (run command _sudo apt-get install vim_, if not present) * __IntelliJ Idea Community Edition (optional)__ (download IDE from http://www.jetbrains.com/idea/download/index.html) * __Cocoa REST Client (optional, this is only for Mac OSX)__ (download from https://code.google.com/p/cocoa-rest-client/)  #### Prerequisite MySQL database and tables * Ensure that MySQL is running and has the __root__ password as __password__ (Else, modify the __hibernate.cfg.xml__ file with correct database credentials) * Ensure that database __test__ exists (Else, create a database __test__ with the access credentials specified in __hibernate.cfg.xml__ file)  #### Prerequisite MongoDB database * Ensure that MongoDB is running (run command _mongod --master --dbpath path_to_directory_that_will_contain_database_data_) * Ensure that database __purchase__ has user with credentials (purchase_user, purchase4user) (Else, modify the credentials in file __resources/mongodb/mongo-config.json__)  #### Steps to compile, install and run code Run the following commands in the given sequence: * git clone https://github.com/swapnildipankar/java_jersey_spring_hibernate_maven.git * cd java_jersey_spring_hibernate_maven * mvn clean install (Depending on the connection speed, this step may take several minutes when executing this command for the first time) * mvn resources:resources liquibase:update -P database-localhost -Dliquibase.password=password (This creates the table __user__ in the database __test__) * mvn tomcat7:run (The tomcat server should now be ready to accept requests on port 8400)  #### Sample test CURL calls (http://localhost:8400/demows/user - this uses MySQL for persistence) * __POST__ _Request:_ ``` 	curl -i -H "Content-Type: application/json" -X POST -d \ 	'{ 		"username":"johndoe", 		"password":"ǝopuɥoɾ", 		"name_first":"John", 		"name_middle":"M", 		"name_last":"Doe", 		"date_of_birth":14, 		"month_of_birth":10, 		"year_of_birth":1978 	}' 'http://localhost:8400/demows/user' ```  * _Response:_ ``` 	'{ 		"id": 1, 		"username": "johndoe", 		"password": "41a87d0faca541b691f11c6d51874f37b989f0cb1b0075cce599e94f82cbbdfc", 		"name_first": "John", 		"name_middle": "M", 		"name_last": "Doe", 		"user_status": "PENDING", 		"user_status_code": "P", 		"date_of_birth": 14, 		"month_of_birth": 10, 		"year_of_birth": 1978, 		"created_at": 1355386501092, 		"updated_at": 1355386501092 	}' ```  * __PUT__ _Request:_ ``` 	curl -i -H "Content-Type: application/json" -X PUT -d \ 	'{ 		"id":"1", 		"username":"johndoe", 		"password":"ǝopuɥoɾ", 		"name_first":"John", 		"name_middle":"M", 		"name_last":"Doe", 		"user_status":"ACTIVE", 		"date_of_birth":14, 		"month_of_birth":10, 		"year_of_birth":1978 	}' 'http://localhost:8400/demows/user/1' ```  * _Response:_ ``` 	'{ 		"id": 1, 		"username": "johndoe", 		"password": "41a87d0faca541b691f11c6d51874f37b989f0cb1b0075cce599e94f82cbbdfc", 		"name_first": "John", 		"name_middle": "M", 		"name_last": "Doe", 		"user_status": "ACTIVE", 		"user_status_code": "A", 		"date_of_birth": 14, 		"month_of_birth": 10, 		"year_of_birth": 1978, 		"created_at": 1355386501092, 		"updated_at": 1355388359982 	}' ```  * __GET (By User ID)__ _Request:_ ``` 	curl 'http://localhost:8400/demows/user/1' ```  * _Response:_ ``` 	'{ 		"id": 1, 		"username": "johndoe", 		"password": "41a87d0faca541b691f11c6d51874f37b989f0cb1b0075cce599e94f82cbbdfc", 		"name_first": "John", 		"name_middle": "M", 		"name_last": "Doe", 		"user_status": "ACTIVE", 		"user_status_code": "A", 		"date_of_birth": 14, 		"month_of_birth": 10, 		"year_of_birth": 1978, 		"created_at": 1355386501092, 		"updated_at": 1355388359982 	}' ```  * __GET (By UserName)__ _Request:_ ``` 	curl 'http://localhost:8400/demows/user/username/johndoe' ```  * _Response:_ ``` 	'{ 		"id": 1, 		"username": "johndoe", 		"password": "41a87d0faca541b691f11c6d51874f37b989f0cb1b0075cce599e94f82cbbdfc", 		"name_first": "John", 		"name_middle": "M", 		"name_last": "Doe", 		"user_status": "ACTIVE", 		"user_status_code": "A", 		"date_of_birth": 14, 		"month_of_birth": 10, 		"year_of_birth": 1978, 		"created_at": 1355386501092, 		"updated_at": 1355388359982 	}' ```  * __DELETE__ _Request:_ ``` 	curl -i -H "Content-Type: application/json" -X DELETE -d \ 	'{ 		"id":"1", 		"username":"johndoe", 		"password":"ǝopuɥoɾ", 		"name_first":"John", 		"name_middle":"M", 		"name_last":"Doe", 		"date_of_birth":14, 		"month_of_birth":10, 		"year_of_birth":1978 	}' 'http://localhost:8400/demows/user/1' ```  * _Response:_ _[Notice that the user gets marked as 'DELETED' and is not removed from the database]_ ``` 	'{ 		"id": 1, 		"username": "johndoe", 		"password": "41a87d0faca541b691f11c6d51874f37b989f0cb1b0075cce599e94f82cbbdfc", 		"name_first": "John", 		"name_middle": "M", 		"name_last": "Doe", 		"user_status": "DELETED", 		"user_status_code": "D", 		"date_of_birth": 14, 		"month_of_birth": 10, 		"year_of_birth": 1978, 		"created_at": 1355386501092, 		"updated_at": 1355388912852 	}' ```  #### Sample test CURL calls (http://localhost:8400/demows/purchase - this uses MongoDB for persistence) * __POST__ _Request:_ ```     curl -i -H "Content-Type: application/json" -X POST -d \     '{         "billing_address": {             "first_name" : "Travis",             "city" : "San Franciso",             "postal_code" : "94000",             "street_2" : "Street 2",             "last_name" : "John",             "street_1" : "Street 1",             "country_code" : "US",             "phone_number" : "415-415-1000",             "name" : "Travis John",             "state" : "CA"         },         "shipping_address": {             "first_name" : "Johnny",             "city" : "San Antonio",             "postal_code" : "32000",             "street_2" : "Street 2",             "last_name" : "Cash",             "street_1" : "Street 1",             "country_code" : "US",             "phone_number" : "420-420-1000",             "name" : "Johnny Cash",             "state" : "TX"         },         "billing_card": {             "card_type" : "AMERICAN_EXPRESS",             "card_number" : "kIXI51g+t88DJjaNguPMEQ==",             "security_code" : "1004",             "expiration_month" : 3,             "expiration_year" : 2020,             "encryption_type" : "sha512"         },         "user_ip_address": "127.127.127.127",         "email": "test@test.com",         "invoice_number": "INVOICE123",         "client_id": "CLIENT124",         "user_id": "PERSON123",         "order_number": "WON123",         "create_date": "MAR-21-2013",         "transaction": {             "currency_code" : "USD",             "transaction_amount" : "1099.99"         }     }' 'http://localhost:8400/demows/purchase' ```  * _Response:_ ``` 	'{        "id" : "5159d6d9456668495a2de972",        "order_number" : "WON123",        "create_date" : "MAR-21-2013",        "user_ip_address" : "127.127.127.127",        "transaction" : {          "currency_code" : "USD",          "transaction_amount" : "1099.99"        },        "invoice_number" : "INVOICE123",        "user_id" : "PERSON123",        "billing_address" : {          "first_name" : "Travis",          "city" : "San Franciso",          "postal_code" : "94000",          "street_3" : null,          "street_2" : "Street 2",          "last_name" : "John",          "street_1" : "Street 1",          "country_code" : "US",          "phone_number" : "415-415-1000",          "name" : "Travis John",          "state" : "CA"        },        "shipping_address" : {          "first_name" : "Johnny",          "city" : "San Antonio",          "postal_code" : "32000",          "street_3" : null,          "street_2" : "Street 2",          "last_name" : "Cash",          "street_1" : "Street 1",          "country_code" : "US",          "phone_number" : "420-420-1000",          "name" : "Johnny Cash",          "state" : "TX"        },        "client_id" : "CLIENT124",        "billing_card" : {          "card_type" : "AMERICAN_EXPRESS",          "card_number" : "kIXI51g+t88DJjaNguPMEQ==",          "security_code" : "1004",          "expiration_month" : "3",          "expiration_year" : "2020",          "encryption_type" : "sha512"        },        "email" : "test@test.com"      }' ```
oberasoftware/jasdb	## NOTE: JasDB is currently in low maintenance mode and does not receive any further big updates  JasDB is an open Document based database that can be used to store unstructured data. The database can be used standalone and accessed through a java client library or the REST interface.  Also the database can be used in process of your Java application, ideal for embedded projects (including on Android) where a small lightweight database is needed.  ## Features JasDB has the folowing features: * Lightweight memory and cpu profile * High throughput on a single machine * Full query capabilities * BTree index structure * REST webservice * Android support * Java client API (both for remote and local embedded mode)  ## Documentation For more information have a look at http://www.oberasoftware.com  Wiki for installation and API usage: https://github.com/oberasoftware/jasdb-open/wiki Javadoc: http://oberasoftware.github.io/jasdb/apidocs/  **Note: The above documentation is for the current LTS version of JasDB, the current master Branch points to version 2.0 which will break compatibility on the API (not the data format)**  ## Quick Installation 1. Install JasDB by unzipping the download 2. Start the database using start.bat or start.sh 3. Open http://localhost:7050  For more details see here: https://github.com/oberasoftware/jasdb-open/wiki/Installing-and-configuring-JasDB  ## Running using Docker We have a docker container available that allows you to very quickly get a running JasDB installation.  In order to run a standard docker container: ``` docker run -d -p 7050:7050 renarj/jasdb:1.1.2 ```  We however recommend running the jasdb data directory in a seperate volume, for example take the following to attach to a host volume ``` docker run -v /Users/devuser/dev/docker/volumes/jasdb-data:/jasdb-data -p 7050:7050 renarj/jasdb:1.1.2 ```  ### Java with Object mapping Example In JasDB we have an object mapper available which allows you to do quick operations using your regular Java objects. All you have to do is add some annotations on top of your Java beans.  Example bean: ```java @JasDBEntity(bagName = "TEST_BAG") public class TestEntity {     private String id;     private String firstName;     private String lastName;      @Id     @JasDBProperty     public String getId() { return id; }     public void setId(String id) { this.id = id; }      @JasDBProperty     public String getFirstName() { return firstName; }     public void setFirstName(String firstName) { this.firstName = firstName; }      @JasDBProperty     public String getLastName() { return lastName; }     public void setLastName(String lastName) { this.lastName = lastName; } } ```  The following code can be used for persisting these beans: ```java DBSession session = new LocalDBSession(); //new RestDBSession(); EntityManager entityManager = session.getEntityManager();  TestEntity entity = new TestEntity(null, "Renze", "de Vries");  String id = entityManager.persist(entity).getInternalId(); ```  The following can be used to query and retrieve the entities back from the database ```java DBSession session = new LocalDBSession(); //new RestDBSession(); EntityManager entityManager = session.getEntityManager(); TestEntity foundEntity = entityManager.findEntity(TestEntity.class, id);  //This will find all entities List<TestEntity> allEntities = entityManager.findEntities(TestEntity.class, QueryBuilder.createBuilder());  //Get all people named Piet QueryBuilder query = QueryBuilder.createBuilder().field("firstName").value("Piet"); List<TestEntity> peopleNamedPiet = entityManager.findEntities(TestEntity.class, query); ```  For more details on the entity mapping, please check this page on the wiki: https://github.com/oberasoftware/jasdb-open/wiki/Object-Entity-Mapping-API  ### Regular Java Client Example Besides using the object mapper you can also use JasDB object model which allows more finer grained control over the data itself. Also the API offers more operations to create indexes and other database operations.  Example on how to insert and retrieve data: ```java //Open DB Session DBSession session = new LocalDBSession(); EntityBag bag = session.createOrGetBag("MyBag");  //Insert some data SimpleEntity entity = new SimpleEntity(); entity.addProperty("title", "Title of my content"); entity.addProperty("text", "Some big piece of text content"); bag.addEntity(entity);  //Retrieve entity by Id SimpleEntity e = bag.getEntity("056f8058-e1f7-4f8e-a2f8-332e62c15961");  //Query the DB QueryExecutor executor = bag.find(QueryBuilder.createBuilder().field("field").value(queryKey)); QueryResult result = executor.execute(); for(SimpleEntity entity : result) {    //access the properties if desired    entity.getProperty("field1"); } ```  For more information: * WIKI: https://github.com/oberasoftware/jasdb-open/wiki/Java-Client-API * JavaDoc API: http://oberasoftware.github.io/jasdb/apidocs/  ## License This software adheres to the MIT X11 license: Copyright (c) 2014 Obera Software  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  There is an additional requirement to the above MIT X11 License: Redistribution of this software in source or binary forms shall be free of all charges or fees to the recipient of this software.
chubbyjiang/MapReduce	# MapReduce Demo  Map-Reduce程序场景代码。  已完成的有：  > 1.网站kpi数据统计    > 2.电信运营商用户基站停留数据统计    > 3.基于物品的协同过滤实现   > 4.测试mahout推荐算法API     > 5.使用自定义的分片策略和庖丁分词进行中文分析   > 6.PeopleRank算法并行化实现-mr的矩阵计算    > 7.简单实现sql的统计、groupby和join    > 8.实现简单的倒排索引    > 9.查找社交二度关系    > 10.广告精准营销  ## 1.网站kpi数据统计  程序中分别对五个kpi指标进行统计操作：    > 1.browser：用户使用的浏览器统计    > 2.ips：页面用户独立ip数统计    > 3.pv：网站pv量统计    > 4.source：用户来源网址统计    > 5.time：时间段用户访问量统计     [数据下载][2]  ## 2.电信运营商用户基站停留数据统计  原始数据分为位置和网络两种    位置数据格式为：     用户标识 设备标识 开关机信息 基站位置 通讯的时间    example:    0000009999  0054785806  3   00000089    2016-02-21 21:55:37  网络数据格式为：    用户标识 设备标识 基站位置 通讯的时间 访问的URL    example:    0000000999  0054776806  00000109    2016-02-21 23:35:18 www.baidu.com  需要得到的数据格式为：    用户标识 时段 基站位置 停留时间  example:    00001 09-18 00003 15    用户00001在09-18点这个时间段在基站00003停留了15分钟  两个reducer：  > 1.统计每个用户在不同时段中各个基站的停留时间    > 2.在1的结果上只保留停留时间最长的基站位置信息     [数据下载][3]  ## 3.基于物品的协同过滤实现  算法解释请看：[ItermCF的MR并行实现][4]  ## 4.测试mahout推荐算法API  > 1.RecommendFactory:推荐相关信息获取的工厂类,包括用户/物品相似度，用户邻居，用户/物品推荐器，数据模型，算法评分器    > 2.RecommendUtil:打印算法的评分结果、推荐结果等    > 3.RecommendEvaluator:从RecommendFactory中获得相似度、推荐器等进行算法组合，借助RecommendUtil打印出评分结果    > 4.RecommendResult:选取RecommendEvaluator中评分结果最好的两个算法，借助RecommendUtil打印出推荐结果     ###**调用mahout推荐算法流程:**  > 1.选择输入数据，创建DataModel对象    > 2.根据DataModel对象创建用户或物品的Similarity对象    > 3.如果是基于用户的，那么需要创建UserNeighborhood对象    > 4.根据以上的条件，创建用户或物品的Recommender对象      **使用算法评估器流程：**      > 1.123步骤同上    > 4.创建RecommenderBuilder对象，调用RecommenderEvaluator的evaluate方法进行评估    > 4.1评估查全率和召回率创建RecommenderIRStatsEvaluator对象，调用其evaluate方法返回IRStatistics对象    > 4.2调用IRStatistics对象的getPrecision,getRecall方法即可     **使用条件过滤的流程：**  > 1.使用自定义的类实现IDRescorer接口，在类中定义成员，在类外使用过滤规则，将过滤之后的结果存入类中      > 2.调用recommend方法进行推荐的时候加入参数IDRescorer      [数据下载][5]  ## 5.使用自定义的分片策略和庖丁分词进行中文分析  [庖丁分词需要的jar包和dic文件下载][7]  **庖丁分词的使用注意:**     > 1.paoding-analysis的包和mahout的包放在一起会冲突，mahout中重写了tokenStream方法为final类型，paoding中又需要重写此方法，会出现class net.paoding.analysis.analyzer.PaodingAnalyzerBean overrides final method tokenStream.的错误        **解决方法:**去掉mahout的包即可      > 2.在IDEA项目中，需要把庖丁的dic目录放入src/main/resources下，否则会报PaodingAnalysisException: not found the dic home directory!的异常，修改paoding-analysis包中的paoding-dic-home文件，设置paoding.dic.home=classpath:dic      > 3.打成jar包时，需要将dic目录复制到hadoop的classpath目录下（最好是第一个），否则会报PaodingAnalysisException: dic home should not be a file, but a directory!，虽然jar包中已经包含了dic。  **包说明：**     > 1.normal：使用默认的mapreduce分切策略，读取全部小文件需要8k+个mapper，cpu占用高达80%，运行时间n个小时     > 2.special：使用自定义的分片策略，将多个小文件合并，每次map处理的数据为一个文件的全部内容，而不是一行，启动的mapper为4个，运行时间大大提高      [自定义的分片策略解决大量小文件的问题][8]  [数据下载][6]  ## 6.PeopleRank算法并行化实现-mr的矩阵计算  使用mapreduce框架实现PeopleRank算法，并在示例数据集上进行测试。  PeopleRank算法实现过程：    > 1.AdjacencyMapper：将原始数据集转换成邻接表    > 2.AdjacencyReducer：计算邻接表的每一行，转换成邻接矩阵之后使用PageRank的计算公式推导邻接概率矩阵    > 3.CalcPeopleRankMapper：输入邻接概率矩阵和pr矩阵，运用矩阵相乘规律将数据输出到reduce进行计算   > 4.CalcPeopleRankReducer：分别用两个map保存邻接矩阵和pr矩阵的值进行计算    > 5.FinallyResultMapper：将计算得到的pr值统一输出到reduce中进行转换计算    > 6.FinallyResultReducer：对每个pr值/pr总值，得到的数据即为最终结果  该算法重点在于**矩阵的计算**     整个mapreduce作业中最核心的就是**CalcPeopleRank**这部分    在mapreduce中进行矩阵计算的技巧在于**从矩阵乘法公式中找出矩阵相乘的规律**     map过程：    > 1.读取第一个矩阵的每一行的**每一个数据**，并做标识处理     > 2.**将行号作为key**，读取第二个矩阵的时候按照**列**读，**以列号为key**，这样一来两个矩阵中**对应的需要计算的值**都都被一起输出到reduce中    > 3.由于两个矩阵的值都会出现在reduce中，所以需要在map的value中设置一个**标识位**，如：A,B等，表示这个数值是第一个矩阵还是第二个矩阵的    > 4.即使将两个矩阵的行和列对应起来了，但是没有**将行列中各个值对应起来**也是没办法计算的，所以map得value中还应该**包含该数值在当前矩阵中是第几列**（对于第二个按列读取的矩阵来说就是第几行）  reduce过程：    > 1.每个输入都是**两个矩阵相对应的行和列**，并且从value中可以得到该值是哪个矩阵，第几行第几列的值    > 2.根据value中**矩阵的标志位**对不同的矩阵值做不同的处理，分别加入**两个map字典**中，key为原本value中包含的行列标志位，value为数值本身    > 3.现在只要遍历任意一个map，**取出一个值的时候就根据该值的key到另外一个map中取出对应的值进行相乘**，最后将结果相加即可  [数据下载][9]  ## 7.简单实现sql的统计、groupby和join  ### 统计最大、最小和平均值  根据给出的文件（表），内容格式为     |Name|age| |---|---| |abc|12| |...|...|  统计年龄的最大、最小和平均值    sql示例:    ```sql select avg(age) as avg,max(age) as max,min(age) as min from xxx; ```  ### group by  根据给出的文件（表），内容格式为     |customer|order_price| |---|---| |1|100| |2|130| |...|...|  实现根据costomer进行分组，统计每个customer的总订单金额    sql示例:    ```sql select customer,sum(order_price) from orders group by customer ```  ### join连接  根据给出的文件（表）    Customer表结构为:     |id|name| |---|---| |1|chubby| |2|xiaohei| |...|...|  Orders表结构为:     |id|cus_id| |1|1| |2|1| |3|2| |...|...|  实现两个表的join连接    sql示例:  ```sql select Customer.name,Orders.id from Customers left join Orders on Customers.id=Orders.cus_id ```  ## 8.实现简单的倒排索引  倒排索引的详情参考：    [mapreduce实现搜索引擎简单的倒排索引](http://www.xiaohei.info/2015/03/19/mapreduce-inverted-index/)  ## 9.查找社交二度关系  一个简单的类似QQ好友推荐的功能，思想就是找到好友之间的二度关系，例如有如下好友关系：    小明 小红    张三 李四    王五 小李    小红 小黑 小李 小红  以小明为例，关系图为：小明->小红,小红->小黑    那么小明和小黑之间就有一个二度关系，可以互相推荐好友（P.S. 著名的xx说世界上的任何两个人最多通过7层关系都可以联系起来）  我们要做的事情就是将有二度关系的人找出来    首先要确定map的key和value应该是什么才能在reduce中得到需要的结果    根据mr程序的尿性，在reduce的时候会将key相同的value聚集在一起，那么可以将二度关系中的链接点作为key，reduce中就可以拿到推荐的好友    如上例中的小红为key，小明和小黑为value  mr程序很简单：  map过程中将每行的1和2作为key-value输出一次，反过来将2和1作为key-value再次输出    reduce中可以拿到所有key相同的一个集合，集合中的每个元素都是有二度关系的，对其进行全排列即可得到推荐好友名单  ## 10.广告精准营销  案例说明请参考：[MapReduce广告精准营销案例](http://www.xiaohei.info/2016/04/08/mapreduce-weibo-ad/)  详情见代码  作者：[@小黑][1]  [1]:http://www.xiaohei.info [2]:http://download.csdn.net/detail/qq1010885678/9439530 [3]:http://download.csdn.net/detail/qq1010885678/9439587 [4]:http://blog.csdn.net/qq1010885678/article/details/50751607 [5]:http://download.csdn.net/detail/qq1010885678/9446510 [6]:http://download.csdn.net/detail/qq1010885678/9447741 [7]:http://download.csdn.net/detail/qq1010885678/9448143 [8]:http://blog.csdn.net/qq1010885678/article/details/50771361 [9]:http://download.csdn.net/detail/qq1010885678/9456762
caskdata/cdap	.. meta::     :author: Cask Data, Inc.     :copyright: Copyright © 2015-2017 Cask Data, Inc.  ===================================== Cask Data Application Platform - CDAP ===================================== .. image:: cdap-docs/developer-manual/source/_images/CDAP.png  .. image:: https://cdap-users.herokuapp.com/badge.svg?t=1     :target: https://cdap-users.herokuapp.com  .. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg     :target: https://opensource.org/licenses/Apache-2.0  .. image:: https://travis-ci.org/caskdata/cdap.svg?branch=release/4.1     :target: https://travis-ci.org/caskdata/cdap   Introduction ============  The Cask™ Data Application Platform (CDAP)is an integrated, open source application development platform for the Hadoop ecosystem that provides developers with data and application abstractions to simplify and accelerate application development, address a broader range of real-time and batch use cases, and deploy applications into production while satisfying enterprise requirements.  CDAP is a layer of software running on top of Apache Hadoop® platforms such as the Cloudera Enterprise Data Hub or the Hortonworks® Data Platform. CDAP provides these essential capabilities:  - Abstraction of data in the Hadoop environment through logical representations of underlying data; - Portability of applications through decoupling underlying infrastructures; - Services and tools that enable faster application creation in development; - Integration of the components of the Hadoop ecosystem into a single platform; - Metadata management that automatically captures metadata and lineage; - CDAP pipelines with an integrated UI for click-and-drag development; and - Higher degrees of operational control in production through enterprise best-practices.  CDAP exposes developer APIs (Application Programming Interfaces) for creating applications and accessing core CDAP services. CDAP defines and implements a diverse collection of services that land applications and data on existing Hadoop infrastructure such as HBase, HDFS, YARN, MapReduce, Hive, and Spark.  You can run applications ranging from simple MapReduce Jobs and complete ETL (extract, transform, and load) pipelines all the way up to complex, enterprise-scale data-intensive applications.  Developers can build and test their applications end-to-end in a full-stack, single-node installation. CDAP can be run either as a Sandbox, deployed within the Enterprise  on-premises or hosted in the Cloud.  For more information, see our collection of `Developers' Manual and other documentation <http://docs.cask.co/cdap/current/en/developers-manual/index.html>`__.   Getting Started ===============  Prerequisites -------------  To install and use CDAP and its included examples, there are a few simple prerequisites:  1. JDK 7+ (required to run CDAP; note that $JAVA_HOME should be set) #. `Node.js <https://nodejs.org/>`__ (required to run the CDAP UI; we recommend any version greater than v4.5.0) #. Apache Maven 3.0+ (required to build the example applications; 3.1+ to build CDAP itself)  Build -----  You can get started with CDAP by building directly from the latest source code::    git clone https://github.com/caskdata/cdap.git   cd cdap   mvn clean package  After the build completes, you will have built all modules for CDAP.  For more build options, please refer to the `build instructions <BUILD.rst>`__.   Introductory Tutorial =====================  Visit our web site for an `introductory tutorial for developers <http://docs.cask.co/cdap/current/en/developers-manual/getting-started/index.html>`__ that will guide you through installing CDAP and running an example application.   Where to Go Next ================  Now that you've had a look at the CDAP Sandbox, take a look at:  - Examples, located in the ``cdap-examples`` directory of the CDAP Sandbox; - `Selected Examples <http://docs.cask.co/cdap/current/en/examples-manual/examples/index.html>`__   (demonstrating basic features of the CDAP) are located on-line; and - Developers' Manual, located in the source distribution in ``cdap-docs/developers-manual/source``   or `online <http://docs.cask.co/cdap/current/en/developers-manual/index.html>`__. - `CDAP Releases and timeline <http://docs.cask.co/cdap/index.html>`__   How to Contribute =================  Interested in helping to improve CDAP? We welcome all contributions, whether in filing detailed bug reports, submitting pull requests for code changes and improvements, or by asking questions and assisting others on the mailing list.  For quick guide to getting your system setup to contribute to CDAP, take a look at our `Contributor Quickstart Guide <DEVELOPERS.rst>`__.  Filing Issues: Bug Reports & Feature Requests --------------------------------------------- Bugs and suggestions should be made by `filing an issue <https://issues.cask.co/browse/cdap>`__.  Existing issues can be browsed at `the CDAP project issues <https://issues.cask.co/browse/CDAP-8373?jql=project%20%3D%20CDAP>`__.  Pull Requests -------------  We have a simple pull-based development model with a consensus-building phase, similar to Apache's voting process. If you’d like to help make CDAP better by adding new features, enhancing existing features, or fixing bugs, here's how to do it:  1. If you are planning a large change or contribution, discuss your plans on the    `cdap-dev@googlegroups.com <https://groups.google.com/d/forum/cdap-dev>`__ mailing list first.    This will help us understand your needs and best guide your solution in a way that fits the project. 2. Fork CDAP into your own GitHub repository. 3. Create a topic branch with an appropriate name. 4. Work on the code to your heart's content. 5. Once you’re satisfied, create a pull request from your GitHub repo (it’s helpful if you fill in    all of the description fields). 6. After we review and accept your request, we’ll commit your code to the caskdata/cdap repository.  Thanks for helping to improve CDAP!  Mailing Lists -------------  CDAP User Group and Development Discussions:  - `cdap-user@googlegroups.com <https://groups.google.com/d/forum/cdap-user>`__  The *cdap-user* mailing list is primarily for users using the product to develop applications. You can expect questions from users, release announcements, and any other discussions that we think will be helpful to the users.  - `cdap-dev@googlegroups.com <https://groups.google.com/d/forum/cdap-dev>`__  The *cdap-dev* mailing list is essentially for developers actively working on the product, and should be used for all our design, architecture and technical discussions moving forward. This mailing list will also receive all JIRA and GitHub notifications.   License and Trademarks ======================  Copyright © 2014-2017 Cask Data, Inc.  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.  Cask is a trademark of Cask Data, Inc. All rights reserved.  Apache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with permission. No endorsement by The Apache Software Foundation is implied by the use of these marks.
lintool/bespin	# Bespin  Bespin is a library that contains reference implementations of "big data" algorithms in MapReduce and Spark.  ## Getting Started  Build the package:  ``` $ mvn clean package ```  Grab the data:  ``` $ mkdir data $ curl http://lintool.github.io/bespin-data/Shakespeare.txt > data/Shakespeare.txt $ curl http://lintool.github.io/bespin-data/p2p-Gnutella08-adj.txt > data/p2p-Gnutella08-adj.txt ```  The datasets are stored in the [Bespin data repo](https://github.com/lintool/bespin-data).  + The file `Shakespeare.txt` contains the [The Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/). + The file `p2p-Gnutella08-adj.txt` contains a [snapshot of the Gnutella peer-to-peer file sharing network from August 2002](http://snap.stanford.edu/data/p2p-Gnutella08.html), where nodes represent hosts in the Gnutella network topology and edges represent connections between the Gnutella hosts. This dataset is available from the [Stanford Network Analysis Project](http://snap.stanford.edu/).   ## Word Count in MapReduce and Spark  Make sure you've downloaded the Shakespeare collection (see "Getting Started" above). Running word count in Java MapReduce:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.wordcount.WordCount \    -input data/Shakespeare.txt -output wc-jmr-combiner ```  To enable the "in-mapper combining" optimization, use the `-imc` option.  Running word count in Scala MapReduce:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.scala.mapreduce.wordcount.WordCount \    --input data/Shakespeare.txt --output wc-smr-combiner ```  To enable the "in-mapper combining" optimization, use the `--imc` option.  And finally, running word count in Spark:  ``` $ spark-submit --class io.bespin.scala.spark.wordcount.WordCount target/bespin-1.0.1-SNAPSHOT-fatjar.jar \     --input data/Shakespeare.txt --output wc-spark-default ```  To enable the "in-mapper combining" optimization in Spark, use the `--imc` option (although this optimization doesn't do anything: exercise left to the reader as to why).  Compare results to make sure they are the same:  ``` $ hadoop fs -cat wc-jmr-combiner/part-r-0000* | awk '{print $1,$2;}' | sort > counts.jmr.combiner.txt $ hadoop fs -cat wc-smr-combiner/part-r-0000* | awk '{print $1,$2;}' | sort > counts.smr.combiner.txt $ hadoop fs -cat wc-spark-default/part-0000* | sed -E 's/^\((.*),([0-9]+)\)$/\1 \2/' | sort > counts.spark.default.txt $ diff counts.jmr.combiner.txt counts.smr.combiner.txt $ diff counts.jmr.combiner.txt counts.spark.default.txt ```  **Tip:** We use `awk` in some cases and `sed` in others because `sed` does not accept control characters such as `\t` (but GNU `sed` does), so you have to insert a literal tab in the command line. This is awkward: on Mac OS X, you need to type `^V^I`; so it's just easier with `awk`.  ## Computing Bigram Relative Frequencies in MapReduce  Make sure you've downloaded the Shakespeare collection (see "Getting Started" above). Running a simple bigram count:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bigram.BigramCount \    -input data/Shakespeare.txt -output bigram-count ```  Computing bigram relative frequencies using the "pairs" implementation:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bigram.ComputeBigramRelativeFrequencyPairs \    -input data/Shakespeare.txt -output bigram-freq-mr-pairs -textOutput ```  Computing bigram relative frequencies using the "stripes" implementation:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bigram.ComputeBigramRelativeFrequencyStripes \    -input data/Shakespeare.txt -output bigram-freq-mr-stripes -textOutput ```  To obtain human-readable output, make sure to use the `-textOutput` option; otherwise, the job defaults to `SequenceFile` output.  Let's spot-check the output to make sure the results are correct. For example, what are the bigrams that begin with "dream"?  ``` $ hadoop fs -cat bigram-count/part* | grep '^dream ' dream again	2 dream and	2 dream are	1 dream as	1 dream away	2 ... ```  What is the sum of all these counts?  ``` $ hadoop fs -cat bigram-count/part* | grep '^dream ' | cut -f 2 | awk '{sum+=$1} END {print sum}' 79 ```  Confirm that the numbers match the "pairs" implementation of the relative frequency computations:  ``` $ hadoop fs -cat bigram-freq-mr-pairs/part* | grep '(dream, ' ```  And the "stripes" implementation of the relative frequency computations:  ``` $ hadoop fs -cat bigram-freq-mr-stripes/part* | awk '/^dream\t/' ```  **Tip:** Note that `grep` in Mac OS X accepts `\t`, but not on Linux; strictly speaking, `grep` uses regular expressions as defined by POSIX, and for whatever reasons POSIX does not define `\t` as tab. One workaround is to use `-P`, which specifies Perl regular expressions; however the `-P` option does not exist in Mac OS X.  Here's how you can verify that the pairs and stripes implementation give you the same results:  ``` $ hadoop fs -cat bigram-freq-mr-pairs/part-r-0000* | awk '{print $1$2,$3;}' | grep -v ",\*)" | sort > freq.mr.pairs.txt $ hadoop fs -cat bigram-freq-mr-stripes/part-r-0000* | perl -ne '%H=();m/([^\t]+)\t\{(.*)\}/; $k=$1; @k=split ", ",$2; foreach(@k){@p=split "=",$_;$H{$p[0]}=$p[1];}; foreach (sort keys %H) {print "($k,$_) $H{$_}\n";}' | sort > freq.mr.stripes.txt $ diff freq.mr.stripes.txt freq.mr.pairs.txt ```  ## Computing Term Co-occurrence Matrix in MapReduce  Make sure you've downloaded the Shakespeare collection (see "Getting Started" above). Running the "pairs" implementation:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.cooccur.ComputeCooccurrenceMatrixPairs \    -input data/Shakespeare.txt -output cooccur-pairs -window 2 ```  Running the "stripes" implementation:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.cooccur.ComputeCooccurrenceMatrixStripes \    -input data/Shakespeare.txt -output cooccur-stripes -window 2 ```  Let's spot check the results. For example, here are all the terms the co-occur with "dream" with the "pairs" implementation:  ``` $ hadoop fs -cat cooccur-pairs/part* | grep '(dream, ' ```  We can verify that the "stripes" implementation gives the same results.  ``` $ hadoop fs -cat cooccur-stripes/part* | awk '/^dream\t/' ```  **Tip:** Note that `grep` in Mac OS X accepts `\t`, but not on Linux; strictly speaking, `grep` uses regular expressions as defined by POSIX, and for whatever reasons POSIX does not define `\t` as tab. One workaround is to use `-P`, which specifies Perl regular expressions; however the `-P` option does not exist in Mac OS X.  ## Inverted Indexing and Boolean Retrieval in MapReduce  Make sure you've downloaded the Shakespeare collection (see "Getting Started" above). Building the inverted index:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.search.BuildInvertedIndex \    -input data/Shakespeare.txt -output index ```  Looking up an individual postings list:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.search.LookupPostings \    -index index -collection data/Shakespeare.txt -term "star-cross'd" ```  Running a boolean retrieval:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.search.BooleanRetrieval \    -index index -collection data/Shakespeare.txt -query "white red OR rose AND pluck AND" ```  Note that the query must be in [Reverse Polish notation](https://en.wikipedia.org/wiki/Reverse_Polish_notation), so the above is equivalent to `(white OR red) AND rose AND pluck` in standard infix notation.  ## Parallel Breadth-First Search in MapReduce  Make sure you've grabbed the sample graph data (see "Getting Started" above). First, convert the plain-text adjacency list representation into Hadoop `Writable` records:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bfs.EncodeBfsGraph \    -input data/p2p-Gnutella08-adj.txt -output graph-BFS/iter0000 -src 367 ```  In the current implementation, you have to run a MapReduce job for every iteration, like this:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bfs.IterateBfs \    -input graph-BFS/iter0000 -output graph-BFS/iter0001 -partitions 5 ```  Here's a bash script to run a bunch of iterations:  ``` #!/bin/bash  for i in `seq 0 14`; do   cur=`echo $i | awk '{printf "%04d\n", $0;}'`   next=`echo $(($i+1)) | awk '{printf "%04d\n", $0;}'`   echo "Iteration $i: reading graph-BFS/iter$cur, writing: graph-BFS/iter$next"   hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bfs.IterateBfs -input "graph-BFS/iter$cur" -output "graph-BFS/iter$next" -partitions 5 done ```  The MapReduce job counters tell you how many nodes are reachable at each iteration:  |Iteration |  Reachable | Distance |---------:|-----------:|--------: |        0 |          1 |       1 |        1 |          9 |       8 |        2 |         65 |      56 |        3 |        257 |     192 |        4 |        808 |     551 |        5 |       1934 |    1126 |        6 |       3479 |    1545 |        7 |       4790 |    1311 |        8 |       5444 |     654 |        9 |       5797 |     353 |       10 |       5920 |     123 |       11 |       5990 |      70 |       12 |       6018 |      28 |       13 |       6026 |       8 |       14 |       6028 |       2 |       15 |       6028 |       0  To find all the nodes that are reachable at a particular iteration, run the following job:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bfs.FindReachableNodes \    -input graph-BFS/iter0005 -output graph-BFS/reachable-iter0005  $ hadoop fs -cat 'graph-BFS/reachable-iter0005/part*' | wc ```  These values should be the same as those in the second column of the table above.  To find all the nodes that are at a particular distance (e.g., the search frontier), run the following job:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bfs.FindNodeAtDistance \    -input graph-BFS/iter0005 -output graph-BFS/d0005 -distance 5  $ hadoop fs -cat 'graph-BFS/d0005/part*' | wc ```  The results are shown in the third column of the table above.  Here's a simple bash script for iterating through the reachability jobs:  ``` #!/bin/bash  for i in `seq 0 15`; do   cur=`echo $i | awk '{printf "%04d\n", $0;}'`   echo "Iteration $i: reading graph-BFS/iter$cur"   hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bfs.FindReachableNodes -input graph-BFS/iter$cur -output graph-BFS/reachable-iter$cur done ```  Here's a simple bash script for extracting nodes at each distance:  ``` #!/bin/bash  for i in `seq 0 15`; do   cur=`echo $i | awk '{printf "%04d\n", $0;}'`   echo "Iteration $i: reading graph-BFS/iter$cur"   hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.bfs.FindNodeAtDistance -input graph-BFS/iter$cur -output graph-BFS/d$cur -distance $i done ```  ## PageRank in MapReduce  Make sure you've grabbed the sample graph data (see "Getting Started" above). First, convert the plain-text adjacency list representation into Hadoop `Writable` records:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.pagerank.BuildPageRankRecords \    -input data/p2p-Gnutella08-adj.txt -output graph-PageRankRecords -numNodes 6301 ```  Create the directory where the graph is going to go:  ``` $ hadoop fs -mkdir graph-PageRank ```  Partition the graph:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.pagerank.PartitionGraph \    -input graph-PageRankRecords -output graph-PageRank/iter0000 -numPartitions 5 -numNodes 6301 ```  Run 15 iterations:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.pagerank.RunPageRankBasic \    -base graph-PageRank -numNodes 6301 -start 0 -end 15 -useCombiner ```  Extract the top 20 nodes by PageRank value and examine the results:  ``` $ hadoop jar target/bespin-1.0.1-SNAPSHOT-fatjar.jar io.bespin.java.mapreduce.pagerank.FindMaxPageRankNodes \    -input graph-PageRank/iter0015 -output graph-PageRank-top20 -top 20  $ hadoop fs -cat graph-PageRank-top20/part-r-00000 367     -6.03734 249     -6.12637 145     -6.18742 264     -6.21511 266     -6.23297 123     -6.28525 127     -6.28685 122     -6.29073 1317    -6.29597 5       -6.30274 251     -6.32983 427     -6.33821 149     -6.40216 176     -6.42350 353     -6.43988 390     -6.44404 559     -6.45491 124     -6.45705 4       -6.47055 7       -6.50145 ```  Compare the results with a sequential PageRank implementation:  ``` $ mvn exec:java -Dexec.mainClass=io.bespin.java.mapreduce.pagerank.SequentialPageRank \    -Dexec.args="-input data/p2p-Gnutella08-adj.txt -jump 0.15" ```  The results should be the same.
marginweb/Wikipedia-noSQL-Benchmark	Please see http://www.nosqlbenchmarking.com for more informations and benchmark results  You can now build this project using Maven. As a few dependencies where not available in any maven repository, I have decided to provide them myself in the lib directory. You still have to add those jar to your local maven repository, to do so execute the script addToLocalRepo.sh or execute each of the three lines that it contains. For Cassandra and HBase, there is a little more configuration needed. For Cassandra edit the file storage-conf.xml to reflect your cluster configuration. For HBase edit the file hbase-site.xml to reflect your cluster configuration.  Then you only have to run "mvn package" to build the jar that contains everything needed. It will be in the folder "target" and it will be called "Wikipedia-nosql-benchmark-1.0-SNAPSHOT.jar".   You can use the jar as a single client or with a controller and several clients if you need (they are mainly usefull if the client's bandwidth is too small). You can also use this jar to insert the documents in the DBs and to measure elasticity.  h1. Fill the database   Call the jar file like this :  *java -jar jarfile.jar fillDB dbType /path/to/documents/ numberOfDocuments nodeAdress firstID numberOfInsertRun*   *dbType* can currently take the following values : * cassandra * scalaris (please note that the current implementation should be optimized) * voldemort (no MapReduce implementation so only read/update can be benchmarked) * terrastore (partial implementation, needs work) * riak * mongodb * hbase  */path/to/documents/* is the path to a directory that contains files named as increasing integers going from 1 to *numberOfDocuments*  *nodeAdress* is the IP address of a working node in the cluster that can handle writes *firstID* is the integer that should be used as the starting ID of the inserts, this is usefull is you already have inserted documents and want to increase the data set with the same documents but using new IDs *numberOfInsertRun* is the number of time you want to insert all the documents   h1. Run the benchmark in standalone mode (only one client)  To *benchmark the read and update performances* call the jarfile like this :   *java -jar jarfile.jar runBenchmark dbType totalNumberOp readPercentage numberOfDocuments IP1 IP2 IP3 ...*    *dbType* can currently take the following values : * cassandra * scalaris (please note that the current implementation should be optimized) * voldemort (no MapReduce implementation so only read/update can be benchmarked) * terrastore (partial implementation, needs work) * riak * mongodb * hbase  *totalNumberOp* is the total number of operations that will be requested to the cluster.  *readPercentage* is the proportion of requests that will be read-only  *numberOfDocuments* the total number of documents already inserted in the DB *IP1 IP2 IP3 ...* is a list of IP that can be as long as desired. For the systems without a load balancer, I put in the list the IP of each  active node in the cluster. For systems with a load balancer, I simply put the IP of the load balancer as many time as there are active  nodes in the cluster.   If you want to *benchmark the MapReduce performances* , call the jar file like this :   *java -jar jarfile.jar dbType search numberOfRuns IP*   *dbType* can take any value listed just before provided that this database has a MapReduce implementation.  *search* is a constant  *numberOfRuns* is the number of time that the search benchmark will run  *IP* is the IP of one of the nodes of the cluster that accept MapReduce jobs  *Please note that the elasticity test is only available in controller/client mode*   h1. Run the benchmark in controller/client mode   This mode use configuration files that must be in the same directory than the jar : * *benchmark_clients* must contain the IP addresses of all the servers running a client, one IP per line without any blank line * *benchmark_nodes* must contain the IP addresses of all the servers that should be benchmarked * *elasticityArgs* must contain the following line if you want to use the elasticity test :  - the upper limit acceptable for the standard deviation on 10 runs (use a statistical test at 95% for example)  - the lower limit acceptable for the standard deviation on 10 runs   - the number of milliseconds that must be waited for between runs  - the maximum number of runs   First you have to start the clients on each server that you specified in *benchmark_clients*, to do so simply run the command  *java -jar jarfile.jar client* on each of them.  To *benchmark the read and update performances* call the jarfile like this :  *java -jar jarfile.jar controller dbType totalNumberOp readPercentage numberOfDocuments*  Please note that the arguments are exactly the same than for the standalone client, except for the IP list that is now provided in the file *benchmark_nodes*  If you want to *benchmark the MapReduce performances* , call the jar file like this :  *java -jar wikipedia.jar controller dbType search numberOfRuns*  The arguments are exactly the same than the ones for the standalone search benchmark, except for the server address that will be used. Note that the IP that will be used is the first one in the file *benchmark_nodes*  If you want to *start the elasticity test* that will run until it sees 5 consecutive runs whose standard deviations are within the bounds defined by the two first lines of the file *elasticityArgs* or until it reach the maximum number of run defined in the same file. Call the jarfile like this : *java -jar jarfile.jar controller dbType totalNumberOp readPercentage numberOfDocuments elasticity*  Finally you can kill all the clients from the controller by running the command : *java -jar jarfile.jar controller kill*
flipkart-incubator/hbase-object-mapper	# HBase Object Mapper  ## Introduction This compact utility library is an annotation based *object mapper* for HBase (written in Java) that helps you:  * convert objects of your bean-like classes to HBase rows and vice-versa     * for use in Hadoop MapReduce jobs that read from and/or write to HBase tables     * and write efficient unit-tests for `Mapper` and `Reducer` classes * define *data access objects* for entities that map to HBase rows     * for single/range/bulk access of rows of an HBase table  ## Usage Let's say you've an HBase table `citizens` with row-key format of `country_code#UID`. Now, let's say your table is created with three column families `main`, `optional` and `tracked`, which may have columns `uid`, `name`, `salary` etc.  This library enables to you represent your HBase table as a bean-like class, as below:  ```java @HBTable(name = "citizens", families = {@Family(name = "main"), @Family(name = "optional", versions = 3), @Family(name = "tracked", versions = 10)}) public class Citizen implements HBRecord<String> {      @HBRowKey     private String countryCode;          @HBRowKey     private Integer uid;          @HBColumn(family = "main", column = "name")     private String name;          @HBColumn(family = "optional", column = "age")     private Short age;          @HBColumn(family = "optional", column = "salary")     private Integer sal;          @HBColumn(family = "optional", column = "custom_details")     private Map<String, Integer> customDetails;          @HBColumn(family = "optional", column = "dependents")     private Dependents dependents;          @HBColumnMultiVersion(family = "tracked", column = "phone_number")     private NavigableMap<Long, Integer> phoneNumber;          @HBColumn(family = "optional", column = "pincode", codecFlags = {@Flag(name = BestSuitCodec.SERIALIZE_AS_STRING, value = "true")})     private Integer pincode;          @Override     public String composeRowKey() {         return String.format("%s#%d", countryCode, uid);     }      @Override     public void parseRowKey(String rowKey) {         String[] pieces = rowKey.split("#");         this.countryCode = pieces[0];         this.uid = Integer.parseInt(pieces[1]);     }          // Constructors, getters and setters }  ``` That is,  * The above class `Citizen` represents the HBase table `citizens`, using the `@HBTable` annotation. * Logics for conversion of HBase row key to member variables of `Citizen` objects and vice-versa are implemented using `parseRowKey` and `composeRowKey` methods respectively. * The data type representing row key is the type parameter to `HBRecord` generic interface (in above case, `String`). Fields that form row key are annotated with `@HBRowKey`. * Names of columns and their column families are specified using `@HBColumn` or `@HBColumnMultiVersion` annotations. * The class may contain fields of simple data types (e.g. `String`, `Integer`), generic data types (e.g. `Map`, `List`), custom class (e.g. `Dependents`) or even generics of custom class (e.g. `List<Dependent>`)  * The `@HBColumnMultiVersion` annotation allows you to map multiple versions of column in a `NavigableMap<Long, ?>`. In above example, field `phoneNumber` is mapped to column `phone_number` within the column family `tracked` (which is configured for multiple versions)  See source files [Citizen.java](./src/test/java/com/flipkart/hbaseobjectmapper/testcases/entities/Citizen.java) and [Employee.java](./src/test/java/com/flipkart/hbaseobjectmapper/testcases/entities/Employee.java) for detailed examples.  ### Serialization / Deserialization  * The default codec of this library has the following behavior:     * uses HBase's native methods to serialize objects of data types `Boolean`, `Short`, `Integer`, `Long`, `Float`, `Double`, `String` and `BigDecimal`     * uses [Jackson's JSON serializer](http://wiki.fasterxml.com/JacksonHome) for all other data types     * serializes `null` as `null` * To control/modify serialization/deserialization behavior, you may define your own codec (by implementing the `Codec` interface) or you may extend the default codec (`BestSuitCodec`). * The optional parameter `codecFlag` (supported by both `@HBColumn` and `@HBColumnMultiVersion` annotations) can be used to pass custom flags to the underlying codec. (e.g. You may write your codec to serialize field `Integer id` in `Citizen` class differently from field `Integer id` in `Employee` class) * The default codec class `BestSuitCodec` takes a flag `BestSuitCodec.SERIALIZE_AS_STRING`, whose value is "serializeAsString" (as in the above `Citizen` class example). When this flag is set to `true` on a field, the default codec serializes that field (even numerical fields) as `String`s.     * Your custom codec may take other such flags to customize serialization/deserialization behavior at a class field level.  ## MapReduce use-cases  ### Mapper If your MapReduce job is reading from an HBase table, in your `map()` method, HBase's `Result` object can be converted to object of your bean-like class using below method:   ```java T readValue(ImmutableBytesWritable rowKey, Result result, Class<T> clazz) ```  For example:  ```java Citizen e = hbObjectMapper.readValue(key, value, Citizen.class); ``` See file [CitizenMapper.java](./src/test/java/com/flipkart/hbaseobjectmapper/testcases/mr/samples/CitizenMapper.java) for full sample code.  ### Reducer If your MapReduce job is writing to an HBase table, in your `reduce()` method, object of your bean-like class can be converted to HBase's `Put` (for row contents) and `ImmutableBytesWritable` (for row key) using below methods:  ```java ImmutableBytesWritable getRowKey(HBRecord<R> obj) ``` ```java Put writeValueAsPut(HBRecord<R> obj) ``` For example, below code in Reducer writes your object as one HBase row with appropriate column families and columns:  ```java Citizen citizen = new Citizen(/*details*/); context.write(hbObjectMapper.getRowKey(citizen), hbObjectMapper.writeValueAsPut(citizen)); ```  See file [CitizenReducer.java](./src/test/java/com/flipkart/hbaseobjectmapper/testcases/mr/samples/CitizenReducer.java) for full sample code.  ### Unit-test for Mapper If your MapReduce job is reading from an HBase table, you would want to unit-test your `map()` method as below.  Object of your bean-like class can be converted to HBase's `Result` (for row contents) and `ImmutableBytesWritable` (for row key) using below methods:  ```java ImmutableBytesWritable getRowKey(HBRecord<R> obj) ``` ```java Result writeValueAsResult(HBRecord<R> obj) ``` Below is an example of unit-test of a Mapper using [MRUnit](https://mrunit.apache.org/):  ```java Citizen citizen = new Citizen(/*params*/); citizenMapDriver .withInput( 	hbObjectMapper.getRowKey(citizen), 	hbObjectMapper.writeValueAsResult(citizen) ) .withOutput( 	hbObjectMapper.toIbw("key"), 	new IntWritable(citizen.getAge()) ) .runTest(); ```  See file [TestCitizenMR.java](./src/test/java/com/flipkart/hbaseobjectmapper/testcases/mr/TestCitizenMR.java) for full sample code.  ### Unit-test for Reducer If your MapReduce job is writing to an HBase table, you would want to unit-test your `reduce()` method as below.  HBase's `Put` object can be converted to your object of you bean-like class using below method:   ```java T readValue(ImmutableBytesWritable rowKey, Put put, Class<T> clazz) ```  Below is an example of unit-test of a Reducer using [MRUnit](https://mrunit.apache.org/):  ```java Pair<ImmutableBytesWritable, Mutation> reducerResult = citizenReduceDriver 	.withInput( 		hbObjectMapper.toIbw("key"), 		inputList 		) 	.run() .get(0); CitizenSummary citizenSummary = hbObjectMapper.readValue( 	reducerResult.getFirst(), 	(Put) reducerResult.getSecond(), 	CitizenSummary.class ); ```  Again, see file [TestCitizenMR.java](./src/test/java/com/flipkart/hbaseobjectmapper/testcases/mr/TestCitizenMR.java) for full sample code.  ## HBase ORM This library provides an abstract class to define your own *data access object*. For example you can create a *data access object* for `Citizen` class in the above example as follows:  ```java import org.apache.hadoop.conf.Configuration;  import java.io.IOException;  public class CitizenDAO extends AbstractHBDAO<String, Citizen> {      public CitizenDAO(Configuration conf) throws IOException {         super(conf); // if you need to customize your codec, you may use super(conf, codec)     } } ``` (see [CitizenDAO.java](./src/test/java/com/flipkart/hbaseobjectmapper/testcases/daos/CitizenDAO.java))  Once defined, you can access, manipulate and persist a row of `citizens` HBase table as below:  ```java Configuration configuration = getConf(); // this is org.apache.hadoop.conf.Configuration  // Create a data access object: CitizenDAO citizenDao = new CitizenDAO(configuration);  // Fetch a row from "citizens" HBase table with row key "IND#1": Citizen pe = citizenDao.get("IND#1");  Citizen[] ape = citizenDao.get(new String[] {"IND#1", "IND#2"}); //bulk get  // In below, note that "IND#1" is inclusive and "IND#5" is exclusive List<Citizen> lpe = citizenDao.get("IND#1", "IND#5"); //range get // ('versioned' variant above method is available)  // for row keys in range ["IND#1", "IND#5"), fetch 3 versions of field 'phoneNumber' as a NavigableMap<row key, NavigableMap<timestamp, column value>>: NavigableMap<String, NavigableMap<Long, Object>> phoneNumberHistory  	= citizenDao.fetchFieldValues("IND#1", "IND#5", "phoneNumber", 3); // (bulk variants of above range method are also available)  pe.setPincode(560034); // change a field  citizenDao.persist(pe); // Save it back to HBase  citizenDao.delete(pe); // Delete a row by it's object reference  citizenDao.delete(Arrays.asList(pe1, pe2)); // Delete multiple rows by list of object references  citizenDao.delete("IND#2"); // Delete a row by it's row key  citizenDao.delete(new String[] {"IND#3", "IND#4"}); // Delete a bunch of rows by their row keys  citizenDao.getHBaseTable() // returns HTable instance (in case you want to directly play around)   ``` (see [TestsAbstractHBDAO.java](./src/test/java/com/flipkart/hbaseobjectmapper/testcases/TestsAbstractHBDAO.java) for more detailed examples)  **Please note:** Since we're dealing with HBase (and not an OLTP data store), fitting a classical ORM paradigm may not make sense. So this library doesn't intend to evolve as a full-fledged ORM. However, if you do intend to use HBase via an ORM library, I suggest you use [Apache Phoenix](https://phoenix.apache.org/).   ## Limitations  * Being an *object mapper*, this library works for pre-defined columns only. For example, this library doesn't provide ways to fetch:  * columns matching a pattern or a regular expression  * unmapped columns of a column family * This library doesn't provide you a way to 'selectively fetch and populate fields of your bean-like class' when you `get` a row by it's key. (However, you can still fetch column values selectively for one or more rows by using `fetchFieldValue` and `fetchFieldValues` methods)  ## Maven Add below entry within the `dependencies` section of your `pom.xml`:  ```xml <dependency> 	<groupId>com.flipkart</groupId> 	<artifactId>hbase-object-mapper</artifactId> 	<version>1.8</version> </dependency> ``` See artifact details: [com.flipkart:hbase-object-mapper on **Maven Central**](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22com.flipkart%22%20AND%20a%3A%22hbase-object-mapper%22) or [com.flipkart:hbase-object-mapper on **MVN Repository**](https://mvnrepository.com/artifact/com.flipkart/hbase-object-mapper). ## How to build? To build this project, follow below steps:   * Do a `git clone` of this repository  * Checkout latest stable version `git checkout v1.8`  * Execute `mvn clean install` from shell  Currently, projects that use this library are running on [Hortonworks Data Platform v2.4](https://hortonworks.com/blog/apache-hadoop-2-4-0-released/) (corresponds to Hadoop 2.7 and HBase 1.1). However, if you're using a different distribution of Hadoop (like [Cloudera](http://www.cloudera.com/)) or if you are using a different version of Hadoop, you may change the versions in [pom.xml](./pom.xml) to desired ones and build the project.  **Please note**: Test cases are very comprehensive - they even spin an [in-memory HBase test cluster](https://github.com/apache/hbase/blob/master/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java) to run data access related test cases (near-realworld scenario). So, build times can sometimes be longer, depending on your machine configuration.  ## Releases  The change log can be found in the [releases](//github.com/flipkart-incubator/hbase-object-mapper/releases) section.  ## Feature requests and bug reporting  If you intend to request a feature or report a bug, you may use [Github Issues for hbase-object-mapper](//github.com/flipkart-incubator/hbase-object-mapper/issues).  ## License  Copyright 2017 Flipkart Internet Pvt Ltd.  Licensed under the [Apache License, version 2.0](http://www.apache.org/licenses/LICENSE-2.0) (the "License"). You may not use this product or it's source code except in compliance with the License.
fabiomaffioletti/jsondoc	# JSONDoc Official website: http://jsondoc.org  [![Build Status](https://travis-ci.org/fabiomaffioletti/jsondoc.svg?branch=master)](https://travis-ci.org/fabiomaffioletti/jsondoc) <a href="https://flattr.com/submit/auto?user_id=fabiomaffioletti&url=http%3A%2F%2Fjsondoc.org&title=JSONDoc&category=software" target="_blank"><img src="http://api.flattr.com/button/flattr-badge-large.png" alt="Flattr this" title="Flattr this" border="0"></a> <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=fabio%2emaffioletti%40gmail%2ecom&lc=US&item_name=JSONDoc&currency_code=EUR&bn=PP%2dDonationsBF%3abtn_donate_SM%2egif%3aNonHosted" target="_blank"> <img src="https://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif" alt="PayPal - The safer, easier way to pay online!"></a>  ## Want to contribute? Please follow these few guidelines:  1. Open issue on Github so that we can discuss it 2. When the issue is discussed and agreed, branch the code starting from the master branch, with branch name equals to "ISSUE-X" where X is the number of the issue on Github 3. Do not develop multiple features or bugfixes in the same branch, each feature/bugfix should have its branch and its code 4. Before submitting a pull request, please write unit tests. I think it's fair to say that pull requests without unit tests will not be merged 5. Submit the pull request  ## How to report a bug Open an issue on Github in this form, where GIVEN is the set of preconditions to reproduce the bug, WHEN is the action that should be taken to reproduce the bug and THEN is the bug description, together with (if possible) screenshots and/or error messages/log that visually describe the bug.  GIVEN * precondition 1 * precondition 2 * precondition n  WHEN * action 1 AND/OR action 2 AND/OR action n  THEN * bug description (with screenshots/messages/logs if possible)
hantsy/angularjs-springmvc-sample-boot	angularjs-springmvc-sample-boot ===============================  An example application using AnguarJS/Bootstrap as frontend and Spring MVC as REST API producer.  **More details about the source codes, please read the online GitBook: [Building REST APIs with Spring MVC](https://www.gitbook.com/book/hantsy/build-a-restful-app-with-spring-mvc-and-angularjs/details).**    Technology stack:  * Spring Boot * Spring MVC * Spring Data JPA * JPA * Hibernate 5.2 * Spring Security * Swagger/Swagger2Markup/Spring Rest Docs * Spring Test/JUnit/Mockito/JBehave/RestAssured * Lombok * ModelMapper * AngularJS * Bootstrap  This version improved [the original version(without Spring Boot)](https://github.com/hantsy/angularjs-springmvc-sample), including:  * Introduction of Gulp build system to processing the static resources * The frontend UI can be run standalone via NodeJS eco-system * An option provided and allow you to package the static resources as part of final jar and run the application via `mvn spring-boot:run` directly  ## Requirements  * JDK 8    Oracle Java 8 is required, go to [Oracle Java website](http://java.oracle.com) to download it and install into your system.      Optionally, you can set **JAVA\_HOME** environment variable and add *&lt;JDK installation dir>/bin* in your **PATH** environment variable.  * Apache Maven    Download the latest Apache Maven from [http://maven.apache.org](http://maven.apache.org), and uncompress it into your local system.     Optionally, you can set **M2\_HOME** environment varible, and also do not forget to append *&lt;Maven Installation dir>/bin* your **PATH** environment variable.    * NodeJS    NodeJS is required to build the frontend static resources.     Download [NodeJS](http://nodejs.org) and install it into your local system.      After it is installed, open terminal, and using `node -v` command to confirm.     ```   node -v    >v4.2.2   ```     `bower` is also requried to install the runtime dependencies, and `gulp` is chosen as our build tools for the statics resources.     ```   npm install -g bower   npm install -g gulp   ```   ## Get the source codes  Get a copy of the source codes into your local system.  ``` git clone https://github.com/hantsy/angularjs-springmvc-sample-boot ```  ## Run the project  You can use one of the following approaches to run this project.  ### Run frontend UI and backend respectively  1. Run the backend API server via Spring Boot.     ```    mvn spring-boot:run    ```     The backend APIs will run on port 9000.  2. Run the frontend UI standalone.        ```    npm install    bower install    gulp serve    ```     By default, gulp serves the frontend UI static resources on port 3000.  3. Go to [http://localhost:3000](http://localhost:3000) to test it.  ### Run the project via Spring Boot maven plugin       1. Run the following command to resovle the dependencies of the frontend static resources.        ```    npm install    bower install    ```  2. Run the backend API server with `spring-boot` command. The parameter `-Dstatic-ui` will copy the static resources and package into the jar archive.     ```    mvn spring-boot:run -Dstatic-ui    ```  3. Go to [http://localhost:9000](http://localhost:9000) to test it.   If you want to explore the REST API docs online, there is a *Swagger UI* configured for visualizing the REST APIs, just go to [http://localhost:9000/swagger-ui.html](http://localhost:9000/swagger-ui.html).   ### Generate static REST API reference documentation  I have moved the REST docs generation configuration into a standalone Maven profile.  Execute the following command to generate HTML and PDF format files for your REST APIs from Swagger API description file and Spring test code snippets(as code samples).  ``` mvn clean package -Drestdocs ```  The detailed configuration is explained in [API documention](https://hantsy.gitbooks.io/build-a-restful-app-with-spring-mvc-and-angularjs/content/swagger.html) section.  When it is done, check the generated static docs in *target/asciidoc* folder, it includes a HTML 5 file(under html folder), and a PDF file(in pdf folder).  Open the pdf document in Adobe Reader, it looks like.  ![pdf](https://github.com/hantsy/angularjs-springmvc-sample-boot/blob/master/restdocs.png)    ### Docker  You can run the project in multistage Docker building development environment, check [Multistage Builds](https://github.com/hantsy/devops-sandbox/blob/master/multistage.md).
shuaiweili/Dubbo-Zookeeper-Netty-SpringMVC	# Dubbo-Zookeeper-Netty-SpringMVC  1. dubbo-common 提供公共接口； 2. dubbo-provide是服务提供者（接口的实现）； 3. dubbo-consumer是服务器消费者（接口的调用）； 4. zookeeper管理注册中心（负载均衡，资源同步-当然前提是集群）； 5. springmvc写restful接口； 6. netty提供服务器，客户端的请求由spring的DispatcherServlet处理；
schema-repo/schema-repo	# Schema Repo  The schema repo is a RESTful web service for storing and serving mappings between schema identifiers and schema definitions. Those mappings are meant to be immutable, since data serialized with a given identifier should be de-serializable forever.  The primary (and initial) use case for having a schema repo is to ease the serialization and de-serialization of Avro payloads within Kafka messages, however, the schema repo is actually protocol-agnostic and does not strictly require Avro.  Please read the [AVRO-1124](https://issues.apache.org/jira/browse/AVRO-1124) ticket for more background information.  Please subscribe to the [mailing list](https://groups.google.com/forum/#!forum/schema-repo) to ask questions or discuss development.  ## Build and Run  In order to build and run the schema repo, execute the following commands in the current directory:      $ mvn install     $ ./run.sh  ## Maven Artifacts  Maven artifacts for the schema repo are published on Sonatype Central Repository, starting with release 0.1.1:  https://oss.sonatype.org/content/repositories/releases/org/schemarepo/  See the official [instructions for integrating with various build tools](https://oss.sonatype.org/content/repositories/releases/org/schemarepo/).  ## Configuration  The schema repo gets configured via a .properties file passed as the first command line argument to the main method. This file can be configured with the following properties:      # FQCN (fully qualified class name) of the schema repo backend implementation to be used:     schema-repo.class=org.schemarepo.InMemoryRepository           # FQCN of the schema repo cache implementation to be used:     schema-repo.cache=org.schemarepo.InMemoryCache           # FQCN of the validators to use. You can specify zero, one or more than one implementation, all of which need to be prefixed with 'schema-repo.validator.' :      schema-repo.validator.my_custom_validator_1=com.xyz.Validator1     schema-repo.validator.my_custom_validator_2=com.xyz.Validator2     schema-repo.validator.my_custom_validator_3=com.xyz.Validator3           # Default validators to apply to all new topics where no validators are explicitly specified. Comma-separated list of Validator names (without the 'schema-repo.validator.' prefix).     schema-repo.validation.default.validators=my_custom_validator_1,my_custom_validator_2  All configuration properties are injected via Guice. However, you are not obligated to use Guice if you do not wish to. You can also feed the required properties to the various constructors directly by code, if you wish to wire in your own config management solution.      ### Local File System Backend  The local file system backend is a single node, persistent, implementation. For production usage, it is recommended to at least use this backend, and not the in-memory one, otherwise a server shutdown or crash will result in the loss of all of its state.  This file-based backend, however, is not considered highly-available nor fault-tolerant. Even if you could somehow set its storage path to be on a mounted file system that you would consider to be highly-available, the current implementation establishes a lock in the file system for the whole duration of the schema repo's runtime, which fences out other instances from sharing the file system.  In order to use the file-based backend, set these configuration properties:      # FQCN of the file-based backend:     schema-repo.class=org.schemarepo.LocalFileSystemRepository           # Relative or absolute path to where you wish to store the state of the repo:     schema-repo.local-file-system.path=relative/path/to/storage/directory/  ### ZooKeeper Backend  The ZooKeeper backend stores its state in a ZooKeeper ensemble. This backend implementation is meant to be highly-available, meaning that multiple instances can share the same ZooKeeper ensemble and synchronize their state through it. All mutation operations to the schema repo's state are shielded behind a shared lock which is acquired temporarily for the time of the mutation and released afterwards.  Disclaimer: the ZooKeeper backend is still considered experimental.  In order to use the ZooKeeper-based backend, set these configuration properties:      # FQCN of the ZooKeeper-based backend:     schema-repo.class=org.schemarepo.zookeeper.ZooKeeperRepository           # Comma-separated list of the ZK hosts and ports:     schema-repo.zookeeper.ensemble=zk-host-1:2181,zk-host-2:2181,zk-host-3:2181           # These additional properties can also be set if the defaults (shown below) need to be overridden:     schema-repo.zookeeper.path-prefix=/schema-repo     schema-repo.zookeeper.session-timeout=5000     schema-repo.zookeeper.connection-timeout=2000     schema-repo.zookeeper.curator.sleep-time-between-retries=2000     schema-repo.zookeeper.curator.number-of-retries=10      ### Jetty Config  The schema repo's REST server implementation uses Jetty. The defaults below will be used if these configs are not specifically overridden in the config file:      # Jetty configs and their defaults:     schema-repo.jetty.host=     schema-repo.jetty.port=2876     schema-repo.jetty.header.size=16384     schema-repo.jetty.buffer.size=16384     schema-repo.jetty.stop-at-shutdown=true     schema-repo.jetty.graceful-shutdown=3000      ## REST API Documentation  The REST endpoints supported by the Schema Repo, their descriptions, as well as example command executions and reponses are documented on the [Service Endpoints wiki page](https://github.com/schema-repo/schema-repo/wiki/Service-Endpoints).  ## Reading List  Here are some interesting resources to get a better understanding of the Schema Repo's motivation and related technologies: * [High Volume Data and Schema Evolution](https://prezi.com/dynn9skazbty/high-volume-data-and-schema-evolution/), by Scott Carey. * [Schema evolution in Avro, Protocol Buffers and Thrift](http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html), by Martin Kleppmann. * The original [AVRO-1124](https://issues.apache.org/jira/browse/AVRO-1124) ticket, by Jay Kreps.  ## Origin Story  The schema repo is a standalone version of the patch submitted on the Apache [AVRO-1124](https://issues.apache.org/jira/browse/AVRO-1124) ticket.  The patch was originally submitted by Jay Kreps and later on substantially refactored by Scott Carey. Some other people then contributed minor fixes and improvements.  The schema repo was separated into a standalone project because it is unclear that Apache Avro is an appropriate parent project for containing it. It was given its own repository in order to ease further development. The whole project is Apache-licensed, so any OSS project can choose to use (or even include) the schema repo.
T-baby/ICERest	# ICEREST概述  ![](http://i4.piimg.com/1949/9b7b792d5b9a1261.jpg)  ICEREST是一个非常轻量级只有200k左右的RESTful路由框架，通过ICEREST你可以处理url的解析，数据的封装,Json的输出，和传统的方法融合，请求的参数便是方法的参数，方法的返回值便是请求的返回值，原则就是：你会写方法，你就会用。  由于ICEREST非常简单所以只需要看一遍文档就能轻松使用。在ICEREST并没有提供orm，所以你还需要选择一个orm哦，推荐使用[MongoPlugin](https://github.com/T-baby/MongoDB-Plugin)与ICEREST搭配，体验RESTful和MongoDB搭配的极速开发。  - 极简设计，几乎0配置。  - 脱离传统MVC，专业的事由专业的做。  - 支持AOP，拦截器配置灵活，配合[MongoPlugin](https://github.com/T-baby/MongoDB-Plugin)轻松校验传输数据。  - 与MOTAN无缝结合。  # RESTful是什么？  RESTful是一种软件架构风格，设计风格而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。  如果希望更深入了解RESTful可以看：  - [《理解本真的REST架构风格》](http://www.infoq.com/cn/articles/understanding-restful-style)  - [《HTTP API设计指南》](http://www.cybermkd.com/restful-api-she-ji-zhi-nan/)  - [《RESTful API设计指南》](http://www.cybermkd.com/restful-apishe-ji-zhi-nan/)  # 相关文档  中文文档：https://github.com/T-baby/ICEREST/wiki/
wesabe/grendel	Grendel =======  Grendel is a RESTful web service which allows for the secure storage of users' documents.  When a Grendel user is created, an OpenPGP keyset (a master key for signing/verifying and a sub key for encrypting/decrypting) is generated. When the user stores a document, the document is signed with the user's master key and encrypted with their sub key.  Other users can be granted read-only access to these documents. For instance, if a web service stores documents securely for users, a user might grant the service administrators temporary shared access to their documents for support  purposes, or may grant permanent access to another user for sharing purposes.  **To get started using Grendel, read [GETTING-STARTED.md](http://github.com/wesabe/grendel/blob/master/GETTING-STARTED.md).**  **To read about how to use Grendel, read [API.md](http://github.com/wesabe/grendel/blob/master/API.md).**  **For a longer overview of the ideas behind Grendel, read "[Protecting 'Cloud' Secrets with Grendel](http://wesabe.wordpress.com/2010/01/04/protecting-cloud-secrets-with-grendel/)."**  **To read Grendel's distribution license, read [LICENSE.md](http://github.com/wesabe/grendel/blob/master/LICENSE.md).**
